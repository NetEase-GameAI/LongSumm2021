{
  "blog_id": "1706.02515",
  "summary": [
    "\"Using the \"SELU\" activation function, you get better results than any other activation function, and you don't have to do batch normalization.",
    "The \"SELU\" activation function is:  if x<0, 1.051\\*(1.673\\*e^x-1.673) if x>0, 1.051\\*x\" Source: narfon2, reddit   ``` import numpy as np  def selu(x):     alpha = 1.6732632423543772848170429916717     scale = 1.0507009873554804934193349852946     return scale*np.where(x>=0.0, x, alpha*np.exp(x)-alpha) ``` Source: CaseOfTuesday, reddit  Discussion here:  [url]"
  ],
  "author_id": "joecohen",
  "pdf_url": "http://arxiv.org/pdf/1706.02515v1",
  "author_full_name": "Joseph Cohen",
  "source_website": "https://www.shortscience.org/user?name=joecohen",
  "id": 8240954
}