{
  "blog_id": "glove-global-vectors-for-word-representation",
  "summary": [
    "GloVe: Global Vectors for Word Representation \u2013 Pennington et al. 2014  Yesterday we looked at some of the amazing properties of word vectors with word2vec .",
    "Pennington et al. argue that the online scanning approach used by word2vec is suboptimal since it doesn\u2019t fully exploit statistical information regarding word co-occurrences.",
    "They demonstrate a Global Vectors (GloVe) model which combines the benefits of the word2vec skip-gram model when it comes to word analogy tasks, with the benefits of matrix factorization methods that can exploit global statistical information.",
    "The GloVe model\u2026  \u2026 produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task.",
    "It also outperforms related models on similarity tasks and named entity recognition.",
    "The source code for the model, as well as trained word vectors can be found at  [url]"
  ],
  "author_id": "ACOLYER",
  "pdf_url": "http://www-nlp.stanford.edu/pubs/glove.pdf",
  "author_full_name": "Adrian Colyer",
  "source_website": "https://blog.acolyer.org/about/",
  "id": 22546727
}