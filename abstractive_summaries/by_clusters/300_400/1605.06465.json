{
  "blog_id": "1605.06465",
  "summary": [
    "This paper presents Swapout, a simple dropout method applied to Residual Networks (ResNets).",
    "In a ResNet, a layer $Y$ is computed from the previous layer $X$ as  $Y = X + F(X)$  where $F(X)$ is essentially the composition of a few convolutional layers.",
    "Swapout simply applies dropout separately on both terms of a layer's equation:  $Y = \\Theta_1 \\odot X + \\Theta_2 \\odot F(X)$  where $\\Theta_1$ and $\\Theta_2$ are independent dropout masks for each term.",
    "The paper shows that this form of dropout is at least as good or superior as other forms of dropout, including the recently proposed [stochastic depth dropout][1].",
    "Much like in the stochastic depth paper, better performance is achieved by linearly increasing the dropout rate (from 0 to 0.5) from the first hidden layer to the last.",
    "In addition to this observation, I also note the following empirical observations:  1.",
    "At test time, averaging the output layers of multiple dropout mask samples (referenced to as stochastic inference) is better than replacing the masks by their expectation (deterministic inference), the latter being the usual standard.",
    "2.",
    "Comparable performance is achieved by making the ResNet wider (e.g. 4 times) and with fewer layers (e.g.",
    "32) than the orignal ResNet work with thin but very deep (more than 1000 layers) ResNets.",
    "This would confirm a similar observation from [this paper][2].",
    "Overall, these are useful observations to be aware of for anyone wanting to use ResNets in practice.",
    "[1]:  [url]"
  ],
  "author_id": "hlarochelle",
  "pdf_url": "http://arxiv.org/pdf/1605.06465v1",
  "author_full_name": "Hugo Larochelle",
  "source_website": "https://www.shortscience.org/user?name=hlarochelle",
  "id": 18304114
}