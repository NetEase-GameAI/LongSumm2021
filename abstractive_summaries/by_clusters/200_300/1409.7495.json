{
  "blog_id": "1409.7495",
  "summary": [
    "The goal of this method is to create a feature representation $f$ of an input $x$ that is domain invariant over some domain $d$.",
    "The feature vector $f$ is obtained from $x$ using an encoder network (e.g. $f = G_f(x)$).",
    "The reason this is an issue is that the input $x$ is correlated with $d$ and this can confuse the model to extract features that capture differences in domains instead of differences in classes.",
    "Here I will recast the problem differently from in the paper:  **Problem:** Given a conditional probability $p(x|d=0)$ that may be different from $p(x|d=1)$:  $$p(x|d=0) \\stackrel{?",
    "}{\\ne} p(x|d=1)$$  we would like it to be the case that these distributions are equal.",
    "$$p(G_f(x) |d=0) = p(G_f(x)|d=1)$$  aka:  $$p(f|d=0) = p(f|d=1)$$  Of course this is an issue if some class label $y$ is correlated with $d$ meaning that we may hurt the performance of a classifier that now may not be able to predict $y$ as well as before.",
    "[url]"
  ],
  "author_id": "joecohen",
  "pdf_url": "http://arxiv.org/pdf/1409.7495v2",
  "author_full_name": "Joseph Cohen",
  "source_website": "https://www.shortscience.org/user?name=joecohen",
  "id": 93218559
}