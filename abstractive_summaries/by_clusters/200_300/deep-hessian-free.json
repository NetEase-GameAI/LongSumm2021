{
  "blog_id": "deep-hessian-free",
  "summary": [
    "James Martens, 2010  This paper introduces a fairly complex optimization algorithm for deep nets that uses approximate 2nd-order gradient information  In Hessian-Free optimization, you can directly approximate a Hessian-vector product $Hv$ with the method of finite-differences; this only costs 1 more gradient evaluation  linear conjugate gradient algorithm allows one to solve for the optimal search direction in $O(N)$ iterations ($N$ is the number of parameters) with only matrix-vector products  Newton\u2019s method is scale invariant, e.g., for a new parameterization $\\hat{\\theta} = A \\theta$ for some invertible matrix $A$, the optimal search direction is now $\\hat{p} = A p$ where $p$ is the original optimal search direction.",
    "Gradient descent is not (need proof!)",
    "- so many bad things about GD, but it\u2019s so easy to implement..",
    "Considerations when applying this technique  Need to use an adaptive damping parameters $\\lambda$ beause the relative scale of $B = H(\\theta)$ is changing and $H(\\theta)$ must remain positive semidefinite.",
    "Recommended heuristic is given in Section 4.1  Gauss-Newton matrix $G$ can produce better search directions than $H$, see this blog post for a summary  Compute gradient on entire dataset, but use minibatches to compute Hessian-vector products.",
    "SGD requires 10\u2019s of thousands of iterations versus ~200 for HF"
  ],
  "author_id": "pemami",
  "pdf_url": "http://www.cs.toronto.edu/~jmartens/docs/Deep_HessianFree.pdf",
  "author_full_name": "Patrick Emami",
  "source_website": "https://pemami4911.github.io/index.html",
  "id": 4206973
}