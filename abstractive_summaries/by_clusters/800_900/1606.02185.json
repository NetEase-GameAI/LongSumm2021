{
  "blog_id": "1606.02185",
  "summary": [
    "This paper can be thought as proposing a variational autoencoder applied to a form of meta-learning, i.e. where the input is not a single input but a dataset of inputs.",
    "For this, in addition to having to learn an approximate inference network over the latent variable $z_i$ for each input $x_i$ in an input dataset $D$, approximate inference is also learned over a latent variable $c$ that is global to the dataset $D$.",
    "By using Gaussian distributions for $z_i$ and $c$, the reparametrization trick can be used to train the variational autoencoder.",
    "The generative model factorizes as  $p(D=(x_1,\\dots,x_N), (z_1,\\dots,z_N), c) = p(c) \\prod_i p(z_i|c) p(x_i|z_i,c)$  and learning is based on the following variational posterior decomposition:  $q((z_1,\\dots,z_N), c|D=(x_1,\\dots,x_N)) = q(c|D) \\prod_i q(z_i|x_i,c)$.",
    "Moreover, latent variable $z_i$ is decomposed into multiple ($L$) layers $z_i = (z_{i,1}, \\dots, z_{i,L})$.",
    "Each layer in the generative model is directly connected to the input.",
    "The layers are generated from $z_{i,L}$ to $z_{i,1}$, each layer being conditioned on the previous (see Figure 1 *Right* for the graphical model), with the approximate posterior following a similar decomposition.",
    "The architecture for the approximate inference network $q(c|D)$ first maps all inputs $x_i\\in D$ into a vector representation, then performs mean pooling of these representations to obtain a single vector, followed by a few more layers to produce the parameters of the Gaussian distribution over $c$.",
    "Training is performed by stochastic gradient descent, over minibatches of datasets (i.e. multiple sets $D$).",
    "The model has multiple applications, explored in the experiments.",
    "One is of summarizing a dataset $D$ into a smaller subset $S\\in D$.",
    "This is done by initializing $S\\leftarrow D$ and greedily removing elements of $S$, each time minimizing the KL divergence between $q(c|D)$ and $q(c|S)$ (see the experiments on a synthetic Spatial MNIST problem of section 5.3).",
    "Another application is few-shot classification, where very few examples of a number of classes are given, and a new test example $x'$ must be assigned to one of these classes.",
    "Classification is performed by treating the small set of examples of each class $k$ as its own dataset $D_k$.",
    "Then, test example $x$ is classified into class $k$ for which the KL divergence between $q(c|x')$ and $q(c|D_k)$ is smallest.",
    "Positive results are reported when training on OMNIGLOT classes and testing on either the MNIST classes or unseen OMNIGLOT datasets, when compared to a 1-nearest neighbor classifier based on the raw input or on a representation learned by a regular autoencoder.",
    "Finally, another application is that of generating new samples from an input dataset of examples.",
    "The approximate posterior is used to compute $q(c|D)$.",
    "Then, $c$ is assigned to its posterior mean, from which a value for the hidden layers $z$ and finally a sample $x$ can be generated.",
    "It is shown that this procedure produces convincing samples that are visually similar from those in the input set $D$.",
    "**My two cents**  Another really nice example of deep learning applied to a form of meta-learning, i.e. learning a model that is trained to take *new* datasets as input and generalize even if confronted to datasets coming from an unseen data distribution.",
    "I'm particularly impressed by the many tasks explored successfully with the same approach: few-shot classification and generative sampling, as well as a form of summarization (though this last probably isn't really meta-learning).",
    "Overall, the approach is quite elegant and appealing.",
    "The very simple, synthetic experiments of section 5.1 and 5.2 are also interesting.",
    "Section 5.2 presents the notion of a *prior-interpolation layer*, which is well motivated but seems to be used only in that section.",
    "I wonder how important it is, outside of the specific case of section 5.2.",
    "Overall, very excited by this work, which further explores the theme of meta-learning in an interesting way."
  ],
  "author_id": "hlarochelle",
  "pdf_url": "http://arxiv.org/pdf/1606.02185v1",
  "author_full_name": "Hugo Larochelle",
  "source_website": "https://www.shortscience.org/user?name=hlarochelle",
  "id": 87827594
}