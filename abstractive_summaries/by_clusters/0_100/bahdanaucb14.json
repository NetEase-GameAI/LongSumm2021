{
  "blog_id": "bahdanaucb14",
  "summary": [
    "One core aspect of this attention approach is that it provides the ability to debug the learned representation by visualizing the softmax output (later called $\\alpha_{ij}$) over the input words for each output word as shown below.",
    "[url]"
  ],
  "author_id": "joecohen",
  "pdf_url": "http://arxiv.org/pdf/1409.0473",
  "author_full_name": "Joseph Cohen",
  "source_website": "https://www.shortscience.org/user?name=joecohen",
  "id": 24431610
}