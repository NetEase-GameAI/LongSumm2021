{"text": [{"content": "we describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language. our method does not assume any knowledge about the target language across eight european languages, our approach results in an average absolute improvement of over a state-of-the-art baseline, and over vanilla hidden markov models induced with the expectation maximization algorithm.", "section_index": 0}, {"content": "supervised learning approaches have advanced the state-of-the-art on a variety of tasks in natural language processing, resulting in highly accurate systems. supervised part-of-speech however, supervised methods rely on labeled training data, which is time-consuming and expensive to generate. unsupervised learning approaches appear to be a natural solution to this problem, as they require only unannotated text for trainthis research was carried out during an internship at google research. unfortunately, the best completely unsupervised english pos tagger , making its practical usability questionable at best. to bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language study related but different multilingual grammar and tagger induction tasks, where it is assumed that no labeled data at all is available. our work is closest to that of yarowsky and ngai (2001), but differs in two important ways. first, we use a novel graph-based framework for projecting syntactic information across language boundaries. to this end, we construct a bilingual graph over word types to establish a connection between the two languages (3), and then use graph label propagation to project syntactic information from english to the foreign language (4). second, we treat the projected labels as features in an unsuper- 1for simplicity of exposition we refer to the resource-poor language as the foreign language. similarly, we use english as the resource-rich language, but any other language with labeled resources could be used instead. syntactic universals are a well studied concept in linguistics for multilingual grammar induction. because there might be some controversy about the exact definitions of such universals, this set of coarse-grained pos categories is defined operationally, by collapsing language (or treebank) specific distinctions to a set of categories that exists across all languages. these universal pos categories not only facilitate the transfer of pos information from one language to another, but also relieve us from using controversial evaluation metrics,2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed english labels. we evaluate our approach on eight european languages , and considerably bridges the gap to fully supervised pos tagging performance (96.6).", "section_index": 1}, {"content": "the focus of this work is on building pos taggers for foreign languages, assuming that we have an english pos tagger and some parallel text between the two languages. central to our approach (see algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus. as discussed in more detail in 3, we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types, while the vertices on the english side are individual word types. graph construction does not require any labeled data, but makes use of two similarity functions. the edge weights between the foreign language trigrams are computed using a co-occurence based similarity function, designed to indicate how syntactically 2see christodoulopoulos et al. (2010) for a discussion of metrics for evaluating unsupervised pos induction systems. algorithm bilingual pos induction require: parallel english and foreign language data de and df , unlabeled foreign training data f ; english tagger. ensure: f , a set of parameters learned using a constrained unsupervised model (5). 1: def word-align-bitext(de,df ) 2: de pos-tag-supervised(de) 3: a extract-alignments(def , de) 4: g construct-graph(f ,df ,a) 5: g graph-propagate(g) 6: extract-word-constraints(g) 7: f pos-induce-constrained(f ,) 8: return f similar the middle words of the connected trigrams are (3.2). to establish a soft correspondence between the two languages, we use a second similarity function, which leverages standard unsupervised word alignment statistics (3.3).3 since we have no labeled foreign data, our goal is to project syntactic information from the english side to the foreign side. to initialize the graph we tag the english side of the parallel text using a supervised model. by aggregating the pos labels of the english tokens to types, we can generate label distributions for the english vertices. label propagation can then be used to transfer the labels to the peripheral foreign vertices (i.e. the ones adjacent to the english vertices) first, and then among all of the foreign vertices (4). the pos distributions over the foreign trigram types are used as features to learn a better unsupervised pos tagger (5). the following three sections elaborate these different stages is more detail.", "section_index": 2}, {"content": "in graph-based learning approaches one constructs a graph whose vertices are labeled and unlabeled examples, and whose weighted edges encode the degree to which the examples they link have the same label graph construction for structured prediction problems such as pos tagging is non-trivial: on the one hand, using individual words as the vertices throws away the context 3the word alignment methods do not use pos information. necessary for disambiguation; on the other hand, it is unclear how to define proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning. more recently, subramanya et al. (2010) defined a graph over the cliques in an underlying structured prediction model. they considered a semi-supervised pos tagging scenario and showed that one can use a graph over trigram types, and edge weights based on distributional similarity, to improve a supervised conditional random field tagger. we extend subramanya et al.’s intuitions to our bilingual setup. because the information flow in our graph is asymmetric on the english side, however, the vertices (denoted by ve) correspond to word types. because all english vertices are going to be labeled, we do not need to disambiguate them by embedding them in trigrams. furthermore, we do not connect the english vertices to each other, but only to foreign language vertices.4 the graph vertices are extracted from the different sides of a parallel corpus (de, df ) and an additional unlabeled monolingual foreign corpus f , which will be used later for training. we use two different similarity functions to define the edge weights among the foreign vertices and between vertices from different languages. our monolingual similarity function we briefly review it here for completeness. we define a symmetric similarity function k(ui, uj) over two for- 4this is because we are primarily interested in learning foreign language taggers, rather than improving supervised english taggers. note, however, that it would be possible to use our graph-based framework also for completely unsupervised pos induction in both languages, similar to snyder et al. eign language vertices ui, uj vf based on the co-occurrence statistics of the nine feature concepts given in table each feature concept is akin to a random variable and its occurrence in the text corresponds to a particular instantiation of that random variable. for each trigram type x2 x3 x4 in a sequence x1 x2 x3 x4 x5, we count how many times that trigram type co-occurs with the different instantiations of each concept, and compute the point-wise mutual information (pmi) between the two.5 the similarity between two trigram types is given by summing over the pmi values over feature instantiations that they have in common. this is similar to stacking the different feature instantiations into long (sparse) vectors and computing the cosine similarity between them. finally, note that while most feature concepts are lexicalized, others, such as the suffix concept, are not. given this similarity function, we define a nearest neighbor graph, where the edge weight for the n most similar vertices is set to the value of the similarity function and to for all other vertices. we use n (u) to denote the neighborhood of vertex u, and fixed n in our experiments. to define a similarity function between the english and the foreign vertices, we rely on high-confidence word alignments. since our graph is built from a parallel corpus, we can use standard word alignment techniques to align the english sentences de 5note that many combinations are impossible giving a pmi value of 0; e.g., when the trigram type and the feature instantiation don’t have words in common. and their foreign language translations df label propagation in the graph will provide coverage and high recall, and we therefore extract only intersected high-confidence ( 0.9) alignments def based on these high-confidence alignments we can extract tuples of the form u v, where u is a foreign trigram type, whose middle word aligns to an english word type v. our bilingual similarity function then sets the edge weights in proportion to these tuple counts. so far the graph has been completely unlabeled. to initialize the graph for label propagation we use a supervised english tagger to label the english side of the bitext.7 we then simply count the individual labels of the english tokens and normalize the counts to produce tag distributions over english word types. these tag distributions are used to initialize the label distributions over the english vertices in the graph. note that since all english vertices were extracted from the parallel text, we will have an initial label distribution for all vertices in ve. a very small excerpt from an italian-english graph is shown in figure as one can see, only the trigrams suo incarceramento ,, suo iter , and suo carattere , are connected to english words. in this particular case, all english vertices are labeled as nouns by the supervised tagger. in general, the neighborhoods can be more diverse and we allow a soft label distribution over the vertices. it is worth noting that the middle words of the italian trigrams are nouns too, which exhibits the fact that the similarity metric connects types having the same syntactic category. in the label propagation stage, we propagate the automatic english tags to the aligned italian trigram types, followed by further propagation solely among the italian vertices. 6we ran six iterations of ibm model , followed by six iterations of the hmm model in both directions. 7we used a tagger based on a trigram markov model , for its fast speed and reasonable accuracy (96.7 on sections 22-24 of the treebank, but presumably much lower on the (out-of-domain) parallel corpus).", "section_index": 3}, {"content": "given the bilingual graph described in the previous section, we can use label propagation to project the english pos labels to the foreign language. we use label propagation in two stages to generate soft labels on all the vertices in the graph. in the first stage, we run a single step of label propagation, which transfers the label distributions from the english vertices to the connected foreign language vertices (say, v lf ) at the periphery of the graph. note that because we extracted only high-confidence alignments, many foreign vertices will not be connected to any english vertices. this stage of label propagation results in a tag distribution ri over labels y, which encodes the proportion of times the middle word of ui vf aligns to english words vy tagged with label y: ri(y) vy ui vy y vy ui vy (1) the second stage consists of running traditional label propagation to propagate labels from these peripheral vertices v lf to all foreign language vertices in the graph, optimizing the following objective: c(q) uivf\\v lf ,ujn (ui) wijqi qj2 uivf\\v lf qi u2 s.t. y qi(y) ui qi(y) ui, y qi ri ui v lf (2) where the qi (i 1, , vf ) are the label distributions over the foreign language vertices and and are hyperparameters that we discuss in we use a squared loss to penalize neighboring vertices that have different label distributions: qi qj2 y(qi(y)qj(y))2, and additionally regularize the label distributions towards the uniform distribution u over all possible labels y it can be shown that this objective is convex in q. the first term in the objective function is the graph smoothness regularizer which encourages the distributions of similar vertices (large wij) to be similar. the second term is a regularizer and encourages all type marginals to be uniform to the extent that is allowed by the first two terms (cf. maximum entropy principle). if an unlabeled vertex does not have a path to any labeled vertex, this term ensures that the converged marginal for this vertex will be uniform over all tags, allowing the middle word of such an unlabeled vertex to take on any of the possible tags. while it is possible to derive a closed form solution for this convex objective function, it would require the inversion of a matrix of order vf instead, we resort to an iterative update based method. we formulate the update as follows: q (m) i (y) ri(y) if ui v l f i(y) i otherwise (3) where ui vf \\ v lf , i(y) and i are defined as: i(y) ujn (ui) wijq (m1) j (y) u(y) (4) i ujn (ui) wij (5) we ran this procedure for iterations.", "section_index": 4}, {"content": "after running label propagation (lp), we compute tag probabilities for foreign word types x by marginalizing the pos tag distributions of foreign trigrams ui x x x over the left and right context words: p(yx) x,x qi(y) x,x,y qi(y ) (6) we then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value : tx(y) if p(yx) otherwise (7) we describe how we choose in this vector tx is constructed for every word in the foreign vocabulary and will be used to provide features for the unsupervised foreign language pos tagger. we develop our pos induction model based on the feature-based hmm of berg-kirkpatrick et al. for a sentence x and a state sequence z, a first order markov model defines a distribution: p(x x,z z) p(z1 z1)x i1 p(zi1 zi1 zi zi) transition p(xi xi zi zi) emission (8) in a traditional markov model, the emission distribution p(xi xi zi zi) is a set of multinomials. the feature-based model replaces the emission distribution with a log-linear model, such that: p(x x z z) exp f(x, z) xval(x) exp f(x, z) : an indicator feature based on the word identity x, features checking whether x contains digits or hyphens, whether the first letter of x is upper case, and suffix features up to length all features were conjoined with the state z. we trained this model by optimizing the following objective function: l() n i1 log z p(x x (i),z z(i)) c22 (10) note that this involves marginalizing out all possible state configurations z for a sentence x, resulting in a non-convex objective. to optimize this function, we used l-bfgs, a quasi-newton method found that this direct gradient method performed better moreover, this route of optimization outperformed a vanilla hmm trained with em by we adopted this state-of-the-art model because it makes it easy to experiment with various ways of incorporating our novel constraint feature into the log-linear emission model. this feature ft incorporates information from the smoothed graph and prunes hidden states that are inconsistent with the thresholded vector tx. the function : f c maps from the language specific fine-grained tagset f to the coarser universal tagset c and is described in detail in 6.2: ft(x, z) log(tx(y)), if (z) y (11) note that when tx for more details about their modification of em, and how gradients are computed for l-bfgs. ravi and knight (2009) instead of the feature-hmm for pos induction on the foreign side. however, we do not explore this possibility in the current work.", "section_index": 5}, {"content": "before presenting our results, we describe the datasets that we used, as well as two baselines. we utilized two kinds of datasets in our experiments: the parallel data came from the europarl corpus (koehn, 2005) and the ods united nations dataset (un, 2006). taking the intersection of languages in these resources, and selecting languages with large amounts of parallel data, yields the following set of eight indo-european languages: danish, dutch, german, greek, italian, portuguese, spanish and swedish. of course, we are primarily interested in applying our techniques to languages for which no labeled resources are available. however, we needed to restrict ourselves to these languages in order to be able to evaluate the performance of our approach. we paid particular attention to minimize the number of free parameters, and used the same hyperparameters for all language pairs, rather than attempting language-specific tuning. we hope that this will allow practitioners to apply our approach directly to languages for which no resources are available. we use the universal pos tagset of petrov et al. (2011) in our experiments.10 this set c consists of the following coarse-grained tags: noun (nouns), verb (verbs), adj (adjectives), adv (adverbs), pron (pronouns), det (determiners), adp (prepositions or postpositions), num (numerals), conj (conjunctions), prt (particles), punc 9we extracted only the words and their pos tags from the treebanks. (punctuation marks) and x (a catch-all for other categories such as abbreviations or foreign words). while there might be some controversy about the exact definition of such a tagset, these categories cover the most frequent part-of-speech and exist in one form or another in all of the languages that we studied. for each language under consideration, petrov et al. (2011) provide a mapping from the fine-grained language specific pos tags in the foreign treebank to the universal pos tags. the supervised pos tagging accuracies (on this tagset) are shown in the last row of table the taggers were trained on datasets labeled with the universal tags. the number of latent hmm states for each language in our experiments was set to the number of fine tags in the language’s treebank. in other words, the set of hidden states f was chosen to be the fine set of treebank tags. therefore, the number of fine tags varied across languages for our experiments; however, one could as well have fixed the set of hmm states to be a constant across languages, and created one mapping to the universal pos tagset. to provide a thorough analysis, we evaluated three baselines and two oracles in addition to two variants of our graph-based approach. we were intentionally lenient with our baselines: em-hmm: a traditional hmm baseline, with multinomial emission and transition distributions estimated by the expectation maximization algorithm. we evaluated pos tagging accuracy using the lenient many-to-1 evaluation approach (johnson, 2007). feature-hmm: the vanilla feature-hmm of berg-kirkpatrick et al. no additional constraint feature) served as a second baseline. model parameters were estimated with l-bfgs and evaluation again used a greedy many-to-1 mapping. projection: our third baseline incorporates bilingual information by projecting pos tags directly across alignments in the parallel data. for unaligned words, we set the tag to the most frequent tag in the corresponding treebank. for each language, we took the same number of sentences from the bitext as there are in its treebank, and trained a supervised feature-hmm. this can be seen as a rough approximation of yarowsky and ngai (2001). we tried two versions of our graph-based approach: no lp: our first version takes advantage of our bilingual graph, but extracts the constraint feature after the first stage of label propagation (eq. because many foreign word types are not aligned to an english word (see table 3), and we do not run label propagation on the foreign side, we expect the projected information to have less coverage. furthermore we expect the label distributions on the foreign to be fairly noisy, because the graph constraints have not been taken into account yet. with lp: our full model uses both stages of label propagation (eq. 2) before extracting the constraint features. as a result, we are able to extract the constraint feature for all foreign word types and furthermore expect the projected tag distributions to be smoother and more stable. our oracles took advantage of the labeled treebanks: tb dictionary: we extracted tagging dictionaries from the treebanks and and used them as constraint features in the feature-based hmm. evaluation was done using the prespecified mappings. supervised: we trained the supervised model of brants (2000) on the original treebanks and mapped the language-specific tags to the universal tags for evaluation. while we tried to minimize the number of free parameters in our model, there are a few hyperparameters that need to be set. fortunately, performance was stable across various values, and we were able to use the same hyperparameters for all languages. we used c as the l2 regularization constant in (eq. 10) and trained both em and l-bfgs for iterations. when extracting the vector tx used to compute the constraint feature from the graph, we tried three threshold values for (see eq. because we don’t have a separate development set, we used the training set to select among them and found to work slightly better than and for seven out of eight languages a threshold of gave the best results for our final model, which indicates that for languages without any validation set, can be used. for graph propagation, the hyperparameter was set to and was not tuned. the graph was constructed using million trigrams; we chose these by truncating the parallel datasets up to the number of sentence pairs that contained million trigrams. table shows our complete set of results. as expected, the vanilla hmm trained with em performs the worst. the feature-hmm model works better for all languages, generalizing the results achieved for english by berg-kirkpatrick et al. our projection baseline is able to benefit from the bilingual information and greatly improves upon the monolingual baselines, but falls short of the no lp model by on an average. the no lp model does not outperform direct projection for german and greek, but performs better for six out of eight languages. overall, it gives improvements ranging from for german to for italian, for an average improvement of over the unsupervised feature-hmm model. for comparison, the completely unsupervised feature-hmm baseline accuracy on the universal pos tags for english is 79.4, and goes up to with a treebank dictionary. our full model (with lp) outperforms the unsupervised baselines and the no lp setting for all languages. it falls short of the projection baseline for german, but is statistically indistinguishable in terms of accuracy. as indicated by bolding, for seven out of eight languages the improvements of the with lp setting are statistically significant with respect to the other models, including the no lp setting.11 overall, it performs better than the hitherto state-of-the-art feature-hmm baseline, and better than direct projection, when we macro-average the accuracy over all languages. our full model outperforms the no lp setting because it has better vocabulary coverage and allows the extraction of a larger set of constraint features. we tabulate this increase in table for all languages, the vocabulary sizes increase by several thousand words. although the tag distributions of the foreign words (eq. 6) are noisy, the results confirm that label propagation within the foreign language part of the graph adds significant quality for every language. figure shows an excerpt of a sentence from the italian test set and the tags assigned by four different models, as well as the gold tags. while the first three models get three to four tags wrong, our best model gets only one word wrong and is the most accurate among the four models for this example. examining the word fidanzato for the no lp and with lp models is particularly instructive. as figure shows, this word has no high-confidence alignment in the italian-english bitext. as a result, its pos tag needs to be induced in the no lp case, while the 11a word level paired-t-test is significant at p for danish, greek, italian, portuguese, spanish and swedish, and p for dutch. gold: si trovava in un parco con il fidanzato paolo f. , anni , rappresentante em-hmm: feature-hmm: no lp: with lp: conj noun det det noun adp det noun noun pron verb adp det noun conj det noun noun noun verb pron verb adp det noun adp det noun noun noun noun verb verb adp det noun adp det adj noun adj noun verb verb adp det noun adp det noun noun noun noun figure 2: tags produced by the different models along with the reference set of tags for a part of a sentence from the italian test set. italicized tags denote incorrect labels. correct tag is available as a constraint feature in the with lp case.", "section_index": 6}, {"content": "we have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages. because we are interested in applying our techniques to languages for which no labeled resources are available, we paid particular attention to minimize the number of free parameters and used the same hyperparameters for all language pairs. our results suggest that it is possible to learn accurate pos taggers for languages which do not have any annotated data, but have translations into a resource-rich language. our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised pos tagging models.", "section_index": 7}], "id": 1000}
{"text": [{"content": "recurrent neural networks (rnns) have had considerable success in classifying and predicting sequences. we demonstrate that rnns can be effectively used in order to encode sequences and provide effective representations. the methodology we use is based on fisher vectors, where the rnns are the generative probabilistic models and the partial derivatives are computed using backpropagation. state of the art results are obtained in two central but distant tasks, which both rely on sequences: video action recognition and image annotation. we also show a surprising transfer learning result from the task of image annotation to the task of video action recognition.", "section_index": 0}, {"content": "fisher vectors have been shown to provide a significant performance gain on many different applications in the domain of computer vision 39, 33, 2, in the domain of video action recognition, fisher vectors and stacked fisher vectors have recently outperformed state-of-theart methods on multiple datasets 33, fisher vectors (fv) have also recently been applied to word embedding (e.g. word2vec 30) and have been shown to provide state of the art results on a variety of nlp tasks 24, as well as on image annotation and image search tasks in all of these contributions, the fv of a set of local descriptors is obtained as a sum of gradients of the loglikelihood of the descriptors in the set with respect to the parameters of a probabilistic mixture model that was fitted on a training set in an unsupervised manner. in spite of being richer than the mean vector pooling method, fisher vectors based on a probabilistic mixture model are invariant to order. this makes them less appealing for annotating, for example, video, in which the sequence of events determines much of the meaning. this work presents a novel approach for fv representation of sequences using a recurrent neural network (rnn). the rnn is trained to predict the next element of a sequence given the previous elements. conveniently, the gradients needed for the computation of the fv are extracted using the available backpropagation infrastructure. the new representation is sensitive to ordering and therefore mitigates the disadvantage of using the standard fisher vector representation. it is applied to two different and challenging tasks: video action recognition and image annotation by sentences. several recent works have proposed to use an rnn for sentence representation 44, 1, 31, the recurrent neural network fisher vector (rnn-fv) method differs from these works in that a sequence is represented by using derived gradient from the rnn as features, instead of using a hidden or an output layer of the rnn. the paper explores two different approaches for training the rnn for the image annotation and image search tasks. in the classification approach, the rnn is trained to predict the following word in the sentence. the regression approach tries to predict the embedding of the following word (i.e. treating it as a regression task). the large vocabulary size makes the regression approach more scalable and achieves better results than the classification approach. in the video action recognition task, the regression approach is the only variant being used, since the notion of a discrete word does not exist. the vgg convolutional neural network (cnn) is used to extract features from the frames of the video and the rnn is trained to predict the embedding of the next frame given the previous ones. similarly, c3d features of sequential video sub-volumes are used with the same training technique. although the image annotation and video action recognition tasks are quite different, a surprising boost in performance in the video action recognition task was achieved by using a transfer learning approach from the image annotation task. specifically, the vgg image embedding of a frame is projected using a linear transformation which was learned on matching images and sentences by the canonical correlation analysis (cca) algorithm the proposed rnn-fv method achieves state-of-theart results in action recognition on the hmdb51 and ucf101 datasets. in image annotation and image search tasks, the rnn-fv method is used for the representation of sentences and achieves state-of-the-art results on the flickr8k dataset and competitive results on other benchmarks.", "section_index": 1}, {"content": "action recognition as in other object recognition problems, the standard pipeline in action recognition is comprised of three main steps: feature extraction, pooling and classification. many works 23, 49, have focused on the first step of extracting local descriptors. extend the notion of spatial interest points into the spatiotemporal domain and show how the resulting features can be used for a compact representation of video data. 51, used low-level hand-crafted features such as histogram of oriented gradients (hog), histogram of optical flow (hof) and motion boundary histogram (mbh). recent works have attempted to replace these handcrafted features by deep-learned features for video action recognition due to its wide success in the image domain. early attempts 45, 12, achieved lower results in comparison to hand-crafted features, proving that it is challenging to apply deep-learning techniques on videos due to the relatively small number of available datasets and complex motion patterns. more recent attempts managed to overcome these challenges and achieve state of the art results with deep-learned features. designed two-stream convnets for learning both the appearance of the video frame and the motion as reflected by the estimated optical flow. designed an effective approach for spatiotemporal feature learning using 3-dimensional convnets. in the second step of the pipeline, the pooling, wang et al. compared different pooling techniques for the application of action recognition and showed empirically that the fisher vector encoding has the best performance. recently, more complex pooling methods were demonstrated by peng et al. who proposed stacked fisher vectors tdd uses both a motion cnn, trained on ucf101, and an appearance cnn, originally trained on imagenet 3, and fine-tuned on ucf101. image annotation and image search in the past few years, the state-of-the-art results in image annotation and image search have been provided by deep learning approaches 42, 29, 18, 14, 27, 16, 4, 13, 48, a typical system is composed of three important components: (i) image representation, (ii) sentence representation, and (iii) matching images and sentences. the image is usually represented by applying a pre-trained cnn on the image and taking the activations from the last hidden layer. there are several different approaches for the sentence representation; socher et al. used a dependency tree recursive neural network. used a tfidf histogram over the vocabulary. used word2vec as the word embedding and then applied fisher vector based on a hybrid gaussian-laplacian mixture model that composes words to different semantic fragments and learns the inter-modal relations between image and the composed fragments at different levels. since a sentence can be seen as a sequence of words, many works have used a recurrent neural network (rnn) in order to represent sentences 14, 48, 27, 16, to address the need for capturing long term semantics in the sentence, these works mainly use long short-term memory (lstm) or gated recurrent unit (gru) cells. generally, the rnn treats a sentence as an ordered sequence of words, and incrementally encodes a semantic vector of the sentence, word-by-word. at each time step, a new word is encoded into the semantic vector, until the end of the sentence is reached. all of the words and their dependencies will then have been embedded into the semantic vector, which can be used as a feature vector representation of the entire sentence. our work also uses an rnn in order to represent sentences but takes the derived gradient from the rnn as features, instead of using a hidden or an output layer of the rnn. a number of techniques have been proposed for the task of matching images and sentences. introduced a deep cca in order to project the images and sentences into a common space and then performed a nearest neighbor search between the images and the sentences in the common space. used a contrastive loss function trained on matching and unmatching pairs of (image,sentence) in order to learn a score function for a given pair. learned a probabilistic model for inferring a sentence given an image and, therefore, are able to compute the probability that a given sentence will be created by a given image and used it as the score.", "section_index": 2}, {"content": "in this section we describe two baseline pooling methods that can represent a multiset of vectors as a single vector. the notation of a multiset is used to clarify that the order of the words in a sentence does not affect the representation, and that a vector can appear more than once. both methods can be applied to sequences, however, the resulting representation will be insensitive to ordering. to address this, we propose in sec. a novel pooling method: rnn-fv. this pooling technique takes a multiset of vectors, x x1, x2, , xn r d, and computes its mean: v 1n n i1 xi. clearly, the vector v that results from the pooling is in rd. the disadvantage of this method is the blurring of the multiset’s content. consider, for example, the text encoding task, where each word is represented by its word2vec embedding. by adding multiple vectors together, the location obtained in the semantic embedding space is somewhere in the convex hull of the words that belong to the multiset. given a multiset of vectors, x x1, x2, , xn rd, the standard fv is defined as the gradient of the log-likelihood of x with respect to the parameters of a pre-trained diagonal-covariance gaussian mixture model (gmm). it is a common practice to limit the fv representation to the partial derivatives with respect to the means, , and the standard deviations, , and ignore the partial derivatives with respect to the mixture weights. it is worth noting the linear structure of the gmm fv pooling. since the likelihood of the multiset is the multiplication of the likelihoods of the individual elements, the log-likelihood is additive. this convenient property would not be preserved in the rnn model, where the probability of an element in the sequence depends on all the previous elements. to all types of fv, we apply the two improvements that were introduced by perronnin et al. the first improvement is to apply an element-wise power normalization function, f(z) sign(z)z where is a parameter of the normalization. the second improvement is to apply an l2 normalization on the fv after applying the power normalization function.", "section_index": 3}, {"content": "the pooling methods described above share a common disadvantage: insensitivity to the order of the elements in the sequence. a way to tackle this, while keeping the power of gradient-based representation, would be to replace the gaussian model by a generative sequence model that takes into account the order of elements in the sequence. a desirable property of the sequence model would be the ability to calculate the gradient (with respect to the model’s parameters) of the likelihood estimate by this model to an input sequence. in this section, we show that such a model can be obtained by training an rnn to predict the next element in a sequence, given the previous elements. having this, we propose, for the first time, the rnn-fv: a fisher vector that is based on such an rnn sequence model. we propose two types of rnn-fvs. one type is based on training a regression problem, and the other on training a classification problem. in practice, only the first type is directly useful for video analysis. for image annotation, the first type outperforms the second. given a sequence of vectors s with n vector elements x1, ..., xn , we convert it to the input sequence x (x0, x1, ..., xn1), where x0 xstart. this special element is used to denote the beginning of the input sequence, and we use xstart throughout this paper. the rnn is trained to predict, at each time step i, the next element xi1 of the sequence, given the previous elements x0, ..., xi. therefore, given the input sequence, the target sequence would be: y (x1, x2, ...xn ). the training data and the training process are application dependent, as is described in sec. for action recognition and in sec. given a sequence of input vectors x , the regression rnn is trained to predict the next vector in the sequence s, i.e., the sequence y the output layer of the network is a fully-connected layer, the size of which would be d, i.e., the dimension of the input vector space. there are several regression loss functions that can be used. here, we consider the following loss function: loss(y, v) y v2 (1) where y is the target vector and v is the predicted vector. after the rnn training is done, and given a new sequence s, the derived sequence x is fed to the rnn. denote the output of the rnn at time step i (i 0, ..., n 1) by rnn(x0, ..., xi) vi rd. the target at time step i is xi1 (the next element in the sequence), and the loss is: loss(xi1, vi) xi1 vi (2) the rnn can be seen as a generative model, and the likelihood of any vector x being the next element of the sequence, given x0, ..., xi, can be defined as: p (xx0, ..., xi) (2) d/2 exp ( x vi ) (3) we are generally interested in the likelihood of the correct prediction, i.e., in the likelihood of the vector xi1 given x0, ..., xi: p (xi1x0, ..., xi). the rnn-based likelihood of the entire sequence x is: p(x) n1 i0 p (xi1x0, ..., xi) (4) the negative log likelihood of x is: l(x) log (p(x)) n1 i0 log (p (xi1x0, ..., xi)) nd log(2) n1 i0 xi1 vi (5) in order to represent x using the fisher vector scheme, we have to compute the gradient of l(x) with respect to our model’s parameters. with rnn being our model, the parameters are the weights w of the network. by (2) and (5), we get that l(x) equals the loss that would be obtained when x is fed as input to the rnn, up to an additive constant. therefore, the desired gradient can be computed by backpropagation: we feed x to the network and perform forward and backward passes. the obtained gradient wl(x) would be the (unnormalized) rnn-fv representation of x notice that this gradient is not used to update the network’s weights as done in training - here we perform backpropagation at inference time. other loss functions may be used instead of the one presented in this analysis. given a sequence, the gradient of the rnn loss may serve as the sequence representation, even if the loss is not interpretable as a likelihood. the classification application is applicable for predicting a sequence of symbols w1,w2,...,wn that have matching vector representations r(w1) x1, r(w2) x2, ..., r(wn ) xn the rnn predicts the sequence u (w1, w2, , wn ) from the sequence x (x0, x1, denote by m the size of our symbol alphabet, i.e., the number of unique symbols in the input sequences. the output layer of the network is a softmax layer with m units, where the j’th element in the output is the probability of the j’th symbol to be the next output element. the loss function for the training of the rnn is the cross-entropy loss. after the rnn is trained, it is ready to be used as a feature vector extractor for new sequences. denote the new sequence by u and its vector representation by x as above. consider feeding the sequence x to the rnn. at time step i (i 0, ..., n 1), the output of the rnn is rnn(x0, ..., xi) (pi1, ..., p i m ), where m j1 p i j here, pij is the probability which the rnn gives to the j’th symbol at time step i. the cross-entropy loss at time step i is derived from the probability given to the correct next symbol: lossi log ( piwi1 ) log (pr (wi1w0, ..., wi)) (6) the rnn can be seen as a generative model which gives likelihood to the sequence u : pr(u) n1 i0 pr (wi1w0, ..., wi) n1 i0 piwi1 (7) the negative log likelihood of u is: l(u) log (pr(u)) n1 i0 log ( piwi1 ) (8) by (6) and (8), we get that l(u) equals the loss that would be obtained when x is fed as input, and u as output to the rnn. therefore, the desired gradient can be computed by backpropagation, i.e. feeding x to the network and performing forward and backward passes. the obtained gradient wl(u) would be the (unnormalized) rnn-fv representation of u it was suggested by that normalizing the fvs by the fisher information matrix is beneficial. we approximated the diagonal of the fisher information matrix (fim), which is usually used for fv normalization. note, however, that we did not observe any empirical improvement due to this normalization, and our experiments are reported without it. let w be a single weight of the rnn. the term in the diagonal of the fim which corresponds to l(xw ) is: f x p (x w ) l(xw ) dx since the probabilistic model which determines p (x w ) is the rnn, it is impossible to derive a closedform expression for this term. therefore, we approximated it directly from the gradients of the training sequences, by computing the mean of l(xw ) for each w the normalized partial derivatives of the fv are then: f 1/2 w l(xw )", "section_index": 4}, {"content": "the action recognition pipeline contains the underlying appearance features used to encode the video, the sequence encoding using the rnn-fv, and an svm classifier on top. the rnn-fv is capable of encoding the sequence properties, and as underlying features, we rely on video encodings that are based on single frames or on fixed length blocks of frames. vgg using the pre-trained vgg convolutional network 41, we extract a 4096-dimensional representation of each video frame. the vgg pipeline is used, namely, the original image is cropped in ten different ways into by pixel images: the four corners, the center, and their xaxis mirror image. the mean intensity is then subtracted in each color channel and the resulting images are encoded by the network. the average of the feature vectors obtained is then used as the single image representation. in order to speed up the method, the input video was sub-sampled, and one in every frames was encoded. empirically, we noticed that recognition performance was comparable to that of using all video frames. to further reduce run-time, the data dimensionality was reduced via pca to 500d. in addition, l2 normalization was applied to each vector. all pcas in this work were trained for each dataset and each training/test split separately, using only the training data. cca using the same vgg representation of video frames as mentioned above and the code of 181, we represented each frame by a vector as follows: we considered the common image-sentence vector space obtained by the cca algorithm, using the best model and l2 normalization were applied. our rnn model consists of three layers: a 200d fullyconnected layer units with leaky-relu activation ( 0.1), a 200-units long short-term memory (lstm) layer, and a 500d linear fully-connected layer. our network is trained for regression with the mean square error (mse) loss function. weight decay and dropouts were also applied. an improvement in recognition performance was noticed when the dropout rate was enlarged, up to a rate of 0.95, due to its ability to ensure the discriminative characteristics of each weight and hence also of each gradient. we train the rnn to predict the next element in our video representation sequence, given the previous elements, as described in sec. in our experiments, we use only 1available at www.cs.tau.ac.il/wolf/code/hglmm the part of gradient corresponding to the weights of the last fully-connected layer. empirically, we saw no improvement when using the partial derivatives with respect to weights of other layers. in order to obtain a fixed size representation, we average the gradients over all time steps. the gradient representation dimension is 500x201100500, which is the number of weights in the last fully-connected layer. we then apply pca to reduce the representation size to 1000d, followed by power and l2 normalization. video classification is performed using a linear svm with a parameter c empirically, we noticed that the the best recognition performance is obtained very quickly and hence early stopping is necessary. in order to choose an early stopping point we use a validation set. some of the videos in the dataset are actually segments of the same original video, and are included in the dataset as different samples. care was taken to ensure that no such similar videos are both in the training and validation sets, in order to guarantee that high validation accuracy will ensure good generalization and not merely over-fitting. after each rnn epoch, we extract the rnn-fv representation as described above, train a linear svm classifier on the training set and evaluate the performance on the validation set. the early stopping point is chosen at the epoch with highest recognition accuracy on the validation set. after choosing our model this way, we train an svm classifier on all training samples (training validation samples) and report our performance on the test set.", "section_index": 5}, {"content": "in the image-sentence retrieval tasks, vector representations are extracted separately for the sentences and the images. these representations are then mapped into a common vector space, where the two are being matched. have presented a similar pipeline for gmm-fv. we replace this representation with rnn-fv. a sentence, being an ordered sequence of words, can be represented as a vector using the rnn-fv scheme. given a sentence with n words w1, ..., wn , (where wn is considered to be the period, namely a wend special token), we treat the sentence as an ordered sequence s (w0, w1, ..., wn1), where w0 wstart. an rnn is trained to predict, at each time step i, the next word wi1 of the sentence, given the previous words w0, ..., wi. therefore, given the input sequence s, the target sequence would be: (w1, w2, ...wn ). the training data may be any large set of sentences. these sentences may be extracted from the dataset of a specific benchmark, or, in order to obtain a generic representation, any external corpus, e.g., wikipedia, may be used. the two network alternatives are explored: classification and regression. as observed in the action recognition case, we did not benefit from extracting partial derivatives with respect to the weights of the hidden layers, and hence we only use those of the output layer as our representation. when the rnn is trained for classification, each word in the dictionary is considered as a class. the input to the network is the word’s embedding, a 300d vector in our case. the hidden layer is lstm with units, which is followed by a softmax output layer. this design creates two challenges. the first is dimensionality: the size of the softmax layer is the size of the dictionary, m , which is typically large. as a result, wl(x) has a high dimensionality. the second issue is with generalization capability: since the softmax layer is fixed, a network cannot handle a sentence containing a word that does not appear in its training data. when training the rnn for regression, the same 300d input is used, followed by an lstm layer of size the output layer, in this case, is fully-connected, where the (300 dimensional) word embedding of next word is predicted. we use no activation function at the output layer. notice that the two issues pointed out regarding the classification rnn are not present in the regression case. first, the size d of the output layer depends only on the dimension of the word embedding. second, the network can naturally handle unseen words, since it predicts vectors in the word vector space rather than an index of a specific word. for matching images and text, each image is represented as a 4096-dimensional vector extracted using the 19-layer vgg, as described in sec. the regularized cca algorithm 47, where the regularization parameter is selected based on the validation set, is used to match the the vgg representation with the sentence rnn-fv representation. in the shared cca space, the cosine similarity is used. we explored several configurations for training the rnn. rnn training data we employed either the training data of each split in the respective benchmark, or the 2010-englishwikipedia-1m dataset made available by the leipzig corpora collection this dataset contains million sentences randomly sampled from english wikipedia. word embedding a word was represented either by word2vec, or by the gmmhglmm representation of 18, projected to a 300d sentence to vgg-encoded-image cca space. we made sure to match the training split according to the benchmark tested. sentence sequence direction we explored both the conventional left-to-right sequence of words and the reverse direction.", "section_index": 6}, {"content": "we evaluated the effectiveness of the various pooling methods on two important yet distinct application domains: action recognition and image textual annotation and search. as mentioned, applying the fim normalization (sec. 4.3) did not seem to improve results. another form of normalization we have tried, is to normalize each dimension of the gradient by subtracting its mean and dividing by its standard deviation. this also did not lead to an improved performance. two normalizations that were found to be useful are the power normalization and the l2 normalization, which were introduced in (see section 2). both are employed, using a constant 1/2. our experiments were conducted on two large action recognition benchmarks. the ucf101 dataset consists of 13,320 realistic action videos, collected from youtube, and divided into action categories. we use the three splits provided with this dataset in order to evaluate our results and report the mean average accuracy over these splits. the hmdb51 dataset consists of action videos, collected from various sources, and divided into action categories. three splits are provided as an official benchmark and are used here. the mean average accuracy over these splits is reported. table compares our rnn-fv pooling method to mean and gmm-fv pooling. three sets of features, as described in sec. are used: vgg coupled with pca, vgg projected by the image to sentence matching cca, and c3d. the parameters were set on the validation split that we created for the provided training set. for gmm-fv, the only parameter is k, which is the number of components in the mixture. the validated values of k were in the set 1, 2, 4, 8, 16, the parameter for rnn-fv was the stopping point of the rnn training, as described in sec. classification is conducted in all experiments using a multiclass (one-vs-all) linear svm with c1. as can be seen in table 1, the rnn-fv pooling outperformed the other pooling methods by a sizable margin. another interesting observation is that with vgg frame representation, cca outperformed pca consistently in all pooling methods. not shown is the performance obtained when using the activations of the rnn as a feature vector. these results are considerably worse than all pooling methods. notice that the representation dimension of mean pooling is (like the features we used), the gmm-fv dimension is k 500, where k is the number of clusters and the rnn-fv dimension is table compares our proposed rnn-fv method, combining multiple features together, with recently published methods on both datasets. the combinations were performed using early fusion, i.e, we concatenated the normalized low-dimensional gradients of the models and train multi-class linear svm on the combined representation. we also tested the combination of our two best models with idt and got state of the art results on both benchmarks. interestingly, when training the rnns on ucf101 and applying to encode hmdb51 videos, a comparable results of (54.47 without idt) is obtained, which is also above current state of the art. dataset hmdb51 ucf101 method mp gmm-fv rnn-fv mp gmmfv rnn-fv vgg pca vgg cca c3d table comparing pooling techniques (mean pooling, gmm-fv and rnn-fv) on hmdb51 and ucf101. three types of features are used: vgg-pca, vggcca, and c3d. the table reports recognition average accuracy (higher is better). method hmdb51 ucf101 idt idt high-d encodings two-stream cnn (2 nets) multi-skip feature stacking c3d (1 net) c3d (3 nets) c3d (3 nets) idt tdd (2 nets) tdd (2 nets) idt stacked fv stacked fv idt rnn-fv(c3d vgg-cca) rnn-fv(c3d vgg-cca) idt table comparison to the state of the art on ucf101 and hmdb51. in order to obtain the best performance, we combine, similar to all other contributions, multiple features. we also present a result where idt is combined, similar to all other top results (multi-skip extends idt). this adds motion based information to our method. the effectiveness of rnn-fv as sentence representation is evaluated on the bidirectional image and sentence retrieval task. we perform our experiments on three benchmarks: flickr8k 8, flickr30k 9, and coco the datasets contain 8, 000, 30, 000, and 123, images respectively. each image is accompanied with sentences describing the image content, collected via crowdsourcing. the flickr8k dataset is provided with training, validation, and test splits. for flickr30k and coco, no training splits are given, and we use the same splits used by there are three tasks in this benchmark: image annotation, in which the goal is to retrieve, given a query image, the five ground truth sentences; image search, in which, given a query sentence, the goal is to retrieve the ground truth image; and sentence similarity, in which the goal is, given a sentence, to retrieve the other four sentences describing the same image. evaluation is performed using recallk, namely the fraction of times the correct result was ranked within the top k items. the median and mean rank of the first ground truth result are also reported. for the sentence similarity task, only mean rank is reported. 6, we explored rnn-fv based on several rnns. the first rnn is a generic one: it was trained with the wikipedia sentences as training data and word2vec as word embedding. in addition, for each of the three datasets, we trained three rnns with the dataset’s training sentences as training data: one with word2vec as word embedding; one with the cca word embedding derived from the semantic vector space of 18, as explained in sec. 6; and one with the cca word embedding, and with feeding the sentences in reverse order. these rnns were all trained for regression. for flickr8k, we also trained an rnn for classification (with flickr8k training sentences, and word2vec embedding). in this network, the softmax layer was of size 8,148, corresponding to the number of unique words in the flickr8k dataset. since the resulting number of weights of the output layer is around million, we reduced the dimension of the gradient feature vector by random sampling of 72,000 coordinates. training a classification model on the larger datasets is virtually impractical, since the number of unique words in these datasets is much higher, resulting in a very large softmax layer and a huge number of weights. in the regression rnns, we used an lstm layer of size we did not observe a benefit in using more lstm units. we used the part of the gradient corresponding to all 30,300 weights of the output layer (including one bias per word2vec dimension). in the case of the larger coco dataset, due to the computational burden of the cca calculation, we used pca to reduce the gradient dimension from 30,300 to 20,000. pca was calculated on a random subset of 300,000 sentences (around 50) of the training set. we also tried pca dimension reduction to a lower dimension of 4,096, for all three datasets. we observed no change in performance (flickr8k) or slightly worse results (flickr30k and coco). the number of rnn training epochs was 400, 100, 20, and 15, for the flickr8k, flickr30k, coco and wikipedia datasets respectively. tables 3, and show the results of the different rnnfv variants compared to the current state of the art methods. we also report results of combinations of models. combining was done by averaging the image-sentence (or sentencesentence) cosine similarities obtained by each model. first, we see that regression-based rnn-fv should be preferred over the classification-based one. in addition to its lower dimension and natural handling of unseen words, the results obtained by regression rnn-fv are better. sec- ond, we notice the competitive performance of the model trained on wikipedia sentences, which demonstrates the generalization power of the rnn-fv, being able to perform well on data different than the one which the rnn was trained on. training using the dataset’s sentences only slightly improves result, and not always. improved results are obtained when using the cca word embedding instead of word2vec. it is interesting to see the result of the reverse model, which is on a par with the other models. it is somewhat complementary to the left-to-right model, as the combination of the two yields somewhat improved results. finally, the combination of rnn-fv with the best model (gmmhglmm) of outperforms the current state of the art on flickr8k, and is competitive on the other datasets.", "section_index": 7}, {"content": "this paper introduces a novel fv representation for sequences that is derived from rnns. the proposed representation is sensitive to the element ordering in the sequence and provides a richer model than the additive bag model typically used for conventional fvs. the rnn-fv representation surpasses the state-of-theart results for video action recognition on two challenging datasets. when used for representing sentences, the rnnfv representation achieves state-of-the-art or competitive results on image annotation and image search tasks. since the length of the sentences in these tasks is usually short and, therefore, the ordering is less crucial, we believe that using the rnn-fv representation for tasks that use longer text will provide an even larger gap between the conventional fv and the rnn-fv. a transfer learning result from the image annotation task to the video action recognition task was shown. the con- ceptual distance between these two tasks makes this result both interesting and surprising. it supports a human development-like way of training, in which visual labeling is learned through natural language, as opposed to, e.g., associating bounding boxes with nouns. while such training was used in computer vision to learn related image to text tasks, and while recently zero-shot action recognition was shown 11, 55, nlp to video action recognition transfer was never shown to be as general as presented here.", "section_index": 8}], "id": 1001}
{"text": [{"content": "currently, no large-scale training data is available for the task of scientific paper summarization. in this paper, we propose a novel method that automatically generates summaries for scientific papers, by utilizing videos of talks at scientific conferences. we hypothesize that such talks constitute a coherent and concise description of the papers’ content, and can form the basis for good summaries. we collected papers and their corresponding videos, and created a dataset of paper summaries. a model trained on this dataset achieves similar performance as models trained on a dataset of summaries created manually. in addition, we validated the quality of our summaries by human experts.", "section_index": 0}, {"content": "the rate of publications of scientific papers is increasing and it is almost impossible for re- searchers to keep up with relevant research. au- tomatic text summarization could help mitigate this problem. in general, there are two com- mon approaches to summarizing scientific papers: citations-based, based on a set of citation sen- tences (nakov et al., 2004; abu-jbara and radev, 2011; yasunaga et al., 2019), and content-based, based on the paper itself (collins et al., 2017; nikola nikolov and hahnloser, 2018). automatic summarization is studied exhaustively for the news domain (cheng and lapata, 2016; see et al., 2017), while summarization of scientific papers is less studied, mainly due to the lack of large-scale training data. the papers’ length and complexity require substantial summarization effort from ex- perts. several methods were suggested to reduce these efforts (yasunaga et al., 2019; collins et al., 2017), still they are not scalable as they require human annotations. the authors contributed equally. recently, academic conferences started publishing videos of talks (e.g., acl1, emnlp1, icml2, and more). in such talks, the presenter (usually a co-author) must describe their paper coherently and concisely (since there is a time limit), provid- ing a good basis for generating summaries. based on this idea, in this paper, we propose a new method, named talksumm (acronym for talk- based summarization), to automatically generate extractive content-based summaries for scientific papers based on video talks. our approach uti- lizes the transcripts of video content of conference talks, and treat them as spoken summaries of pa- pers. then, using unsupervised alignment algo- rithms, we map the transcripts to the correspond- ing papers’ text, and create extractive summaries. table gives an example of an alignment between 1vimeo.com/aclweb icml.cc/conferences/2017/videos a paper and its talk transcript (see table in the appendix for a complete example). summaries generated with our approach can then be used to train more complex and data- demanding summarization models. although our summaries may be noisy (as they are created auto- matically from transcripts), our dataset can easily grow in size as more conference videos are aggre- gated. moreover, our approach can generate sum- maries of various lengths. our main contributions are as follows: (1) we propose a new approach to automatically gener- ate summaries for scientific papers based on video talks; (2) we create a new dataset, that contains summaries for papers from several computer science conferences, that can be used as training data; (3) we show both automatic and human eval- uations for our approach. we make our dataset and related code publicly available3. to our knowl- edge, this is the first approach to automatically cre- ate extractive summaries for scientific papers by utilizing the videos of conference talks.", "section_index": 1}, {"content": "several works focused on generating train- ing data for scientific paper summariza- tion (yasunaga et al., 2019; jaidka et al., 2018; collins et al., 2017; cohan and goharian, 2018). most prominently, the cl-scisumm shared tasks provide a total of human generated summaries; there, a citations- based approach is used, where experts first read citation sentences (citances) that reference the paper being summarized, and then read the whole paper. then, they create a summary of words on average. recently, to mitigate annotation cost, yasunaga et al. (2019) proposed a method, in which human annotators only read the abstract in addition to citances (not reading the full paper). using this approach, they generated summaries, costing person-hours. conversely, we generate summaries, given transcripts of conference talks, in a fully automatic manner, and, thus, our approach is much more scalable. (2017) also aimed at generating labeled data for scientific paper summarization, based on highlight statements that authors can provide in some publication venues. using external data to create summaries was url also proposed in the news domain. wei and gao (2014, 2015) utilized tweets to decide which sen- tences to extract from news article. finally, alignment between different modali- ties (e.g., presentation, videos) and text was stud- ied in different domains. both kan (2007) and bahrani and kan (2013) studied the problem of document to presentation alignment for schol- arly documents. kan (2007) focused on the the discovery and crawling of document-presentation pairs, and a model to align between documents to corresponding presentations. in bahrani and kan (2013) they extended previous model to include also visual components of the slides. align- ing video and text was studied mainly in the setting of enriching videos with textual infor- mation (bojanowski et al., 2015; malmaud et al., 2015; zhu et al., 2015). (2015) used hmm to align asr transcripts of cook- ing videos and recipes text for enriching videos with instructions. (2015) utilized books to enrich videos with descriptive explanations. (2015) proposed to align video and text by providing a time stamp for every sen- tence. the main difference between these works and ours is in the alignment being used to gener- ate textual training data in our case, rather than to enrich videos.", "section_index": 2}, {"content": "recently, many computer science academic asso- ciations including acl, acm, imls and more, have started recording talks in different confer- ences, e.g., acl, naacl, emnlp, and other co- located workshops. a similar trend occurs in other domains such as physics4, biology5, etc. in a conference, each speaker (usually a co- author) presents their paper given a timeframe of 15-20 minutes. thus, the talk must be coherent and concentrate on the most important aspects of a paper. hence, the talk can be considered as a sum- mary of the paper, as viewed by its authors, and is much more comprehensive than the abstract, which is written by the authors as well. in this work, we focused on nlp and ml conferences, and analyzed video talks from acl, naacl, emnlp, sigdial (2015-2018), and icml (2017-2018). we downloaded the 4www.cleoconference.org igem.org/videos/lecturevideos videos and extracted the speech data. then, via a publicly available asr service6, we extracted transcripts of the speech, and based on the video metadata (e.g., title), we retrieved the correspond- ing paper (in pdf format). we used scienceparse7 to extract the text of the paper, and applied a simple processing in order to filter-out some noise (e.g. lines starting with the word copyright). at the end of this process, the text of each paper is associated with the transcript of the corresponding talk. the transcript itself cannot serve as a good sum- mary for the corresponding paper, as it constitutes only one modality of the talk (which also consists of slides, for example), and hence cannot stand by itself and form a coherent written text. thus, to create an extractive paper summary based on the transcript, we model the alignment between spo- ken words and sentences in the paper, assuming the following generative process: during the talk, the speaker generates words for describing ver- bally sentences from the paper, one word at each time step. thus, at each time step, the speaker has a single sentence from the paper in mind, and produces a word that constitutes a part of its ver- bal description. then, at the next time-step, the speaker either stays with the same sentence, or moves on to describing another sentence, and so on. thus, given the transcript, we aim to retrieve those source sentences and use them as the sum- mary. the number of words uttered to describe each sentence can serve as importance score, in- dicating the amount of time the speaker spent de- scribing the sentence. this enables to control the summary length by considering only the most im- portant sentences up to some threshold. we use an hmm to model the assumed genera- tive process. the sequence of spoken words is the output sequence. each hidden state of the hmm corresponds to a single paper sentence. we heuris- tically define the hmm’s probabilities as follows. denote by y , we define the emission www.ibm.com/watson/services/speech-to-text/ github.com/allenai/science-parse probabilities to be: p(y (t) ys(t) k) max wwords(k) sim(y,w) where words(k) is the set of words in the k’th sentence, and sim is a semantic- similarity measure between words, based on word-vector distance. we use pre-trained glove as the semantic vector representations for words. as for the transition probabilities, we must model the speaker’s behavior and the transitions between any two sentences in the paper. this is unlike the simpler setting in malmaud et al. (2015), where transition is allowed between con- secutive sentences only. to do so, denote the en- tries of the transition matrix by t (k, l) p(s(t 1) ls(t) k). we rely on the following assumptions: (1) t (k, k) (the probability of staying in the same sentence at the next time-step) is rel- atively high. (2) there is an inverse relation be- tween t (k, l) and l k, i.e., it is more probable to move to a nearby sentence than jumping to a farther sentence. (3) s(t 1) s(t) is more probable than the opposite (i.e., transition to a later sentence is more probable than to an earlier one). although these assumptions do not perfectly re- flect reality, they are a reasonable approximation in practice. following these assumptions, we define the hmm’s transition probability matrix. first, de- fine the stay-probability as max, using to fit it to our case where transitions be- tween any two sentences are allowed, and to handle rare cases where k is close to, or even larger than t then, for each sentence index k 1, ...,k, we define: t (k, k) t (k, k j) k j1, j t (k, k j) k j1, j where , , k (0, 1), and are factors reflecting assumptions (2) and (3) respectively, and for all k, k is normalized s.t. the values of , , and were fixed through- out our experiments at 0.75, 0.5, and the average value of , across all papers, was around the values of these parameters were determined based on eval- uation over manually-labeled alignments between the transcripts and the sentences of a small set of papers. finally, we define the start-probabilities assum- ing that the first spoken word must be conditioned on a sentence from the introduction section, hence p(s(1)) is defined as a uniform distribution over the introduction section’s sentences. note that sentences which appear in the ab- stract, related work, and acknowledgments sec- tions of each paper are excluded from the hmm’s hidden states, as we observed that presenters sel- dom refer to them. to estimate the map sequence of sentences, we apply the viterbi algorithm. the sentences in the obtained sequence are the candidates for the pa- per’s summary. for each sentence s appearing in this sequence, denote by count(s) the number of time-steps in which this sentence appears. thus, count(s) models the number of words generated by the speaker conditioned on s, and, hence, can be used as an importance score. given a desired summary length, one can draw a subset of top- ranked sentences up to this length.", "section_index": 3}, {"content": "data for evaluation we evaluate the quality of our dataset generation method by training an extractive summarization model, and evaluating this model on a human-generated dataset of sci- entific paper summaries. for this, we choose the cl-scisumm shared task (jaidka et al., 2016, 2018), as this is the most established bench- mark for scientific paper summarization. in this dataset, experts wrote summaries of words length on average, after reading the whole paper. the evaluation is on the same test data used by yasunaga et al. (2019), namely examples from cl-scisumm 2016, and examples from cl- scisumm as validation data. training data using the hmm importance scores, we create four training sets, two with fixed-length summaries (150 and words), and two with fixed ratio between summary and paper lengths (0.3 and 0.4). we train models on each training set, and select the model yielding the best performance on the validation set (evaluation is always done with generating a 150-words sum- mary). summarization model we train an extractive summarization model on our talksumm dataset, using the extractive variant of chen and bansal (2018). we test two summary generation ap- proaches, similarly to yasunaga et al. first, for talksumm-only, we generate a 150- words summary out of the top-ranked sentences extracted by our trained model (sentences from the acknowledgments section are omitted, in case the model extracts any). in the second approach, a 150-words summary is created by augmenting the abstract with non-redundant sentences extracted by our model, similarly to the hybrid ap- proach of yasunaga et al. we perform early-stopping and hyper-parameters tuning using the validation set. baselines we compare our results to scisumm- net trained on sci- entific papers summarized by human annotators. as we use the same test set as in yasunaga et al. (2019), we directly compare their reported model performance to ours, including their abstract baseline which takes the abstract to be the paper’s summary. automatic evaluation table summarizes the results: both gcn cited text spans and talksumm-only models, are not able to obtain better performance than abstract8 however, for the hybrid approach, where the abstract is aug- mented with sentences from the summaries emit- ted by the models, our talksumm-hybrid out- performs both gcn hybrid and abstract. importantly, our model, trained on automatically- generated summaries, performs on par with mod- els trained over scisummnet, in which training data was created manually. 8while the abstract was input to gcn cited text spans, it was excluded from talksumm-only. human evaluation we conduct a human eval- uation of our approach with support from authors who presented their papers in conferences. as our goal is to test more comprehensive summaries, we generated summaries composed of sentences (approximately of a long paper). we randomly selected presenters from our corpus and asked them to perform two tasks, given the gen- erated summary of their paper: (1) for each sen- tence in the summary, we asked them to indicate whether they considered it when preparing the talk (yes/no question); (2) we asked them to globally evaluate the quality of the summary (1-5 scale, ranging from very bad to excellent, means good). for the sentence-level task (1), of the sentences were considered while preparing the talk. as for the global task (2), the quality of the sum- maries was on average, with standard deviation of these results validate the quality of our generation method.", "section_index": 4}, {"content": "we propose a novel automatic method to gener- ate training data for scientific papers summariza- tion, based on conference talks given by authors. we show that the a model trained on our dataset achieves competitive results compared to models trained on human generated summaries, and that the dataset quality satisfies human experts. in the future, we plan to study the effect of other video modalities on the alignment algorithm. we hope our method and dataset will unlock new opportu- nities for scientific paper summarization.", "section_index": 5}], "id": 1002}
{"text": [{"content": "emotion detection from text has become a popular task due to the key role of emotions in human-machine interaction. current approaches represent text as a sparse bag-of-words vector. in this work, we propose a new approach that utilizes pre-trained, dense word embedding representations. we introduce an ensemble approach combining both sparse and dense representations. our experiments include ve datasets for emotion detection from di erent domains and show an average improvement of in macro average f1-score.", "section_index": 0}, {"content": "emotions are an important element of human nature and detecting them in the textual messages written by users has many applications in information retrieval and human-computer interaction a common approach to emotion analysis and modeling is categorization, e.g., according to ekman’s basic emotions; namely, anger, disgust, fear, happiness, sadness, and surprise approaches to categorical emotion classi cation based on text often employ supervised machine learning classi ers, which require labeled training data. currently, two types of datasets labeled with emotions are publicly available: manually labeled, and pseudo-labeled. manual annotation requires high cognitive capabilities of multiple human annotators per sample. as a result, the quality of these datasets is usually high. however, the task is tedious, time-consuming, and expensive 4, and thus, these datasets are usually small (in the order of thousands of annotated samples). manual annotations are usually applied to domain speci c datasets (e.g., news headlines). to overcome these limitations, pseudo-labeled datasets are gathered from social media platforms where social media posts are explicitly tagged by the author by using the hashtag symbol () or by adding emoticons this tagged data can be used to create large-scale training data labeled with emotions in a non-speci c domain as in given such a dataset (manually or pseudo labeled), it is then common to train a linear classi er based on bag-of-words (bow) 1url/ permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. copyrights for components of this work owned by others than acm must be honored. abstracting with credit is permitted. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. request permissions from permissionsacm.org. ictir’17, october 14, 2017, amsterdam, the netherlands. doi: 10.1145/3121050.3121093 representation: representing text samples as sparse vectors, where each vector entry corresponds to the presence of a speci c feature (such as n-grams, punctuation and other) as reported recently by 11, deep learning is a promising approach for solving nlp tasks including text classi cation. while the aforementioned approach utilizes bow representation and linear classi ers, neural network methods are based on dense vector representations of text samples (word embedding) and are nonlinear. such word embedding representation captures syntactic and semantic knowledge, which can improve the emotion detection task. for example, in such a representation the words awful and terrible are expected to have similar vector representations. generating high quality word vectors requires large-scale data and computing power. when generation is not an option, one can utilize pre-trained representations; the most popular pre-trained representations are based on word2vec and glove algorithms, and were trained on large corpora. inspired by the good results of the deep learning approach, we aim at using these techniques for emotion detection. pseudo-labeled large-scale data is suitable for deep learning techniques, however, it exhibits low accuracy when classifying domain speci c data, even after domain adaptation (using linear models). on the other hand, the manually annotated data is highly accurate, but it is not large enough to form an appropriate base for deep learning techniques, as these models tend to over t small size datasets. in order to utilize the high quality but small size datasets, we propose an ensemble approach that combines both a linear model based on bow, and a non-linear model based on the pre-trained word vectors. in addition, we propose a new method for realizing a sentence level representation from the single words vectors. to our knowledge, this is the rst research that shows how to utilize pre-trained word vectors to improve emotion detection from text in domain-speci c datasets.", "section_index": 1}, {"content": "approaches to categorical emotion classi cation often employ onevs-rest machine learning classi ers (a binary classi er for each emotion), typically svm 19, or logistic regression 32, using textual features such as n-grams, lexicon based, and pos features. in the domain of sentiment analysis, which is closely related to emotion detection, tang et al. showed that encoding sentiment information into the word embeddings using twitter data, as a part of a neural network based system, outperformed previous approaches. 16, focused on generating document level embeddings in sentiment classi cation. these works all require the availability of large-scale data. in our setting, we only have small datasets, thus, we used pre-trained vectors. pre-trained word vectors were used as an input to neural networks to improve sentiment analysis classi cation 14, this approach also requires large-scale data for the neural network training. forgues used pre-trained word vectors and a linear classi er to classify user intents in dialog systems, however their task and methodology is di erent than ours.", "section_index": 2}, {"content": "in this section we describe the two di erent classi ers we created and an ensemble method that combines their output. the two classi ers are based on di erent document representations. depending on the dataset being used, the classi cation task can be represented as a multi-class or a multi-label problem. for both types of problems we used a one-vs-rest svm classi er. thus, given some test sample, a classi er outputs the decision function value for each emotion that appears in the training data. the classes associated with the test sample are then taken to be the emotion with the highest decision function value (for multi-class) or the set of emotions with a positive decision function value (for multi-label). in our rst approach we used an svm classi er with a linear kernel, and represented every document as a bow. we extracted various ngrams (after lemmatization), punctuation and social media features. namely, unigrams, bigrams, nrc lexicon features (number of terms in a post associated with each a ect label in nrc lexicon 22), and presence of exclamation marks, question marks, usernames, links, happy emoticons, and sad emoticons. we used tfidf weights for the values of n-gram features (this was experimentally superior to using binary weights), and removed n-grams that appeared in less than two documents. we further removed of the remaining features that got the lowest scores in a statistical test. we handled negation similarly to word embedding based vectors can be combined to represent a document into a xed size vector. we have experimented with several document representations, combining the word vectors, following the notation: dwe (t1, ..., tk ) 1k i1 ai k i1 ai v (ti ) , (1) where dwe is the word embedding based vector representation for document d with k terms, v is some pre-trained word-to-vector mapping (described in section 5), and ai is some weight indicating the relative importance of term ti the document representations we experimented with include: cbow (continuous bag of words) 18: in this case, i,ai 1, which means uniform weights for all terms. tfidf weights: ai is the tfidf weight for term ti in case ti was not present in the training data, we smoothed its idf weight as if it appeared in one document (this yielded better performance than discarding the term). classi er weights (class): in this approach we calculated a weight function, w (t , e ) for each term t in the training data which indicates its importance in classifying a document as expressing emotion e we did this by rst representing the documents in a bow binary vector representation where we only extracted unigram features. then, for each e we trained an svm model with a linear kernel and took m(e, t ) to be the weight associated by the model with each term t in the training data. motivated by guyon who showed that m(e, t ) is an indicative feature selection criterion, we de ne: w (t , e ) m (e, t ) e e , (2) where e and e are the corresponding average and standard deviation of model weights in absolute value. we now de ne: ai ti v or w (ti , e ) w (ti , e ) else (3) , where v is the vocabulary generated from the training data. in words, a low constant weight is assigned to terms which did not appear in the training data, or were found to be less discriminative. for other terms, the weight is proportional to the words discriminative power. this method captures the notion that some terms in a document are more important in its embedded representation since they are more informative given the classi cation task. note that, in this method, a di erent document representation is used for every emotion e since the discriminative power of every word (and hence its weight ai ) is di erent for each emotion. this is a novel embedded document representation method, and as detailed in section 5, it is superior in comparison to the other representation methods we experimented with. similarly to our bow classi er, we used an svm classi er in this approach as well, but since we represented a document as a low-dimensional vector, we allowed non-linearity by using an rbf kernel. this yielded better results than using a linear kernel. ensembles tend to achieve better results when there is a signi cant diversity among the classi ers thus, we utilized the above classi ers that are based on di erent document representations, to form an ensemble model. as a preliminary step, we transformed the above classi ers’ decision function value output to represent probabilities, using softmax transformation for multi-class problems 8, and sigmoid transformation for multi-label problems. the ensemble methods we experimented with, follow the notation: men (d ) (mbow (d )) (1 ) (mwe (d )) , (4) wheremen (d ) is the output probability vector for the ensemble classi er given a test document d ,mbow (d ) andmwe (d ) are the output probability vectors of the bow and the word embedding based classi ers respectively, and is a parameter which corresponds to the speci c ensemble method used. we have experimented with the following weighed average probabilities methods: equal weights ( 0.5), stacking ( is learned by an additional classi er) and precision-based weighting ( re ects the ratio between the macro precision scores for the two classi ers over the training data). we have found in our setting that precision-based weighting achieved the best performance, thus we report results using this method only.", "section_index": 3}, {"content": "we experimented with the following ve emotion detection datasets from di erent domains (summarized in table 1): isear contains labeled sentences where participants who have di erent cultural backgrounds reported experiences and reactions for seven emotions. semeval contains newspaper headlines labeled with the six ekman emotions by six annotators. for our experiments, we considered the most dominant emotion as the headline label as in fairy tales includes sentences from fairy tales, labeled with ve emotions by six annotators. for our experiments, we used only sentences with high annotation agreement of four identical emotion labels, as in blog posts consists of emotion-rich sentences collected from blogs labeled with emotions by four annotators. we considered only sentences for which the annotators agreed on the emotion category, as in customer support dialogs in twitter consists of customer turns from customer support dialogs in twitter, labeled by ve annotators with nine emotions relevant to customer care.", "section_index": 4}, {"content": "we tuned the hyper-parameters for our classi ers using a grid search over a validation dataset collected from twitter, presented in this process yielded the following hyper-parameter values that we used in all of our experiments below. penalty parameter for bow classi er: c ; penalty and kernel parameters for the word embedding based classi er: c 4, we evaluated our methods for each dataset by using 10-fold crossvalidation. our baseline is the bow classi er described above. this was used as a state-of-the-art approach for emotion detection in short texts in many cases, e.g., 19, and more. emotion detection datasets are labeled with multiple emotions and are imbalanced. thus, we evaluated the classi cation performance for all emotion classes by using macro average f1-score. we used scikit-learn for an svm implementation and spacy for n-grams extraction. we compared our bow classi er baseline with previous work which presented cross-validation results on the datasets we experimented with. table shows results presented in the original work that introduced the dataset, or in an advanced work. the table shows that our bow classi er results correspond to previous work, which validates our baseline’s implementation. we compared the quality of two publicly available pre-trained word vector sources, based on glove4 and word2vec (googlenews)5, in terms of emotion detection performance. vectors from both sources are dimensional. table depicts the macro f1-scores for each dataset and each document representation method that is based on word vectors, as detailed in section results show that for all datasets and representation methods, word vectors trained by glove achieved higher performance than using word2vec based vectors. for example, on average, cbow representation for glove source showed a improvement in f1-score relative to word2vec. thus, we used glove based pre-trained word vectors below. also, the class method we 2url/ 3url/ 4url 5url/ proposed outperformed the other embedded document representation methods. table depicts the macro f1-scores for each dataset, and for the di erent models: bow is our baseline, presented in section en-cbow, en-tfidf and en-class are the ensemble models of bow and the corresponding embedded representations presented in section our ensemble methods outperformed the baseline for each document representation method. the best result, which is also signi - cantly better for each dataset, is of en-class model that achieved an average relative improvement of in f1-score over all datasets. these results indicate the advantage in combining both bow and embedded document representations for emotion detection from text.", "section_index": 5}, {"content": "this work studied the use of pre-trained word vectors for emotion detection. we presented class, a novel method for representing a document as a dense vector based on the importance of the document’s terms in respect to emotion classi cation. our results show that an ensemble that combines bow and embedded representations using our class method, outperforms previous approaches for domain-speci c datasets. in comparison to other deep-learning methods, our approach ts a small number of model parameters and requires little computing power. for future work we plan to investigate the use of deep learning models trained on domain adapted pseudo-labeled large-scale datasets. we also plan to investigate transfer learning for multidomain emotion detection.", "section_index": 6}], "id": 1003}
{"text": [{"content": "providing customer support through social media channels is gaining increasing popularity. in such a context, automatic detection and analysis of the emotions expressed by customers is important, as is identification of the emotional techniques (e.g., apology, empathy, etc.) in the responses of customer service agents. result of such an analysis can help assess the quality of such a service, help and inform agents about desirable responses, and help develop automated service agents for social media interactions. in this paper, we show that, in addition to text based turn features, dialogue features can significantly improve detection of emotions in social media customer service dialogues and help predict emotional techniques used by customer service agents.", "section_index": 0}, {"content": "an interesting use case for social media is customer support that can now take place over public social media channels. using this medium has its advantages as described, for example, in (demers, 2014): customers appreciate the simplicity and immediacy of social media conversations, the ability to reach real human beings, the transparency, and the feeling that someone listens to them. businesses also benefit from the publicity of giving good services almost in real-time, online, building an online community of customers and encouraging more brand mentions in social media. a recent study shows that one in five (23) customers in the u.s. say they have used social media for customer service in 2014, up from in obviously, companies hope that such 1url/ news/docs/2014x/2014-global-customer- uses are associated with a positive experience. yet there are limited tools for assessing this. in this paper, we analyze customer support dialogues using the twitter platform and show the utility of such analyses. the particular aspect of such dialogues that we concentrate on is emotions. emotions are a cardinal aspect of inter-personal communication: they are an implicit or explicit part of essentially any communication, and of particular importance in the setting of customer service, as they relate directly to customer satisfaction and experience (oliver, 2014). typical emotions expressed by customers in the context of social media service dialogues include anger and frustration, as well as gratitude and more (gelbrich, 2010). on the other hand, customer service agents also express emotions in service conversations, for example apology or empathy. however, it is important to note that emotions expressed by service agents are typically governed by company policies that specify which emotions should be expressed in which situation (rafaeli and sutton, 1987). this is why we talk in this paper about agent emotional techniques rather than agent emotions. consider, for example, the real (anonymized) twitter dialogue depicted in figure in this dialogue, customer disappointment is expressed in the first turn (’bummer. /’), followed by customer support empathy (’uh oh!’). then in the last two turns both customer and support express gratitude. the analysis of emotions being expressed in customer support conversations can take two applications: (1) to discern and compute quality of service indicators and (2) to provide real-time clues to customer service agents regarding the cus- service-barometer-us.pdf tomer emotion expressed in a conversation. a possible application here is recommending to customer service agents what should be their emotional response (for example, in each situation, should they apologize, should they thank the customer, etc.) another interesting trend in customer service, in addition to the use of social media described above, is the automation of various functions of customer interaction. several companies are developing text-based chat agents, typically accessible through corporate web sites, and partially automatized: in these platforms, a computer program handles simple conversations with customers, and more complicated dialogues are transferred to a human agent. such partially automated systems are also in use for social media dialogues. the automation in such systems helps save human resources and, with further development based on artificial intelligence, more automation in customer service chats is likely to appear. given the importance of emotions in service dialogues, such systems will benefit from the ability to detect (customer) emotions and will need to guide employees (and machines) regarding the right emotional technique in various situations (e.g., apologizing at the right point). thus, our goal, in this paper, is to show that the functionality of guiding employees regarding appropriate responses can be developed based on the analysis of textual dialogue data. we show first that it is possible to automatically detect emotions being expressed and, second that it is possible to predict the emotional technique that is likely to be used by a human agent in a given situation. this analysis reflects our ultimate goal: to enable a computer system to discern the emotions expressed by human customers, and to develop computerized tools that mimic the emotional technique used by a human customer service agent in a particular situation. we see the main contributions of this paper as follows: (1) to our knowledge, this is the first research focusing on automatic analysis of emotions expressed in customer service provided through social media. (2) this is the first research using unique dialogue features (e.g., emotions expressed in previous dialogue turns by the agent and customer, time between dialogue turns) to improve emotion detection. (3) this is the first research studying the prediction of the agent emotional techniques to be used in the response to customer turns. the rest of this paper is organized as follows. we start by reviewing the related work and a description of the data that we collected. then we formally define the methodology for detection and prediction of emotion expression in dialogues. finally, we describe our experiments, evaluate the various models, conclude and suggest future directions.", "section_index": 1}, {"content": "approaches to categorical emotion classification often employ machine learning classifiers, and svm has typically outperformed other classifiers. in a series of binary svm classifiers (one for each emotion) were trained over datasets from different domains (news headlines, social media). these works utilize unigrams and bigrams among other lexical based features (e.g., utilizing the nrc emotion lexicon (mohammad and turney, 2013)) and punctuation based features. in our work, we also used an svm classifier, however, while these works aim at classifying single posts (i.e., sentence, tweet, etc.) without context, our work utilizes the context while con- sidering dialogues. the work in showed how to predict and elicit emotions in online dialogues. their approach for emotion classification is different from ours, for example they only considered the last turn as informative (we consider the full context of the dialogue), and focused on eliciting emotions, while we focus on predicting the agent emotional technique. the works in presented dialogue systems that sense the user emotions, such that the system further optimizes its affect response. both systems use rulebased approaches to generate responses, however, the authors do not discuss how they developed the rules. it is worth mentioning the works in that are focused on data-driven response generation in the context of dialogues in social media. these works generated general responses, while we focused on predicting the appropriate emotional response. in the domain of customer support, several papers studied emotions as part of written interactions. the work in , analyzed emotions in textual email communications and the authors focused on prioritizing customer support emails based on detected emotions. in the setting of online customer service the authors studied the impact of emotional text on the customer’s perception of the service agent. to extract the emotions, the authors used relatively basic features such as emoticons, exclamation marks, all caps, and some internet acronyms (such as ’lol’ or ’imho’). emotion detection is also applied to the domain of call centers and this differs from our focus since call center data are voice, and, thus, emotion detection is mainly based on paralinguistic aspects rather than on the text. in addition, if the textual part is considered, then the texts are transcripts of calls that are very different from written text (wallace chafe, 1987), and even more different from the social media setting where the dialogue is fully public.", "section_index": 2}, {"content": "in this section we describe the data collection process and provide some statistics about the twitter dialogue dataset we have collected. companies that utilize the twitter platform as a channel for customer service use a dedicated twitter account which provides real-time support by monitoring tweets that customers address to it. at the same time corporate support agents reply to these tweets also through the twitter platform. a customer and an agent, can use the twitter reply mechanism to discuss until the issue is solved (e.g., a solution is provided, or the customer is directed to another channel), or until the customer is no longer active. in the present work, we define a dialogue to be a sequence of turns between a specific customer and an agent, where the customer initiates the first turn. consecutive posts of the same party (customer or agent) uninterrupted by the other party, are considered as a single turn (even if there are several tweets). given the nature of customer support services, we assume the last turn in the dialogue is an agent turn (e.g., you’re very welcome. :) hit us back any time you need support). thus, we expect an even number of turns in the dialogue. we filtered out dialogues in which more than one customer or one agent are involved. formally, we define a dialogue to be an ordered list of turns t1, t2, , tn where odd turns are customer turns, and even turns are agent turns, and n is even. each turn ti is a tuple consisting of turn number, timestamp, content where turn number represents the sequential position of the turn in the dialogue, timestamp captures the time the message was published on twitter, and content is the textual message. we gathered data for two north america based customer support services twitter accounts that provide support for customers from north america (so tweets are in english). one service is for general customer care (denoted as gen), and the other is for technical customer support (denoted as tech). we extracted this data from december until june specifically, for each customer that posted a tweet to the customer support accounts, we searched for the previous, if any, turn to which it replied. given this method we traced back previous turns and reconstructed entire dialogues. table summarizes some statistics about the collected data, and figure depicts the frequencies of dialogue lengths which follow a power-law relationship. table shows differences between the two services; the dialogues in tech tend to be longer (i.e., typically include more turns), with an average of turns vs. average of turns for gen. as most of the dialogues include at most turns (88 and for gen and tech, respectively), we removed dialogues longer than turns. in addition, we removed dialogues that contained only turns as these are too short to be meaningful as the customer never replied or provided more details about the issue. after applying these preprocessing steps, we had dialogues of gen support, and dialogues of tech support.", "section_index": 3}, {"content": "the first objective of our work is to detect emotions expressed in customer turns and the second is to predict the emotional technique in agent turns. we treated these two objectives as two classification tasks. we generated a classifier for each task, where the classification output of one classifier can be part of the input to the other classifier. while both classifiers work at the level of turns, i.e., classify the current turn to emotions ex- pressed in it, they are inherently different. when detecting emotions in a customer turn, the turn’s content is available at classification time (as well as the history of the dialogue) - meaning, the customer has already provided her input and the system must now understand what is the emotion being expressed. whereas, when predicting the emotional technique for an agent turn, the turn’s content is not available during classification time, but only the agent action and the history of the dialogue since the agent did not respond yet. this difference stems from the fact that in order to train an automated service agent to respond based on customer input, the agent’s emotional technique needs to be computed before the agent generates its response sentence. we defined a different set of relevant emotion classes for each party in the dialogue (customer or agent), based on our above survey of research on customer service (e.g., (gelbrich, 2010)). relevant customer emotions to be detected are: confusion, frustration, anger, sadness, happiness, hopefulness, disappointment, gratitude, and politeness. relevant agent emotional techniques to be predicted are: empathy, gratitude, apology, and cheerfulness. we utilized the context of the dialogue to extract informative features that we refer to as dialogue features. using these features for emotion classification in written dialogues is novel, and as our experimental results show, it improves performance compared to a model based only on features extracted from the turn’s text. we used the following features in our models. comprises three contextual feature families: integral, emotional, and temporal. a feature can be global, namely its value is constant across an entire dialogue or it can be a local, meaning that its value may change at each turn. in addition, a feature can be historical (as will be discussed below). the integral family of features includes three sets of features: dialogue topic: a set of global binary features representing the intent of the customer who initiated the support inquiry. multiple intents can be assigned to a dialogue from a taxonomy of popular topics, which are adapted to the specific service. examples of topics include ac- count issues, payments, technical problem and more this feature set captures the notion that customer emotions are influenced by the event that led the customer to contact the customer service agent essence: a set of local binary features that represent the action used by the agent to address the last customer turn, independently of any emotional technique expressed. we refer to these actions as the essence of the agent turn. multiple essences can be assigned to an agent turn from a predefined taxonomy. for instance, asking for more information and offering a solution are possible essences this feature set captures the notion that customer emotions are influenced by actions of agents turn number: a local categorical feature representing the number of the turn. the emotional family of features includes agent emotion and customer emotion: these two sets of local binary features represent emotions predicted for previous turns. our model generates predictions of emotions for each customer and agent turn, and uses these predictions as features to classify a later customer or agent turn with emotion expression. the temporal family of features includes the following features extracted from the timeline of the dialogue: customer/agent response time: two local features that indicate the time elapsed between the timestamp of the last customer/agent turn and the timestamp of the subsequent turn. this is a categorical feature with values low, medium or high (using categorical values yielded better results than using a continuous value). median customer/agent response time: two local categorical features defined as the median of the customer/agent response times preceding the current turn. the categories are the same as the previous temporal features. 2currently this feature is not supported in social media. in other channels, for example, customer support on the phone, the customer is requested to provide a topic before she is connected to a support agent (usually using an ivr system). as this feature is inherent in other customer support channels, we assume that in the future it will also be supported in social media. 3we assume that if the agent is human, then this input is known to her e.g., based on company policies. for the automated service agent case, we assume that the dialogue system will manage and provide this input. day of week: a local categorical feature indicating the day of the week when the turn was published monday - sunday. this feature captures the effects of weekend versus weekday influences on emotions when representing a turn, ti, as a feature vector, we added some features originating in previous turns j i to ti. these features, that are historical, include the emotional features family and local integral features (namely agent emotions, customer emotions and agent essence). we do not include the turn number of previous turns, as this is dependent on the turn number of ti. we denote these features as historical features. the value of history, that is a parameter of our models, defines the number of sequential turns that precede ti which propagate historical features to ti. figure shows an example of the historical features in relation to the classification of customer turn ti, for history size between and these features are extracted from the text of a customer turn, without considering the context of the dialogue. we use various state-of-the-art text based features that have been shown to be effective for the social media domain (mohammad, 2012; roberts et al., 2012). these features include various n-grams, punctuation and social media features. namely, unigrams, bigrams, nrc lexicon features (number of terms in a post associated with each affect label in nrc lexicon), and presence of exclamation marks, question marks, usernames, links, happy emoticons, and sad emoticons. we note that these are the features we used in our baseline model detailed below, in the description of our experiments. for both of the agent and customer turn classification tasks, we implemented two different models which incorporate all of the feature sets we have detailed above. we considered these tasks as multi-label classification tasks. this captures the notion that a party can express multiple emotions (e.g., confusion and anger) in a turn. we chose to use a problem transformation approach which maps the multi-label classification task into several binary classification tasks, one for each emotion class which participates in the multi-label problem (tsoumakas and katakis, 2006). for each emotion e, a binary classifier is created using the one-vs.-all approach which classifies a turn as expressing e or not. a test sample is fully classified by aggregating the classification results from all independent binary classifiers. we next define our two modeling approaches. in our first approach we trained an svm classifier for each emotion class as explained above. the feature vector we used to represent a turn incorporates dialogue and textual features. the history size is also a parameter of this model. feature extraction for a training/testing feature vector representing a turn ti, works as follows. textual features are extracted for ti if it is a customer turn, or for ti1 if it is an agent turn (recall that the system does not have the content of agent turn ti at classification time). the temporal features are also extracted using time lapse values between previous turns as explained above. as discussed above, agent essence is assumed to be an input to our module, while agent emotion and customer emotion features are propagated from classification results of previous turns during testing (or from ground truth labels during training), where the number of previous turns is determined according to the value of history. these historical features are also appended to the feature vector of ti, similarly to where this method was used for classifying dialogue acts. our second approach to classifying dialogue turns is to use a sequence classification method used svm-hmm and conditional random fields for dialogue act classification. since emotions expressed in customer and agent turns are different, we treated them as different classification tasks (like in our previous approach) and trained a separate classifier for each emotion. we made the following changes when using svm-hmm: (1) we treated the emotion classification problem of turn ti as a sequence classification problem of the sequence t1, t3, ..., ti (i.e., only customer turns) if ti is a customer turn and t2, t4, ..., ti (i.e., only agent turns) if it is an agent turn. (2) the svm-hmm classifier generates models that are isomorphic to a kth-order hidden markov model. under this model, dependency in past classification results is captured internally by modeling transition probabilities between emotion states. thus, we removed historical customer emotion (resp. agent emotion) feature sets when representing a feature vector for a customer (resp. (3) we note that in our setting we provide classifications in real-time during the progress of the dialogue, so at classification time we have access only to previous turns and global information, and we cannot change classification decisions for past turns. thus, we tagged a test turn, ti, by classifying the sequence which ends in ti. then, ti was tagged with its sequence classification result.", "section_index": 4}, {"content": "a first step in building a classification model is to obtain ground truth data. for this, we sampled dialogues from our dataset, as detailed in table 2, based on each data source’s dialogue length distribution. this sample included customer turns and agent turns in total. the sampled dialogues were tagged using amazon mechanical turk4. each dialogue was tagged by five different mechanical turk’s master level judges. each 4url/ judge performed the following tagging tasks given the full dialogue: emotion tagging: indicate the intensity of emotion expressed in each turn (customer or agent) for each emotion, on a scale of (0...5), such that defines no emotion, a low emotion intensity and a high emotion intensity. the intraclass correlation (icc) among the judges was which indicates a moderate agreement which is common in this setting (lebreton and senter, 2007). dialogue topic tagging: select one or several topic: account issues, pricing, payments, customer service, customer experience, technical problem, technical question, order and delivery issues, behavior of a staff member, company policy issues and general statement. agent essence tagging: select one or several of the following for each agent’s turn, to describe the agent’s action in the specific turn: recognizing the issue raised, asking for more information, providing an explanation, offering a solution, general statement and assurance of efforts. the taxonomy is based on (zomerdijk and voss, 2010). we generated true binary labels from the emotion tagging. for turn ti, we considered it to express emotion e if tag(e, ti) where tag(e, t) is the average judges’ tag value of e in t. this process generated the class sizes detailed in table dialogue topic tagging was converted to binary features representing the top-2 selected topics. agent essence feature set representation for each turn was defined analogously. the temporal response time values were translated to low/medium/high categorical values according to their relation to the 33-th and 66-th percentiles. we evaluated our methods by using leave-onedialogue-out cross-validation ), over the whole dataset (for the two cus- tomer service data sources together). each test dialogue was classified by its order of turns, where each turn type (customer or agent) is classified by its corresponding classifier. our baseline in all experiments is an svm classifier that uses only the textual features described above, which do not utilize the dialogue context. this was used as a state-of-the-art single sentence emotion detection approach in many cases, e.g., and more. as described above, agent turn emotion prediction is performed before its content is known. thus, the baseline representation of an agent turn consisted of textual features extracted from its preceding customer turn. we evaluated each emotion’s classification performance by using precision (p ), recall (r) and f1score (f ). we evaluated the total performance for all emotion classes using micro and macro averages. we used liblinear5 as an svm implementation and svm-hmm6 for sequence classification. additionally, we used clearnlp7 for textual features extraction. since history size is a parameter of our models, we first tested the classification results for all possible history sizes (given that that maximum dialogue size in our dataset is 8). for each task and for each possible history size, we generated svm dialogue and svm-hmm dialogue models and evaluated them as detailed above. we compared the macro and micro average f1-score of our classifiers against the baseline classifier performance. as depicted in figure both the svm dialogue and svm-hmm dialogue models were superior 5url/ 6url/ svmlight/svmhmm.html 7url for all history ranges and for both tasks. examining the customer turns emotion detection performance, we can see in figure 4(a) that it increases until history 3, and then remains relatively stable for larger history sizes. this means that information about the behavior of the customer and agent in past turns is beneficial for detecting customer emotions in a current turn. for assessing the performance of our predictions of agent turns emotion techniques, we first note that we tested with history range, since we assume that the minimal information needed for agent turn classification is the information extracted from the last customer turn. figure 4(b) shows that overall, performance is highest when history 1, and does not decline much for higher history values. this indicates that for agent emotion technique prediction the last customer turn is the most informative one. in all of our experiments, we used the wilcoxon signed-rank test to validate the statistical significance of our models’ micro and macro average f1-score comparing to baseline performance. additionally, we used mcnemar’s test on the contingency tables aggregated over all emotions. these tests showed that both of our models were significantly different from the baseline model, under a value of 0.001, for both classification tasks and all history sizes. table depicts the detailed classification results for optimal history values that obtained maximal macro f1-score, namely for customer emotion detection history and for agent emotion technique prediction history the table presents performance for each emotion, for macro and micro average results over all dialogues, and for each data source (gen or tech) separately. for both classification tasks, both of our models outperformed baseline results for almost all emotions, where average macro and micro results are statistically significant compared to the baseline, as described above. for customer turn emotion detection, the svmhmm dialogue model performed better than the svm dialogue model, and reached a macro and micro average f1-score improvements over all dialogues of and 11.7, respectively. furthermore, the macro and micro average f1-score results of the svm-hmm dialogue model (0.519 and 0.6, respectively) are satisfying given the moderate icc score between the judges (0.53). for predicting the agent emotional technique, the svm dialogue model obtained slightly better results than svm-hmm dialogue model, and reached a macro and micro average f1-score improvements over all dialogues of and 43.5, respectively. these results emphasize the differences between the svm dialogue and svm-hmm dialogue models. specifically, when history size is large, as in customer emotion prediction, svm-hmm dialogue model, which internally captures dependencies in past classifications, outperforms the simplistic svm dialogue model. we note that an improvement is also obtained when calculating macro and micro average performance for each data source separately. this highlights our models’ superiority as well as their general applicability and robustness for different data sources. we examined the contribution of different feature sets in an incremental fashion, using the optimal history value detailed above. based on the families of feature sets that we defined in the methodology section, we tested the performance of different feature set combinations in our models, added in the following order: baseline (textual features), emotional, temporal and integral. figure depicts the results for both classification tasks. the x-axis represents specific combination of features sets, and the y-axis represents the macro or micro average f1-score value obtained. figure shows that adding each feature set improved performance for all models, for both tasks, which indicates the informative value of each feature set. additionally, the figure suggests that the most informative dialogue feature sets are the integral and emotional.", "section_index": 5}, {"content": "in this work we studied emotions being expressed in customer service dialogues in the social media. specifically, we described two classification tasks, one for detecting customer emotions and the other for predicting the emotional technique used by support service agent. we have proposed two different models (svm dialogue and svm-hmm dialogue models) for these tasks. we studied the impact of dialogue features and dialogue history on the quality of the classification and showed improvement in performance for both models and both classification tasks. we also showed the robustness of our models across different data sources. as for future work we plan to work on several aspects: (1) in this work, we showed that it is possible to predict the emotional technique. in the future, we plan to run experiments in which the predicted emotional technique is actually applied in the context of new dialogues to measure the effect of such predictions on real support dialogues. (2) distinguish between dialogues that have positive outcomes (e.g., high customer satisfaction) and others.", "section_index": 6}], "id": 1004}
{"text": [{"content": "few-shot learning (fsl) is a topic of rapidly growing interest. typically, in fsl a model is trained on a dataset consisting of many small tasks (meta-tasks) and learns to adapt to novel tasks that it will encounter during test time. this is also referred to as meta-learning. another topic closely related to meta-learning with a lot of interest in the community is neural architecture search (nas), automatically finding optimal architecture instead of engineering it manually. in this work we combine these two aspects of meta-learning. so far, meta-learning fsl methods have focused on optimizing parameters of pre-defined network architectures, in order to make them easily adaptable to novel tasks. moreover, it was observed that, in general, larger architectures perform better than smaller ones up to a certain saturation point (where they start to degrade due to over-fitting). however, little attention has been given to explicitly optimizing the architectures for fsl, nor to an adaptation of the architecture at test time to particular novel tasks. in this work, we propose to employ tools inspired by the differentiable neural architecture search (d-nas) literature in order to optimize the architecture for fsl without over-fitting. additionally, to make the architecture task adaptive, we propose the concept of metadapt controller’ modules. these modules are added to the model and are meta-trained to predict the optimal network connections for a given novel task. using the proposed approach we observe state-of-the-art results on two popular few-shot benchmarks: miniimagenet and fc100.", "section_index": 0}, {"content": "recently, there has been a lot of exciting progress in the field of few-shot learning in general, and in few-shot classification (fsc) in particular. a popular method for approaching fsc is meta-learning, or learning-to-learn. in meta-learning, the inputs to both train and test phases are not images, but instead a set of few-shot tasks, ti, each k-shot / n -way task containing a small amount k (usually 1-5) of labeled support images and some amount of unlabeled query images for each of the n categories of the task. the goal of meta-learning is to find a base model that is easily adapted to the specific task at hand, so that it will generalize well to tasks built from novel unseen categories and fulfill the goal of fsc (see section for further review). equal contributors corresponding authors: sivan doveh sivan.dovehibm.com and leonid karlinsky leonidkail.ibm.com ar x iv :1 2v cs .c v m ar many successful meta-learning based approaches have been developed for fsc 60,55,13,39,51,41,29 advancing its state-of-the-art. besides continuous improvements offered by the fsc methods, some general trends affecting the performance of fsc have become apparent. one of such major factors is the cnn backbone architecture at the basis of all the modern fsc methods. carefully reviewing and placing on a single chart the test accuracies of top-performing fsc approaches w.r.t. the backbone architecture employed reveals an interesting trend (figure 1). it is apparent that larger architectures increase fsc performance, up to a certain size, where performance seems to saturate or even degrade. this happens since bigger backbones carry higher risk of over-fitting. it seems the overall performance of the fsc techniques cannot continue to grow by simply expanding the backbone size. in light of the above, in this paper we set to explore methods for architecture search, their meta-adaptation and optimization for fsc. neural architecture search (nas) is a very active research field that has contributed significantly to overall improvement of the state of the art in supervised classification. some of the recent nas techniques, and in particular differentiable-nas (d-nas), such as darts 34, are capable of finding optimal (and transferable) architectures given a particular task using a single gpu in the course of 1-2 days. this is due to incorporating the architecture as an additional set of neural network parameters to be optimized, and solving this optimization using sgd. due to this use of additional architectural parameters, the training tends to over-fit. d-nas optimization techniques are especially designed to mitigate over-fitting, making them attractive to extreme situations with the greatest risk of overfitting, such as in the case of fsc. so far, d-nas techniques have been explored mainly in the context of large scale tasks, involving thousands of labeled examples for each class. very little work has been done on nas for few-shot. d-nas in particular, to the best of our knowledge, has not been applied to few-shot problems yet. meta-adaption of the architecture in task dependent manner to accommodate for novel tasks also has not been explored. in this work, we build our few-shot task-adaptive architecture search upon a technique from d-nas (darts 34). our goal is to learn a neural network where connections are controllable and adapt to the few-shot task with novel categories. similarly to darts, we have a neural network in the form of a directed acyclic graph (dag), where the nodes are the intermediate feature maps tensors, and edges are operations. each edge is a weighted sum of operations (with weights summing to 1), each operation is a different preset sequence of layers (convolution, pooling, batchnorm and non-linearity). the operations set includes the identity-operation and the zero-operation to either keep the representation untouched or cut the connection. to avoid over-fitting, a bi-level (two-fold) optimization is performed where first the operation layers’ weights are trained on one fold of the data and then the connections’ weights are trained on the other fold. however, unlike darts, our goal is not to learn a one time architecture to be used for all tasks. to be successful at fsc, we need to make our architecture task adaptive so it would be able to quickly rewire for each new target task. to this end, we employ a set of small neural networks, metadapt controllers, responsible for controlling the connections in the dag given the current task. the metadapt controllers adjust the weights of the different operations, such that if some operations are better for the current task they will get higher weights, thus, effectively modifying the architecture and adapting it to the task. to summarize, our contributions in this work are as follows: (1) we show that darts-like bi-level iterative optimization of layer weights and network connections performs well for few-shot classification without suffering from overfitting due to over-parameterization; (2) we show that adding small neural networks, metadapt controllers, that adapt the connections in the main network according to the given task further (and significantly) improves performance; (3) using the proposed method, we obtain improvements over fsc state-of-the-art on two popular fsc benchmarks: miniimagenet and fc100", "section_index": 1}, {"content": "the major approaches to few-shot learning include: metric learning, generative (or augmentation) based methods, and meta learning (or learning-to-learn). few-shot learning by metric learning. this type of methods 64,55,49 learn a non-linear embedding into a metric space where l2 nearest neighbor (or similar) approach is used to classify instances of new categories according to their proximity to the few labeled training examples embedded in the same space. additional proposed variants include using a metric learning method based on graph neural networks 16, that goes beyond the l2 metric. similarly, 52,58 introduce metric learning methods where the similarity is computed by an implicit learned function rather than via the l2 metric over an embedding space. the embedding space based metric-learning approaches are either posed as a general discriminative distance metric learning 49,8, or optimized on the fewshot tasks 55,64,16,41, via the meta-learning paradigm that will be described next. these approaches show a great promise, and in some cases are able to learn embedding spaces with some meaningful semantics embedded in the metric improved performance in the metric learning based methods has been achieved when combined with some additional semantic information. in 24, class conditioned embedding is used. in 66, the visual prototypes are refined using a corresponding label embedding and in it is extended to using multiple semantics, such as textual descriptions. augmentation-based few-shot learning. this family of approaches refers to methods that (learn to) generate more samples from the one or a few examples available for training in a given few-shot learning task. these methods include synthesizing new data from few examples using a generative model, or using external data for obtaining additional examples that facilitate learning on a given few shot task. these approaches include: (i) semi-supervised approaches using additional unlabeled data 9,14; (ii) fine tuning from pre-trained models 31,62,63; (iii) applying domain transfer by borrowing examples from relevant categories or using semantic vocabularies 3,15; (iv) rendering synthetic examples 42,10,56; (v) augmenting the training examples using geometric and photometric transformations or learning adaptive augmentation strategies 21; (vi) example synthesis using generative adversarial networks (gans) 69,25,20,48,45,35,11,23,2. in 22,54 additional examples are synthesized via extracting, encoding, and transferring to the novel category instances, of the intra-class relations between pairs of instances of reference categories. in 61, a generator sub-net is added to a classifier network and is trained to synthesize new examples on the fly in order to improve the classifier performance when being fine-tuned on a novel (few-shot) task. in 48, a few-shot class density estimation is performed with an auto-regressive model, augmented with an attention mechanism, where examples are synthesized by a sequential process. in 7,67 label and attribute semantics are used as additional information for training an example synthesis network. in models are trained to perform set-operations (e.g. union) and then can be used to synthesise samples for few-shot multi-label classifications. few-shot meta-learning (learning-to-learn). these methods are trained on a set of few-shot tasks (also known as episodes’) instead of a set of object instances, with the motivation to learn a learning strategy that will allow effective adaptation to new such (few-shot) tasks using one or few examples. an important sub-category of meta learning methods is metric-meta-learning, combining metric learning as explained above with task-based (episodic) training of meta-learning. in matching networks 60, a non-parametric k-nn classifier is meta-learned such that for each few-shot task the learned model generates an adaptive embedding space for which the task can be better solved. in the metric (embedding) space is optimized such that in the resulting space different categories form compact and well separated uni-modal distributions around the category prototypes’ (centers of the category modes). another family of meta-learning approaches is the so-called gradient based approaches’, that try to maximize the adaptability’, or speed of convergence, of the networks they train to new (few-shot) tasks (usually assuming an sgd optimizer). in other words, the meta-learned classifiers are optimized to be easily fine-tuned on new few-shot tasks using small training data. the first of these approaches is maml that due to its universality was later extended through many works such as, meta-sgd 30, demlmeta-sgd 68, metalearn lstm 46, and meta-networks in leo 51, a maml like loss is applied not directly on the model parameters, but rather on a latent representation encoding them. this approach featured an encoder and a decoder to and from that latent space and achieved state-of-the-art results on miniimagenet few-shot benchmark among models relying on visual information alone. in metaoptnet a cnn backbone is trained end-to-end with an unrolled convex optimization solution of an optimal classifier, such as svm. in this work, we use their suggested construction of performing sgd through an unrolled svm optimization to train the backbone. our work is focused on optimizing the backbone architecture. other methods focus on regularization for mitigating the over-fitting. in bf3s auxiliary self-supervision tasks are added, such as predicting image rotation or patch location. in robust-dist first an ensemble of up to models is learned, so each model by itself cannot overfit the data. then, the the final model is distilled from all those models. notably, our method which also deals with training large architecture without over-fitting is orthogonal to these two approaches. it is likely that further improvement can be achieved by combining these methods with ours. notably, in all previous meta-learning methods, only the parameters of a (fixed) neural network are optimized through meta-learning in order to become adaptable (or partially adaptable) to novel few-shot tasks. in this work, we both learn a specialized backbone architecture that would facilitate this adaptability, as well as meta-learn to become capable of adapting that architecture itself to the task, thus going beyond the parameter only adaptation of all previous metalearning approaches. neural architecture search. over the last few years neural architecture search (nas) have enabled automatic design of novel architectures that outperformed previous hand-designed architectures in terms of accuracy. two notable works on nas are amoebanet and nasnet the first one used a genetic algorithm and the second used a reinforcement learning based method. although achieving state of the art performance at the time, these methods required enormous amount of gpu-hours efficient nas (enas) 43, a reinforcement learning based method, used weight sharing across its child models, which are sub graphs of a larger one. by that, they managed to accelerate the search process. the work in shows how to scale the size of such learned architectures with the size of the input data. recently, differentiable methods with lower demand for computing have been introduced. notable among them are differentiable architecture search (darts) and snas these methods managed to search for architecture in just a few gpu days. darts relaxes the search space into a continuous one, allowing a differentiable search. the darts method includes two stages. first, basic structures are learned, by placing coefficients attached to operations between feature maps. these coefficients indicate the importance of the attached operations and connections. after the search is done, the final basic structures are formed by pruning and keeping only the most important operations. at the second stage, the final network is built by repeatedly concatenating the found basic structures. asap addresses the issue that harsh pruning at the end of the search makes the found architecture sub-optimal. it does so by performing gradual pruning. asap achieves higher accuracy with a shorter training time. in snas 65, the search is done by learning a continuous architectures distribution and sampling from it. this distribution is pushed closer to binary by using a temperature parameter and gradually decreasing it. then, the chosen architecture is the one that has the higher probability. in a binary mask is learned and used to keep a single path of the network graph. by doing so, they managed to search for the whole network and not only cells. pnas suggested a method for progressively searching for a larger architecture. p-darts do the same but with differentiable architecture search. these methods are mostly focused, and perform well, on datasets such as cifar and imagenet. so far, little attention has been given to their adaptation to few-shot learning. auto-meta used pnas based search for few-shot classification, but with a focus on searching for a small architecture (resulting in a relatively low performance w.r.t. to current state-of-the-art). in particular, the possibility of adapting the architecture at test-time to a specific novel task, as proposed in this work, has not been explored before.", "section_index": 2}, {"content": "in this section we describe the architecture and training procedure for metadapt. we introduce the task-adaptable block, it has a graph structure with adaptable connections that can modulate the architecture, adapting it to the few-shot task at hand. we then describe the sub-models, metadapt controllers, that predict the change in connectivity that is needed in the learned graph as a function of the current task. finally, we describe the training procedure. the architecture of the adaptable block used in metadapt is defined, similarly to darts 34, as a directed acyclic graph (dag). the block is built from feature maps v xi that are linked by mixtures of operations. the input feature map to the block is x0 and its output is xv a mixed operation, o(i,j), is defined as o(i,j)(x) oo exp( (i,j) o )o(x) oo exp( (i,j) o ) , (1) where o is a set of the search space operations, o(x) is an operation applied to x, and (i,j) o is an optimised coefficient for operation o at edge (i, j). later, we will describe how s can be adapted per task (k-shot, n -way episode). the list of search space operations used in our experiments is provided in table this list includes the zero-operation and identity-operation that can fully or partially (depending on the corresponding (i,j) o ) cut the connection or make it a residual one (skip-connection). each feature map xi in the block is connected to all previous maps by setting it to be: xi ji o(j,i)(xj). (2) the task-adaptive block defined above can be appended to any backbone feature extractor. potentially, more than one block can be used. we use resnet9 followed by a single task-adaptive block with nodes (v 4) in our experiments, resulting in about times more parameters compared with the original resnet12 (due to large set of operations on all connections combined). note that as we use nodes in our block, there exists a single path in our search space that is the regular residual block (resnet3 block). figure 2a schematically illustrates the block architecture. the task-adaptive block is accompanied by a set of metadapt controller modules, one per edge. they are responsible for predicting, given a few-shot task, the best way of adapting the mixing coefficients ( (i,j) o ) for the corresponding edge operations. let (i,j) be the vector of all (i,j) o let (i,j) be the globally optimized coefficients (optimization process described below), then metadapt controllers predict the task-specific residuals (i,j), that is the modification required to make to (i,j) for the current task (few-shot episode). finally, (i,j) (i,j) (i,j) (3) are the final task-adapted coefficients used for the mixed operation calculation as in equation the architecture for each metadapt controller, predicting (i,j), is as follows. it receives the input feature maps of the corresponding edge xi, computed for all the support samples of the episode. for a support-set of size s, number of channels d and feature map spatial resolution m m , the input is a tensor of dimensions (s,d,m,m). we perform global average pooling to obtain a (s,d) tensor, followed by a bottleneck linear layer (with relu activation) that operates on each sample individually, to get a (s,dbottleneck) size tensor. then, all support samples representations are concatenated to form a single vector of size s dbottleneck. finally, another linear layer maps the concatenation of all support-samples to the predicted (i,j). the metadapt controller architecture and the way it is used in our adaptable block structure are schematically illustrated on figure 2bc. figures and present an example of adaptation made by the metadapt controller for a specific episode. replacing simple sequence of convolutional layers with the suggested dag, with its many layers and parameters, in conventional gradient descent training will result in a larger over-fitting. this is even worse for fsl, where it is harder to achieve generalization due to scarcity of the data and the domain differences between the training and test sets. researchers have faced the same problem with differentiable architecture search, where the objective is to train a large neural network with weighted connections that are then pruned to form the final chosen architecture. we follow the solution proposed in darts 34, solving a bi-level iterative optimization of the layers’ weights w and the coefficients of operations between the nodes. the training set is split to trainw for weights training and train for training the ’s. iteratively optimizing w and to convergence is prohibitively slow. so, like in darts, w is optimized with a standard sgd: w w wlosstrainw(w,), (4) where is the learning rate. the ’s are optimized using sgd with a secondorder approximation of the model after convergence of w, by applying: losstrain(w losstrainw(w,), ) (5) where is the learning rate for the metadapt controllers’ parameters are trained as a final step, with all other parameters freezed, using sgd on the entire training set for a single epoch.", "section_index": 3}, {"content": "we use the popular miniimagenet and fc100 few-shot benchmarks to evaluate our method. the miniimagenet dataset is a standard benchmark for few-shot image classification, that has randomly chosen classes from ilsvrc-2012 these classes are randomly split into meta-training, meta-validation, and meta-testing classes. each class has images of size we use the same classes splits as and prior works. the fc100 dataset is constructed from the cifar-100 dataset 27, which contains classes that are grouped into super-classes. these are in turn partitioned into classes from super-classes for meta-training, classes from super-classes for meta-validation, and classes from super-classes for meta-testing. this minimizes the semantic overlap between classes of different splits. each class contains images of size we use the svm classifier head as suggested in metaoptnet we begin with training a resnet12 backbone on the training set of the relevant dataset for epochs. we then replace the last residual block of the resnet12 backbone with our dag task-adaptive block, keeping the first resnet blocks (resnet9) fixed and perform the architecture search for epochs. finally, we train the metadapt controllers’ for a single epoch. each epoch consists of episodes with mini-batch of episodes. for the initial training we use the sgd optimizer with intial learning rate 0.1, momentum and weight decay decreasing the learning rate to at epoch 20, at epoch and at epoch for weights optimization during the search and meta adaptation phases we use the sgd optimizer with learning rate 0.001, momentum and weight decay for the architecture optimization we use adam optimizer with learning rate 104, 0.5, 0.99, weight decay and the cosine annealing learning rate scheduler with min following previous works, e.g. 13,5, we perform test time augmentations and fine-tuning. we perform horizontal flip augmentation, effectively doubling the number of support-set. we fine-tune the dag weights for iterations where the horizontally flipped support set serves as our labeled query set. tables and compare the metadapt performance with the state-of-the-art few-shot classification methods that use plain resnet backbones. we observe improved results for fc100 1-shot (3.46) and 5-shot (3.17) and also for miniimagenet 1-shot (1.74) and similar results for 5-shot. we see that despite having a larger model we do not suffer from severe over-fitting and perform comparably or better than top performing methods. architecture transferability. it has been shown, in the case of architecture search, that it is possible to learn an architecture on a smaller dataset, e.g. cifar-10, and then the optimized architecture is transferable to a larger datasets, e.g. this helps mitigating the costly architecture search process. we follow this route, showing in our experiment that we can learn the values on fc100 and transfer them to miniimagenet (and then train the weights w of the transferred architecture on this dataset). for miniimagenet we set the ’s to be fixed to values obtained for the fc100, while the rest of the parameters of the searched top block are randomly initialized. we find that the architecture transferred from fc100 to miniimagenet is performing well, with results comparable to other state-of-the-art methods, 62.82/79.35 for 1/5-shot. but a search performed on the actual dataset (miniimagenet training set) is outperforming the transferred one. next, we explore the effect of the different design choices made in our approach. we hypothesize that simply using the same algorithm with a larger model architecture would not result in better performance and it might even harm performance. this is evident in figure when comparing the performance of different methods across increasingly larger architectures. this is also evident by observing the architectures usually used in the few-shot literature. it is already been shown in darts that training together with w simultaneously decrease performance. they attribute this decrease to over-fitting the training-set. to confirm our hypothesis, we used sgd to train our suggested dag architecture, using fixed uniform instead of the learned (making it even less likely to over-fit compared to the ablation in darts). to this end i,j are initialized so each operation is given the same weight and are kept fixed. we observed that indeed in this case our large architecture is not performing as well as resnet12 (a smaller architecture). 5: training curves with and without optimization of for fc100. the generalization gap (gap between training-set and validationset accuracies) is smaller when is optimized using our method, suggesting it has a regularization effect. the uniform- and optimized- experiments are described in sec. fc100 method 1-shot 5-shot protonet tadam metaoptnet metadapt (ours) table 4: metadapt vs. s-metadapt (stochastic metadapt); cifar-100 (fc100) 5-way accuracy fc100 method 1-shot 5-shot s-metadapt (ours) metadapt (ours) darts without meta-adaptation. next we test the effect of optimizing using iterative intermittent optimization for w and using different folds of the training set. here, w and are updated intermittently one mini-batch at a time. in order to see the importance of using second-order approximation of w after convergence, we perform training with and without it. without: the updates are done without the second-order approximation of w after a gradient descent step, i.e., the updates are performed according to: w w wlosstrainw(w,) (6) we find the optimization is helping at improving the performance by about compared to the fixed architecture (see table 5c). with: the updates are done not according to current value of w but at an approximation of its value after convergence (see equation 5). the update of w is performed according to equations we find that this change gives a moderate improvement of about (see table 5d). figure presents the training curves for training with the proposed bi-level optimization of w and vs. training the large model when is fixed. it shows that the generalization gap is larger for the latter case and confirms our hypothesis that simply adding more parameters is not sufficient for good performance. in the ablation experiments described till now, we used a slightly smaller model. each edge is composed of operations out of the listed in table 1, excluding the operations. now, we add these operations to test the effect of a larger set of operations. adding these operations improves slightly further the performance (see table 5e). then, we add the metadapt controllers, so the architecture is adapted to the current task according to the support samples. this brings us to the full metadapt method. we find that indeed the adaptations to each task are beneficial. the meta-adaptations improve the accuracy by around (see table 5f). test time augmentations and fine-tuning. finally, we add test time flip augmentations and fine-tuning as described in this helps in the case of 1-shot with improvement, but has no noticeable effect for 5-shot (see table 5g-h). a recent approach suggested for architecture search is stochastic neural architecture search (snas 65). darts, at search time the training is done on the full model at each iteration where each edge is a weighted-sum of its operations according to i,j contrarily, in snas i,j are treated as probabilities of a multinomial distribution and at each iteration a single operation is sampled accordingly. so at each iteration only a single operation per edge affects the classification outcome and only this operation is be updated in the gradient descent backward step. of course sampling from a multinomial distribution directly is not differentiable, so at training time the gumbel distribution is used as a differentiable approximation. we tested a snas version of metadapt, named s-metadapt, on the few-shot classification task. other than the modifications specified below s-metadapt is similar to metadapt. at training time, instead of the mixed operation defined in equation 1, we define the mixed operation to be: oi,j(x) oo zi,jo o(x) (8) where z(i,j) is a continuous approximation of a one-hot vector sampled from a gumbel distribution: zi,j gumbel(i,j). (9) here i,j are after softmax normalization and summed to at test time, rather than the one-hot approximation, we use the operation with the top probability zi,jk 1, if k argmax(i,j) 0, otherwise (10) using this method we get better results for fc100 1-shot and comparable results for 5-shot, compared to vanilla metaoptnet. however, it does not perform as well as the non-stochastic version of metadapt.", "section_index": 4}, {"content": "in this work we have proposed metadapt, a few-shot learning approach that enables meta-learned network architecture that is adaptive to novel few-shot tasks. the proposed approach effectively applies tools from the neural architecture search (nas) literature, extended with the concept of metadapt controllers’, in order to learn adaptive architectures. these tools help mitigate over-fitting to the extremely small data of the few-shot tasks and domain shift between the training set and the test set. we demonstrate that the proposed approach successfully improves state-of-the-art results on two popular few-shot benchmarks, miniimagenet and fc100, and carefully ablate the different optimization steps and design choices of the proposed approach. some interesting future work directions include extending the proposed approach to progressively searching the full network architecture (instead of just the last block), applying the approach to other few-shot tasks such as detection and segmentation, and researching into different variants of task-adaptivity including global connections modifiers and inter block adaptive wiring.", "section_index": 5}], "id": 1005}
{"text": [{"content": "virtual agents are becoming a prominent channel of interaction in customer service. not all customer interactions are smooth, however, and some can become almost comically bad. in such instances, a human agent might need to step in and salvage the conversation. detecting bad conversations is important since disappointing customer service may threaten customer loyalty and impact revenue. in this paper, we outline an approach to detecting such egregious conversations, using behavioral cues from the user, patterns in agent responses, and useragent interaction. using logs of two commercial systems, we show that using these features improves the detection f1-score by around over using textual features alone. in addition, we show that those features are common across two quite different domains and, arguably, universal.", "section_index": 0}, {"content": "automated conversational agents (chatbots) are becoming widely used for various tasks such as personal assistants or as customer service agents. recent studies project that of businesses plan to use chatbots by 20201, and that chatbots will power of customer service interactions by the year this increasing usage is mainly due to advances in artificial intelligence and natural language processing (hirschberg and manning, 2015) 1url 2url along with increasingly capable chat development environments, leading to improvements in conversational richness and robustness. still, chatbots may behave extremely badly, leading to conversations so off-the-mark that only a human agent could step in and salvage them. consequences of these failures may include loss of customer goodwill and associated revenue, and even exposure to litigation if the failures can be shown to include fraudulent claims. due to the increasing prevalence of chatbots, even a small fraction of such egregious3 conversations could be problematic for the companies deploying chatbots and the providers of chatbot services. in this paper we study detecting these egregious conversations that can arise in numerous ways. for example, incomplete or internally inconsistent training data can lead to false classification of user intent. bugs in dialog descriptions can lead to dead ends. failure to maintain adequate context can cause chatbots to miss anaphoric references. in the extreme case, malicious actors may provide heavily biased (e.g., the tay chatbot4) or even hacked misbehaviors. in this article, we focus on customer care systems. in such setting, a conversation usually becomes egregious due to a combination of the aforementioned problems. the resulting customer frustration may not surface in easily detectable ways such as the appearance of all caps, shouting to a speech recognizer, or the use of profanity or extreme punctuation. consequently, the chatbot will continue as if the conversation is proceeding well, usually 3defined by the dictionary as outstandingly bad. 4url leading to conversational breakdown. consider, for example, the anonymized but representative conversation depicted in figure here the customer aims to understand the details of a flight ticket. in the first two turns, the chatbot misses the customer’s intentions, which leads to the customer asking are you a real person?. the customer then tries to explain what went wrong, but the chatbot has insufficient exposure to this sort of utterance to provide anything but the default response (i’m not trained on that). the response seems to upset the customer and leads to a request for a human agent, which is rejected by the system (we don’t currently have live agents). such rejection along with the previous responses could lead to customer frustration (amsel, 1992). being able to automatically detect such conversations, either in real time or through log analysis, could help to improve chatbot quality. if detected in real time, a human agent can be pulled in to salvage the conversation. as an aid to chatbot improvement, analysis of egregious conversations can often point to problems in training data or system logic that can be repaired. while it is possible to scan system logs by eye, the sheer volume of conversations may overwhelm the analyst or lead to random sampling that misses important failures. if, though, we can automatically detect the worst conversations (in our experience, typically under of the total), the focus can be on fixing the worst problems. our goal in this paper is to study conversational features that lead to egregious conversations. specifically, we consider customer inputs throughout a whole conversation, and detect cues such as rephrasing, the presence of heightened emotions, and queries about whether the chatbot is a human or requests to speak to an actual human. in addition, we analyze the chatbot responses, looking for repetitions (e.g. from loops that might be due to flow problems), and the presence of not trained responses. finally, we analyze the larger conversational context exploring, for example, where the presence of a not trained response might be especially problematic (e.g., in the presence of strong customer emotion). the main contributions of this paper are twofold: (1) this is the first research focusing on detecting egregious conversations in conversational agent (chatbot) setting and (2) this is the first research using unique agent, customer, and customer-agent interaction features to detect egregiousness. the rest of this paper is organized as follows. we review related work, then we formally define the methodology for detecting egregious conversations. we describe our data, experimental setting, and results. we then conclude and suggest future directions.", "section_index": 1}, {"content": "detecting egregious conversations is a new task, however, there is related work that aim at measuring the general quality of the interactions in conversational systems. these works studied the complementary problem of detecting and measuring user satisfaction and engagement. early work by discussed a framework that maximizes the user satisfaction by considering measures such as number of inappropriate utterances, recognition rates, number of times user requests repetitions, number of turns per interaction, etc. shortcomings of this approach are discussed by specifically, these works evaluated chat functionality by asking users to make conversations with an intelligent agent and measured the user satisfaction along with other features such as the automatic speech recognition the authors presented a conversational system enhanced with emotion analysis, and suggested using emotions as triggers for human escalation. in our work, we likewise use emotion analysis as predictive features for egregious conversation. the works of studied reasons why users reformulated utterances in such systems. specifically, in they focused on how to automatically predict the reason for user’s dissatisfaction using different features. our work also explores user reformulation the authors also looked for problems in a specific setting of spoken conversations. the main difference with our work is that we focus on chat logs for domains for which the expected user utterances are a bit more diverse, using interaction features as well as features that are not sensitive to any architectural aspects of the conversational system they measured dialogue success at the turn level as a way of predicting the success of a conversation as a whole. created a measure of dialogue appropriateness to determine its role in maintaining a conversation. recently, evaluated a number of popular measures for dialogue response generation systems and highlighted specific weaknesses in the measures. simi- larly, in they developed a taxonomy of available measures for an enduser’s quality of experience for multimodel dialogue systems, some of which touch on conversational quality. all these measures may serve as reasons for a conversation turning egregious, but none try to capture or predict it directly. in the domain of customer service, researchers mainly studied reasons for failure of such systems along with suggestions for improved design in the authors analyzed reasons sales chatbots fail by interviewing chatbots experts. they found that a combination of exaggerated customer expectations along with a reduction in agent performance they studied service quality dimensions (i.e., reliability, empathy, responsiveness, and tangibility) and how to apply them during agent design. the main difference between those works and ours is that they focus on qualitative high-level analysis while we focus on automatic detection based on the conversations logs.", "section_index": 2}, {"content": "the objective of this work is to reliably detect egregious conversations between a human and a virtual agent. we treat this as a binary classification task, where the target classes are egregious and non-egregious. while we are currently applying this to complete conversations (i.e., the classification is done on the whole conversation), some of the features examined here could likely be used to detect egregious conversations as they were unfolding in real time. to perform egregious conversation detection, features from both customer inputs and agent responses are extracted, together with features related to the combination of specific inputs and responses. in addition, some of these features are contextual, meaning that they are dependent on where in the conversation they appear. using this set of features for detecting egre- gious conversations is novel, and as our experimental results show, improves performance compared to a model based solely on features extracted from the conversation’s text. we now describe the agent, customer, and combined customer-agent features. a virtual agent is generally expected to closely simulate interactions with a human operator (reeves and nass, 1996; nass and moon,y, 2000; kramer, 2008). when the agent starts losing the context of a conversation, fails in understanding the customer intention, or keeps repeating the same responses, the illusion of conversing with a human is lost and the conversation may become extremely annoying. with this in mind, we now describe the analysis of the agent’s responses and associated features (summarized in the top part of table 1). as typically implemented, the virtual agent’s task is to reliably detect the intent of each customer’s utterance and respond meaningfully. accurate intent detection is thus a fundamental characteristic of well-trained virtual agents, and incorrect intent analysis is reported as the leading cause of user dissatisfaction (sarikaya, 2017). moreover, since a classifier (e.g., svm, neural network, etc.) is often used to detect intents, its probabilistic behavior can cause the agent to repeat the same (or semantically similar) response over and over again, despite the user’s attempt to rephrase the same intent. such agent repetitions lead to an unnatural interaction (kluwer, 2011). to identify the agent’s repeating responses, we measured similarity between agent’s subsequent (not necessarily sequential) turns. we represented each sentence by averaging the pre-trained embeddings5 of each word in the sentence, calculating the cosine similarity between the representations. turns with a high similarity value6 are considered as repeating responses. 5url 6empirically, similarity values given that the knowledge of a virtual agent is necessarily limited, we can expect that training would not cover all customer intents. if the classifier technology provides an estimate of classification confidence, the agent can respond with some variant of i’m not trained on that when confidence is low. in some cases, customers will accept that not all requests are supported. in other cases, unsupported intents can lead to customer dissatisfaction (sarikaya, 2017), and cascade to an egregious conversation (as discussed below in section 3.3). we extracted the possible variants of the unsupported intent messages directly from the system, and later matched them with the agent responses from the logs. from the customer’s point of view, an ineffective interaction with a virtual agent is clearly undesirable. an ineffective interaction requires the expenditure of relatively large effort from the customer with little return on the investment these efforts can appear as behavioral cues in the customer’s inputs, and include emotions, repetitions, and more. we used the following customer analysis in our model. customer features are summarized in the middle part of table when a customer repeats or rephrases an utterance, it usually indicates a problem with the agent’s understanding of the customer’s intent. this can be caused by different reasons as described in to measure the similarity between subsequent customer turns to detect repetition or rephrasing, we used the same approach as described in section turns with a high similarity value6 are considered as rephrases. the customer’s emotional state during the conversation is known to correlate with the conversation’s quality (oliver, 2014). in order to analyze the emotions that customers exhibit in each turn, we utilized the ibm tone analyzer service, available publicly online7. 7url this service was trained using customer care interactions, and infers emotions such as frustration, sadness, happiness. we focused on negative emotions (denoted as neg emo) to identify turns with a negative emotional peak (i.e., single utterances that carried high negative emotional state), as well as to estimate the aggregated negative emotion throughout the conversation (i.e., the averaged negative emotion intensity). in order to get a more robust representation of the customer’s negative emotional state, we summed the score of the negative emotions (such as frustration, sadness, anger, etc.) into a single negative sentiment score (denoted as neg sent). note that we used the positive emotions as a filter for other customer features, such as the rephrasing analysis. usually, high positive emotions capture different styles of thanking the agent, or indicate that the customer is somewhat satisfied (rychalski and hudson, 2017), thus, the conversation is less likely to become egregious. in examining the conversation logs, we noticed that it is not unusual to find a customer asking to be transferred to a human agent. such a request might indicate that the virtual agent is not providing a satisfactory service. moreover, even if there are human agents, they might not be available at all times, and thus, a rejection of such a request is sometimes reasonable, but might still lead to customer frustration (amsel, 1992). in addition to the above analyses, we also detected customer turns that contain exactly one word. the assumption is that single word (unigram) sentences are probably short customer responses (e.g., no, yes, thanks, okay), which in most cases do not contribute to the egregiousness of the conversation. hence, calculating the percentage of those turns out of the whole conversation gives us another measurable feature. we also looked at features across conversation utterance-response pairs in order to capture a more complete picture of the interac- tion between the customer and the virtual agent. here, we considered a pair to be customer utterance followed by an agent response. for example, a pair may contain a turn in which the customer expressed negative emotions and received a response of not trained by the agent. in this case, we would leverage the two analyses: emotional and unsupported intent. figure gives an example of this in the customer’s penultimate turn. such interactions may divert the conversation towards becoming egregious. these features are summarized in the last part of table we also calculated the similarity between the customer’s turn and the virtual agent’s response in cases of customer rephrasing. this analysis aims to capture the reason for the customer rephrasing. when a similarity score between the customer’s turn and the agent’s response is low, this may indicate a misclassified intent, as the agent’s responses are likely to share some textual similarity to the customer’s utterance. thus, a low score may indicate a poor interaction, which might lead the conversation to become egregious. another similarity feature is between two customer’s subsequent turns when the agent’s response was not trained. we trained a binary svm classifier with a linear kernel. a feature vector for a sample in the training data is generated using the scores calculated for the described features, where each feature value is a number between 0,1. after the model was trained, test conversations are classified by the model, after being transformed to a feature vector in the same way a training sample is transformed. the svm classification model (denoted egr) outputs a label egregious or non-egregious as a prediction for the conversation.", "section_index": 3}, {"content": "we extracted data from two commercial systems that provide customer support via conversational bots (hereafter denoted as company a and company b). both agents are using similar underlying conversation engines, each embedded in a larger system with its own unique business logic. company a’s system deals with sales support during an online purchase, while company b’s system deals with technical support for purchased software products. each system logs conversations, and each conversation is a sequence of tuples, where each tuple consists of conversation id, turn id, customer input, agent response. from each system, we randomly extracted conversations. we further removed conversations that contained fewer than turns, as these are too short to be meaningful since the customer never replied or provided more details about the issue at hand. figure depicts the frequencies of conversation lengths which follow a power-law relationship. the conversations from company a’s system tend to be longer, with an average of turns vs. an average of turns for company b. the first step in building a classification model is to obtain ground truth data. for this purpose, we randomly sampled conversations from our datasets. this sample included and conversations for company a and company b respectively. the sampled conversations were tagged using an in-house tagging system designed to increase the consistency of human judgements. each conversation was tagged by four different expert judges8. given the full conversation, each judge tagged whether the conversation was egregious or not following this guideline: conversations which are extraordinarily bad in some way, those conversations where you’d like to see a human jump in and save the conversation. we generated true binary labels by considering a conversation to be egregious if at least three of the four judges agreed. the interrater reliability between all judges, measured by cohen’s kappa, was which indicates high level agreement. this process generated the egregious class sizes of (8.6) and (8) for company a and company b, respectively. this verifies the unbalanced data expectation as previously discussed. we also implemented two baseline models, rule-based and text-based, as follows: rule-based. in this approach, we look for cases in which the virtual agent responded with a not trained reply, or occurrences of the customer requesting to talk to a human agent. as discussed earlier, these may be indicative of the customer’s dissatisfaction with the nature of the virtual agent’s responses. a model that was trained to predict egregiousness given the conversation’s text (all customer and agent’s text dur- 8judges that are hci experts and have experience in designing conversational agents systems. this model was implemented using state-of-the-art textual features as in in emotions are detected from text, which can be thought of as similar to our task of predicting egregious conversations. we evaluated these baseline methods against our classifier using 10-fold crossvalidation over company a’s dataset (we did not use company b’s data for training due to the low number of tagged conversations). since class distribution is unbalanced, we evaluated classification performance by using precision (p), recall (r) and f1-score (f) for each class. the egr classifier was implemented using an svm with a linear kernel9. table depicts the classification results for both classes and the three models we explored. the egr model significantly outperformed both baselines10. specifically, for the egregious class, the precision obtained by the text-based and egr models were similar. this indicates that the text analyzed by both models encodes some information about egregiousness. on the other hand, for the recall and hence the f1-score, the egr model relatively improved the text-based model by and 18, respectively. we will further analyze the models below. to better understand the contributions of different sets of features to our egr model, we examined various features in an incremental fashion. based on the groups of feature sets that we defined in section 3, we tested the performance of different group combinations, added in the following order: agent, customer and customer-agent interactions. 9url 10egr with p 0.001, using mcnemar’s test. figure depicts the results for the classification task. the x-axis represents specific combinations of groups, and the y-axis represents the performance obtained. figure shows that adding each group improved performance, which indicates the informative value of each group. the figure also suggests that the most informative group in terms of prediction ability is the customer group. we also studied how robust our features were: if our features generalize well, performance should not drop much when testing company b with the classifier trained exclusively on the data from company a. although company a and company b share similar conversation engine platforms, they are completely different in terms of objectives, domain, terminology, etc. for this task, we utilized the annotated conversations of company b as test data, and experimented with the different models, trained on company a’s data. the rule-based baseline does not require training, of course, and could be applied directly. table summarizes the results showing that the performance of the egr model is relatively stable (w.r.t the model’s performance when it was trained and tested on the same domain), with a degradation of only in f1-score11. in addition, the results also show that the text-based model performs poorly when applied to a different domain (f1-score of 0.11). this may occur since textual features are closely tied to the training domain. 11egr model results are statistically significant compared to the baselines models with p 0.001, using mcnemar’s test. inspired by we analyzed the customer rephrasing motivations for both the egregious and the non-egregious classes. first, we detected customer rephrasing as described in section 3.2.1, and then assigned to each its motivation. specifically, in our setting, the relevant motivations are12: (1) natural language understanding (nlu) error - the agent’s intent detection is wrong, and thus the agent’s response is semantically far from the customer’s turn; (2) language generation (lg) limitation - the intent is detected correctly, but the customer is not satisfied by the response (for example, the response was too generic); (3) unsupported intent error - the customer’s intent is not supported by the agent. in order to detect nlu errors, we measured the similarity between the first customer turn claiming that the best answer given by the system has the highest similarity value between the customer turn and the agent answer. thus, if the similarity was we considered this as an erroneous detection. if the similarity was we considered the detection as correct, and thus the rephrasing occurred due to lg limitation. to detect unsupported intent error we used the approach described in section as reported in table 4, rephrasing due to an unsupported intent is more common in egregious conversations (18 vs. 14), whereas, rephrasing due to generation limitations (lg limitation) is more common in 12we did not consider other motivations like automatic speech recognition (asr) errors, fallback to search, and backend failure as they are not relevant to our setting. non-egregious conversations (37 vs. 33). this indicates that customers are more tolerant of cases where the system understood their intent, but the response is not exactly what they expected, rather than cases where the system’s response was not trained. finally, the percentage of rephrasing due to wrong intent detection (nlu errors) is similar for both classes, which is somewhat expected as similar underlying systems provided nlu support. we further investigated why the egr model was better at identifying egregious conversations (i.e., its recall was higher compared to the baseline models). we manually examined egregious conversations that were identified justly so by the egr model, but misclassified by the other models. those conversations were particularly prevalent with the agent’s difficulty to identify correctly the user’s intent due to nlu errors or lg limitation. we did not encounter any unsupported intent errors leading to customer rephrasing, which affected the ability of the rule-based model to classify those conversations as egregious. in addition, the customer intents that appeared in those conversations were very diverse. while customer rephrasing was captured by the egr model, for the text-based model some of the intents were new (did not appear in the training data) and thus were difficult for the model to capture.", "section_index": 4}, {"content": "in this paper, we have shown how it is possible to detect egregious conversations using a combination of customer utterances, agent responses, and customer-agent interactional features. as explained, the goal of this work is to give developers of automated agents tools to detect and then solve problems cre- ated by exceptionally bad conversations. in this context, future work includes collecting more data and using neural approaches (e.g., rnn, cnn) for analysis, validating our models on a range of domains beyond the two explored here. we also plan to extend the work to detect egregious conversations in real time (e.g., for escalating to a human operators), and create log analysis tools to analyze the root causes of egregious conversations and suggest possible remedies.", "section_index": 5}], "id": 1006}
{"text": [{"content": "we present an analysis into the inner workings of convolutional neural networks (cnns) for processing text. cnns used for computer vision can be interpreted by projecting filters into image space, but for discrete sequence inputs cnns remain a mystery. we aim to understand the method by which the networks process and classify text. we examine common hypotheses to this problem: that filters, accompanied by global max-pooling, serve as ngram detectors. we show that filters may capture several different semantic classes of ngrams by using different activation patterns, and that global max-pooling induces behavior which separates important ngrams from the rest. finally, we show practical use cases derived from our findings in the form of model interpretability (explaining a trained model by deriving a concrete identity for each filter, bridging the gap between visualization tools in vision tasks and nlp) and prediction interpretability (explaining predictions).", "section_index": 0}, {"content": "convolutional neural networks as well as other traditional natural language processing , even when considering relatively simple one-layer models (kim, 2014). as with other architectures of neural networks, explaining the learned functionality of cnns is still an active research area. the ability to interpret neural models can be used to increase trust in model predictions, analyze errors or improve the model the problem of interpretability in machine learning can be divided into two concrete tasks: given a trained model, model interpretability aims to supply a structured explanation which captures what the model has learned. given a trained model and a single example, prediction interpretability aims to explain how the model arrived at its prediction. these can be further divided into white-box and black-box techniques. while recent works have begun to supply the means of interpreting predictions , interpreting neural nlp models remains an under-explored area. accompanying their rising popularity, cnns have seen multiple advances in interpretability when used for computer vision tasks , which is likely different than the role it has when processing text. in this work, we examine and attempt to understand how cnns process text, and then use this information for the more practical goals of improving model-level and prediction-level explanations. we identify and refine current intuitions as to how cnns work. specifically, current common wisdom suggests that cnns classify text by working through the following steps (goldberg, 2016): 1) 1-dimensional convolving filters are used as ngram detectors, each filter specializing in a closely-related family of ngrams. 2) max-pooling over time extracts the relevant ngrams for making a decision. 3) the rest of the network classifies the text based on this information. we refine items and and show that: max-pooling induces a thresholding behavior, and values below a given threshold are ignored when (i.e. irrelevant to) making a prediction. specifically, we show an experiment for which of the pooled ngrams on average can be dropped with no loss of performance (section 4). filters are not homogeneous, i.e. a single filter can, and often does, detect multiple distinctly different families of ngrams (section 5.3). filters also detect negative items in ngrams they not only select for a family of ngrams but often actively suppress a related family of negated ngrams (section 5.4). we also show that the filters are trained to work with naturally-occurring ngrams, and can be easily misled (made to produce values substantially larger than their expected range) by selected nonnatural ngrams. these findings can be used for improving model-level and prediction-level interpretability (section 6). concretely: 1) we improve model interpretability by deriving a useful summary for each filter, highlighting the kinds of structures it is sensitive to. 2) we improve prediction interpretability by focusing on informative ngrams and taking into account also the negative cues.", "section_index": 1}, {"content": "we focus on the task of text classification. we consider the common architecture in which each word in a document is represented as an embedding vector, a single convolutional layer with m filters is applied, producing an m-dimensional vector for each document ngram. the vectors are combined using max-pooling followed by a relu activation. the result is then passed to a linear layer for the final classification. for an n-words input text w1, ..., wn we embed each symbol as d dimensional vector, resulting in word vectors w1, ...,wn rd. the resulting dn matrix is then fed into a convolutional layer where we pass a sliding window over the text. for each l-words ngram: ui wi, ...,wi1 rd ; i n and for each filter fj rd we calculate ui, fj. the convolution results in matrix f rnm. applying max-pooling across the ngram dimension results in p rm which is fed into relu non-linearity. finally, a linear fullyconnected layer w rcm produces the distribution over classification classes from which the strongest class is outputted. formally: ui wi; ...;wi1 fij ui, fj pj relu(max i fij) o softmax(wp) in practice, we use multiple window sizes l, l ( n by using multiple convolution layers in parallel and concatenating the resulting p vectors. we note that the methods in this work are applicable for dilated convolutions as well.", "section_index": 2}, {"content": "for our empirical experiments and results presented in this work we use three text classification datasets for sentiment analysis, which involves classifying the input text (user reviews in all cases) between positive and negative. the specific datasets were chosen for their relative variety in size and domain as well as for the relative simplicity and interpretability of the binary sentiment analysis task. the three datasets are: a) mr: sentence polarity dataset v1.0 introduced by pang and lee , containing 200k train and 25k test evenly split reviews. c) yelp review polarity: introduced by zhang et al. (2015) from the yelp dataset challenge 2015, containing 560k train and 38k test evenly split business reviews. for word embeddings, we use the pre-trained glove wikipedia 2014gigaword embeddings , which we fine-tune with the model. we use embedding dimension of 50, filter sizes of 2, 3, words, and m 10, filters. models are implemented in pytorch and trained with the adam optimizer.", "section_index": 3}, {"content": "current common wisdom posits that filters serve as ngram detectors: each filter searches for a specific class of ngrams, which it marks by assigning them high scores. these highest-scoring detected ngrams survive the max-pooling operation. the final decision is then based on the set of ngrams in the max-pooled vector (represented by the set of corresponding filters). intuitively, ngrams which any filter scores highly (relative to how it scores other ngrams) are ngrams which are highly relevant for the classification of the text. in this section we refine this view by attempting to answer the questions: what information about ngrams is captured in the max-pooled vector, and how is it used for the final classification?1 consider the pooled vector p rm on which the classification is based. each value pj relu(maxiui, fj) stems from a filter-ngram interaction, and can be traced back to the ngram ui wi, ...,wi1 that triggered it. denote the set of ngrams contributing to p as sp. ngrams not in sp do not influence the decision of the classifier. but what about the ngrams that are in sp? previous attempts in prediction-based interpretation of cnns for text highlight the ngrams in sp and their scores as means of explaining the prediction. we take here a more refined view. note that the final classification does not observe the ngram identities directly, but only through the scores assigned to them by the filters. hence, the information in p must rely on the assigned scores. conceptually, we separate ngrams in sp into two classes, deliberate and accidental. deliberate ngrams end up in sp because they were scored high by their filter, likely because they are informative regarding the final decision. in contrast, accidental ngrams end up in sp despite having a low score, because no other ngram scored higher than them. these ngrams are likely not informative for the classification decision. can we tease apart the deliberate and accidental ngrams? 1although this work focuses on text classification, the findings in this section apply to any neural architecture which utilizes global max pooling, for both discrete and continuous domains. to our knowledge this is the first work that examines the assumption that max-pooling induces classifying behavior. previously, ruderman et al. (2018) showed that other assumptions to the functionality of max-pooling as deformation stabilizers (relevant only in continuous domains) do not necessarily hold true. we assume that there is threshold for each filter, where values above the threshold signal informative information regarding the classification, while values below the threshold are uninformative and can be ignored for the purpose of classification. we thus search for the threshold that separate the two classes. however, as we cannot measure directly which values pj influence the final decision, we opt instead for measuring correlation between pj values and the predicted label for the vector p. the linearity of the decision function wp allows to measure exactly how much pj is weighted for the logit of label class k. the class which filter fj contributes to is cj argmaxk wkj we refer to class cj as the class identity of filter fj by assigning each filter a class identity cj and comparing it to the predicted label we arrive at a correlation labelwhether the filter’s identity class matches the final decision by the network. concretely, we run the classifier over a set of texts, resulting in pooled vectors pi and network predictions ci. for each filter j we then consider the values pij and whether c i cj for each filter, we obtain a dataset (p1j , c cj), ..., (p d j , c d cj), and we look for a threshold tj that separates pij for which ci cj from those where ci cj (x,y )j (pij , ci cj) j m i d in an ideal case, the set is linearly separable and we can easily separate informative from uninformative values: if pij tj then the classifier’s prediction agrees with the filter’s label, and otherwise they disagree. in practice, the set is not separable. we instead work with the purity of a filter-threshold combination, defined as the percentage of informative (correlative) ngrams which were scored above the threshold3. formally, given threshold dataset (x,y ): purity(f, t) (x, y) (x,y )f x t y true (x, y) (x,y )f x t we heuristically set the threshold of a filter to the lowest value that achieves a sufficiently high 2in the case of non-linear fully-connected layers, the question of how each feature contributes to each class is significantly harder to answer. possible methods include saliency map methods or gradient-based methods. (2018) has attributed labels to filters using bayesian inference and other image annotations. 3the purity metric can be considered as the precision metric for this task. purity (we experimentally find that a purity value of works well). in figure 2b,c we show examples for threshold datasets for a model trained on the mr sentiment analysis task. threshold effectiveness we described a method for obtaining per-filter threshold values. but is the threshold assumptionthat items below a given threshold do not participate in the decisioneven correct? to assess the quality of threshold obtained by our proposal and validate the thresholding assumption, we discard values that do not pass the threshold for each filter and observe the performance of the model. practically, we replace the relu non-linearity with a threshold function: threshold(x, t) x, if x t 0, otherwise figure presents the results on the mr dataset (we observed similar results on the elec dataset). where the threshold is set for each filter separately, based on a shared purity value. if the thresholding assumption is correct and our way of deriving the threshold is effective, we expect to not see a drop in accuracy. indeed, for purity value of 0.75, we observe that the model performance improves slightly when replacing the relu with a per-filter threshold, indicating that the thresholding model is indeed a good approximation for the feature behavior. the percentage of informative (non-accidental) values in p is roughly a linear function of the purity (figure 1c). with a purity value of 0.754, we discard roughly of the values in pand hence of the ngrams in sp. not all filters behave in a similar way, however. in figure we show an example for a filter6 in the figurewhich is especially uninformative: by applying the lowest threshold which satisfies a purity of 0.75, we discard of activations. therefore in the experiments in figure 1, this filter is effectively unused, yet it does not cause loss in performance. in essence, the threshold classifier 4we note that empirically and intuitively, the more filters we utilize in the network, the less correlation there is between each filter’s class and the final classification, as the decision is being made by a greater consensus. this means that demanding a higher purity will be accompanied by lower coverage, relative to other experiments, and more ngrams will be discarded. the correct purity level for a filter then is a function of the model and dataset used, and should be investigated using the train or validation datasets. identified and effectively discarded a filter which is not useful to the model. to summarize, we validated our assumptions and shown empirically that global max-pooling indeed induces a functionality of separating important and not important activation signals using a latent (presumably soft) threshold. for the rest of this work we will assume a known threshold value for every filter in the model which we can use to identify important ngrams.", "section_index": 4}, {"content": "previous work looked at the top-k scoring ngrams for each filter. however, focusing on the top-k does not tell a complete story. we insead look at the set of deliberate ngrams: those that pass the filter’s threshold value. common intuition suggests that each filter is homogeneous and specializes in detecting a specific classes of ngrams. for example, a filter may specializing in detecting ngrams such as had no issues, had zero issues, and had no problems. we challenge this view and show that filters often specialize in multiple distinctly different semantic classes by utilizing activation patterns which are not necessarily maximized. we also show that filters may not only identify good ngrams, but may also actively supress bad ones. as discussed in section 2, for each ngram u w1, ...,w and for each filter f we calculate the score u, f. the ngram score can be decomposed as a sum of individual word scores by considering the inner products between every word embedding wi in u and every parallel slice in f : u, f i0 wi, fid:i(d1) we refer to slice fid:i(d1) as slot i of the filter weights, denoted as f(i). instead of taking the sum of these inner products, we can instead interpret them directlysaying that wi, f(i) captures how much slot i in f is activated by the ith word in the ngram5. we can now move from examining the activation of an ngram-filter pair u : w1; ...;w, f to examining its slot activation vector: (w1, f(1), ..., w, f()). the slot ac- tivation vector captures how much each word in the ngram contributes to its activation. we distinguish naturally occurring or observed ngrams, which are ngrams that are observed in a large corpus, from possible ngrams which are any combination of words from the vocabulary. the possible ngrams are a superset of the naturally occurring ones. given a filter, we can find its topscoring naturally occurring ngram by searching over all ngrams in a corpus. we can find its topscoring possible ngram by maximizing each slot value individually. we observe there is a big and consistent gap in scores between the top-scoring natural ngrams and top-scoring possible ngrams. in our elec model, when averaging over all filters, the top naturally-occurring ngrams score less than the top possible ngrams. interestingly, the we note that this breakdown does not consider the filter’s bias, if one is used. this bias is a single number (per filter) which is added to the sum of slot activations to arrive at the ngram activation which is passed to the max-pooling layer. bias can be accommodated by appending an additional bias word with an embedding vector of 1, ..., to every ngram. regardless, as this bias is identical for all ngrams for the filter in question, it has no role in identifying which ngrams the filter is most similar to, and we can ignore it in this context. top-scoring natural ngrams almost never fully activate all slots in a filter. table shows the top-scoring naturally occurring and possible ngrams for nine filters in the elec model. in each of the top scoring natural ngrams, at least one slot receives a low activation. table zooms in on one of the filters and shows its top7 naturally occurring ngrams and top-7 most activated words in each slot. here, most top-scoring ngrams maximize slot with words such as invaluable and perfect, however some ngrams such as works as good and still holding strong maximize slots and respectively, instead. additionally, most top-scoring words do not appear to be utilized in high-scoring ngrams at all. this can be explained with the following: if a word such as crt rarely or never appears in slot alongside other high-scoring words in other slots, then crt can score highly with no consequence. since an ngram containing crt at slot will rarely pass the max-pooling layer, its score at that slot is essentially random. on naturally occurring ngrams, the filters do not achieve maximum values in all slots but only on some of them. we consider two hypotheses to explain this behavior: (i) each filter captures multiple semantic classes of ngrams, and each class has some dominating slots and some non-dominating slots (which we define as a slot activation pattern). (ii) a slot may not be maximized because it’s not used to detect word existence, but rather lack of existenceensuring that specific words do not occur. we investigate both hypotheses in sections and respectively. adversarial potential we note in passing that this discrepancy in scores between naturally occurring and possible ngrams can be used to derive adversarial examples that cause a trained model to misclassify. by inserting a few seemingly random ngrams, we can cause filters to activate beyond their expected range, potentially driving the model to misclassification. we reserve this area of exploration for future work. we explore hypothesis (i) by clustering thresholdpassing (naturally occurring) ngrams in each filter according to their activation vectors. we use mean shift clustering (fukunaga and hostetler, 1975; cheng, 1995), an algorithm that does not require specifying an a-priori number of clusters, and does not make assumptions about their shapes. mean shift considers the feature vectors as sampled from an underlying probability density function6. each cluster captures a different slot activation pattern. we use the cluster’s centroid as the prototypical slot activation for that cluster. table shows a sample clustering output. the clustering algorithm identified two clusters: one primarily containing ngrams of the pattern det intensity-adverb positive-word, while the second contains ngrams that begin with phrases like go wrong.7 the centroids for these clusters capture the activation patterns well: low-medium-high and highhigh-low for clusters and respectively. to summarize, by discarding noisy ngrams which do not pass the filter’s threshold and then clustering those that remain according to their slot activation patterns, we arrived at a clearer image 6intuitively, we can think of the sampling noise as the ngram embeddings, and the probability distribution as defined by a function of the filter weights. 7in the yelp dataset, go wrong overwhelmingly occurs in a negated context such as can’t go wrong and won’t go wrong, which explains why it is detected by a positive filter. of the semantic classes of ngrams that a given filter specializes in capturing. in particular, we reveal that filters are not necessarily homogeneous: a single filter may detect several different semantic patterns, each one of them relying on a different slot activation pattern. our second theory to explain the discrepancy between the activations of naturally occurring and possible ngrams is that certain filter slots are not used to detect a class of highly activating words, but rather to rule out a class of highly negative words. we refer to these as negative ngrams. for example, table shows an ngram pattern for which slot contains determiners and other filler tokens such as hyphens, periods and commas with relatively weak slot activations. hypothesis (ii) suggests that this slot may receive a strong negative score for words such as not and n’t, causing such negated patterns to drop below the threshold. indeed, ngrams containing not or n’t in slot do not pass the threshold for this filter. we are interested in a more systematic method of identifying these cases. identifying negative slot activations would be very useful for understanding the semantics captured by a filter and the reasoning behind the dismissal of an ngram, as we discuss in sections and respectively. we achieve this by searching the belowthreshold ngram space for ngrams which are flipped versions of above-threshold ngrams. concretely: given ngram u which was scored highly by filter f , we search for low-scoring ngrams u such that the hamming distance between u and u is low. by doing this for the topk scoring ngrams per cluster, we arrive at a comprehensive set of negative ngrams. in table we show a sample output of this algorithm. furthermore, we can divide negative ngrams into two cases: 1) lowering the ngram score below the threshold by replacing high-scoring words with low-scoring words. 2) lowering the ngram score below the threshold by replacing words with a low positive score with words with a highly-negative score. case is more interesting because it embodies cases where hypothesis (ii) is relevant. additionally, it highlights ngrams where a strongly positive word in one slot was negated with another strongly negative word in another slot. table shows examples in bold. in order to identify case negative ngrams, we heuristically test whether the changed words’ scores directly influence the status of the activation relative to the threshold: given an already identified negative ngram, if the ngram scoresans the bottom-k negative slot activations (considering a hamming distance of k and given that there are k negative slot activations)passes the threshold, yet it does not pass the threshold by including the negative slot activations, then the ngram is considered a case negative ngram.", "section_index": 5}, {"content": "in this section we show two practical implications of the findings above: improvements in both model-level and prediction-level interpretability of 1d cnns for text classification. as in computer vision, we can now interpret a trained cnn model by visualizing its filters and interpreting the visible shapesin other words, defining a high-level description of what the filter detects. we propose to associate each filter with the following items: 1) the class which this filter’s strong signals contribute to (in the sentiment task: positive or negative); 2) the threshold value for the filter, together with its purity and coverages percentages (which essentially capture how informative this filter is); 3) a list of semantic patterns identified by this filter. each list item corresponds to a slot-activations cluster. for each cluster we present the top-k ngrams activating it, and for each ngram we specify its total activation, its slot-activation vector, and its list of bottom-k negative ngrams with their activations and slot activations. in particular, by clustering the activated ngrams according to their slot activation patterns and showing the top-k in each clusters, we get a much more refined coverage of the linguistic patterns that are captured by the filter. previous prediction-based interpretation attempts traced back the ngrams from the max-pooling layer. here we improve these previous attempts by considering only ngrams that pass the threshold for their filter. this results in a more concise and relevant explanation (figure 1). figure shows two examples. note that in example 1, many negative-class filters were forced to choose an ngram in max-pooling despite there not being strongly negative phrasesbut those ngrams do not pass the threshold and are thus cleaned from the explanation. additionally we can use the individual slot activations to tease-apart the contribution of each word in the ngram. finally, we can also mark cases of negative-ngrams (section 5.4), where an ngram has high slot activations for some words, but these are negated by a highly-negative slot and as a consequence are not selected by max-pooling, or are selected but do not pass the filter’s threshold.", "section_index": 6}, {"content": "we have refined several common wisdom assumptions regarding the way in which cnns process and classify text. first, we have shown that maxpooling over time induces a thresholding behavior on the convolution layer’s output, essentially separating between features that are relevant to the final classification and features that are not. we used this information to identify which ngrams are important to the classification. we also associate each filter with the class it contributes to. we decompose the ngram score into word-level scores by treating the convolution of a filter as a sum of word-level convolutions, allowing us to examine the word-level composition of the activation. specifically, by maximizing the word-level activations by iterating over the vocabulary, we observed that filters do not maximize activations at the word-level, but instead form slot activation patterns that give different types of ngrams similar activation strengths. this provides empirical evidence that filters are not homogeneous. by clustering high-scoring ngrams according to their slotactivation patterns we can identify the groups of linguistic patterns captured by a filter. we also show that filters sometimes opt to assign negative values to certain word activations in order to cause the ngrams which contain them to receive a low score despite having otherwise highly activating words. finally, we use these findings to suggest improvements to model-based and predictionbased interpretability of cnns for text.", "section_index": 7}], "id": 1007}
{"text": [{"content": "we suggest a new idea of editorial network a mixed extractive-abstractive summarization approach, which is applied as a postprocessing step over a given sequence of extracted sentences. we further suggest an effective way for training the editor based on a novel soft-labeling approach. using the cnn/dailymail dataset we demonstrate the effectiveness of our approach compared to state-of-the-art extractive-only or abstractiveonly baselines.", "section_index": 0}, {"content": "automatic text summarizers condense a given piece of text into a shorter version (the summary). this is done while trying to preserve the main essence of the original text and keeping the generated summary as readable as possible. existing summarization methods can be classified into two main types, either extractive or abstractive. extractive methods select and order text fragments (e.g., sentences) from the original text source. such methods are relatively simpler to develop and keep the extracted fragments untouched, allowing to preserve important parts, e.g., keyphrases, facts, opinions, etc. yet, extractive summaries tend to be less fluent, coherent and readable and may include superfluous text. abstractive methods apply natural language paraphrasing and/or compression on a given text. a common approach is based on the encoder-decoder , with the original text sequence being encoded while the summary is the decoded sequence. work was done during a summer internship in ibm research ai while such methods usually generate summaries with better readability, their quality declines over longer textual inputs, which may lead to a higher redundancy moreover, such methods are sensitive to vocabulary size, making them more difficult to train and generalize a common approach for handling long text sequences in abstractive settings is through attention mechanisms, which aim to imitate the attentive reading behaviour of humans two main types of attention methods may be utilized, either soft or hard. soft attention methods first locate salient text regions within the input text and then bias the abstraction process to prefer such regions during decoding on the other hand, hard attention methods perform abstraction only on text regions that were initially selected by some extraction process compared to previous works, whose final summary is either entirely extracted or generated using an abstractive process, in this work, we suggest a new idea of editorial network (editnet) a mixed extractive-abstractive summarization approach. a summary generated by editnet may include sentences that were either extracted, abstracted or of both types. moreover, per considered sentence, editnet may decide not to take either of these decisions and completely reject the sentence. using the cnn/dailymail dataset we demonstrate that, editnet’s summarization quality is highly competitive to that obtained by both state-of-the-art abstractive-only and extractive-only baselines.", "section_index": 1}, {"content": "figure depicts the architecture of editnet. editnet is applied as a post-processing step over a given input summary whose sentences were initially selected by some extractor. the key idea behind editnet is to create an automatic editing process to enhance summary quality. let s denote a summary which was extracted from a given text (document) d. the editorial process is implemented by iterating over sentences in s according to the selection order of the extractor. for each sentence in s, the editor may make three possible decisions. the first decision is to keep the extracted sentence untouched (represented by label e in figure 1). the second alternative is to rephrase the sentence (represented by label a in figure 1). such a decision, for example, may represent the editor’s wish to simplify or compress the original source sentence. the last possible decision is to completely reject the sentence (represented by label r in figure 1). for example, the editor may wish to ignore a superfluous or duplicate information expressed in the current sentence. an example mixed summary generated by our approach is depicted in figure in the appendix, further emphasizing the various editor’s decisions. for a given sentence s d, we now denote by se and sa its original (extracted) and paraphrased (abstracted) versions. to obtain sa we use an abstractor, whose details will be shortly explained (see section 2.2). let es rn and as rn further denote the corresponding sentence representations of se and sa, respectively. such representations allow to compare both sentence versions on the same grounds. recall that, for each sentence si s (in order) the editor makes one of the three possible decisions: extract, abstract or reject si. therefore, the editor may modify summary s by paraphrasing or rejecting some of its sentences, resulting in a mixed extractive-abstractive summary s. let l be the number of sentences in s. in each step i 1, 2, , l, in order to make an educated decision, the editor considers both sentence representations esi and asi as its input, together with two additional auxiliary representations. the first auxiliary representation is that of the whole document d itself, hereinafter denoted d rn. such a representation provides a global context for decision making. assuming document d has n sentences, let e 1n n sd es. following (chen and bansal, 2018; wu and hu, 2018a), d is then calculated as follows: d tanh (wde bd) , where wd rnn and bd rn are learnable parameters. the second auxiliary representation is that of the summary that was generated by the editor so far, denoted at step i as gi1 rn, with g0 such a representation provides a local context for decision making. given the four representations as an input, the editor’s decision for sentence si s is implemented using two fully-connected layers, as follows: softmax (v tanh (wcesi , asi , gi1, d bc) b) , (1) where denotes the vectors concatenation, v r3m, wc rm4n, bc rm and b r3 are learnable parameters. in each step i, therefore, the editor chooses the action i e,a,r with the highest likelihood (according to eq. upon decision, in case it is either e or a, the editor appends the corresponding sentence version (i.e., either sei or s a i ) to s ; otherwise, the decision is r and sentence si is discarded. depending on its decision, the current summary representation is further updated as follows: gi gi1 tanh (wghi) , (2) where wg rnn are learnable parameters, gi1 is the summary representation from the previous decision step; and hi esi , asi ,0, depending on which decision is made. such a network architecture allows to capture various complex interactions between the different inputs. for example, the network may learn that given the global context, one of the sentence versions may allow to produce a summary with a better coverage. as another example, based on the interaction between both sentence versions with either of the local or global contexts (and possibly among the last two), the network may learn that both sentence versions may only add superfluous or redundant information to the summary, and therefore, decide to reject both. as a proof of concept, in this work, we utilize the extractor and abstractor that were previously used in (chen and bansal, 2018), with a slight modification to the latter, motivated by its specific usage within our approach. we now only highlight important aspects of these two sub-components and kindly refer the reader to (chen and bansal, 2018) for the full implementation details. the extractor of (chen and bansal, 2018) consists of two main sub-components. the first is an encoder which encodes each sentence s d into es using an hierarchical representation1. the second is a sentence selector using a pointernetwork for the latter, let p (s) be the selection likelihood of sentence s. the abstractor of yet, instead of applying it directly only on a single given extracted sentence sei s, we apply it on a chunk of three consecutive sentences2 (se, s e i , s e ), where s e and s e denote the sentence that precedes and succeeds sei in d, respectively. this in turn, allows to generate an abstractive version of sei (i.e., s a i ) that benefits from a wider local context. inspired by previous softattention methods, we further utilize the extractor’s sentence selection likelihoods p () for enhancing the abstractor’s attention mechanism, as follows. letc(wj) denote the abstractor’s original attention value of a given word wj occurring in (se, s e i , s e ); we then recalculate this value to be c (wj) c(wj)p (s) z , with wj s and s s e , s e i , s e ; z sse,sei ,se wjs c(wj)p (s ) denotes the normalization term. recall that, in order to compare sei with s a i , we need to represent both sentence versions on as similar grounds as possible. to achieve that, we first replace sei with s a i within the original document d. by doing so, we basically treat sentence sai as if it was an ordinary sentence within d, where the rest of the document remains untouched. we then obtain sai ’s representation by encoding it using the extractor’s encoder in a similar way in which sentence sei was originally supposed to be encoded. this results in a representation asi that provides a comparable alternative to esi , whose encoding is expected to be effected by similar contextual grounds. we conclude this section with the description of how we train the editor using a novel soft labeling approach. given text s (with l extracted sentences), let (1, , l) denote its editing decisions 1such a representation is basically a combination of a temporal convolutional model followed by a bilstm encoder. 2the first and last chunks would only have two consecutive sentences. we define the following soft crossentropy loss: l(s) l sis ie,a,r y(i) log p(i), (3) where, for a given sentence si s, y(i) denotes its soft-label for decision. we next explain how each soft-label y(i) is estimated. to this end, we utilize a given summary quality metric r(s) which can be used to evaluate the quality of any given summary s (e.g., rouge (lin, 2004)). overall, for a given text input s with l sentences, there are 3l possible summaries s to consider. , l ) denote the best decision sequence which results in the summary which maximizes r(). , i1, i) denote the average r() value obtained by decision sequences that start with the prefix (1, based on , the soft label y(i) is then calculated3 as follows: y(i) r(1,", "section_index": 2}, {"content": "we trained, validated and tested our approach using the non-annonymized version of the cnn/dailymail dataset following , we used the story highlights associated with each article as its ground truth summary. we further used the f-measure versions of rouge-1, rouge-2 and rougel as our evaluation metrics (lin, 2004). the extractor and abstractor were trained similarly to , we set the reward metric to be r() r-1() r-2() r-l(); with 0.4, and 0.5, which were further suggested by (wu and hu, 2018a). we further applied the teacher-forcing approach during training, where we considered the true-label instead of the 3for i we have: r(1 , editor’s decision (including when updating gi at each step i according to eq. following (chen and bansal, 2018), we set m and n we trained for epochs, which has taken about hours on a single gpu. we chose the best model over the validation set for testing. finally, all components were implemented in python using the pytorch package. table compares the quality of editnet with that of several state-of-the-art extractive-only or abstractive-only baselines. this includes the extractor (rnn-ext-rl) and abstractor (rnn-ext-absrl) components of (chen and bansal, 2018) that we utilized for implementing editnet we further report the quality of editnet when it was being enforced to take an extract-only or abstract-only decision, denoted hereinafter as editnete and editneta, respectively. the comparison of editnet to both editnete and editneta variants provides a strong empirical proof that, by utilizing an hybrid decision approach, a 4the rnn-ext-rl extractor results reported in table are the ones that were reported by (chen and bansal, 2018). training the public extractor released by these authors, we obtained the following significantly lower results: see editnete better summarization quality is obtained. overall, editnet provides a highly competitive summary quality, where it outperforms most baselines. interestingly, editnet’s summarization quality is quite similar to that of neusum yet, while neusum applies an extraction-only approach, summaries generated by editnet include a mixture of sentences that have been either extracted or abstracted. two models outperform editnet, bertsum the bertsum model gains an impressive accuracy, yet it is an extractive model that utilizes many attention layers running in parallel with millions of parameters dca gains a comparable quality to editnet, it outperforms on r-2 and slightly on r-1. the contextual encoder of dca is comprised of several lstm layers one on top of the other with varied number of agents (hyper-tuned) that transmit messages to each other. considering the complexity of these models, and the slow down that can incur during training and inference, we think that editnet still provides a useful, high quality and relatively simple extension on top of standard encoder aligned decoder architectures. on average, and of editnet’s decisions were to abstract (a) or reject (r), respectively. moreover, on average, per summary, editnet keeps only of the original (extracted) sentences, while the rest (67) are abstracted ones. this demonstrates that, editnet has a high capability of utilizing abstraction, while being also able to maintain or reject the original extracted text whenever it is estimated to provide the best benefit for the summary’s quality.", "section_index": 3}, {"content": "we have proposed editnet a novel alternative summarization approach that instead of solely applying extraction or abstraction, mixes both together. moreover, editnet implements a novel sentence rejection decision, allowing to correct initial sentence selection decisions which are predicted to negatively effect summarization quality. as future work, we plan to evaluate other alternative extractor-abstractor configurations and try to train the network end-to-end. we further plan to explore reinforcement learning (rl) as an alternative decision making approach.", "section_index": 4}], "id": 1008}
{"text": [{"content": "phonetic similarity algorithms identify words and phrases with similar pronunciation which are used in many natural language processing tasks. however, existing approaches are designed mainly for indo-european languages and fail to capture the unique properties of chinese pronunciation. in this paper, we propose a high dimensional encoded phonetic similarity algorithm for chinese, dimsim. the encodings are learned from annotated data to separately map initial and final phonemes into n-dimensional coordinates. pinyin phonetic similarities are then calculated by aggregating the similarities of initial, final and tone. dimsim demonstrates a 7.5x improvement on mean reciprocal rank over the state-of-theart phonetic similarity approaches.", "section_index": 0}, {"content": "performing the mental gymnastics of transforming i’m hear’ to i’m here,’ or, i can’t so buttons’ to i can’t sew buttons,’ is familiar to anyone who has encountered autocorrected text messages, punny social media posts, or just friends with bad grammar. although at first glance it may seem that phonetic similarity can only be quantified for audible words, this problem is often present in purely textual spaces, such as social media posts or text messages. incorrect homophones and synophones, whether used in error or in jest, pose challenges for a wide range of nlp tasks, such as named entity identification, text normalization and spelling correction these tasks must therefore successfully transform incorrect words or phrases (hear’,’so’) to their phonetically similar correct counterparts (’here’,’sew’), which in turn requires a robust representation of phonetic similarity between word pairs. a reli- able approach for generating phonetically similar words is equally crucial for chinese text unfortunately, most existing phonetic similarity algorithms such as soundex (archives and administration, 2007) and double metaphone (dm) philips (2000) are motivated by english and designed for indo-european languages. words are encoded to approximate phonetic presentations by ignoring vowels (except foremost ones), which is appropriate where phonetic transcription consists of a sequence of phonemes, such as for english. in contrast, the speech sound of a chinese character is represented by a single syllable in pinyin consisting of two or three parts: an initial (optional), a final or compound finals, and tone (table 1). as a result, phonetic similarity approaches designed for indo-european languages often fall short when applied to chinese text. note that we use pinyin as the phonetic representation because it is a widely accepted romanization system (san, 2007; iso, 2015) of chinese syllables, used to teach pronunciation of standard chinese. table shows two sentences from chinese microblogs, containing informal words derived from phonetic transcription. the dm and soundex encodings for 1chinese has five tones, represented on a 1-5 scale. near-homonyms of from table are shown in table since both dm and soundex ignore vowels and tones, words with dissimilar pronunciations are incorrectly assigned to the same encoding (e.g. and ), while true nearhomonyms are encoded much further apart (e.g. on the other hand, additional candidates with similar phonetic distances such as xin1fan2xi1fang1 for should be generated, for consumption by downstream applications such as text normalization. the example highlights the importance of considering all pinyin components and their characteristics when calculating chinese phonetic similarity one recent work (yao, 2015) manually assigns a single numerical number to encode and derive phonetic similarity. however, this single-encoding approach is inaccurate since the phonetic distances between pinyins are not captured well in a one dimensional space. figure illustrates the similarities between a subset of initials. initial groups z, c, zh, ch, z, zh and zh, ch are all similar, which cannot be captured using a one dimensional representation (e.g., an encoding of zh0,z1,c2,ch3 fails to identify the zh, ch pair as similar.) aline (kondrak, 2003) is another illustration of the challenge of manually assigning numerical values in order to accurately represent the complex relative phonetic similarity relationships across various languages. therefore, given the perceptual nature of the problem of phonetic similarity, it is critical to learn the distances based on as much empirical data as possible (kessler, 2005), rather than using a manually encoded metric. this paper presents dimsim, a learned ndimensional phonetic encoding for chinese along with a phonetic similarity algorithm, which uses the encoding to generate and rank phonetically similar words. to address the complexity of relative phonetic similarities in pinyin components, we propose a supervised learning approach to learn n dimensional encodings for finals and initials where n can be easily extended from one to two or higher dimensions. the learning model derives accurate encodings by jointly considering pinyin linguistic characteristics, such as place of articulation and pronunciation methods, as well as high quality annotated training data sets. we compare dimsim to double metaphone(dm), minimum edit distance(med) and aline demonstrating that dimsim outperforms these algorithms by 7.5x on mean reciprocal rank, 1.4x on precision and 1.5x on recall on a real-world dataset. an encoding for chinese pinyin leveraging chinese pronunciation characteristics. a simple and effective phonetic similarity algorithm to generate and rank phonetically similar chinese words. an implementation and a comprehensive evaluation showing the effectiveness of dimsim over the state-of-the-art algorithms. a package release of the implemented algorithm and a constructed dataset of chinese words with phonetic corrections.2", "section_index": 1}, {"content": "dimsim generates ranked candidate words with similar pronunciation to a seed word. similarity is measured by a phonetic distance metric based on n-dimensional encodings, as introduced below. an important characteristic of pinyin is that the three components, initial, final and tone, can be independently phonetically compared. for example, the phonetic similarity of the finals ie and ue is identical in the pinyin pairs xie2,xue2 and lie2,lue2, in spite of the varying initials. english, by contrast, does not have this characteristic. consider as an example, the letter group ough, which is pronounced quite differently in rough, through and though. note that depending on the initials, a final of same written form can represent different finals. for instance, u is written as u after j, q and x; uo is written as o after b, p,m, f or w. there are a total 2url. of six rewritten rules in pinyin (iso, 2015). since these rules are fixed, we preprocess the pinyins according to these rules, transforming them into the original form for our internal representation (e.g., we represent ju as ju and bo as buo.) dimsim represents a given word w as a list of characters ci1 i k where k is the number of characters and pci denotes the pinyin of ith character. the initial, final, and tone components of pci are denoted as p i ci , p f ci , and p t ci , respectively. formally, the phonetic similarity s between the pronunciation of ci and ci is computed using manhattan distance as the sum of the distances between the three pairs of components, as follows: 1ik s(ci, c i) 1ik sp(pici , pici) sp(p f ci , p f ci ) st (p t ci , p t ci ) (1) manhattan distance is an appropriate metric since the three components are independent. any single change does not affect more than one component, and any change affecting several components is the result of multiple independent and additive changes. it follows that the similarity between two words is computed as the sum of the phonetic distances of characters. for example, the pinyins of and are tong2xie2 and tong2xue2. the distance between (tong2) and (tong2) is zero; the distance between (xie2) and (xue2) is calculated as s( , ) sp(x, x) sp(ie, ue) st (2, 2). although the characters (xie2) and (xue2) are completely different, their pinyins only differ in their finals. the next task is to compute encodings for initials, finals, and tones. while tonal similarity is easily handled (see section 2.4), pairwise similarity for initials and finals is more complex. we adopt a supervised learning approach to obtain these encodings, using linguistic characteristics combined with a labeled dataset. the latter consists of word pairs, with specific pairs of initials or finals manually annotated for phonetic similarity. the set of annotated pairs between initials and finals are then used to learn the n-dimensional encodings of initials and finals, which will in turn be used for generating phonetically similar candidates. confidential review copy. for instance, u is written as u after j, q, x. uo is written as o after b, p, m, f or w. there are a total of six rewritten rules in pinyin (iso, 2015). since these rules are fixed, it is straightforward to preprocess the pinyins according to these rules to turn them into the original form of pinyins as a internal representation before conducting phonetic comparison. for example, we represent ju as ju, bo as buo. after the preprocessing step, we independently compare ompon nts. measuring phonetic similarity dimsim represents a given chinese word w as a list of chinese characters ci1 i k where k is the umber of characters in w and pci denotes the pinyin of ith character. the initial, final, and tone components of the pinyin pci are denoted as pici , p f ci , and p t ci , respectively. formally, the phonetic similarity s between the pronunciation of two characters, ci and c0i is computed using manhattan distance as the sum of the distances between the three pairs of components, as follows: x 1ik s(ci, c i) x 1ik sp(pici , p i c0i ) sp(p f ci , p f c0i ) st (p t ci , p t c0i ) (1) manhattan distance is an appropriate metric since the three components are independent. a single change in a pinyin is therefore a change to the initial, the final, or the tone, but not to more than one of the components simultaneously. a change that affects more than one component is the result of multiple independent and therefore additive changes. following the same logic, the phonetic similarity between two words w and w0 is computed as the sum of the distances between the pinyins. for example, the pinyins of and f are tong2xie2 and tong2xue2 respectively. the distance between (tong2) and (tong2) is zero; the distance between (xie2) and f(xue2) is calculated as s(ie, ue) sp(x, x)sp(ie, ue)st (2, 2). we see that although the characters are completely different, (xie2) and f(xue2) only differ in their fina s, b t not thei initials and tones. learning pinyin encodings therefore, the next task is to generate an accurate representation of phonetic similarity for every pair of initials, finals, and tones. as there are only table 4: table of pinyin initials (colors denote clusters). tones in chinese, pairwise tonal similarity is easily handled (see section 2.4). however, pairwise similarity for initials and finals is more complex and must be learned. we use a supervised machine learning approach that uses pinyin linguistic characteristics combined with manually labeled data sets of phonetic similarity. the training data sets consist of word pairs that highlight a pair of initials (or finals), and are used as the context for an annotator-provided phonetic similarity score. the manually labeled scores are transformed into similarity scores. the set of initials (or finals) is then mapped to the n-dimensional encodings by minimizing the difference between the resulting pairwise distances, and the distances obtained from the training data sets. generating similar word pairs phonetically similar word pairs are used to create annotations representing the phonetic similarity of a pair of initials, or finals. chinese has pairs of initials and pairs of finals. manually annotating each pair similarity requires a very large number of examples: assuming ten or twenty word pairs are provided as context for each pair, the task quickly blows up to nine or eighteen thousand annotations. we observe that the phonetic similarity of chinese pinyin is greatly impacted by the pronunciation methods and the place of articulation. leveraging known pinyin linguistic characteristics can improve the accuracy of our model and reduce the size of the annotation task. specifically, this is done by grouping the pinyin components into initial clusters according to the pinyin pronunciation tables (iso, 2015) and only annotating the pairs within each cluster along with a single pairwise distance between clusters. table shows the the clustering of initials according to the pinyin linguistic characteristics. we partition initials into clusters, consisting phonetically sim lar wor pairs are used t create an otations repres nting the phon tic simil rity of initials, or finals. chinese has pairs of initials and pairs of finals. annotating examples of all these pairs is labor intensive and error-prone. assuming twenty word pairs are provided as context per pair, the task quickly blows up to eighteen thousand annotations. however, we observe that the phonetic similarity of pinyin is greatly impacted by the pronunciation methods and the place of articulation - this allows us to improve the accuracy and simplify the annotation task. specifically, this is done by grouping pinyin components into initial clusters and only annotating pairs within each cluster, and represent tive cluster pairs. figure partitions initials nto clusters, consisti g of bp,dt,gk,hf,nl,r, jqx, zcs, zhchsh, m ,y and w, based on the pronunciatio method and the place of rticulation. f and h are grouped together as they are both fricative and sound very similar, especially for people from the southeast of china (zhishihao, 2017). we then eliminate the comparison of pairs that are highly similar or highly dissimilar. for example, as the semivowel initials y and w are dissimilar to all other initials, we label every initial pair containing one of them with the lowest possible score. to compare between clusters, we randomly choose one initial from each cluster and generate just those comparison pairs. the number of pairs of initials decreases from to we use a similar method for finals, partitioning them into six groups by the six basic vowels ( ,o,e,i,u,u) (e.g., i,in,ing are clustered together.) we t n use ed t distance and common sequence length con traints to g ide the pair generation; specifically, we compare a pair of finals if the edit distance between them is or since the length of finals on average is two, an edit distance of three means a complete change to the final, resulting in pairs with the lowest similarity. to compare finals across clusters, since the edit distance between any such pair is at least two, we compare pairs only when the length of the common sequence is at least two (for example, ian and uan), and otherwise assign the lowest possible similarity to the pairs. this drops the number of comparison pairs of finals down to after generating the comparison pairs, we create word pairs whose pinyins only differ in the these pairs. we identify and account for several confounding factors that may affect annotation: 1) the position of the character containing the initial or final being compared; 2) the word length; and 3) the combination of initials and finals. since most chinese words are of length two, we only generate word pairs of length two for this task. providing word pairs of length greater than two would not make much difference to learned encodings as long as word pairs are representative. for a given initial (or final) pair (p1, p2), such as (b, p), we first generate the all possible pinyins with a component of p1 such as bao and bing. for each pinyin py, we retrieve all the words with length two in the dictionary which also have first or second character with the same py. example words for pybao includebao1fu2. for each created word w, we change the initial (or final) from p1 to p2, retrieve the corresponding words from the dictionary and generate the word pairs to compare. one such example is (bao1fu2, pao4fu2). finally, from the full list we randomly select five word pairs that vary the first character, and five word pairs that vary the second character. we invite three native chinese speakers to perform the annotations. for each word pair, the annotators give a label on a point scale representing their agreement, where the labels range from ’completely disagree’ (1) to ’completely agree’ (7). we calculate krippendorff’s (hayes and krippendorff, 2007) for the initials and finals annotations to be and 0.54, representing the inter-annotator agreement. for each word pair, we use equation to calculate the distance with the average value of labels across the annotators. equation inverts the labels so that the output can be used as a distance metric (phonetically similar initials or finals are closer together), and scales the result to more accurately measure phonetic similarities. the parameters a and b are set and by default, but we also show that the performance of our method is not sensitive to the parameter settings (see section 3.2). once the average distances between pairs are computed from the annotated data sets, we define a constrained optimization to compute encodings of the initials and finals. the final goal is to map each initial (or final) to an n-dimensional point. the distance sp of a pair p of points (x1, x2, ..., xn), (y1, y2, ..., yn) is calculated using euclidean distance as shown in equation sp 1in (xi yi)2 (3) the model aims to minimize the sum of the absolute differences between the euclidean distances of component pairs and the average distances obtained from the annotated training data across all pairs for initials (or finals) c. we also incorporate a penalty function, p, for pairs deviating from the manually annotated distance so that more phonetically similar pairs are penalized more highly (we discuss further in section 3.2). equation represents the cost function: min pc s2p 2p p (4) one main advantage of our learning model is that it is generic and can easily extend to any ndimensional space. based on the structured of table 2, we intuit that extending beyond one dimension will yield more accurate encodings. figures and visualize the computed encodings of initials when setting n1 and n2 we see that when n 2, the locations of initial coordinates align well with table 2,. in particular, the twelve groups are clustered in a pattern that is defined in section for example, bp,gk,jqx are separated into different clusters. however, while table indicated the basic clusters for the initials, our learned model goes further than table by actually quantifying the inter- and intra-cluster similarities. specifically, clusters c, ch, j, q, x are tighter than clusters c, c, h and d, t, whereas the clusters m and n, r, l are well separated from other clusters. interestingly, the learning algorithms organically discovers new clusters that are not reflected in table 2; namely that r,n and r,l are pairs of phonetically similar initials. when n 1, the learned model collapses the coordinates into one dimension (figure 3). we observe that the predefined clusters are not well aligned, and many clusters are mixed together (e.g., bp,gk,nl,dt), preventing dimsim from considering variations within a cluster to be more similar than variations between clusters. visually comparing figures and gives the intuition for why dimsim with n performs better than dimsim with n 1, which is in turn reflected in our evaluation results. section presents the effects that varying the number of dimensions has on evaluation results. there are five tones in chinese, represented by a tone number scale ranging from to it is simple to use tone numbers for tone encodings and the difference between the tones of two pinyins as the raw measure of distance, ranging in value from to (e.g., st (xue2, xue4) 2). one exception is that we encode tone as the numerical value of since tone is more similar to tone compared to tone according to the relative pitch changes of the four tones (iso, 2015). however, this measure must first be scaled to be comparable to the pairwise phonetic distances of initials and finals. there is an additional constraint: any pairwise difference in initials or finals must have input : word w, threshold th,dict dict; output: words outws; begin pys getpinyins(w,dict); headpys getsimpinyins(pys(0), th); headwords getwordswithheadpy(headpys, dict); for cw headwords do if cw.size w.size then continue; end sim getsimilarity(cw,w); if sim th then outws.add(cw); end end sortbyascsim(outws); return outws; end algorithm 1: generating phonetic candidates. a greater negative effect on the phonetic similarity between characters than any difference in tones. for example, s(xue1,lue1)s(xue1,xue5) even though xue1 and xue5 are at opposite ends of the tone scale. we therefore scale st such that max(st ) min(sp). having determined the phonetic encodings and the mechanism to compute the phonetic similarity using learned phonetic encodings, we now describe how to generate and rank similar candidates in algorithm given a word w, a similarity threshold th, and a chinese pinyin dictionary dict, we retrieve the pinyin py of w from dict. we derive a list of pinyins pys whose similarity to py falls within the threshold th. these are used to generate a list of words with the same pinyin in pys and the same number of characters as w. we calculate the similarity of each candidate word with w using equation and filter out candidates that fall outside the similarity threshold th. thus, th is a parameter that affects the precision and recall of the generated candidates. a larger th generates more candidates, increasing recall while decreasing precision.3 finally, we output the candidates ranked in ascending order by similarity distance. 3we study the impact of varying th in section", "section_index": 2}, {"content": "we collect words from social media we calculate recall automatically using the the full test set of word pairs (wu, 2016). since downstream applications will only consider a limited number of candidates in practice, we evaluate precision via a manual annotation task on the top-ranked candidates generated by each approach. dm considers word spelling, pronunciation and other miscellaneous characteristics to encode the word into a primary and a secondary code. dm as one of the baselines is known to perform poorly at ranking the candidates (carstensen, 2005) since only two codes are used. we therefore use our method (equation 1) to rank the dm-generated candidates, to create a second baseline, dm-rank.4 the third baseline, aline, measures phonetic similarity based on manually coded multi-valued articulatory features weighted by their relative importance with respect to feature salience (again, manually determined). med, the last baseline, computes similarity as the minimum-weight series of edit operations that transforms one sound component into another. recall and mrr: we compare dimsim to dm, dm-rank, aline and med. dimsim1 and dimsim2 denotes dimsim encoding dimension n and n 2, respectively. as shown in figure 5, dimsim2 improves recall by factors of 1.5, 1.5, and 1.2, and improves mrr by factors of 7.5, 1.4, and over dm, dm-rank, aline and med, respectively. dm performs relatively poorly, as it is designed for english, and does not accurately reflect chinese pronunciation. ranking dm candidates using the dimsim phonetic distance defined in equation improves its average mrr by a factor of however, even dm-rank is outperformed by the simple med 4we do not compare with soundex as dm is accepted to be an improved phonetic similarity algorithm over soundex. baseline, demonstrating the inherent problem with dm’s coarse encodings. while aline has a similar recall to dimsim, it performs worse on mrr than dimsim2 because it does not have a direct representation of compound vowels for pinyin. it measures distance between compound vowels using phonetic features of basic vowels which leads to inaccuracy. in turn, med struggles with representing accurate phonetic distances between initials, since most initials are of length 1, and the edit distance between any two characters of length is identical. in contrast, dimsim encodes initials and finals separately, and thus even a 1-dimensional encoding (dimsim1) outperforms the other baselines. finally, the intuition of figures and is reflected in the data, as dimsim2 outperforms dimsim1 by (mrr). precision and mrr: here we evaluate the quality of the candidate ranking since in practice, downstream applications consider only a small number of possible candidates for every word. we ask two native chinese speakers to annotate the quality of the generated candidates. choosing words randomly from the test set, we use dmrank, med, aline and dimsim2 to generate top-k candidates for each seed word (k 5).5 the annotators mark each candidate as phonetically similar to the seed word (1) or not (0), also marking the one candidate they believe to be the most similar-sounding (2), which may be any of 5we do not evaluate dm and dimsim1 as they perform worse than dm-rank and dimsim2, respectively. we then compute precision and average mrr using the obtained annotations. we achieve inter-level agreement(ila) of for p and ila of for average mrr. dimsim once again outperforms med and dm-rank by up to 1.4x for precision and 1.24x for mrr. since the only criteria for picking the best top-k candidate is phonetic similarity, this demonstrates that dimsim ranks the most phonetically similar candidates higher than the other baselines. we study the sensitivity of dimsim to varying the scoring and penalty functions, using recall and average mrr for evaluation. table shows four different scoring functions and penalty functions (including the variation of not using a penalty function) to convert the annotator scores to pairwise distances s, following equation figure depicts the values of the four scoring functions as a function of the annotator scores on a log scale, to demonstrate the effect of varying a and b, as well as using as the base or exponent. figure demonstrates how sensitive our model is to the different combinations of scoring and penalty functions. we see that although recall is entirely insensitive to the variations, the performance of mrr is impacted. there is a clear preference for the variations on the diagonal of table 4: f11, f22, f33, f44, but the nearidentical performance of these variations demonstrates dimsim’s robustness to the particular scoring and penalty functions used. note that not using a penalty function impacts mrr significantly. as demonstrated above, encoding initials and finals into a two-dimensional space is more effective than a one-dimensional space. figure presents the results of continuing to increase the number of dimensions, n 1, we observe that recall is barely affected, with all variations able to successfully identify the targeted words to of the time. we also see that moving from n1 to n2 increases the average mrr by 1.14x however, further increasing the number of dimensions to n2 no longer improves average mrr, indicating that learning a two-dimensional encoding is enough to capture the phonetic relationships between pinyin components. we examine how the similarity distance threshold (th) impacts dimsim by varying th from to (figure 10) (using the scoring function f22). as th increases, recall increases from to 0.99, converging when th reaches by increasing th dimsim matches more characters that are simi- lar to the first character of the given word, which in turn increases the number of candidates within the distance. thus, the probability of including the labeled gold standard words in the results increases. mmr is less sensitive to th, converging when th reaches however, the generated set of candidate words is reduced too much for th 128, hurting the performance of mmr. to ensure both high recall and mrr we set th while generating more candidates improves the recall, presenting too many candidates to a downstream application is not desirable. to find a balance, we study the impact of varying the upper limit of the number of generated candidates nc from to (figure 11). we find that mrr converges at candidates, while recall takes longer; however, setting the upper limit at candidates already achieves almost recall, suggesting it as a reasonable cutoff in practice. unless otherwise mentioned, we set nc 1, for experiments, to isolate the impact of this parameter. we analyze and summarize three types of errors made by dimsim. the first occurs when targeted words are out of vocabulary(oov). for instance, for the original word , the targed word is which is oov. as is commonly the case in text normalization applications which convert informal language to well-formed terms, our method works as long as the targeted words are in the dictionary. this shortcoming is generally alleviated by adding new terms to the dictionary. second, dimsim cannot derive phonetic candidates from dialects that are not encoded in our mapping table. for example, for (dong4)(suan4) , the targeted word (dang1)(xuan2) is obtained using the pronunciation of southern fujian dialect. however, our approach can easily be extended to incorporate and capture such variants by learning mapping tables for each dialect and using them to generate corresponding candidates. finally, we constrain dimsim to not identify candidates that differ in length from the seed word, as we observe that most transcriptions have the same word length - though some corner cases do occur.", "section_index": 3}, {"content": "there is a plethora of work focusing on the phonetic similarities between words and characters these algorithms encode words with similar pronunciation into the same code. for example, soundex (archives and administration, 2007) converts words into fixed length code through a mapping table of initial groups to ordinal numbers. these algorithms fail to capture chinese phonetic similarity since the conversion rules do not consider pronunciation properties of pinyin. linguists in the phonetic and phonology community have also proposed several phonetic comparison algorithms (kessler, 2005; mak and barnard, 1996; nerbonne and heeringa, 1997; ladefoged, 1969; kondrak, 2003) for determining the similarity between speech forms. however, as features of articulatory phonetics are manually assigned, these algorithms fall short in capturing the perceptual essence of phonetic similarity through empirical data (kessler, 2005). in contrast, dimsim achieves high accuracy by learning the encodings both from high quality training data sets and linguistic pinyin features. several works in named entity translation focus on learning the phonetic similarity between english and chinese automatically. these approaches first represent english and chinese words in basic phoneme units and apply edit distance algorithms to compute the similarity. training frameworks are then used to learn the similarity. however, the phonetic similarity used in these systems cannot be applied to chinese words since pinyin has its own specific characteristics, which do not easily map to english, for determining phonetic similarity. another main application of phonetic similarity algorithms is text normalization , where phonetic similarity is measured by a combination of initial and final similarities. however, the encodings used in these approaches are too coarse-grained, yielding low f1 measures. dimsim learns separate high dimensional encodings for initials and finals, and uses them to calculate and rank the distances between pinyin representations of chinese word pairs. karl stratos (stratos, 2017) proposes a sub-character architecture to deal with the data sparsity problem in korean language processing by breaking down each korean character into a small set of primitive phonetic units. however, this work does not address the problem of the phonetic similarity and is thus orthogonal to dimsim.", "section_index": 4}, {"content": "motivated by phonetic transcription as a widely observed phenomenon in chinese social media and informal language, we have designed an accurate phonetic similarity algorithm. dimsim generates phonetically similar candidate words based on learned encodings that capture the pronunciation characteristics of pinyin initial, final, and tone components. using a real world dataset, we demonstrate that dimsim effectively improves mrr by 7.5x , recall by 1.5x and precision by 1.4x over existing approaches. the original motivation for this work was to improve the quality of downstream nlp tasks, such as named entity identification, text normalization and spelling correction. these tasks all share a dependency on reliable phonetic similarity as an intermediate step, especially for languages such as chinese where incorrect homophones and synophones abound. we therefore plan to extend this line of work by applying dimsim to downstream applications, such as text normalization.", "section_index": 5}], "id": 1009}
{"text": [{"content": "relation detection is a core component of many nlp applications including knowledge base question answering (kbqa). in this paper, we propose a hierarchical recurrent neural network enhanced by residual learning which detects kb relations given an input question. our method uses deep residual bidirectional lstms to compare questions and relation names via different levels of abstraction. additionally, we propose a simple kbqa system that integrates entity linking and our proposed relation detector to make the two components enhance each other. our experimental results show that our approach not only achieves outstanding relation detection performance, but more importantly, it helps our kbqa system achieve state-of-the-art accuracy for both single-relation (simplequestions) and multi-relation (webqsp) qa benchmarks.", "section_index": 0}, {"content": "kb query, which can be executed to retrieve the answers from a kb. figure illustrates the process used to parse two sample questions in a kbqa system: ; and (b) a more complex case, where some constraints need to be handled for multiple entities in the question. the kbqa system in the figure performs two key tasks: (1) entity linking, which links n-grams in questions to kb entities, and (2) relation detection, which identifies the kb relation(s) a question refers to. the main focus of this work is to improve the relation detection subtask and further explore how it can contribute to the kbqa system. although general relation detection1 methods are well studied in the nlp community, such studies usually do not take the end task of kbqa into consideration. as a result, there is a significant gap between general relation detection studies and kb-specific relation detection. first, in most general relation detection tasks, the number of target relations is limited, normally smaller than in contrast, in kbqa even a small kb, like freebase2m , contains more than 6,000 relation types. second, relation detection for kbqa often becomes a zero-shot learning task, since some test instances may have unseen relations in the training data. for example, the simplequestions data set has of the golden test relations not observed in golden training tuples. third, as shown in figure 1, we need to predict a chain of relations instead of a single relation. this increases the number of target relation types and the sizes of candidate relation pools, further increasing the difficulty of kb relation detection. owing to these reasons, kb relation detection is significantly more challenging compared to general relation detection tasks. this paper improves kb relation detection to cope with the problems mentioned above. first, in order to deal with the unseen relations, we propose to break the relation names into word sequences for question-relation matching. second, noticing 1in the information extraction field such tasks are usually called relation extraction or relation classification.", "section_index": 1}, {"content": "that original relation names can sometimes help to match longer question contexts, we propose to build both relation-level and word-level relation representations. third, we use deep bidirectional lstms (bilstms) to learn different levels of question representations in order to match the different levels of relation information. finally, we propose a residual learning method for sequence matching, which makes the model training easier and results in more abstract (deeper) question representations, thus improves hierarchical matching. in order to assess how the proposed improved relation detection could benefit the kbqa end task, we also propose a simple kbqa implementation composed of two-step relation detection. given an input question and a set of candidate entities retrieved by an entity linker based on the question, our proposed relation detection model plays a key role in the kbqa process: (1) re-ranking the entity candidates according to whether they connect to high confident relations detected from the raw question text by the relation detection model. this step is important to deal with the ambiguities normally present in entity linking results. (2) finding the core relation (chains) for each topic entity2 selection from a much smaller candidate entity set after re-ranking. the above steps are followed by an optional constraint detection step, when the question cannot be answered by single relations (e.g., multiple entities in the question). finally the highest scored query from the above 2following yih et al. (2015), here topic entity refers to the root of the (directed) query tree; and core-chain is the directed path of relation from root to the answer node. steps is used to query the kb for answers. our main contributions include: (i) an improved relation detection model by hierarchical matching between questions and relations with residual learning; (ii) we demonstrate that the improved relation detector enables our simple kbqa system to achieve state-of-the-art results on both single-relation and multi-relation kbqa tasks.", "section_index": 2}, {"content": "relation extraction relation extraction recent research benefits a lot from the advancement of deep learning: from word embeddings to deep networks like cnns and lstms and attention models the above research assumes there is a fixed (closed) set of relation types, thus no zero-shot learning capability is required. the number of relations is usually not large: the widely used ace2005 has 11/32 coarse/fine-grained relations; semeval2010 task8 has relations; tac- kbp2015 has relations although it considers open-domain wikipedia relations. all are much fewer than thousands of relations in kbqa. as a result, few work in this field focuses on dealing with large number of relations or unseen relations. (2016) proposed to use relation embeddings in a low-rank tensor method. however their relation embeddings are still trained in supervised way and the number of relations is not large in the experiments. relation detection in kbqa systems relation detection for kbqa also starts with featurerich approaches and attention models many of the above relation detection research could naturally support large relation vocabulary and open relation sets ), in order to fit the goal of open-domain question answering. different kbqa data sets have different levels of requirement about the above open-domain capacity. for example, most of the gold test relations in webquestions can be observed during training, thus some prior work on this task adopted the close domain assumption like in the general re research. while for data sets like simplequestions and paralex, the capacity to support large relation sets and unseen relations becomes more necessary. to the end, there are two main solutions: ), like ; split relations to word sequences for single-relation detection. use character tri-grams as inputs on both question and relation sides. golub and he (2016) propose a generative framework for single-relation kbqa which predicts relation with a character-level sequenceto-sequence model. another difference between relation detection in kbqa and general re is that general re re- search assumes that the two argument entities are both available. thus it usually benefits from features or attention mechanisms based on the entity information (e.g. entity types or entity embeddings). for relation detection in kbqa, such information is mostly missing because: (1) one question usually contains single argument (the topic entity) and (2) one kb entity could have multiple types (type vocabulary size larger than 1,500). this makes kb entity typing itself a difficult problem so no previous used entity information in the relation detection model.3", "section_index": 3}, {"content": "kb relations previous research formulates kb relation detection as a sequence matching problem. however, while the questions are natural word sequences, how to represent relations as sequences remains a challenging problem. here we give an overview of two types of relation sequence representations commonly used in previous work. (1) relation name as a single token (relationlevel). in this case, each relation name is treated as a unique token. the problem with this approach is that it suffers from the low relation coverage due to limited amount of training data, thus cannot generalize well to large number of opendomain relations. for example, in figure 1, when treating relation names as single tokens, it will be difficult to match the questions to relation names episodes written and starring roles if these names do not appear in training data their relation embeddings hrs will be random vectors thus are not comparable to question embeddings hqs. (2) relation as word sequence (word-level). in this case, the relation is treated as a sequence of words from the tokenized relation name. it has better generalization, but suffers from the lack of global information from the original relation names. for example in figure 1(b), when doing only word-level matching, it is difficult to rank the target relation starring roles higher compared to the incorrect relation plays produced. this is because the incorrect relation contains word plays, which is more similar to the question 3such entity information has been used in kbqa systems as features for the final answer re-rankers. (containing word play) in the embedding space. on the other hand, if the target relation co-occurs with questions related to tv appearance in training, by treating the whole relation as a token (i.e. relation id), we could better learn the correspondence between this token and phrases like tv show and play on. the two types of relation representation contain different levels of abstraction. as shown in table 1, the word-level focuses more on local information (words and short phrases), and the relation-level focus more on global information (long phrases and skip-grams) but suffer from data sparsity. since both these levels of granularity have their own pros and cons, we propose a hierarchical matching approach for kb relation detection: for a candidate relation, our approach matches the input question to both word-level and relation-level representations to get the final ranking score. section gives the details of our proposed approach.", "section_index": 4}, {"content": "this section describes our hierarchical sequence matching with residual learning approach for relation detection. in order to match the question to different aspects of a relation (with different abstraction levels), we deal with three problems as follows on learning question/relation representations. we provide our model with both types of relation representation: word-level and relationlevel. therefore, the input relation becomes r rword1 , , rwordm1 rrel1 , , rrelm2, where the first m1 tokens are words (e.g. episode, written), and the last m2 tokens are relation names, e.g., episode written or starring roles, series (when the target is a chain like in figure 1(b)). we transform each token above to its word embed- ding then use two bilstms (with shared parameters) to get their hidden representations bword1:m1 : brel1:m2 (each row vector i is the concatenation between forward/backward representations at i). we initialize the relation sequence lstms with the final state representations of the word sequence, as a back-off for unseen relations. we apply one max-pooling on these two sets of vectors and get the final relation representation hr. from table 1, we can see that different parts of a relation could match different contexts of question texts. usually relation names could match longer phrases in the question and relation words could match short phrases. yet different words might match phrases of different lengths. as a result, we hope the question representations could also comprise vectors that summarize various lengths of phrase information (different levels of abstraction), in order to match relation representations of different granularity. we deal with this problem by applying deep bilstms on questions. the first-layer of bilstm works on the word embeddings of question words q q1, , qn and gets hidden representations (1) 1:n (1) ; ; (1) n the second-layer bilstm works on (1)1:n to get the second set of hidden representations (2)1:n since the second bilstm starts with the hidden vectors from the first layer, intuitively it could learn more general and abstract information compared to the first layer. note that the first(second)-layer of question representations does not necessarily correspond to the word(relation)-level relation representations, instead either layer of question representations could potentially match to either level of relation representations. this raises the difficulty of matching between different levels of relation/question representations; the following section gives our proposal to deal with such problem. now we have question contexts of different lengths encoded in (1)1:n and (2) 1:n unlike the standard usage of deep bilstms that employs the representations in the final layer for prediction, here we expect that two layers of question representations can be complementary to each other and both should be compared to the relation representation space (hierarchical matching). this is important for our task since each relation token can correspond to phrases of different lengths, mainly because of syntactic variations. for example in table 1, the relation word written could be matched to either the same single word in the question or a much longer phrase be the writer of. we could perform the above hierarchical matching by computing the similarity between each layer of and hr separately and doing the (weighted) sum between the two scores. however this does not give significant improvement (see table 2). our analysis in section shows that this naive method suffers from the training difficulty, evidenced by that the converged training loss of this model is much higher than that of a single-layer baseline model. this is mainly because (1) deep bilstms do not guarantee that the two-levels of question hidden representations are comparable, the training usually falls to local optima where one layer has good matching scores and the other always has weight close to (2) the training of deeper architectures itself is more difficult. to overcome the above difficulties, we adopt the idea from residual networks for hierarchical matching by adding shortcut connections between two bilstm layers. we proposed two ways of such hierarchical residual matching: (1) connecting each (1)i and (2) i , resulting in a i (1) i (2) i for each position i. then the final question representation hq becomes a maxpooling over all is, 1in (2) applying max- pooling on (1)1:n and (2) 1:n to get h (1) max and h (2) max, respectively, then setting hq h(1)max h (2) max. finally we compute the matching score of r given q as srel(r;q) cos(hr,hq). intuitively, the proposed method should benefit from hierarchical training since the second layer is fitting the residues from the first layer of matching, so the two layers of representations are more likely to be complementary to each other. this also ensures the vector spaces of two layers are comparable and makes the second-layer training easier. during training we adopt a ranking loss to maximizing the margin between the gold relation r and other relations r in the candidate pool r. lrel max0, srel(r;q) srel(r ;q) where is a constant parameter. fig summarizes the above hierarchical residual bilstm (hr-bilstm) model. remark: another way of hierarchical matching consists in relying on attention mechanism, e.g. , to find the correspondence between different levels of representations. this performs below the hr-bilstm (see table 2).", "section_index": 5}, {"content": "this section describes our kbqa pipeline system. we make minimal efforts beyond the training of the relation detection model, making the whole system easy to build. following previous work , our kbqa system takes an existing entity linker to produce the top-k linked entities, elk(q), for a question q (initial entity linking). then we generate the kb queries for q following the four steps illustrated in algorithm algorithm 1: kbqa with two-step relation detection input : question q, knowledge base kb, the initial top-k entity candidates elk(q) output: top query tuple (e, r, (c, rc)) entity re-ranking (first-step relation detection): use the raw question text as input for a relation detector to score all relations in the kb that are associated to the entities in elk(q); use the relation scores to re-rank elk(q) and generate a shorter list el0k0(q) containing the top-k0 entity candidates (section 5.1) relation detection: detect relation(s) using the reformatted question text in which the topic entity is replaced by a special token e (section 5.2) query generation: combine the scores from step and 2, and select the top pair (e, r) (section 5.3) constraint detection (optional): compute similarity between q and any neighbor entity c of the entities along r (connecting by a relation rc) , add the high scoring c and rc to the query (section 5.4). compared to previous approaches, the main difference is that we have an additional entity reranking step after the initial entity linking. we have this step because we have observed that entity linking sometimes becomes a bottleneck in kbqa systems. for example, on simplequestions the best reported linker could only get top-1 accuracy on identifying topic entities. this is usually due to the ambiguities of entity names, e.g. in fig 1(a), there are tv writer and baseball player mike kelley, which is impossible to distinguish with only entity name matching. having observed that different entity candidates usually connect to different relations, here we propose to help entity disambiguation in the initial entity linking with relations detected in questions. sections and elaborate how our relation detection help to re-rank entities in the initial entity linking, and then those re-ranked entities enable more accurate relation detection. the kbqa end task, as a result, benefits from this process. in this step, we use the raw question text as input for a relation detector to score all relations in the kb with connections to at least one of the entity candidates in elk(q). we call this step relation detection on entity set since it does not work on a single topic entity as the usual settings. we use the hr-bilstm as described in sec. for each question q, after generating a score srel(r; q) for each relation using hr-bilstm, we use the top l best scoring relations (rlq) to re-rank the original entity candidates. concretely, for each entity e and its associated relations re, given the original entity linker score slinker, and the score of the most confident relation r rlq\\re, we sum these two scores to re-rank the entities: srerank(e; q) slinker(e; q) (1 ) max r2rlq\\re srel(r; q). finally, we select top k k entities according to score srerank to form the re-ranked list el k0(q). we use the same example in fig 1(a) to illustrate the idea. given the input question in the example, a relation detector is very likely to assign high scores to relations such as episodes written, author of and profession. then, according to the connections of entity candidates in kb, we find that the tv writer mike kelley will be scored higher than the baseball player mike kelley, because the former has the relations episodes written and profession. this method can be viewed as exploiting entity-relation collocation for entity linking. in this step, for each candidate entity e el0k(q), we use the question text as the input to a relation detector to score all the relations r re that are associated to the entity e in the kb.4 because we have a single topic entity input in this step, we do the following question reformatting: we replace the the candidate e’s entity mention in 4note that the number of entities and the number of relation candidates will be much smaller than those in the previous step. this helps the model better distinguish the relative position of each word compared to the entity. we use the hr-bilstm model to predict the score of each relation r re: srel(r; e, q). finally, the system outputs the entity, relation (or core-chain) pair (e, r) according to: s(e, r; q) max e2el0 k0 (q),r2re ( srerank(e; q) (1 ) srel(r; e, q)) , where is a hyperparameter to be tuned. similar to , we adopt an additional constraint detection step based on text matching. our method can be viewed as entitylinking on a kb sub-graph. it contains two steps: (1) sub-graph generation: given the top scored query generated by the previous steps5, for each node v (answer node or the cvt node like in figure 1(b)), we collect all the nodes c connecting to v (with relation rc) with any relation, and generate a sub-graph associated to the original query. (2) entity-linking on sub-graph nodes: we compute a matching score between each n-gram in the input question (without overlapping the topic entity) and entity name of c (except for the node in the original query) by taking into account the maximum overlapping sequence of characters between them (see appendix a for details and b for special rules dealing with date/answer type constraints). if the matching score is larger than a threshold (tuned on training set), we will add the constraint entity c (and rc) to the query by attaching it to the corresponding node v on the core-chain.", "section_index": 6}, {"content": "we use the simplequestions and webqsp datasets. each question in these datasets is labeled with the gold semantic parse. hence we can directly evaluate relation detection performance independently as well as evaluate on the kbqa end task. 5starting with the top-1 query suffers more from error propagation. however we still achieve state-of-the-art on webqsp in sec.6, showing the advantage of our relation detection model. we leave in future work beam-search and feature extraction on beam for final answer re-ranking like in previous research. simplequestions , in order to compare with previous research. , we use s-mart (yang and chang, 2015) entity-linking outputs.7 in order to evaluate the relation detection models, we create a new relation detection task from the webqsp data set.8 for each question and its labeled semantic parse: (1) we first select the topic entity from the parse; and then (2) select all the relations and relation chains (length 2) connected to the topic entity, and set the corechain labeled in the parse as the positive label and all the others as the negative examples. we tune the following hyper-parameters on development sets: (1) the size of hidden states for lstms (50, 100, 200, 400)9; (2) learning rate (0.1, 0.5, 1.0, 2.0); (3) whether the shortcut connections are between hidden states or between max-pooling results (see section 4.3); and (4) the number of training epochs. for both the relation detection experiments and the second-step relation detection in kbqa, we have entity replacement first the embeddings of relation names are randomly initialized, since existing pre-trained relation embeddings (e.g. transe) usually support limited sets of relation names. we leave the usage of pre-trained relation embeddings to future work. table shows the results on two relation detection tasks. the ampcnn result is from , which yielded state-of-the-art scores by outperforming several attention-based meth- 6the two resources have been downloaded from https: //github.com/gorov/simplequestions-entitylinking 7url 8the dataset is available at url/ simplequestions-entitylinking. 9for cnns we double the size for fair comparison. we re-implemented the bicnn model from , where both questions and relations are represented with the word hash trick on character tri-grams. the baseline bilstm with relation word sequence appears to be the best baseline on webqsp and is close to the previous best result of ampcnn on simplequestions. our proposed hr-bilstm outperformed the best baselines on both tasks by margins of 2-3 (p and compared to the best baseline bilstm w/ words on sq and wq respectively). note that using only relation names instead of words results in a weaker baseline bilstm model. the model yields a significant performance drop on simplequestions (91.2 to 88.9). however, the drop is much smaller on webqsp, and it suggests that unseen relations have a much bigger impact on simplequestions. ablation test: the bottom of table shows ablation results of the proposed hr-bilstm. first, hierarchical matching between questions and both relation names and relation words yields improvement on both datasets, especially for simplequestions and its one-way variations, where the one-way model gives better results10. note that residual learning significantly helps on webqsp (80.65 to 10we also tried to apply the same attention method on deep bilstm with residual connections, but it does not lead to better results compared to hr-bilstm. we hypothesize that the idea of hierarchical matching with attention mechanism may work better for long sequences, and the new advanced attention mechanisms might help hierarchical matching. we leave the above directions to future work. 82.53), while it does not help as much on simplequestions. on simplequestions, even removing the deep layers only causes a small drop in performance. webqsp benefits more from residual and deeper architecture, possibly because in this dataset it is more important to handle larger scope of context matching. finally, on webqsp, replacing bilstm with cnn in our hierarchical matching framework results in a large performance drop. yet on simplequestions the gap is much smaller. we believe this is because the lstm relation encoder can better learn the composition of chains of relations in webqsp, as it is better at dealing with longer dependencies. analysis next, we present empirical evidences, which show why our hr-bilstm model achieves the best scores. we use webqsp for the analysis purposes. first, we have the hypothesis that training of the weighted-sum model usually falls to local optima, since deep bilstms do not guarantee that the two-levels of question hidden representations are comparable. this is evidenced by that during training one layer usually gets a weight close to thus is ignored. for example, one run gives us weights of -75.39/0.14 for the two layers (we take exponential for the final weighted sum). it also gives much lower training accuracy (91.94) compared to hr-bilstm (95.67), suffering from training difficulty. second, compared to our deep bilstm with shortcut connections, we have the hypothesis that for kb relation detection, training deep bilstms is more difficult without shortcut connections. our experiments suggest that deeper bilstm does not always result in lower training accuracy. in the experiments a two-layer bilstm converges to 94.99, even lower than the achieved by a single-layer bilstm. under our setting the twolayer model captures the single-layer model as a special case (so it could potentially better fit the training data), this result suggests that the deep bilstm without shortcut connections might suffers more from training difficulty. finally, we hypothesize that hr-bilstm is more than combination of two bilstms with residual connections, because it encourages the hierarchical architecture to learn different levels of abstraction. to verify this, we replace the deep bilstm question encoder with two single-layer bilstms (both on words) with shortcut connections between their hidden states. this decreases test accuracy to it gives similar training accuracy compared to hr-bilstm, indicating a more serious over-fitting problem. this proves that the residual and deep structures both contribute to the good performance of hr-bilstm. table compares our system with two published baselines , the stateof-the-art on webqsp11 and , the state-of-the-art on simplequestions. since these two baselines are specially designed/tuned for one particular dataset, they do not generalize well when applied to the other dataset. in order to highlight the effect of different relation detection models on the kbqa end-task, we also implemented another baseline that uses our kbqa system but replaces hr-bilstm with our implementation of ampcnn (for simplequestions) or the char-3-gram bicnn (for webqsp) relation detectors (second block in table 3). compared to the baseline relation detector (3rd row of results), our method, which includes an improved relation detector (hr-bilstm), improves the kbqa end task by 2-3 (4th row). note that in contrast to previous kbqa systems, our system does not use joint-inference or feature-based re-ranking step, nevertheless it still achieves better or comparable results to the state-of-the-art. the third block of the table details two ablation tests for the proposed components in our kbqa systems: (1) removing the entity re-ranking step significantly decreases the scores. since the reranking step relies on the relation detection models, this shows that our hr-bilstm model contributes to the good performance in multiple ways. 11the stagg score on sq is from appendix c gives the detailed performance of the re-ranking step. , constraint detection is crucial for our system12. this is probably because our joint performance on topic entity and core-chain detection is more accurate (77.5 top-1 accuracy), leaving a huge potential (77.5 vs. 58.0) for the constraint detection module to improve. finally, like stagg, which uses multiple relation detectors for the three models used), we also try to use the top-3 relation detectors from section as shown on the last row of table 3, this gives a significant performance boost, resulting in a new state-of-the-art result on simplequestions and a result comparable to the state-of-the-art on webqsp.", "section_index": 7}, {"content": "kb relation detection is a key step in kbqa and is significantly different from general relation extraction tasks. we propose a novel kb relation detection model, hr-bilstm, that performs hierarchical matching between questions and kb relations. our model outperforms the previous methods on kb relation detection tasks and allows our kbqa system to achieve state-of-the-arts. for future work, we will investigate the integration of our hr-bilstm into end-to-end systems. for example, our model could be integrated into the decoder in , to provide better sequence prediction. we will also investigate new emerging datasets like graphquestions (su et al.,", "section_index": 8}], "id": 1010}
{"text": [{"content": "dictionaries and ontologies are foundational elements of systems extracting knowledge from unstructured text. however, as new content arrives keeping dictionaries up-to-date is a crucial operation. in this paper, we propose a human-in-the-loop (huml) dictionary expansion approach that employs a lightweight neural language model coupled with tight huml supervision to assist the user in building and maintaining a domain-specific dictionary from an input text corpus. the approach is based on the explore/exploit paradigm to effectively discover new instances (explore) from the text corpus as well as predict new unseen terms not currently in the corpus using the accepted dictionary entries (exploit). we evaluate our approach on a real-world scenario in the healthcare domain, in which we construct a dictionary of adverse drug reactions from user blogs as input text corpus. the evaluation shows that using our approach the user can easily extend the input dictionary, where tight human-in-the-loop integration results in a improvement in effectiveness.", "section_index": 0}, {"content": "dictionary expansion is one area where close integration of humans into the discovery loop has been shown to enhance task performance substantially over more traditional post-adjudication. this is not surprising, as dictionary membership is often a fairly subjective judgment (e.g., should a fruit dictionary include tomatoes?) thus even with a system which finds similar terms (e.g., word2vec) guidance is important to keep the system focused on the subject matter expert’s notion of lexicon. in this work we propose a feature agnostic approach for dictionary expansion based on lightweight neural language models, such as word2vec to prevent semantic drift during the dictionary expansion, we effectively include humanin-the-loop (huml). given an input text corpus and a set of seed examples, the proposed approach runs in two phases, explore and exploit, to identify new potential dictionary entries. the explore phase tries to identify similar instances to the dictionary entries that are present in the input text corpus, using term vectors from the neural language model to calculate a similarity score. the exploit phase tries to construct more complex multi-term phrases based on the instances already in the input dictionary. multi-term phrases are a challenge for word2vec style systems as they need to be known prior to model creation. to identify multi-term phrases, most commonly a simple phrase detection model is used, which is based on a term’s co-occurrence score, i.e., terms that often appear together probably are part of the same phrase the phrase detection must be done before the model is built, and they remain unchanged after the model is built. however, depending on the domain and the task, the instances of interest evolve, or the example corpus may not be complete. for example, valid phrase combinations may simply not occur (e.g., acute joint pain may appear in the sample corpus, but for some reason chronic hip pain may not). however, these phrases are likely to occur in future texts from the same source, and thus are important to include in any entity extraction lexicon. in the exploit phase, the approach generates new phrases by analyzing the single terms of the instances in the input dictionary. we use two phrase generation algorithms: (i) modify the phrases by replacing single terms with similar terms from the text corpus, e.g., abnormal behavior can be modified to strange behavior; (ii) extend the instances with terms from the text corpus that are related to the terms in the instance, e.g., abnormal blood clotting problems is a an adverse drug reaction, which doesn’t appear as such in a large text corpus, however the instances abnormal blood count, blood clotting and clotting problems appear several times in the corpus, which can be used to build the more complex instance. the approach allows us to construct new multi-term instances that don’t appear as such in the text corpus, but there is enough statistical evidence in the corpus that such instances might be of interest for the user. combining the explore and exploit approaches in an unsupervised fashion (or an infrequently supervised fashion) is not particularly effective. it tends to generate many spurious results that the human subject matter expert needs to wade through. close supervision, however, results in a much more performant system. the evaluation shows that high promptness of the huml (tighter computer/human partnership) results in nearly perfect performance of the system, i.e., nearly all the candidates identified by the system are valid entries in the dictionary. more precisely, the experiments show that the system is more effective when receiving huml feedback after identified candidates, compared to receiving huml feedback after identified candidates, while both cases require equal amount of human effort. the rest of this paper is structured as follows. in section 2, we give an overview of related work. in section 3, we present our interactive dictionary expansion approach, followed by an evaluation in section 4; we conclude with a summary and an outlook on future work.", "section_index": 1}, {"content": "dictionaries and ontologies are the backbone of many nlp and information retrieval systems. hence, a lot of work in the literature focuses on identifying new approaches for more efficient and more effective dictionary extraction from unstructured text. riloff and jones 12, is one of the first works to propose an automatic iterattive approach for dictionary extraction from unstructured text. the approach uses mutual bootstrapping technique that learns extraction patterns from the seed terms and then exploits the learned extraction patterns to identify more terms that belong to the semantic category. in the following years, many similar approaches have been developed 13,3,7,2. however, all these approaches require nlp parsing for feature extraction, and have a reliance on syntactic information for identifying quality patterns. hence, such approaches underperform on not-so-well structured texts, like user-generated text. furthermore, without human-in-the-loop, iterative methods can easily generate semantic drift. one of the major challenges with concept extraction involves dealing with not-so-well structured text, given the importance of user generated content, which can prove to be extremely valuable source of information for many domains, pharmacovigilance being one of those.1 to this end, lee et al. propose a semi-supervised model which uses a random twitter stream as unlabeled training data and prove it successful for the recognition of adverse drug reaction. another hurdle is the fact that the dictionary to be created can be highly dependent on the task at hand, especially when dealing with positive/negative words which are highly domain-dependent 6,11. while completely automatic techniques are highly appealing they need to be fine-tuned for every new task. we propose a human-in-the-loop approach where the tuning is an integral part of the process, i.e. the human works in partnership with the statistical method to drive the semantic of the task effectively and efficaciously. many works rely on machine learning techniques and tailor the algorithms to certain specific domains (e.g. drugs): these methods are in general expensive, requiring an annotated corpus and/or domain specific feature extraction (a comprehensive overview can be found in 10). our work is closely related to glimpse and glimpseld glimpse is a statistical algorithm for dictionary extraction based on spot with a faster underlying matching engine. the input is a large text corpus and a set of seed examples. starting from these it evaluates the contexts (the set of words surrounding an item) in which the seeds occur and identifies good contexts. contexts are scored retrospectively in terms of how many good results they generate. all contexts are kept which have a score over a given threshold and the candidates that appear in the most good contexts are provided first to the huml. the approach has been extended to glimpseld 1, which is language agnostic and uses linked data to as a bootstrapping source. while both approaches have been proven to achieve high effectiveness for dictionary extension, both of the approaches can only identify new dictionary entries that are only present in the input text corpus. in this work, we adopt the glimpse computer/human part- psb2016 is a recent benchmarking initiative on the problem url/ sharedtaskeval.html. nership architecture and extend it with the explore/exploit algorithm for more effective dictionary expansion.", "section_index": 2}, {"content": "the input of the algorithm is a text corpus tc and a set of dictionary seed example terms s. in the preprocessing step, we build a word2vec model 9, with the skip-gram implementation using tc as an input. word2vec is a particularly computationally-efficient two-layer neural net model for learning term embeddings from raw text. the output of the model is an embedding matrix w, where each term (word or phrase) from the corpus vocabulary vtc is represented as an n-dimensional vector. projecting such latent representations of words into a lower dimensional feature space shows that semantically similar words appear closer to each other. our approach is based on the explore/exploit paradigm to effectively discover new instances (explore) from the text corpus and generate new unseen instances based on user feedback (exploit). the approach runs in iterations, where each iteration runs first the explore phase then the exploit phase. the explore phase uses the instances available in the input dictionary to identify similar candidates that are already present in the corpus vocabulary vtc , which are then accepted or rejected by the huml. the accepted candidates are then added to the input dictionary and are used in the exploit phase as well as the next explore iteration. during the exploit phase, we use the instances in the input dictionary to construct more complex phrases that might be of interest for the user. as previously mentioned, in the word2vec feature embedding space, semantically similar words appear close to each other in the feature space. therefore, the problem of calculating the similarity between two instances is a matter of calculating the distance between two instances in the given feature space. to do so we use the standard cosine similarity measure which is applied on the vectors of the instances. formally, the similarity between two terms w1 and w2, with vectors v1 and v2, is calculated as the cosine similarity between the vectors v1 and v2: sim(w1, w2) v1 v2 v1 v2 (1) we calculate the similarity between the instances in the input dictionary and all the words in the corpus vocabulary vtc we sort the vocabulary in descending order using the cumulative similarity score, and choose the top-n candidates to present to the huml. the accepted candidates are added in the input dictionary, which are then used in the exploit phase and the next iteration. in the exploit phase we try to identify more complex phrases that don’t exist in the corpus vocabulary by analyzing the structure of the instances in the input dictionary. this is critical to help future proof a lexicon against new text. for a surveillance application (e.g., drug side effects mentioned on twitter) it reduces how frequently a human needs to tune up the lexicon to make sure it is catching all relevant entity instances. we use two phrase generation algorithms. in the first approach, we first break each instance in to a set of single terms t t1, t2, ..., tn, then for each term ti in t we identify a set of similar terms tsti ts1, ts2, ..., tss in the vocabulary vtc using equation in the next step, we build new phrases by replacing ti with a term tsi from tsti the new phrases are sorted based on the similarity score and the top-n are selected as candidates. for example, given the entry abnormal behavior the approach will identify strange behavior, abnormal attitude and strange attitude. in the second approach, we generate new phrases by extending the instances with terms from the text corpus that are related to the terms in the instance. related terms are terms that often share the same context, which means they often are surrounded by similar words. given a word2vec model, we calculate the relatedness between two terms w1 and w2, as the probability p(w1w2) calculated using the softmax function, p(w1w2) exp(vtw1vw2)v w1 exp(v t w vw2) , (2) where vw and v w are the input and the output vector of the word w, and v is the complete vocabulary of words. as before, we first break each instance in to a set of single terms t t1, t2, ..., tn, then for each term ti in t we identify a set of similar terms trti tr1, tr2, ..., trr in the vocabulary vtc using equation in the next step, we build new phrases by appending a term tri from trti to each term ti from t the new phrases are sorted based on the relatedness score and the top-n are selected as candidates. for example, given the instance clotting problems in the input dictionary the approach first tries to identify related terms in the text corpus for clotting. for which the top word is blood, because in many sentences blood clotting appears as a phrase, which can be used to generate new instances blood clotting problems. in the next iteration the phrase can be further extended, by identifying new related words. for example, in the top-n related words for blood we will find abnormal, which can be used to generate the instance abnormal blood clotting problems.", "section_index": 3}, {"content": "to evaluate our approach we conduct two experiments, i.e., (i) count the number of newly discovered dictionary entries per iteration; (ii) the impact of the promptness of the huml on the system performance. for the experiments we use data from the healtcare domain, specifically tackling the problem of identifying adverse drug reactions in user generated data. as an input text corpus we use user blogs extracted from url. askapatient.com (a forum where patients report their experience with medication drugs). as an input set of seed examples we use a set of instances referring to adverse drug events, which were labeled by a medical doctor in this experiment we compare the performance of the explore, exploit and the explore/exploit approaches for discovering new dictionary instances. we run the evaluation in iterations, where after each iteration we count how many new instances are discovered in the top proposed candidates by the algorithm. the accepted instances are then added in the dictionary and used for the next iteration. for the explore/exploit approach we run explore to identify candidates, and exploit to identify another candidates. the results are shown in fig. the results show that using the explore/exploit approach we are able to discover significantly more instances in each iteration compared to the other approaches. we can observe that when using the explore approach the number of newly discovered instances quickly decreases as the number of available instances in the whole corpus is decreasing in each iteration. when using the exploit approach the number of newly discovered instances sharply decreases as no new base terms are introduced, thus the exploit cannot generate new instances that can be added in the dictionary. the results show that using explore and exploit alternately leads to the best performances. in this experiment we show the importance of the promptness of the huml on the number of newly discovered instances, i.e., we evaluate if the user gives their feedback to the system sooner it will improve the performance of the system. to do so, we run the explore/exploit approach with different feedback intervals. the feedback interval indicates how many candidates the system needs to identify before the user gives their feedback to the system. for example, when using feedback interval of 10, the user gives their feedback after candidates are identified by the system. we evaluate feedback intervals of 10, 50, 100, and after each iteration we count the number of accepted candidates, and include them in the dictionary to be used for the next iteration. the results are shown in fig the results show that the tighter the huml integration is, the more quickly new instances are discovered. we see that with a large examples feedback interval the huml system discovers new instances, but requires the human to consider candidates. a more tightly integrated system with a examples feedback interval finds new instances in just iterations, requiring the human to consider only candidates. after iterations the system discovered new dictionary entries, compared to only new entries when using examples feedback interval. that yields improvement in effectivness of the system.", "section_index": 4}, {"content": "this paper proposes an interactive dictionary expansion tool using a lightweight neural language model. our algorithm is iterative and purely statistical, hence does not require any feature extraction beyond tokenization. it incorporates human feedback to improve performance and control semantic drift at every iteration cycle. the experiments showed high importance of tight huml integration on discovery efficiency. in this work, we have considered only lightweight language models, which can be efficiently built and updated on large text corpora. in future work, we will analyze more complex language neural network models, such as recurrent neural networks (rnn), long short term memory networks (lstm), and bidirectional lstm, which might improve the search for similar and related terms, at the expense of higher training time. furthermore, future work will include an evaluation of the approach on multiple datasets covering different domains.", "section_index": 5}], "id": 1011}
{"text": [{"content": "textual grounding is an important but challenging task for human-computer interaction, robotics and knowledge mining. existing algorithms generally formulate the task as selection from a set of bounding box proposals obtained from deep net based systems. in this work, we demonstrate that we can cast the problem of textual grounding into a unified framework that permits efficient search over all possible bounding boxes. hence, the method is able to consider significantly more proposals and doesn’t rely on a successful first stage hypothesizing bounding box proposals. beyond, we demonstrate that the trained parameters of our model can be used as word-embeddings which capture spatial-image relationships and provide interpretability. lastly, at the time of submission, our approach outperformed the current state-of-the-art methods on the flickr 30k entities and the referitgame dataset by and respectively.", "section_index": 0}, {"content": "grounding of textual phrases, i.e., finding bounding boxes in images which relate to textual phrases, is an important problem for human-computer interaction, robotics and mining of knowledge bases, three applications that are of increasing importance when considering autonomous systems, augmented and virtual reality environments. for example, we may want to guide an autonomous system by using phrases such as the bottle on your left,’ or the plate in the top shelf.’ while those phrases are easy to interpret for a human, they pose significant challenges for present day textual grounding algorithms, as interpretation of those phrases requires an understanding of objects and their relations. existing approaches for textual grounding, such as 38, take advantage of the cognitive performance improvements obtained from deep net features. more specifically, deep net models are designed to extract features from given bounding boxes and textual data, which are then compared to measure their fitness. to obtain suitable bounding boxes, many of the textual grounding frameworks, such as 38, 15, make use of region proposals. while being easy to obtain, automatic extraction of region proposals is limiting, because the performance of the visual grounding is inherently constrained by the quality of the proposal generation procedure. in this work we describe an interpretable mechanism which additionally alleviates any issues arising due to a limited number of region proposals. our approach is based on a number of image concepts’ such as semantic segmentations, detections and priors for any number of objects of interest. based on those image concepts’ which are represented as score maps, we formulate textual grounding as a search over all possible bounding boxes. we find the bounding box with highest accumulated score contained in its interior. the search for this box can be solved via an efficient branch and bound 31st conference on neural information processing systems (nips 2017), long beach, ca, usa. a woman in a green shirt is getting ready to throw her bowling ball down the lane... two women wearing hats covered in flowers are posing. young man wearing a hooded jacket sitting on snow in front of mountain area. second bike from right in front painting next to the two on theleft person all the way to the right figure 1: results on the test set for grounding of textual phrases using our branch and bound based algorithm. top row: flickr 30k entities dataset. bottom row: referitgame dataset (groundtruth box in green and predicted box in red). scheme akin to the seminal efficient subwindow search of lampert et al. the learned weights can additionally be used as word embeddings. we are not aware of any method that solves textual grounding in a manner similar to our approach and hope to inspire future research into the direction of deep nets combined with powerful inference algorithms. we evaluate our proposed approach on the challenging referitgame and the flickr 30k entities dataset 35, obtaining results like the ones visualized in fig. at the time of submission, our approach outperformed state-of-the-art techniques on the referitgame and flickr 30k entities dataset by and respectively using the iou metric. we also demonstrate that the trained parameters of our model can be used as a word-embedding which captures spatial-image relationships and provides interpretability.", "section_index": 1}, {"content": "textual grounding: related to textual grounding is work on image retrieval. classical approaches learn a ranking function using recurrent neural nets 30, 6, or metric learning 13, correlation analysis 22, and neural net embeddings 9, beyond work in image retrieval, a variety of techniques have been considered to explicitly ground natural language in images and video. one of the first models in this area was presented in 31, the authors describe an approach that jointly learns visual classifiers and semantic parsers. propose a canonical correlation analysis technique to associate images with descriptive sentences using a latent embedding space. in spirit similar is work by wang et al. 42, which learns a structure-preserving embedding for image-sentence retrieval. it can be applied to phrase localization using a ranking framework. in 11, text is generated for a set of candidate object regions which is subsequently compared to a query. the reverse operation, i.e., generating visual features from query text which is subsequently matched to image regions is discussed in in 23, 3d cuboids are aligned to a set of nouns relevant to indoor scenes using a markov random field based technique. a method for grounding of scene graph queries in images is presented in grounding of dependency tree relations is discussed in and reformulated using recurrent nets in subject-verb-object phrases are considered in to develop a visual knowledge extraction system. their algorithm reasons about the spatial consistency of the configurations of the involved entities. in 15, caption generation techniques are used to score a set of proposal boxes and returning the highest ranking one. to avoid application of a text generation pipeline on bounding box proposals, improve the phrase encoding using a long short-term memory (lstm) based deep net. additional modeling of object context relationship were explored in 32, video 9/5/2017 bbestredraw 1/1 datasets, although not directly related to our work in this paper, were used for spatiotemporal language grounding in 27, common datasets for visual grounding are the referitgame dataset and a newly introduced flickr 30k entities dataset 35, which provides bounding box annotations for noun phrases of the original flickr 30k dataset in contrast to all of the aforementioned methods, which are largely based on region proposals, we suggest usage of efficient subwindow search as a suitable inference engine. efficient subwindow search: efficient subwindow search was proposed by lampert et al. for object localization. it is based on an extremely effective branch and bound scheme that can be applied to a large class of energy functions. the approach has been applied to very efficient deformable part models 43, for object class detection 26, for weakly supervised localization 5, indoor scene understanding 40, diverse object proposals and also for spatio-temporal object detection proposals", "section_index": 2}, {"content": "we outline our approach for textual grounding in fig. in contrast to the aforementioned techniques for textual grounding, which typically use a small set of bounding box proposals, we formulate our language grounding approach as an energy minimization over a large number of bounding boxes. the search over a large number of bounding boxes allows us to retrieve an accurate bounding-box prediction for a given phrase and an image. importantly, by leveraging efficient branch-and-bound techniques, we are able to find the global minimizer for a given energy function very effectively. our energy is based on a set of image concepts’ like semantic segmentations, detections or image priors. all those concepts come in the form of score maps which we combine linearly before searching for the bounding box containing the highest accumulated score over the combined score map. it is trivial to add additional information to our approach by adding additional score maps. moreover, linear combination of score maps reveals importance of score maps for specific queries as well as similarity between queries such as skier’ and snowboarder.’ hence the framework that we discuss in the following is easy to interpret and extend to other settings. general problem formulation: for simplicity we use x to refer to both given input data modalities, i.e., x (q, i), with query text, q, and image, i we will differentiate them in the narrative. in addition, we define a bounding box y via its top left corner (y1, y2) and its bottom right corner (y3, y4) and subsume the four variables of interest in the tuple y (y1, every integral coordinate yi, i 1, , yi,max, and y denotes the product space of all four coordinates. for notational simplicity only, we assume all images to be scaled to identical dimensions, i.e., yi,max is not dependent on the input data x. we obtain a bounding box prediction y given our data x, by solving the energy minimization y arg min yy e(x, y, w), (1) to global optimality. note that w refers to the parameters of our model. despite the fact that we are only’ interested in a single bounding box, the product space y is generally too large for exhaustive minimization of the energy specified in eq. therefore, we pursue a branch-and-bound technique in the following. to apply branch and bound, we assume that the energy function e(x, y, w) depends on two sets of parameters w wtt , w t r t , i.e., the top layer parameters wt of a neural net, and the remaining parameters wr. in light of this decomposition, our approach requires the energy function to be of the following form: e(x, y, w) wtt (x, y, wr). note that the features (x, y, wr) may still depend non-linearly on all but the top-layer parameters. this assumption does not pose a severe restriction since almost all of the present-day deep net models typically obtain the logits e(x, y, w) using a fully-connected layer or a convolutional layer with kernel size as the last computation. energy function details: our energy function e(x, y, w) is based on a set of image concepts,’ such as semantic segmentation of object categories, detections, or word priors, all of which we subsume in the set c. importantly, all image concepts c c are attached a parametric score map c(x,wr) rwh following the image width w and height h note that those parametric score maps may depend nonlinearly on some parameters wr. given a bounding box y, we use the scalar c(x, y, wr) r to refer to the score accumulated within the bounding box y of score map c(x,wr). to define the energy function we also introduce a set of words of interest, i.e., s. note that this set contains a special symbol denoting all other words not of interest for the considered task. we use the given query q, which is part of the data x, to construct indicators, s (s q) 0, 1, denoting for every token s s its existence in the query q, where denotes the indicator function. based on this definition, we formulate the energy function as follows: e(x, y, w) ss:s1 cc ws,cc(x, y, wr), (2) where ws,c is a parameter connecting a word s s to an image concept c c. in other words, wt (ws,c : s s, c c). this energy function results in a sparse wt, which increases the speed of inference. score maps: the energy is given by a linear combination of accumulated score maps c(x, y, wr). in our case, we use c k1 k2 k3 of those maps, which capture three kinds of information: (i) k1 word-priors; (ii) k2 geometric information cues; and (iii) k3 image based segmentations and detections. we discuss each of those maps in the following. approach accuracy () scrc (2016) dspe (2016) grounder (2016) cca (2017) ours (prior geo seg det) ours (prior geo seg bdet) table 1: phrase localization performance on flickr 30k entities. approach accuracy () scrc (2016) grounder (2016) grounder (2016) spat ours (prior geo) ours (prior geo seg) ours (prior geo seg det) table 2: phrase localization performance on referitgame. for the top k1 words in the training set we construct word prior maps like the ones shown in fig. to obtain the prior for a particular word, we search a given training set for each occurrence of the word. with the corresponding subset of image-text pairs and respective bounding box annotations at hand, we compute the average number of times a pixel is covered by a bounding box. to facilitate this operation, we scale each image to a predetermined size. investigating the obtained word priors given in fig. (a) more carefully, it is immediately apparent that they provide accurate location information for many of the words. the k2 geometric cues provide the aspect ratio and the area of the hypothesized bounding box y. note that the word priors and geometry features contain no information about the image specifics. to encode measurements dedicated to the image at hand, we take advantage of semantic segmentation and object detection techniques. the k3 image based features are computed using deep neural nets as proposed by 4, 37, we obtain probability maps for a set of class categories, i.e., a subset of the nouns of interest. the feature accumulates the scores within the hypothesized bounding box y. inference: the algorithm to find the bounding box y with lowest energy as specified in eq. (1) is based on an iterative decomposition of the output space y 25, summarized in fig. to this end we search across subsets of the product space y and we define for every coordinate yi, i 1, , a corresponding lower and upper bound, yi,low and yi,high respectively. more specifically, considering the initial set of all possible bounding boxes y , we divide it into two disjoint subsets y1 and y2. for example, by constraining y1 to 0, , y1,max for y1 and y2 respectively, while keeping all the other intervals unchanged. it is easy to see that we can repeat this decomposition by choosing the largest among the four intervals and recursively dividing it into two parts. given such a repetitive decomposition strategy for the output space, and since the energy e(x, y, w) for a bounding box y is obtained using a linear combination of word priors and accumulated segmentation masks, we can design an efficient branch and bound based search algorithm to exactly solve the inference problem specified in eq. the algorithm proceeds by iteratively decomposing a product space y into two subspaces y1 and y2. for each subspace, the algorithm computes a lower bound e(x,yj , w) for the energy of all possible bounding boxes within the respective subspace. intuitively, we then know, that any bounding box within the subspace yj has a larger energy than the lower bound. the algorithm proceeds by choosing the subspace with lowest lower-bound until this subspace consists of a single element, i.e., until y we summarize this algorithm in alg. to this end, it remains to show how to compute a lower bound e(x,yj , w) on the energy for an output space, and to illustrate the conditions which guarantee convergence to the global minimum of the energy function. for the latter, we note that two conditions are required to ensure convergence to the optimum: (i) the bound of the considered product space has to lower-bound the true energy for each of its bounding box hypothesis y y , i.e., y y , e(x, y, w) e(x, y, w); (ii) the bound has to be exact for all possible bounding boxes y y , i.e., e(x, y, w) e(x, y, w). given those two conditions, global convergence of the algorithm summarized in alg. is apparent: upon termination we obtain an interval’ containing a single bounding box, and its energy is at least as low as the one for any other interval. for the former, we note that bounds on score maps for bounding box intervals can be computed by considering either the largest or the smallest possible bounding box in the bounding box hypothesis, y , depending on whether the corresponding weight in wt is positive or negative and whether the feature maps contain only positive or negative values. intuitively, if the weight is positive and the feature mask contains only positive values, we obtain the smallest lower bound e(x, y, w) by considering the content within the smallest possible bounding box. note that the score maps do not necessarily contain only positive or negative numbers. however we can split the given score maps into two separate score maps (i.e., one with only positive values, and another with only negative values) while applying the same weight. it is important to note that computation of the bound e(x, y, w) has to be extremely effective for the algorithm to run at a reasonable speed. however, computing the feature mask content for a bounding box is trivially possible using integral images. this results in a constant time evaluation of the bound, which is a necessity for the success of the branch and bound procedure. learning the parameters: with the branch and bound based inference procedure at hand, we now describe how to formulate the learning task. support-vector machine intuition can be applied. formally, we are given a training set d (x, y) containing pairs of input data x and groundtruth bounding boxes y. we want to find the parameters w of the energy function e(x, y, w) such that the energy of the groundtruth is smaller than the energy of any other configuration. negating this statement results in the following desiderata when including an additional margin term l(y, y), also known as task-loss, which measures the loss between the groundtruth y and another configuration y: e(x, y, w) e(x, y, w) l(y, y) y y. since we want to enforce this inequality for all configurations y y , we can reduce the number of constraints by enforcing it for the highest scoring right hand side. we then design a cost function which penalizes violation of this requirement linearly. we obtain the following structured support vector machine based surrogate loss minimization: min w c w22 (x,y)d max yy (e(x, y, w) l(y, y)) e(x, y, w) (3) where c is a hyperparameter adjusting the squared norm regularization to the data term. for the task loss l(y, y) we use intersection over union (iou). by fixing the parameters wr and only learning the top layer parameters wt, eq. (3) is equivalent to the problem of training a structured svm. we found the cutting-plane algorithm to work well in our context. the cutting-plane algorithm involves solving the maximization task. this maximization over the output space y is commonly referred to as loss-augmented inference. loss augmented inference is structurally similar to the inference task given in eq. since maximization is identical to negated minimization, the computation of the bounds for the energy e(x, y, w) remains identical. to bound the iou loss, we note that a quotient can be bounded by bounding nominator and denominator independently. to lower bound the intersection of the groundtruth box with the hypothesis space we use the smallest hypothesized bounding box. to upper bound the union of the groundtruth box with the hypothesis space we use the largest bounding box. further, even though not employed to obtain the results in this paper, we mention that it is possible to backpropagate through the neural net parameters wr that influence the energy non-linearly. this underlines that our initial assumption is merely a construct to design an effective inference procedure.", "section_index": 3}, {"content": "in the following we first provide additional details of our implementation before discussing the results of our approach. language processing: in order to process free-form textual phrases efficiently, we restricted the vocabulary size to the top most frequent words in the training set for the referitgame, and to the top most frequent training set words for flickr 30k entities; both choices cover about of all phrases in the training set. we map all the remaining words into an additional token. we don’t differentiate between uppercase and lower case characters and we also ignore punctuation. segmentation and detection maps: we employ semantic segmentation, object detection, and poseestimation. for segmentation, we use the deeplab system 4, trained on pascal voc-2012 semantic image segmentation task, to extract the probability maps for categories. for detection, we use the yolo object detection system 37, to extract categories, trained on pascal voc-2012, and trained on mscoco for pose estimation, we use the system from to extract the body part location, then post-process to get the head, upper body, lower body, and hand regions. for the referitgame, we further fine-tuned the last layer of the deeplab system to include the categories of sky,’ ground,’ building,’ water,’ tree,’ and grass.’ for the flickr 30k entities, we also fine-tuned the last layer of the deeplab system using the eight coarse-grained types and eleven colors from preprocessing and post-processing: for word prior feature maps and the semantic segmentation maps, we take an element-wise logarithm to convert the normalized feature counts into logprobabilities. the summation over a bounding box region then retains the notion of a joint logprobability. we also centered the feature maps to be zero-mean, which corresponds to choosing an initial decision threshold. the feature maps are resized to dimension of for efficient computation, and the predicted box is scaled back to the original image dimension during evaluation. we re-center the prediction box by a constant amount determined using the validation set, as resizing truncate box coordinates to an integer. efficient sub-window search implementation: in order for the efficient subwindow search to run at a reasonable speed, the lower bound on e needs to be computed as fast as possible. observe that, e(x, y, w), is a weighted sum of the feature maps over the region specified by a hypothesized bounding box. to make this computation efficient, we pre-compute integral images. given an integral image, the computation for each of the bounding box is simply a look-up operation. this trick can similarly be applied for the geometric features. since we know the range of the ratio and areas of the bounding boxes ahead of time, we cache the results in a look up table as well. the referitgame dataset consists of more than 99,000 regions from 20,000 images. bounding boxes are assigned to natural language expressions. we use the same bounding boxes as and the same training test set split, i.e., 10,000 images for testing, 9,000 images for training and 1,000 images for validation. the flickr 30k entities dataset consists of more than 275k bounding boxes from 31k image, where each bounding box is annotated with the corresponding natural language phrase. we us the same training, validation and testing split as in quantitative evaluation: in tab. we quantitatively compare the results of our approach to recent state-of-the-art baselines, where prior word priors, geo geometric information, seg segmentation maps, det detection maps, bdet detection maps body parts detection. an example is considered as correct, if the predicted box overlaps with the ground-truth box by more than iou. we observe our approach to outperform competing methods by around on the flickr 30k entities dataset and by around on the referitgame dataset. we also provide an ablation study of the word and image information as shown in tab. we analyze the results for each phrase type provided by flicker30k entities dataset. as can be seen, our system outperforms the state-of-the-art in all phrase types except for clothing. we note that our results have been surpassed by 3, 7, 34, where they fine-tuned the entire network including the feature extractions or trained more feature detectors; cca, grounder and our approach uses a fixed pre-trained network for extracting image features. qualitative evaluation: next we evaluate our approach qualitatively. we observe that our method successfully captures a variety of objects and scenes. we illustrate failure cases. we observe that for a few cases word prior may hurt the prediction (e.g., shoes are typically on the bottom half of the image.) also our system may fail when the energy is not a linear combination of the feature scores. for example, the score of dirt bike should not be the score of dirt the score of bike. we provide additional results in the supplementary material. learned parameters word embedding: recall, in eq. (2), our model learns a parameter per phrase word and concept pair, ws,c. we visualize its magnitude in fig. (a) for a subset of words and concepts. as can be seen, ws,c is large, when the phrase word and the concept are related, (e.g. this demonstrates that our model successfully learns the relationship between phrase words and image concepts. this also means that the word vector, ws ws,1, ws,2, ...ws,c, can be interpreted as a word embedding. (b), we visualize the cosine similarity between pairs of word vectors. expected groups of words form, for example (bicycle, bike), (camera, cellphone), (coffee, cup, drink), (man woman), (snowboarder, skier). the word vectors capture image-spatial relationship of the words, meaning items that can be replaced in an image are similar; (e.g., a snowboarder can be replaced with a skier and the overall image would still be reasonable). computational efficiency: overall, our method’s inference speed is comparable to cca and much faster than grounder. the inference speed can be divided into three main parts, (1) extracting image features, (2) extracting language features, and (3) computing scores. for extracting image features, grounder requires a forward pass on vgg16 for each image region, where cca and our approach requires a single forward pass which can be done in ms. for extracting language features, our method requires index lookups, which takes negligible amount of time (less than 1e-6 ms). cca, uses word2vec for processing the text, which takes ms. grounder uses a long-short-term memory net, which takes ms. computing the scores with our c implementation takes 1.05ms on a cpu. cca needs to compare projections of the text and image features, which takes 13.41ms on a gpu and 609ms on a cpu. grounder uses a single fully connected layer, which takes ms on a gpu.", "section_index": 4}, {"content": "we demonstrated a mechanism for grounding of textual phrases which provides interpretability, is easy to extend, and permits globally optimal inference. in contrast to existing approaches which are generally based on a small set of bounding box proposals, we efficiently search over all possible bounding boxes. we think interpretability, i.e., linking of word and image concepts, is an important concept, particularly for textual grounding, which deserves more attention. acknowledgments: this material is based upon work supported in part by the national science foundation under grant no. this work is supported by nvidia corporation with the donation of a gpu. this work is supported in part by ibm-illinois center for cognitive computing systems research (c3sr) - a research collaboration as part of the ibm cognitive horizons network.", "section_index": 5}], "id": 1012}
{"text": [{"content": "perceptual features (pfs) have been used with great success in tasks such as transfer learning, style transfer, and super-resolution. however, the efficacy of pfs as key source of information for learning generative models is not well studied. we investigate here the use of pfs in the context of learning implicit generative models through moment matching (mm). more specifically, we propose a new effective mm approach that learns implicit generative models by performing mean and covariance matching of features extracted from pretrained convnets. our proposed approach improves upon existing mm methods by: (1) breaking away from the problematic min/max game of adversarial learning; (2) avoiding online learning of kernel functions; and (3) being efficient with respect to both number of used moments and required minibatch size. our experimental results demonstrate that, due to the expressiveness of pfs from pretrained deep convnets, our method achieves stateof-the-art results for challenging benchmarks.", "section_index": 0}, {"content": "the use of features from deep convolutional neural networks (dcnns) pretrained on imagenet has led to important advances in computer vision. dcnn features, usually called perceptual features (pfs), have been used in tasks such as transfer learning 40, 16, style transfer and super-resolution while there have been previous works on the use of pfs in the context of image generation and transformation 7, 17, exploration of pfs as key source of information for learning generative models is not well studied. particularly, the efficacy of pfs for implicit generative models trained through moment matching is an open question. moment matching approaches for generative modeling are based on the assumption that one can learn the data distribution by matching the moments of the model distribution to the empirical data distribution. two representative meth- equal contribution. ods of this family are based on maximum mean discrepancy by using a specially designed objective function. in this work we demonstrate that, by using pfs to perform moment matching, one can overcome some of the difficulties found in current moment matching approaches. more specifically, we propose a simple but effective moment matching method that: (1) breaks away from the problematic min/max game completely; (2) does not use online learning of kernel functions; and (3) is very efficient with regard to both number of used moments and required minibatch size. our proposed approach, named generative feature matching networks (gfmn), learns implicit generative models by performing mean and covariance matching of features extracted from all convolutional layers of pretrained deep convnets. some interesting properties of gfmns include: (a) the loss function is directly correlated to the generated image quality; (b) mode collapsing is not an issue; and (c) the same pretrained feature extractor can be used across different datasets. we perform an extensive number of experiments with different challenging datasets: cifar10, stl10, celeba and lsun. we demonstrate that our approach can achieve state-of-the-art results for challenging benchmarks such as cifar10 and stl10. moreover, we show that the same ar x iv :1 2v cs .c v a pr feature extractor is effective across different datasets. the main contributions of this work can be summarized as follows: (1) we propose a new effective moment matchingbased approach to train implicit generative models that does not use adversarial or online learning of kernel functions, provides stable training, and achieves state-of-the-art results; (2) we show theoretical results that demonstrate gfmn convergence under the assumption of the universality of perceptual features; (3) we propose an adam-based moving average method that allows effective training with small minibatches; (4) our extensive quantitative and qualitative experimental results demonstrate that pretrained autoencoders and dcnn classifiers can be effectively used as (cross-domain) feature extractors for gfmn training.", "section_index": 1}, {"content": "let g be the generator implemented as a neural network with parameters , and let e be a pretrained neural network with l hidden layers. our proposed approach consists in training g by minimizing the following loss function: min m j1 jpdata jpg()2 jpdata jpg()2 (1) where: jpdata expdataej(x) rdj jpg() ezn (0,inz )ej(g(z; )) r dj jpdata, expdataej,(x) j,pdata 2, dj and is the l2 loss; x is a real data point sampled from the data generating distribution pdata; z rnz is a noise vector sampled from the normal distribution n (0, inz ); ej(x), denotes the output vector/feature map of the hidden layer j from e; m l is the number of hidden layers used to perform feature matching. note that 2pdata and pg denote the variances of the features from real data and generated data, respectively. we use diagonal covariance matrices as computing full covariance matrices is impractical for large numbers of features. in practice, we train g by first precomputing estimates of jpdata and j pdata on the training data, then running multiple training iterations where we sample a minibatch of generated (fake) data and optimize the parameters using stochastic gradient descent (sgd) with backpropagation. the network e is used for the purpose of feature extraction only and is kept fixed during the training of g. fig. presents gfmn training pipeline. autoencoder features: a natural choice of unsupervised method to train a feature extractor is the autoencoder (ae) framework. the decoder part of an ae consists exactly of an image generator that uses features extracted by the encoder. therefore, by design, the encoder network should be a good feature extractor for the purpose of generation. classifier features: we experiment with different dcnn architectures pretrained on imagenet to play the role of the feature extractor e. our hypothesis is that imagenet-based pfs are informative enough to allow the training of (crossdomain) generators by feature matching. from feature matching loss to moving averages. in order to train with a mean and covariance feature matching loss, one needs large minibatches to obtain good mean and covariance estimates. with images larger than 3232, dcnns produce millions of features, resulting easily in memory issues. we propose to alleviate this problem by using moving averages of the difference of means (covariances) of real and generated data. instead of computing the (memory) expensive feature matching loss in eq. 1, we keep moving averages vj of the difference of feature means (covariances) at layer j between real and generated data. we detail our moving average strategy for the mean features only, but the same approach applies for the covariances. the mean features from the first term of eq. 1, jpdataezn (0,inz )ej(g(z; ))2 can be approximated by: vj ( jpdata n n k1 ej(g(zk; )) ) , where n is the minibatch size and vj is a moving average on j , the difference of the means of the features extracted by the j-th layer of e: j j pdata n n k1 ej(g(zk; )). (2) using these moving averages we replace the first term of the loss given in eq. by min m j1 vj ( jpdata n n k1 ej(g(zk; )) ) (3) the moving average formulation of features matching above has a major advantage on the naive formulation of eq. since we can now rely on vj to get better estimates of the population feature means of real and generated data while using a small minibatch of sizen for a similar result using the feature matching loss given in eq. 1, one would need a minibatch with large size n , which is problematic for large number of features. adam moving average: from sgd to adam updates. note that for a rate , the moving average vj has the following update: vj,new (1 ) vj,old j ,j .m it is easy to see that the moving average is a gradient descent update on the following loss: min vj vj j (4) hence, writing the gradient update with learning rate we have equivalently: vj,new vj,old(vj,oldj) (1)vj,oldj with this interpretation of the moving average, we propose to get a better moving average estimate by using the adam optimizer on the loss of the moving average given in eq. 4, such that vj,new vj,old adam(vj,old j). adam(x) function is computed as follows: mt mt1 (1 1) x mt mt/(1 t1) ut ut1 (1 2) x2 ut ut/(1 t2) adam(x) mt/( ut ), where x is the gradient for the loss function in eq. 4, t is the iteration number,mt and ut are the first and second moment vectors at iteration t, .9, and are constants. m0 and u0 are initialized as proposed by we refer to for a detailed adam optimizer description. this moving average formulation, which we call adam moving average (ama) promotes stable training when using small minibatches. although we detail ama using mean feature matching only, we use this approach for both mean and covariance matching. the main advantage of ama over simple moving average (ma) is in its adaptive first and second order moments that ensure stable estimation of the moving averages vj in fact, this is a non-stationary estimation since the mean of the generated data changes in the training, and it is well known that adam works well for such online non-stationary losses in section we provide experimental results supporting: (1) the memory advantage that the ama formulation of feature matching offers over the naive implementation; (2) the stability advantage and improved generation results that ama allows compared to the naive implementation. we discuss in appendix the advantage of ama on ma from a regret bounds point of view", "section_index": 2}, {"content": "our proposed approach is related to the recent body of work on mmd or mm based generative models 22, 8, 21, 3, we highlight the main differences between mmd-gans and gfmn in terms of requirements on the kernel for mmd-gan and on the feature map (extractor) for gfmn, that ensure convergence of the generator to the data distribution. gmmn, mmd-gan convergence: mmd matching with universal kernels. we start by reviewing known results on mmd. let hk be a reproducing kernel hilbert space (rkhs) defined with a continuous kernel k. informally, k is universal if any bounded continuous function can be approximated to an arbitrary precision in hk (formal definition in appendix ). theorem shows that the mmd is a well defined metric for universal kernels. given a kernel k, let p, q be two distributions, their mmd is: mmd2(k, p, q) p q2hk , where p expkx is the mean embedding. if k is universal then mmd2(k, p, q) if and only if p q. given a universal kernel such as a gaussian kernel as outlined in gmmn 22, 8, one can learn implicit generative models g that defines a family of distribution q by minimizing the mmd distance: inf mmd(k, pdata, q) (5) assuming pdata is in the family q (, q pdata), the infimum of mmd minimization for a universal kernel is achieved for q pdata (immediate consequence of theorem 1). this elegant setup for mmd matching with universal kernels, while avoiding the difficult min/max game in gan, does not translate into good results in image generation. to remedy that, other discrepancies introduced in 21, 3, compose universal kernels k with a feature map as follows: dmmd(p, q) sup mmd(k , p, q). for learning implicit generative models replaces mmd in eq. under conditions on the kernel and the learned feature map this discrepancy is continuous in the weak topology (prop. nevertheless, learning generative models remains challenging with it as it boils down to a min/max game as in original gan gfmn convergence: mmd matching with universal features. while universality is usually thought on the kernel level, it is not straightforward to define universality for kernels defined by feature maps. define universality of feature maps and how it connects to their corresponding kernels. specifically for a fixed feature set on a space x ), are dense in the set of continuous bounded functions (formal definition in appendix 1). this is of interest since gfmn corresponds to mmd matching with a kernel k defined on a fixed feature map (x)j(x)ji , where i is finite. we have k(x, y) (x),(y)ji j(x)j(y) and mmd2(k, p, q) exp(x) exq(x)2 for mmd2(k, p, q) to be a metric it is enough to have the set features s be universal (by thm.1 and thm. gives conditions for gfmn convergence: proposition assume pdata belongs to the family defined by the generator q. gfmn converges to the real distribution by matching in a feature space s j , j i, where i is a countable set, if the features set s is universal (informally means that any continuous functions can be written as linear combination in the span of s) s is universal k is universal hence mmd(k, pdata, q) iff q pdata. gfmn solves inf mmd2(k, pdata, q), and the infimum is achieved for such that q pdata ( pdata q ). the analysis covers here mean matching but the same applies to covariance matching considering s j , jk, j, k i. universality of perceptual features in computer vision. we see that for gfmn to be convergent with pretrained feature extractors ej that are perceptual features (such as features from vgg or resnet pretrained on imagenet), we need to assume universality of those features in the image domain. we know from transfer learning that features from imagenet pretrained vgg/resnet can express any functions for a downstream task by finding a linear weight in their span. note that this is the definition of universal feature as given in 26: continuous functions can be approximated in the linear span of those features. hence, assuming universality of pfs defined by imagenet pretrained vgg or resnet, gfmn is guaranteed to converge to the data distribution by prop. our results complement the common wisdom on universality of pfs in transfer learning and style transfer by showing that they are sufficient for learning implicit generative models.", "section_index": 3}, {"content": "gfmn is related to the recent body of work on mmd and moment matching based generative models 22, 8, 21, 3, the closest to our method is the generative moment matching network autoencoder (gmmnae) proposed in in gmmnae, the objective is to train a generator g that maps from a prior uniform distribution to the latent code learned by a pretrained ae, and then uses the frozen pretrained decoder to map back to image space. as discussed in section one key difference in our approach is that, while gmmnae uses a gaussian kernel to perform moment matching using the ae low dimensional latent code, gfmn performs mean and covariance matching in a pf space induced by a non-linear kernel function (a dcnn) that is orders of magnitude larger than the ae latent code, and that we argued is universal in the image domain. demonstrate that gmmnae is not competitive with gans for challenging datasets such as cifar10. mmd-gans, discussed in section 3, demonstrated competitive results with the use of adversarial learning by learning a feature map in conjuction with a gaussian kernel 21, recently proposed a method to perform online learning of the moments while training the generator. our proposed method differs by using fixed pretrained pf extractors for moment matching. proposed the generative latent optimization (glo) model that jointly optimizes model parameters and noise input vectors z, while avoiding adversarial training. our work relates also to plug and play generative models of where a pretrained classifier is used to sample new images, using mcmc sampling methods. our work is also related to ae-based generative models variational ae (vae) 19, adversarial ae (aae) and wasserstein ae (wae) however, gfmn is quite distinct from these methods because it uses pretrained aes to play the role of feature extractors only, while these methods aim to impose a prior distrib. on the latent space of aes. another recent line of work that involves the use of aes in generative models consists in applying aes to improve gans stability 42, finally, our objective function is related to the mcgan loss function 29, where authors match first and second order moments.", "section_index": 4}, {"content": "datasets: we evaluate our proposed approach on images from cifar10 (50k train., 10k test, classes), stl10 (5k train., 8k test, 100k unlabeled, classes), celeba (200k) and lsun bedrooms datasets. stl10 images are rescaled to 3232, while celeba and lsun images are rescaled to either or 128128, depending on the experiment. celeba images are center-cropped to before rescaling. gfmn generator: in most of our experiments the generator g uses a dcgan-like architecture for cifar10, stl10, lsun and celeba6464, we use two extra layers as commonly used in previous works 28, for celeba128128 and some experiments with cifar10 and stl10, we use a resnet-based generator such as the one in architecture details are in the supplementary material. autoencoder features: for most ae experiments, we use an encoder network whose architecture is similar to the discriminator in dcgan (strided convolutions). we use batch normalization and relu non-linearity after each convolution. we set the latent code size to 128, 128, and for cifar10, stl10 and celeba, respectively. to perform feature extraction, we get the output of each relu in the network. additionally, we also perform some experiments where the encoder uses a vgg19 architecture. the decoder network d uses a network architecture similar to our generator g. more details in the supplementary material. classifier features: we perform our experiments on classifier features with vgg19 and resnet18 networks which we pretrained using the whole imagenet dataset with classes. pretrained imagenet classifiers details can be found in the supplementary material. gfmn training: gfmns are trained with an adam optimizer; most hyperparameters are kept fixed across datasets. we use nz and minibatch of dataset dependent learning rates are used for updating g (104 or 5105) and ama (5105 or 105). we use ama moving average (sec. 2.2) in all reported experiments. this section presents a comparative study on the use of pretrained autoencoders and cross-domain classifiers as feature extractors in gfmn. shows the inception score (is) and frechet inception distance (fid) for gfmn trained on cifar10 using different feature extractors e. the two first rows in tab. correspond to gfmn models that use pretrained encoders ase, while the last four rows use pretrained vgg19/resnet18 imagenet classifiers. that there is a large boost in performance when imagenet classifiers are used as feature extractors instead of encoders. despite the classifiers being trained on a different domain (imagenet vs. cifar10), the classifier features are significantly more effective. while the best is with encoders is 4.95, the lowest is with imagenet classifier is additionally, when using simultaneously vgg19 and resnet18 as feature extractors (two last rows), which increases the number of features to 832k, we get even better performance. finally, we achieve the best performance in terms of both is and fid (last row1) when using a generator architecture that contains residual blocks, similar to the one propose in random samples from gfmnvgg19resnet18 trained with cifar10 and stl10 are shown in figs. 2c shows random samples from gfmnvgg19 trained with lsun bedrooms dataset (resolution 6464). presents samples from gfmnvgg19 trained with celeba dataset with resolution 128128, which shows that gfmn can achieve good performance with image resolutions larger than these results also demonstrate that: (1) the same classifier (vgg19 trained on imagenet) can be successfully applied to train gfmn models across different domains; (2) perceptual features from dcnns encapsulate enough statistics to allow the learning of good generative models through moment matching. shows is and fid for increasing number of layers (i.e. number of features) in our extractor vgg19. we se- 1average result of five runs with different random seeds. lect up to layers, excluding the output of fully connected layers. using more layers dramatically improves the performance of the feature extractor, reaching is and fid peak performance when the maximum number of layers is used. note that the features are relu activation outputs, meaning the encodings may be quite sparse. in appendix we show qualitative results that corroborate these results. to verify whether the number of features is the main factor for performance, we conducted an experiment where we train an ae with an encoder using a vgg19 architecture. this encoder is pretrained on imagenet and produces a total of 296k features. shows the results for this experiment. although there is improvement in both is and fid compared to the dcgan encoder (first row), the boost is not comparable to the one obtained with a vgg19 classifier. in other words, features from classifiers are significantly more informative than aes features for the purpose of training generators by feature matching. this section presents experimental results that evidence the advantage of our proposed adam moving average (ama) over the simple moving average (ma). the main benefit of ama is the promotion of stable training when using small minibatches. the ability to train with small minibatches is essential due to gfmn’s need for large number of features from dcnns, which becomes a challenge in terms of gpu memory usage. our pytorch implementation of gfmn can only handle minibatches of size up to when using vgg19 as a feature extractor and image size on a tesla k40 gpu w/ 12gb of memory. a more optimized implementation minimizing memory overhead could, in principle, handle somewhat larger minibatch sizes (as could a more recent tesla v100 w/ gb). however, increase image size or feature extractor size and the memory footprint increases quickly. we will always run out of memory when using larger minibatches, regardless of implementation or hardware. all experiments in this section use celeba training set, and a feature extractor using the encoder from an ae following a dcgan-like architecture. this feature extractor is smaller than vgg19/resnet18 allowing for minibatches of size up to for image size shows generated images from gfmn trained with either ma or our proposed ama. for ma, generated images from gfmn trained with and minibatch size are presented in figs. 4c shows results for minibatch size in ma training, the minibatch size has a tremendous impact on the quality of generated images: with minibatches smaller than 512, almost all images generated are quite distorted. on the other hand, when using ama, gfmn generates much better images with minibatch size (fig. for ama, increasing the minibatch size from to does not improve the quality of generated images for the given dataset and feature extractor. in the supplementary material, we show a comparison between ma and ama with vgg19 imagenet classifier as feature extractor for a minibatch size of ama also displays a very positive effect on the quality of generated images when a stronger feature extractor is used. an alternative for training with larger minibatches would be the use of multi-gpu, multi-node setups. however, performing large scale experiments is beyond the scope of the current work. moreover, many practitioners do not have access to a gpu cluster, and the development of methods that can also work on a single gpu with small memory footprint is essential. an important advantage of gfmn over adversarial methods is its training stability. shows the evolution of the generator loss per epoch and generated examples when using ama. there is a clear correlation between the quality of generated images and the loss. moreover, mode collapsing was not observed in our experiments with ama. 4, we compare gfmn results with different adversarial and non-adversarial approaches for cifar10 and stl10. in the middle part of the table, we report results for recent unsupervised models that use a dcganlike architecture in the generator. despite using a frozen cross-domain feature extractor, gfmn outperforms the unsupervised systems in is and fid for both datasets. includes results for supervised approaches. some of these models use a resnet architecture in the generator as indicated in parenthesis. note that ganbased methods that perform conditional generation use direct feedback from the labels in the form of log likelihoods from the discriminator (e.g. in contrast, our generator is trained with a loss function that only performs feature matching. our generator is agnostic to the labels and there is no feedback in the form of a log likelihood from the labeled data. despite that, gfmn produces results that are at the same level of supervised gan models that use labels from the target dataset. we performed additional experiments with a wgangp architecture where: (1) the discriminator is a vgg19 or a resnet18; (2) the discriminator is pretrained on imagenet. the goal was to evaluate if wgan-gp can benefit from dcnn classifiers pretrained on imagenet. although we tried different hyperparameter combinations, we were not able to successfully train wgan-gp with vgg19 or resnet18 discriminators (details in appendix 8).", "section_index": 5}, {"content": "we achieve successful non-adversarial training of implicit generative models by introducing different key ingredients: (1) moment matching on perceptual features from all layers of pretrained neural networks; (2) a more robust way to compute the moving average of the mean features by using adam optimizer, which allows us to use small minibatches; and (3) the use of perceptual features from multiple neural networks at the same time (vgg19 resnet18). our quantitative results in tab. show that gfmn achieves better or similar results compared to the state-ofthe-art spectral gan (sn-gan) for both cifar10 and stl10. this is an impressive result for a nonadversarial feature matching-based approach that uses pretrained cross-domain feature extractors and has stable train- ing. when compared to mmd approaches 22, 8, 21, 3, 33, gfmn presents important distinctions (some of them already listed in secs. and 4) which make it an attractive alternative. compared to gmmn and gmmnae 22, we can see in tab. that gfmn achieves far better results. in the supplementary material, we also show a qualitative comparison between gfmn and gmmn results. compared to recent adversarial mmd methods (mmd gan) 21, gfmn also presents significantly better results while avoiding the problematic min/max game. gfmn achieves better results than the method of learned moments (molm) 33, while using a much smaller number of features to perform matching. the best performing model from 33, molm1536, uses around million moments to train the cifar10 generator, while our best gfmn model uses around 850k moments/features only, almost 50x less. one may argue that the best gfmn results are obtained with feature extractors trained with classifiers. however, there are two important points to note: (1) we use a cross domain feature extractor and do not use labels from the target datasets (cifar10, stl10, lsun, celeba); (2) classifier accuracy does not seem to be the most important factor for generating good features: vgg19 classifier produces features as good as the ones from resnet18, although the former is less accurate (more details in supplementary material). we are confident that gfmn can achieve state-ofthe-art results with features from classifiers trained with unsupervised methods such as in conclusion, this work presents important theoretical and practical contributions that shed light on the effectiveness of perceptual features for training implicit generative models through moment matching.", "section_index": 6}, {"content": "convergence we summarize here the main definitions and theorems from regarding universality of kernels and feature maps. the following defines a universal kernel definition (universal kernel). given a kernel k defined on x x let z be any compact subset of x define the space of kernel sections: k(z) spanky, y z, where ky : x r, ky(x) k(x, y). let c(z) be the space of all continuous real valued functions defined on z a kernel is said universal if for any choice of z (compact subset of x ) k(z) is dense in c(z). in other words a kernel is universal if c(z) k(z). meaning if any continuous function can be expressed in the span of ky universal feature maps.we turn now for kernels defined by feature maps and how to characterize their universality. consider a continuous feature map : x w , where (w, , w) is a hilbert space; the kernel k has the following form: k(x, y) (x),(y)w (6) let y be an orthonormal basis of w define the following continuous function fy c(z) defined at x z: fy(x) (x), yw , and let: (y) spanfy, y y definition (universal feature map). a feature map is universal if (y) is dense in c(z), for all z compact subsets of x .i.e a feature map is universal if (y) c(z). the following theorem shows the relation between universality of a kernel defined by feature map and the universality of the feature map: theorem (26, thm 4, relation between k(z) and (y) ). for kernel defined by feature maps in (6) we have k(z) (y). a kernel of form (6) is universal if and only if its feature map is universal. hence the following theorem from 26: theorem (26). let s j , j i, where i is a countable set and j : x r continuous function. define the following kernel k(x, y) ji j(x)j(y). as we already discussed the moving average of v of the difference of features means t n n i1 e(xi) n n i1 e(g(zi, t)) between real and generated data at each time step t in the gradient descent up to time t , can be seen as a gradient descent in an online setting on the following cost : f min v t t1 ft(v) t t1 v t22 note that we are in the online setting since t is only known when t of the generator is updated. the sequence vt generated by ma (moving average) and by ama (adam moving average) is the sgd updates and adam updates respectively applied to the cost function ft. hence we can bound the regret of the sequence vmat and vamat using known results on sgd and adam. let d be the dimension of the encoding e. for ma, using classic regret bounds for gradient descents we obtain: rmat t t1 vmat t22 f o( dt ). for ama, using adam regrets bounds from let us define ramat t t1 vamat t22 f. we have: ramat o( t d i1 u t, i ) o d i1 t t1 (t,i vamat,i )2 c where u are defined in the adam updates as moving averages of second order moments of the gradients. the regret bound of ama is better than ma especially ifd i1 u t, i d and d i1 t t1 (t,i vamat,i )2 td. in this appendix, we present comparative results between gfmn with mean feature matching vs. gfmn with mean covariance feature matching. using the first and second moments to perform feature matching gives statistical advantage over using the first moment only. in table 5, we can see that for different feature extractors, performing mean covariance feature matching produces significantly better results in terms of both is and fid. have also demonstrated the advantages of using mean covariance matching in the context of gans. in tables and 7, and figure we detail the neural net architectures used in our experiments. in both dcgan-like generator and discriminator, an extra layer is added when using images of size in vgg19 architecture, after each convolution, we apply batch normalization and relu. the resnet generator is used for celeba128128 experiments and also for some experiments with cifar10 and stl10. for these two last datasets, the resnet generator has resblocks only, and the output size of the dense layer is both vgg19 and resnet18 networks are trained with sgd with fixed learning rate, momentum term, and weight decay set to we pick models with best top-1 accuracy on the validation set over epochs of training; for vgg19 (image size 3232), and for resnet18 (image size 3232). when training the classifiers we use random cropping and random horizontal flipping for data augmentation. when using vgg19 and resnet18 as feature extractors in gfmn, we use features from the output of each relu that follows a conv. layer, for a total of layers for vgg and for resnet18. in our experiments with autoencoders (ae) we pretrained them using either mean squared error (mse) or the laplacian pyramid loss 23, let e and d be the encoder and the decoder networks with parameters and , respectively. min , epdata xd(e(x;);)2 or the laplacian pyramid loss lap1(x, x ) j 22j lj(x) lj(x)1 where lj(x) is the j-th level of the laplacian pyramid representation of x. the laplacian pyramid loss provides better signal for learning high frequencies of images and overcome some of the blurriness issue known from using a simple mse loss. recently demonstrated that the lap1 loss produces better results than l2 loss for both autoencoders and generative models. we evaluate our models using two quantitative metrics: inception score (is) and frechet inception distance (fid) we followed the same procedure used in previous work to calculate is 36, 27, for each trained generator, we calculate the is for randomly generated images and repeat this procedure times (for a total of 50k generated images) and report the average and the standard deviation of the is. we compute fid using two sample sizes of generated images: 5k and 50k. in order to be consistent with previous works 27, and be able to directly compare our quantitative results with theirs, the fid is computed as follows: cifar10: the statistics for the real data are computed using the 50k training images. this (real data) statistics are used in the fid computation of both 5k and 50k samples of generated images. this is consistent with both miyato et al. procedure to compute fid for cifar10 experiments. stl10: when using 5k generated images, the statistics for the real data are computed using the set of 5k (labeled) training images. this is consistent with the fid fid computation is repeated times and the average is reported. there is very small variance in the fid results. extraction figure shows generated images from generators that were trained with a different number of layers employed to feature matching. in all the results in fig.7, the vgg19 network was used to perform feature extraction. we can see a significant improvement in image quality when more layers are used. better results are achieved when or more layers are used, which corroborates the quantitative results in sec. wgan-gp the objective of the experiments presented in this section is to evaluate if wgan-gp can benefit from dcnn classifiers pretrained on imagenet. in the experiments, we used a wgan-gp architecture where: (1) the discriminator is a vgg19 or a resnet18; (2) the discriminator is pretrained on imagenet; (3) the generator is pretrained on cifar10 through autoencoding. although we tried different hyperparameter combinations, we were not able to successfully train wgan-gp with vgg19 or resnet18 discriminators. indeed, the discriminator, being pretrained on imagenet, can quickly learn to distinguish between real and fake images. this limits the reliability of the gradient information from the discriminator, which in turn renders the training of a proper generator extremely challenging or even impossible. this is a well-known issue with gan training where the training of the generator and discriminator must strike a balance. this phenomenon is covered in section (illustrated in their figure 2) as one motivation for work like wassertein gans. if a discriminator can distinguish perfectly between real and fake early on, the generator cannot learn properly and the min/max game becomes unbalanced, having no good discriminator gradients for the generator to learn from, producing degenerate models. figure shows some examples of images generated by the unsuccessfully trained models. in this appendix, we present a comparison between the simple moving average (ma) and adam moving average (ama) for the case where vgg19 imagenet classifier is used as a feature extractor. this experiment uses a minibatch size of that ama has a very positive effect in the quality of generated images. gfmn trained with ma produces various images with some sort of crossing line artifacts. figure shows a visual comparison between images generated by gfmn (figs. 10a and 10b) and generative moment matching networks (gmmn) (figs. gmmn generated images were obtained from li et al. in this experiment, both gmmn and gfmn use a dcgan-like architecture in the generator. images generated by gfmn have significantly better quality compared to the ones generated by gmmn, which corroborates the quantitative results in sec. in this appendix, we present a comparison in image quality for autoencoder features vs. vgg19 features for the celeba dataset. we show results for both simple moving average (ma) and adam moving average (ama), for both cases we use a minibatch size of 11, we show generated images from gfmn trained with either vgg19 features (top row) or autoencoder (ae) features (bottom row). we show images generated by gfmn models trained with simple moving average (ma) and adam moving average (ama). we can note in the images that, although vgg19 features are from a cross-domain classifier, they lead to much better generation quality than ae features, specially for the ma case.", "section_index": 7}], "id": 1013}
{"text": [{"content": "recommendation for e-commerce with a mix of durable and nondurable goods has characteristics that distinguish it from the well-studied media recommendation problem. the demand for items is a combined effect of form utility and time utility, i.e., a product must both be intrinsically appealing to a consumer and the time must be right for purchase. in particular for durable goods, time utility is a function of inter-purchase duration within product category because consumers are unlikely to purchase two items in the same category in close temporal succession. moreover, purchase data, in contrast to rating data, is implicit with non-purchases not necessarily indicating dislike. together, these issues give rise to the positive-unlabeled demand-aware recommendation problem that we pose via joint low-rank tensor completion and product category inter-purchase duration vector estimation. we further relax this problem and propose a highly scalable alternating minimization approach with which we can solve problems with millions of users and millions of items in a single thread. we also show superior prediction accuracies on multiple real-world datasets.", "section_index": 0}, {"content": "e-commerce recommender systems aim to present items with high utility to the consumers utility may be decomposed into form utility: the item is desired as it is manifested, and time utility: the item is desired at the given point in time 28; recommender systems should take both types of utility into account. economists define items to be either durable goods or nondurable goods based on how long they are intended to last before being replaced a key characteristic of durable goods is the long duration of time between successive purchases within item categories whereas this duration for nondurable goods is much shorter, or even negligible. thus, durable and nondurable goods have differing time utility characteristics which lead to differing demand characteristics. although we have witnessed great success of collaborative filtering in media recommendation, we should be careful when expanding its application to general e-commerce recommendation involving both durable and nondurable goods due to the following reasons: since media such as movies and music are nondurable goods, most users are quite receptive to buying or renting them in rapid succession. however, users only purchase durable goods when the time is right. for instance, most users will not buy televisions the day after they have already bought one. therefore, recommending an item for which a user has no immediate demand can hurt user experience and waste an opportunity to drive sales. now at tencent ai lab, bellevue, wa, usa 31st conference on neural information processing systems (nips 2017), long beach, ca, usa. a key assumption made by matrix factorization- and completion-based collaborative filtering algorithms is that the underlying rating matrix is of low-rank since only a few factors typically contribute to an individual’s form utility however, a user’s demand is not only driven by form utility, but is the combined effect of both form utility and time utility. hence, even if the underlying form utility matrix is of low-rank, the overall purchase intention matrix is likely to be of high-rank,2 and thus cannot be directly recovered by existing approaches. an additional challenge faced by many real-world recommender systems is the one-sided sampling of implicit feedback 15, unlike the netflix-like setting that provides both positive and negative feedback (high and low ratings), no negative feedback is available in many e-commerce systems. for example, a user might not purchase an item because she does not derive utility from it, or just because she was simply unaware of it or plans to buy it in the future. in this sense, the labeled training data only draws from the positive class, and the unlabeled data is a mixture of positive and negative samples, a problem usually referred to as positive-unlabeled (pu) learning to address these issues, we study the problem of demand-aware recommendation. given purchase triplets (user, item, time) and item categories, the objective is to make recommendations based on users’ overall predicted combination of form utility and time utility. we denote purchases by the sparse binary tensor p to model implicit feedback, we assume that p is obtained by thresholding an underlying real-valued utility tensor to a binary tensor y and then revealing a subset of y’s positive entries. the key to demand-aware recommendation is defining an appropriate utility measure for all (user, item, time) triplets. to this end, we quantify purchase intention as a combined effect of form utility and time utility. specifically, we model a user’s time utility for an item by comparing the time t since her most recent purchase within the item’s category and the item category’s underlying inter-purchase duration d; the larger the value of d t, the less likely she needs this item. in contrast, d t may indicate that the item needs to be replaced, and she may be open to related recommendations. therefore, the function h max(0, d t) may be employed to measure the time utility factor for a (user, item) pair. then the purchase intention for a (user, item, time) triplet is given by x h, where x denotes the user’s form utility. this observation allows us to cast demand-aware recommendation as the problem of learning users’ form utility tensor x and items’ inter-purchase durations vector d given the binary tensor p although the learning problem can be naturally formulated as a tensor nuclear norm minimization problem, the high computational cost significantly limits its application to large-scale recommendation problems. to address this limitation, we first relax the problem to a matrix optimization problem with a label-dependent loss. we note that the problem after relaxation is still non-trivial to solve since it is a highly non-smooth problem with nested hinge losses. more severely, the optimization problem involves mnl entries, where m, n, and l are the number of users, items, and time slots, respectively. thus a naive optimization algorithm will take at least o(mnl) time, and is intractable for largescale recommendation problems. to overcome this limitation, we develop an efficient alternating minimization algorithm and show that its time complexity is only approximately proportional to the number of nonzero elements in the purchase records tensor p since p is usually very sparse, our algorithm is extremely efficient and can solve problems with millions of users and items. compared to existing recommender systems, our work has the following contributions and advantages: (i) to the best of our knowledge, this is the first work that makes demand-aware recommendation by considering inter-purchase durations for durable and nondurable goods; (ii) the proposed algorithm is able to simultaneously infer items’ inter-purchase durations and users’ real-time purchase intentions, which can help e-retailers make more informed decisions on inventory planning and marketing strategy; (iii) by effectively exploiting sparsity, the proposed algorithm is extremely efficient and able to handle large-scale recommendation problems.", "section_index": 1}, {"content": "our contributions herein relate to three different areas of prior work: consumer modeling from a microeconomics and marketing perspective 6, time-aware recommender systems 4, 29, 8, 19, and pu learning 20, 9, 13, 14, 23, the extensive consumer modeling literature is concerned with descriptive and analytical models of choice rather than prediction or recommendation, but nonetheless 2a detailed illustration can be found in the supplementary material forms the basis for our modeling approach. a variety of time-aware recommender systems have been proposed to exploit time information, but none of them explicitly consider the notion of time utility derived from inter-purchase durations in item categories. much of the pu learning literature is focused on the binary classification problem, e.g. 20, 9, whereas we are in the collaborative filtering setting. for the papers that do examine collaborative filtering with pu learning or learning with implicit feedback 14, 23, 2, 32, they mainly focus on media recommendation and overlook users’ demands, thus are not suitable for durable goods recommendation. temporal aspects of the recommendation problem have been examined in a few ways: as part of the cold-start problem 3, to capture dynamics in interests or ratings over time 17, and as part of the context in context-aware recommenders however, the problem we address in this paper is different from all of those aspects, and in fact could be combined with the other aspects in future solutions. to the best of our knowledge, there is no existing work that tries to take inter-purchase durations into account to better time recommendations as we do herein.", "section_index": 2}, {"content": "throughout the paper, we use boldface euler script letters, boldface capital letters, and boldface lower-case letters to denote tensors (e.g., a), matrices (e.g., a) and vectors (e.g., a), respectively. scalars such as entries of tensors, matrices, and vectors are denoted by lowercase letters, e.g., a. in particular, the (i, j, k) entry of a third-order tensor a is denoted by aijk. given a set of m users, n items, and l time slots, we construct a third-order binary tensor p 0, 1mnl to represent the purchase history. specifically, entry pijk indicates that user i has purchased item j in time slot k. we denote p0 as the number of nonzero entries in tensor p since p is usually very sparse, we have p0 mnl. also, we assume that the n items belong to r item categories, with items in each category sharing similar inter-purchase durations.3 we use an n-dimensional vector c 1, 2, , rn to represent the category membership of each item. given p and c, we further generate a tensor t rmrl where ticjk denotes the number of time slots between user i’s most recent purchase within item category cj until time k. if user i has not purchased within item category cj until time k, ticjk is set to in this work, we formulate users’ utility as a combined effect of form utility and time utility. to this end, we use an underlying third-order tensor x rmnl to quantify form utility. in addition, we employ a non-negative vector d rr to measure the underlying inter-purchase duration times of the r item categories. it is understood that the inter-purchase durations for durable good categories are large, while for nondurable good categories are small, or even zero. in this study, we focus on items’ inherent properties and assume that the inter-purchase durations are user-independent. the problem of learning personalized durations will be studied in our future work. as discussed above, the demand is mediated by the time elapsed since the last purchase of an item in the same category. let dcj be the inter-purchase duration time of item j’s category cj , and let ticjk be the time gap of user i’s most recent purchase within item category cj until time k. then if dcj ticjk, a previously purchased item in category cj continues to be useful, and thus user i’s utility from item j is weak. intuitively, the greater the value dcj ticjk, the weaker the utility. on the other hand, dcj ticjk indicates that the item is nearing the end of its lifetime and the user may be open to recommendations in category cj we use a hinge loss max(0, dcj ticjk) to model such time utility. the overall utility can be obtained by comparing form utility and time utility. in more detail, we model a binary utility indicator tensor y 0, 1mnl as being generated by the following thresholding process: yijk 1xijk max(0, dcj ticjk) , (1) where 1() : r 0, is the indicator function, and is a predefined threshold. 3to meet this requirement, the granularity of categories should be properly selected. for instance, the category smart tv is a better choice than the category electrical equipment, since the latter category covers a broad range of goods with different durations. note that the positive entries of y denote high purchase intentions, while the positive entries of p denote actual purchases. generally speaking, a purchase only happens when the utility is high, but a high utility does not necessarily lead to a purchase. this observation allows us to link the binary tensors p and y: p is generated by a one-sided sampling process that only reveals a subset of y’s positive entries. given this observation, we follow and include a label-dependent loss trading the relative cost of positive and unlabeled samples: l(x ,p) ijk: pijk1 max1 (xijk max(0, dcj ticjk)), (1 ) ijk: pijk0 l(xijk, 0), where l(x, c) (x c)2 denotes the squared loss. in addition, the form utility tensor x should be of low-rank to capture temporal dynamics of users’ interests, which are generally believed to be dictated by a small number of latent factors by combining asymmetric sampling and the low-rank property together, we jointly recover the tensor x and the inter-purchase duration vector d by solving the following tensor nuclear norm minimization (tnnm) problem: min xrmnl, drr ijk: pijk1 max1 (xijk max(0, dcj ticjk)), (1 ) ijk: pijk0 x2ijk x, (2) where x denotes the tensor nuclear norm, a convex combination of nuclear norms of x ’s unfolded matrices given the learned x and d, the underlying binary tensor y can be recovered by (1). we note that although the tnnm problem (2) can be solved by optimization techniques such as block coordinate descent and admm 10, they suffer from high computational cost since they need to be solved iteratively with multiple svds at each iteration. an alternative way to solve the problem is tensor factorization however, this also involves iterative singular vector estimation and thus not scalable enough. as a typical example, recovering a rank tensor of size takes the state-of-the-art tensor factorization algorithm tenals more than 20, seconds on an intel xeon ghz processor with gb main memory. in this subsection, we discuss how to significantly improve the scalability of the proposed demandaware recommendation model. to this end, we assume that an individual’s form utility does not change over time, an assumption widely-used in many collaborative filtering methods 25, under this assumption, the tensor x is a repeated copy of its frontal slice x::1, i.e., x x::1 e, (3) where e is an l-dimensional all-one vector and the symbol represents the outer product operation. in this way, we can relax the problem of learning a third-order tensor x to the problem of learning its frontal slice, which is a second-order tensor (matrix). for notational simplicity, we use a matrix x to denote the frontal slice x::1, and use xij to denote the entry (i, j) of the matrix x. since x is a low-rank tensor, its frontal slice x should be of low-rank as well. hence, the minimization problem (2) simplifies to: min xrmn drr ijk: pijk1 max1 (xij max(0, dcj ticjk)), (1 ) ijk: pijk0 x2ij x : f(x,d), (4) where x stands for the matrix nuclear norm, the convex surrogate of the matrix rank function. by relaxing the optimization problem (2) to the problem (4), we recover a matrix instead of a tensor to infer users’ purchase intentions. 4url/swoh/software/optspace/code.html", "section_index": 3}, {"content": "although the learning problem has been relaxed, optimizing (4) is still very challenging for two main reasons: (i) the objective is highly non-smooth with nested hinge losses, and (ii) it contains mnl terms, and a naive optimization algorithm will take at least o(mnl) time. to address these challenges, we adopt an alternating minimization scheme that iteratively fixes one of d and x and minimizes with respect to the other. specifically, we propose an extremely efficient optimization algorithm by effectively exploring the sparse structure of the tensor p and low-rank structure of the matrix x. we show that (i) the problem (4) can be solved within o(p0(k log(p0)) (nm)k2) time, where k is the rank of x, and (ii) the algorithm converges to the critical points of f(x,d). in the following, we provide a sketch of the algorithm. the detailed description can be found in the supplementary material. when x is fixed, the optimization problem with respect to d can be written as: min d ijk: pijk1 max ( (xij max(0, dcj ticjk)), )2 : g(d) : ijk: pijk1 gijk(dcj ). (5) problem (5) is non-trivial to solve since it involves nested hinge losses. fortunately, by carefully analyzing the value of each term gijk(dcj ), we can show that gijk(dcj ) max(1 xij , 0)2, if dcj ticjk max(xij 1, 0) (1 (xij dcj ticjk))2, if dcj ticjk max(xij 1, 0). for notational simplicity, we let sijk ticjk max(xij 1, 0) for all triplets (i, j, k) satisfying pijk now we can focus on each category : for each , we collect the set q (i, j, k) pijk and cj and calculate the corresponding sijks. we then sort sijks such that s(i1j1k1) s(iqjqkq). for each interval s(iqjqkq), s(iq1jq1kq1), the function is quadratic, thus can be solved in a closed form. therefore, by scanning the solution regions from left to right according to the sorted s values, and maintaining some intermediate computed variables, we are able to find the optimal solution, as summarized by the following lemma: lemma the subproblem (5) is convex with respect to d and can be solved exactly in o(p0 log(p0)), where p0 is the number of nonzero elements in tensor p therefore, we can efficiently update d since p is a very sparse tensor with only a small number of nonzero elements. by defining aijk max(0, dcj ticjk), if pijk 0, otherwise the subproblem with respect to x can be written as min xrmn h(x)x where h(x) : ijk: pijk1 max(aijkxij , 0)2(1) ijk: pijk0 x2ij (6) since there are o(mnl) terms in the objective function, a naive implementation will take at least o(mnl) time, which is computationally infeasible when the data is large. to address this issue, we use proximal gradient descent to solve the problem. at each iteration, x is updated by x s(x h(x)), (7) where s() is the soft-thresholding operator for singular values.5 5if x has the singular value decomposition x uvt , then s(x) u( i)vt where a max(0, a). in order to efficiently compute the top singular vectors of x h(x), we rewrite it as x h(x) 2(1 )l x 2(1 ) ijk: pijk1 xij ijk: pijk1 max(aijk xij , 0) since fa(x) is of low-rank and fb(x) is sparse, multiplying (x h(x)) with a skinny m by k matrix can be computed in o(nk2 mk2 p0k) time. as shown in 12, each iteration of proximal gradient descent for nuclear norm minimization only requires a fixed number of iterations of randomized svd (or equivalently, power iterations) using the warm start strategy, thus we have the following lemma. a proximal gradient descent algorithm can be applied to solve problem (6) within o(nk2t mk2t p0kt ) time, where t is the number of iterations. we note that the algorithm is guaranteed to converge to the true solution. this is because when we apply a fixed number of iterations to update x via problem (7), it is equivalent to the inexact gradient descent update where each gradient is approximately computed and the approximation error is upper bounded by a constant between zero and one. intuitively speaking, when the gradient converges to 0, the error will also converge to at an even faster rate. see for the detailed explanations. combining the two subproblems together, the time complexity of each iteration of the proposed algorithm is: o(p0 log(p0) nk2t mk2t p0kt ). remark: since each user should make at least one purchase and each item should be purchased at least once to be included in p , n and m are smaller than p0. also, since k and t are usually very small, the time complexity to solve problem (4) is dominated by the term p0, which is a significant improvement over the naive approach with at least o(mnl) complexity. since our problem has only two blocks d, x and each subproblem is convex, our optimization algorithm is guaranteed to converge to a stationary point indeed, it converges very fast in practice. as a concrete example, our experiment shows that it takes only iterations to optimize a problem with million users, million items, and more than million purchase records.", "section_index": 4}, {"content": "we first conduct experiments with simulated data to verify that the proposed demand-aware recommendation algorithm is computationally efficient and robust to noise. to this end, we first construct a low-rank matrix x wht , where w rm10 and h rn10 are random gaussian matrices with entries drawn from n (1, 0.5), and then normalize x to the range of 0, we randomly assign all the n items to r categories, with their inter-purchase durations d equaling 10, 20, we then construct the high purchase intension set (i, j, k) ticjk dcj and xij 0.5, and sample a subset of its entries as the observed purchase records. we let n m and vary them in the range 10, 000, 20, 000, 30, 000, 40, we also vary r in the range 10, 20, , given the learned durations d, we use d d2/d2 to measure the prediction errors. accuracy figure 1(a) and 1(b) clearly show that the proposed algorithm can perfectly recover the underlying inter-purchase durations with varied numbers of users, items, and categories. to further evaluate the robustness of the proposed algorithm, we randomly flip some entries in tensor p from to to simulate the rare cases of purchasing two items in the same category in close temporal succession. figure 1(c) shows that when the ratios of noisy entries are not large, the predicted durations d are close enough to the true durations, thus verifying the robustness of the proposed algorithm. scalability to verify the scalability of the proposed algorithm, we fix the numbers of users and items to be million, the number of time slots to be 1, 000, and vary the number of purchase records (i.e., p0). table summarizes the cpu time of solving problem (4) on an intel xeon ghz server with gb main memory. we observe that the proposed algorithm is extremely efficient, e.g., even with million users, million items, and more than million purchase records, the running time of the proposed algorithm is less than hours. in the real-world experiments, we evaluate the proposed demand-aware recommendation algorithm by comparing it with the six state-of the-art recommendation methods: (a) m3f, maximum-margin matrix factorization 24, (b) pmf, probabilistic matrix factorization 25, (c) wr-mf, weighted regularized matrix factorization 14, (d) cp-apr, candecomp-parafac alternating poisson regression 7, (e) rubik, knowledge-guided tensor factorization and completion method 30, and (f) bptf, bayesian probabilistic tensor factorization among them, m3f and pmf are widely-used static collaborative filtering algorithms. we include these two algorithms as baselines to justify whether traditional collaborative filtering algorithms are suitable for general e-commerce recommendation involving both durable and nondurable goods. since they require explicit ratings as inputs, we follow to generate numerical ratings based on the frequencies of (user, item) consumption pairs. wr-mf is essentially the positive-unlabeled version of pmf and has shown to be very effective in modeling implicit feedback data. all the other three baselines, i.e., cp-apr, rubik, and bptf, are tensor-based methods that can consider time utility when making recommendations. we refer to the proposed recommendation algorithm as demand-aware recommender for one-sided sampling, or daross for short. our testbeds are two real-world datasets tmall6 and amazon review7. since some of the baseline algorithms are not scalable enough, we first conduct experiments on their subsets and then on the full set of amazon review. in order to generate the subsets, we randomly sample item categories for tmall dataset and select the users who have purchased at least items within these categories, leading to the purchase records of users and items. for amazon review dataset, we randomly select users who have provided reviews to at least item categories on amazon.com. this leads to a total of 5, items belonging to categories. time information for both datasets is provided in days, and we have and time slots for tmall and amazon review subsets, respectively. the full amazon review dataset is significantly larger than its subset. after removing duplicate items, it contains more than million product reviews from million users and million items that 6url 7url/ belong to item categories. the collected reviews span a long range of time: from may to july 2014, which leads to 6, time slots in total. comparing to its subset, the full set is a much more challenging dataset both due to its much larger size and much higher sparsity, i.e., many reviewers only provided a few reviews, and many items were only reviewed a small number of times. for each user, we randomly sample of her purchase records as the training data, and use the remaining as the test data. for each purchase record (u, i, t) in the test set, we evaluate all the algorithms on two tasks: (i) category prediction, and (ii) purchase time prediction. in the first task, we record the highest ranking of items that are within item i’s category among all items at time t. since a purchase record (u, i, t) may suggest that in time slot t, user u needed an item that share similar functionalities with item i, category prediction essentially checks whether the recommendation algorithms recognize this need. in the second task, we record the number of slots between the true purchase time t and its nearest predicted purchase time within item i’s category. ideally, good recommendations should have both small category rankings and small time errors. thus we adopt the average top percentages, i.e., (average category ranking) / n and (average time error) / l 100, as the evaluation metrics of category and purchase time prediction tasks, respectively. the algorithms m3f, pmf, and wr-mf are excluded from the purchase time prediction task since they are static models that do not consider time information. figure displays the predictive performance of the seven recommendation algorithms on tmall and amazon review subsets. as expected, m3f and pmf fail to deliver strong performance since they neither take into account users’ demands, nor consider the positive-unlabeled nature of the data. this is verified by the performance of wr-mf: it significantly outperforms m3f and pmf by considering the pu issue and obtains the second-best item prediction accuracy on both datasets (while being unable to provide a purchase time prediction). by taking into account both issues, our proposed algorithm daross yields the best performance for both datasets and both tasks. table reports the inter-review durations of amazon review subset estimated by our algorithm. although they may not perfectly reflect the true inter-purchase durations, the estimated durations clearly distinguish between durable good categories, e.g., automotive, musical instruments, and non-durable good categories, e.g., instant video, apps, and food. indeed, the learned inter-purchase durations can also play an important role in applications more advanced than recommender systems, such as inventory management, operations management, and sales/marketing mechanisms. we do not report the estimated durations of tmall herein since the item categories are anonymized in the dataset. finally, we conduct experiments on the full amazon review dataset. in this study, we replace category prediction with a more strict evaluation metric item prediction 8, which indicates the predicted ranking of item i among all items at time t for each purchase record (u, i, t) in the test set. since most of our baseline algorithms fail to handle such a large dataset, we only obtain the predictive performance of three algorithms: daross, wr-mf, and pmf. note that for such a large dataset, prediction time instead of training time becomes the bottleneck: to evaluate average item rankings, we need to compute the scores of all the million items, thus is computationally inefficient. therefore, we only sample a subset of items for each user and estimate the rankings of her purchased items. using this evaluation method, the average item ranking percentages for daross, wr-mf, and pmf are 16.7, 27.3, and 38.4, respectively. in addition to superior performance, it only takes our algorithm iterations and hour to converge to a good solution. since wr-mf and pmf are both static models, our algorithm is the only approach evaluated here that considers time utility while being scalable enough to handle the full amazon review dataset. note that this dataset has more users, items, and time slots but fewer purchase records than our largest synthesized dataset, and the running time of the former dataset is lower than the latter one. this clearly verifies that the time complexity of our algorithm is dominated by the number of purchase records instead of the tensor size. interestingly, we found that some inter-review durations estimated from the full amazon review dataset are much smaller than the durations estimated from its subset. this is because the durations may be underestimated when many users reviewed items within a same durable goods category in close temporal succession. on the other hand, this result verifies the effectiveness of the pu formulation even if the durations are underestimated, our algorithm still outperforms the competitors by a considerable margin. as a final note, we want to point out that tmall and amazon review may not take full advantage of the proposed algorithm, since (i) their categories are relatively coarse and may contain multiple sub-categories with different durations, and (ii) the time stamps of amazon review reflect the review time instead of purchase time, and inter-review durations could be different from inter-purchase durations. by choosing a purchase history dataset with a more appropriate category granularity, we may obtain more accurate duration estimations and also a better recommendation performance.", "section_index": 5}, {"content": "in this paper, we examine the problem of demand-aware recommendation in settings when interpurchase duration within item categories affects users’ purchase intention in combination with intrinsic properties of the items themselves. we formulate it as a tensor nuclear norm minimization problem that seeks to jointly learn the form utility tensor and a vector of inter-purchase durations, and propose a scalable optimization algorithm with a tractable time complexity. our empirical studies show that the proposed approach can yield perfect recovery of duration vectors in noiseless settings; it is robust to noise and scalable as analyzed theoretically. on two real-world datasets, tmall and amazon review, we show that our algorithm outperforms six state-of-the-art recommendation algorithms on the tasks of category, item, and purchase time predictions.", "section_index": 6}], "id": 1014}
{"text": [{"content": "we present a neural response generation model that generates responses conditioned on a target personality. the model learns high level features based on the target personality, and uses them to update its hidden state. our model achieves performance improvements in both perplexity and bleu scores over a baseline sequence-to-sequence model, and is validated by human judges.", "section_index": 0}, {"content": "automated conversational agents are becoming popular for various tasks, such as personal assistants, shopping assistants, or as customer service agents. automated agents benefit from adapting their personality according to the task at hand thus, it is desirable for automated agents to be capable of generating responses that express a target personality. personality is defined as a set of traits which represent durable characteristics of a person. many models of personality exist while the most common one is the big five model (digman, 1990) , including: openness, conscientiousness, extraversion, agreeableness, and neuroticism. these traits were correlated with linguistic choices including lexicon and syntax (mairesse and walker, 2007). in this paper we study how to encode personality traits as part of neural response generation for conversational agents. our approach builds upon a sequence-to-sequence by adding an additional layer that represents the target set of personality traits, and a hidden layer that learns high-level personality based features. the response is then generated conditioned on these features. specifically, we focus on conversational agents for customer service; in this context, many studies examined the effect of specific personality traits of human agents on service performance. results indicate that conscientiousness figure shows examples of customer utterances, followed by two automatically generated responses. the first response (in each example), is generated by a standard seq2seq response generation system that ignores personality modeling and in effect generates the consensus response of the humans represented in the training data. the second response is generated by our system, and is aimed to generate data for an agent that expresses a high level of a specific trait. in example 1, the agreeableness-agent is more compassionate (expresses empathy) and is more cooperative (asks questions). in example 2, the conscientiousness-agent is more thoughtful (will check the issue). we experimented with a dataset of 87.5k real customer-agent utterance pairs from social media. we find that leveraging personality encoding improves relative performance up to in bleu score, compared to a baseline seq2seq model. to our knowledge, this work is the first to train a neural response generation model that encodes target personality traits.", "section_index": 1}, {"content": "generating responses that express a target personality was previously discussed in different settings. early work on the personage system (mairesse and walker, 2007; mairesse and walker, 2008; mairesse and walker, 2010; mairesse and walker, 2011) presented a framework projecting different traits throughout the different modules of an nlg system. the authors explicitly defined linguistic features as generation parameters, and then learned how to weigh them to generate a desired set of traits. while we aim at the same objective, our methodology is different and does not require feature engineering. our approach utilizes a neural network that automatically learns to represent high level personality based features. neural response generation models are based on a seq2seq architecture and employ an encoder to represent the user utterance and an attention-based decoder that generates the agent response one token at a time. models that aim to generate a coherent persona also exist. generated responses for customer service re- quests on social media using standard seq2seq, while we modify it to generate a target personality.", "section_index": 2}, {"content": "we review the seq2seq attention based model on which our model is based. neural response generation can be viewed as a sequence-to-sequence problem , where a sequence of input language tokens x x1, , xm , describing the user utterance, is mapped to a sequence of output language tokens y1, , yn , describing the agent response. the encoder is an lstm generates output tokens one at a time. at each time step j, it generates yj based on the current hidden state sj , then updates the hidden state sj1 based on sj and yj formally, the decoder is defined by the following equations: s1 tanh(w (s)bm), (1) p(yj w x, y1:j1) exp(u sj , cj ), (2) sj1 lstm((out)(yj), cj , sj), (3) where i 1, , n and the context vector, cj , is the result of global attention ). the matricesw (s),w (a), u , and the embedding function (out) are decoder parameters. the entire model is trained end-to-end by maximizing p(y x) nj1 p(yj x, y1:j1).", "section_index": 3}, {"content": "the model described in section generates responses with maximum likelihood which reflect the consensus of the agents that appear in the training data. this kind of response does not characterize a specific personality and thus can result in inconsistent or unwanted personality cues. in this section we present our personality-based model (figure 2) which generates responses conditioned on a target set of personality traits values which the responses should express. the target set of personality traits is represented as a vector p, where pi represents the desired value for the ith trait. this value encodes how strongly should this trait be expressed in the response. consequently, the size of p depends on the selected personality model (e.g., five traits for the big five model). as in (mairesse and walker, 2011), we argue that personality traits are exhibited as different types of stylistic linguistic variation. thus, our model’s response is conditioned on generation parameters which are based on personality traits. in comparison to (mairesse and walker, 2011) where generation parameters were defined manually, we learn these high-level features automatically during training. we introduce a personality based features hidden layer hp (w (p)p b), where w (p) and b are parameters learned by the model during training. each personality feature hi is a weighted sum of the targeted traits values (following a sigmoid activation). now, at each token generation, the decoder updates the hidden state conditioned on the personality traits features hp, as well as on the previous hidden state, the output token and the context. formally, equation is changed to: sj1 lstm((out)(yj), cj , hp, sj), (4) conditioning on hp captures the relation of text generation to the underlining personality traits.", "section_index": 4}, {"content": "our model is designed to generate text conditioned on a target set of personality traits. specifically, we verified its performance in a scenario of customer service. for our experiments we utilized the dataset presented in , which exhibits a large variety of customer service properties. this dataset is a collection of 1m conversations over customer service twitter channels of different brands which cover a large variety of product categories. several preprocessing steps were performed for our purposes: we first split the data to pairs consisting of a single customer utterance and its corresponding agent response. we removed pairs containing non-english sentences. we further removed pairs for agents that participated in less than conversation pairs, so we would have sufficient data for each agent to extract their personality traits (see below). this resulted in 87.5k conversation pairs in total including different agents (138160 pairs per agent on average). following we used bleu for evaluation. besides bleu scores, we also report perplexity as an indicator of model capability. for implementation details, refer to appendix a. we experimented with two different settings to measure our model’s performance. warm start: in the first experiment, data for each agent in the dataset was split between training, validation and test data sets with a fraction of 80/10/10, respectively. we then extracted the agents’ personality traits using an external service , which learns a persona vector for each agent in the training data. the results in table show that the standard seq2seq model achieved the lowest performance in terms of both perplexity and bleu score while the competing models which learn a representation for the agents achieved higher performance. the persona-based model achieved similar perplexity but higher bleu score than our model. this is reasonable since persona-based is not restricted to personality based features. however, this model can not generate content for agents which do not appear in the training data, and thus, it is limited. cold start: in our second experiment, we split the dataset such that of the agents only formed the validation and test sets (half of each agent’s examples for each set). data for the other of the agents formed the training set. in this setting, data for agents in the test set does not appear in the training set. these agents represent new personality distributions we would like to generate responses for. note that, we extracted target personality traits for agents in the training set using their training data, or, for agents in the test set, using validation data. in this setting, it is not possible to test the persona-based model since no representation is learned during training for agents in the test set. thus, we only compare our model to the baseline seq2seq model. table shows that, in this setting, we get better performance by utilizing personality based representation: our model achieves a relative decrease in perplexity, and a relative improvement in bleu score. results from both experiments demonstrate that we can better model the linguistic variation in agent responses by conditioning on target personality traits. we conducted a human evaluation of our personality-based model using a crowd-sourcing service. this evaluation measures whether the responses generated by our model are correlated with the target personality traits. we focused on two personality traits from the big five model that are important to customer service: agreeableness and conscientiousness we extracted customer utterances from the validation set of the cold start setting described above. we selected customer utterances that convey a negative sentiment, since re- sponses to this kind of utterances vary much. after sentences were selected, we generated corresponding agent responses in the following way. we generated a high-trait target personality distribution (trait was either agreeableness or conscientiousness), where trait was set to a value of 0.9, and all other traits to similarly, we created a low-trait version where trait was set to for each trait and customer utterance we generated a response for the high-trait and low-trait versions. each triplet methodology, the two responses were presented in a random order, and judged on a 5-point zero-sum scale. a score of (2) was assigned if one response was judged to express the trait more (less) than the other response, and (1) if one response expressed the trait somewhat more (less) than the other. ties were assigned a score of zero. the judges rated each pair, and their scores were averaged and mapped into equal-width bins. after discarding ties, we found that the high-trait responses generated by our personality-based model were judged either more expressive or somewhat more expressive than the low-trait corresponding responses in of cases. if we ignore the somewhat more expressive judgments, the high-trait responses win in of cases.", "section_index": 5}, {"content": "we have presented a personality-based response generation model and tested it in customer care tasks, outperforming baseline seq2seq model. in future work, we would like to generate responses adapted to the personality traits of the customer as well, and to apply our model to other tasks such as education systems.", "section_index": 6}], "id": 1015}
{"text": [{"content": "we introduce a stereo correspondence system implemented fully on event-based digital hardware, using a fully graph-based non von-neumann computation model, where no frames, arrays, or any other such data-structures are used. this is the first time that an end-to-end stereo pipeline from image acquisition and rectification, multi-scale spatiotemporal stereo correspondence, winner-take-all, to disparity regularization is implemented fully on event-based hardware. using a cluster of truenorth neurosynaptic processors, we demonstrate their ability to process bilateral event-based inputs streamed live by dynamic vision sensors (dvs), at up to 2,000 disparity maps per second, producing high fidelity disparities which are in turn used to reconstruct, at low power, the depth of events produced from rapidly changing scenes. experiments on real-world sequences demonstrate the ability of the system to take full advantage of the asynchronous and sparse nature of dvs sensors for low power depth reconstruction, in environments where conventional frame-based cameras connected to synchronous processors would be inefficient for rapidly moving objects. system evaluation on event-based sequences demonstrates a improvement in terms of power per pixel per disparity map compared to the closest stateof-the-art, and maximum latencies of up to 11ms from spike injection to disparity map ejection.", "section_index": 0}, {"content": "sparsity and parallel asynchronous computation are two key principles of information processing in the brain. they allow to solve complex tasks using a tiny fraction of the energy consumed by stored-program computers while the successful artificial neural networks may not operate the same way as the brain, both of them utilize highly parallel and hierarchical architectures that gradually abstract input data to more meaningful concepts 8, 51, however, event-based computation has not been equally adopted work done as an intern at ibm research - almaden. cognitive anteater robotics lab (carl), university of california, irvine another barrier for sparse computation are traditional sensors, such as frame-based cameras, which provide regular inputs. for autonomous vehicles, drones, and satellites, energy consumption is a challenge event-based processing dramatically reduces power consumption by computing only what is new while omitting unchanged input parts. recently developed event-based cameras such as dynamic vision sensor (dvs) 37, and atis 50, inspired by the biological retina, encode pixel illumination changes as events. these sensors solve two major drawbacks of frame-based cameras. first, temporal resolution of frame-based applications is limited by the camera frame rate, usually frames per second. event-based cameras can generate events at microsecond resolution. second, consecutive frames in videos are usually highly redundant, which waste downstream data transfer, computing resources and power. since events are sparse, event-based cameras lead to better downstream resource usage. moreover, eventbased cameras have high dynamic range ( db), which is useful for real world variations in lighting conditions. to achieve the low energy and high temporal resolution benefits of event-based inputs, computations must be performed asynchronously. to benefit from sparse and asynchronous computation, neuromorphic processors have been developed 44, 24, 30, 9, these processors represent input events as spikes and process them in parallel using a large neuron population. they are stimulus-driven and the propagation delay of an event through the neuron layers is usually a few milliseconds, suitable for many real-time applications. for example, the truenorth neuromorphic chip has been used for high throughput convolutional neural networks (cnns) 22, character recognition 53, optic flow 11, saliency 3, and gesture recognition depth perception is an important task for autonomous mobile agents to navigate in the real world. the speed and low power requirements of these applications can be effectively met using event-based sensors. event-based stereo provides additional advantages over other depth estimation methods that increase accuracy and save energy, such as high temporal resolution, high dynamic range, and robustness to interference with other agents. several methods have been proposed to solve event- based stereo correspondence. most global methods 40, 17, 49, are derived from the marr and poggio cooperative stereo algorithm the algorithm assumes depth continuity and often event-based implementations are not tested with objects tilted in depth. local methods can be parallelized and find corresponding events using either local features over a spatiotemporal window or event-to-event features 13, 58, 52, 32, however, most approaches use non-event-based hardware, such as cpu or dsp. we propose a fully neuromorphic event-based stereo disparity algorithm. a live-feed version of the system running on nine truenorth chips is shown to calculate disparity maps per second, and the ability to increase this up to 2,000 disparities per second (subject to certain trade-offs) is demonstrated, for use with high speed event cameras, such as dvs. the main advantages of the proposed method, compared to the related work 17, 49, 45, 52, 57, are simultaneous end-to-end neuromorphic disparity calculation, low power, high throughput, low latency (9-11 ms), and linear scalability to multiple neuromorphic processors for larger input sizes. compared to frame-based computation, in the asynchronous, event-based computation supported by truenorth, at each time cycle, in general only neurons that have input spikes are computed, and only spike events are communicated. when the data in a cycle is sparse, as is the case with a dvs sensor, most neurons would not compute for most of the time, resulting in low active power this processing differs from traditional architectures that use frame-buffers and other conventional data structures; where same memory fetching and computation is repeated for each pixel every frame, independent of scene activity. the proposed event-based disparity method is implemented using a stereo pair of davis sensors (a version of dvs) and nine truenorth ns1e boards however, the method is applicable to other spiking neuromorphic architectures, and it is also tested offline on larger models using a truenorth simulator. input rectification, spatiotemporal scaling, feature matching, search for best matches, morphological erosion and dilation, and bidirectional consistency check are all performed on truenorth, for a fully neuromorphic disparity solution. with respect to the most relevant state-of-the-art approach 17, our method uses less power per pixel per disparity map. we also release the event-based stereo dataset used, which includes kinect-based registered ground-truth.", "section_index": 1}, {"content": "frame-based stereo disparity methods calculate matching cost using a spatial similarity metric 25, 27, or a cost function learned from a dataset (see reviews 62, 55, 34, 63). cnns have been used to learn stereo matching cost 66, ground truth disparity maps from benchmark frame-based datasets 27, 54, 26, are used to train these models, followed by sparse-to-dense conversions 18, feature based matching techniques, such as color, edge, histogram, and sift based matching, produce sparse disparity maps 28, 38, 21, in contrast, event-based stereo correspondence literature is relatively new. mahowald and delbruck implemented the marr and poggio cooperative stereo algorithm 42, a global approach, in an analog vlsi circuit. the algorithm converges well when object surfaces are fronto-parallel and candidate matches injected to the network are close together 40, later mahowald modified the vlsi embodied algorithm to solve tilted depth maps using a network of analog valued disparity units, which linearly interpolates the cooperative network output. however, most of the recent event-based implementations of the cooperative algorithm do not consider depth gradients 47, 48, 23, inject neighborhood similarity of candidate matches into the cooperative network. use six spinnaker processor boards to implement the cooperative network for pixels of stereo event data. propose an fpga based implementation of spiking neurons as the nodes of the cooperative network. employ message passing on a markov random field with depth continuity for a global solution. local event-based stereo correspondence approaches are area-based or time-based. area-based methods assume that object shapes appear identically on left and right sensors. 13, propose to match edge orientations in event frames accumulated over ms. schraml et al. 60, propose dsp implementation of a spatiotemporal similarity method using two live event sensors use a rotating pair of event-based line (vertical) sensors in static scenes and render events from each rotation to an edge map 33, which is subsequently processed using a frame-based panoramic stereo algorithm time-based methods utilize event timestamps for matching. although spike dynamics vary among pixels and sensors and events cannot be matched based on exact timestamps. 52, propose to use event-toevent constraints for calculating matching cost, such as time window, distance to the epipolar line, ordering constraint, and polarity. 32, calculate similarity as the inverse of temporal distance and average them within each depth plane. the proposed method and its fpga implementations 20, are equivalent to the cooperative stereo algorithm with noisy time difference inputs. 59, propose a cost function for the rotating stereo panorama setup in based on temporal event difference.", "section_index": 2}, {"content": "our implementation uses a pair of synchronized davis240c cameras 10, connected via ethernet to a cluster of truenorth ns1e boards (fig. the use of davis sensors improve speed, power, dynamic range, and computational requirements. 2, fast moving objects are more challenging for frame-based cameras. the ibm truenorth is a reconfigurable, non-von neumann neuromorphic chip containing million spiking neurons and million synapses distributed across parallel, event-driven, neurosynaptic cores cores are tiled in a array, embedded in a fully asynchronous network-on-chip. the chip consumes 70mw when operating at a ms computation tick and normal workloads. depending on event dynamics and network architecture, faster tick period is possible, which we take advantage of in this work to achieve as low as ms per tick, thus doubling the maximum throughput achievable. each neurosynaptic core connects inputs to neurons using a crossbar of binary synapses with a lookup table of weights for bits of precision, plus a sign bit. a neuron state variable, called membrane potential, integrates synaptically weighted input events with an optional leak decay. each neuron can generate an output event deterministically, if the membrane potential v (t) exceeds a threshold; or stochastically, with a probability that is a function of the difference between the membrane potential and its threshold 2, the membrane potential is updated at each tick t to v (t) v (t 1) v (t) t , followed by the application of an activation function an(v (t)) where an(v (t)) 1, if v (t) n 0, otherwise (1) each neuron is assigned an initial membrane potential v (0). furthermore, upon producing an event, a neuron is reset to a user-specified value. unless specified otherwise, we assume initial membrane potentials and reset values of zero. truenorth programs are written in the corelet programming language a hierarchical, compositional, object-oriented language", "section_index": 3}, {"content": "the proposed local event-based stereo correspondence algorithm is implemented end-to-end as a neuromorphic event-based algorithm. this consists of systems of equations defining the behavior of truenorth neurons, encased in modules called corelets 1, and the subsequent composition of the inputs and outputs of these modules. depicts the sequence of operations performed by the corelets using inputs from stereo event sensors. the stereo rectification is defined by a pair of functions l, r which map each pixel in the left and right sensor’s rectified space to a pixel in the left and right sensor’s native resolution respectively. on truenorth, this is implemented using h w splitter neurons per sensor polarity channel, arranged in an h w retinotopic map. the events at each rectified pixel p h w l,r,, , are generated through splitter neurons which replicate corresponding sensor pixels. their membrane potential v splp (t) is defined by v splp (t) t i(t 1; p) where i(t; p) 0, denotes whether a sensor event is produced at time t and the sensor pixel p corresponding to the rectified pixel p. a1(v spl p (t)) defines the activation of the corresponding neuron. potentials are initialized to zero and set to also reset to zero upon spiking. the event rate of an event-based sensor depends on factors, such as scene contrast, sensor bias parameters, and object velocity. to add invariance across event rates, we accumulate spikes over various temporal scales through the use of temporally overlapping sliding windows. these temporal scales are implemented through the use of splitter neurons which cause each event to appear at its corresponding pixel multiple times, depending on the desired temporal scale, or through the use of temporal ring buffer mechanisms, which lead to lower event rates. the ring buffer is implemented by storing events in membrane potentials of memory cell neurons in a circular buffer, and through the use of control neurons which spike periodically to polarize appropriate memory cell neurons. buffers can encode the input at various temporal scales. for example at a scale t the buffer denotes if an event occurred at the corresponding pixel during the last ticks (logical disjunction). a control neuron that produces events with period t and phase is defined by a neuron at (v ctrl ) that satisfies v ctrl (t) t 1, v (0) and resets to zero upon pro- ducing an event. through populations of such neurons one can also define at (v ctrl ,) corresponding to phase intervals , (where t ), defining periodic intervals of events. such control neurons are used to probe (prb) or reset (rst) neuron membrane potentials. a memory cell neuron is a recurrent neuron which accepts as input either its own output (so that it does not lose its stored value whenever the neuron is queried for its stored value), input axons to set the neuron value and control axons for resetting and querying the memory cell. in more detail the output at index r 0, ..., t of a t size memory cell ring-buffer at a given pixel p, is multiplexed via two copies (m 0, 1) and is defined as a2(v mem p,m,r) where v memp,m,r(t 1) t at2(v rst s1(t)) (a1(v spl p (t)) r r a2(v mem p,m,r(t 1)) m t ) at2(v prb 3r,t2r(t)) m t at2(v rst 2r,t1r(t)) 1m t (2) where probe/reset (prb/rst) control neurons are used, r t mod (t 2), s t r mod (t 2), t t mod 2, is logical disjunction1, xrr max 0, x, if r r 0, otherwise (3) and x def x11 defines a relu function. defines a ring-buffer with t memory cells, where probe pulses periodically and uniformly query t of the t2 cells for the stored memory contents at each tick, where m neurons are probed at odd ticks and m neurons are probed at even ticks. reset pulses control when to reset one of the t memory cells to zero in preparation of a new input. notice that new inputs (a1(v spl p ())) are always routed to the cell r that was reset in the previous tick. the probe pulses result in the creation of an output event if during the last t ticks a1(v spl p ()) produced an event. after a probe event, a reset event decrements the previous membrane potential increase, followed by the restoring of the memory event output during the last probe (a2(v mem p,m,r(t 1))). binary morphological erosion and dilation is optionally applied on the previous module’s outputs to denoise the image. given a 2-d neighborhood n(p) centered around each pixel p, the erosion neuron’s membrane potential v ep is guided by the system of equations v ep (t) t n(p) qn(p) m r a2(v mem q,m,r (t 1)) and uses an a1 activation function. similarly, dilation neurons v dp with receptive fields n(p) evolve according to v dp (t) t qn(p) a1(v e q (t 1)) where a1 is also used as the dilation neurons’ activation function. the neuron potentials are initialized to zero and set to also reset to zero upon producing a spike. in practice pixel neighborhoods are used. at each tick, erosion and dilation neurons output the minimum and maximum value respectively, of their receptive fields. cascades of erosion and dilation neurons, are used to denoise retinotopic binary inputs (fig. each feature extracted around a rectified pixel p is a concatenation of event patches, extracted from one or more spatiotemporal scales. spatial scaling consists of spatially sub-sampling each output map of the temporal scale phase (sec. 4.2/4.3), as specified in the corelet parameters, to apply the window matching (sec. this results in spatiotemporal coordinate tensors xl,p, xr,p defining the coordinates where events form feature vectors. the ith of these coordinates is represented by neuron activations a1(v l, x (i) l,p (t)) and a1(v r, x (i) r,p (t)) in 1disjunction is implemented by sending input events to the same neuron input axon, effectively merging any input events to a single input event. the left and right sensor’s positive () or negative () polarity channel.2 given a pair of spatiotemporal coordinate tensors xl,p, xr,q centered at coordinates p, q in the left and right rectified image respectively and representing k coordinates each, we calculate the binary hadamard product fl(p, t) fr(q, t) associated with the corresponding patches at time t, where fl(p, t) ia1(v l x (i) l,p (t)) 0, 1k and fr(q, t) ia1(v r x (i) r,q (t)) 0, 1k the product is calculated in parallel across multiple neurons, as k pairwise logical and operations of corresponding feature vector entries, resulting in (a1(v dot p,q,1), ...,a1(v dot p,q,k)) where v dotp,q,i(t) t a1(v l x (i) l,p (t 1)) a1(v r x (i) r,q (t 1)) the population code representation of the hadamard product output is converted to a thermometer code3, which is passed to the winner-take-all circuit described below. the winner-take-all (wta) system is a feed-forward neural network that takes as input d thermometer code representations of the hadamard products for d distinct candidate disparity levels, and finds the disparity with the largest value, at every tick. for designing a scalable and compact wta system on a neuromorphic hardware, we introduced a novel encoding technique for inputs. in a binary eventbased system, numbers can be efficiently coded using base4 representation where each digit is encoded using a 3-bits thermometer code. we denote it as quaternary thermometer code (qtc). note that a thermometer code of length 2n bits can be represented by a qtc of length n/2 bits. for example, values between are represented by a qtc of bits. while it takes a few more bits than an bits binary code, it allows designing a feed-forward wta network comprising only four cascaded subnetworks, compared to eight for a binary representation, requiring fewer hardware resources as well as half the latency. latency is further improved with larger bases, but the growth in thermometer code length for each digit results in consuming more hardware resources. table shows binary, base-4 and qtc representation of different decimal numbers. we assume a maximum thermometer code length of 4b1 k for some b n. then for any 0, 1, 2, 0, 1, ..., b, we define the conversion of candidate disparity level d 0, ..., d to a qt-coded membrane potential v cnv,,d (t) as 2for notational simplicity we henceforth drop the , superscripts: the left and right sensors could produce distinct event streams based on event polarity, or could merge events in a single polarity-agnostic stream. 3e.g., given a population code (1, 1, 0, 1, 0) for value 3, its thermometer code is the right-aligned juxtaposition of all events: (0, 0, 1, 1, 1). v cnv,,d (t) t iu() vid(t1) iu(1) vid(t1) (4) where vid(t) is the i-th element of the input thermometer code4 for dth disparity level at time t and u() n n : n (mod 4), n 4b1. all the conversion neurons use an a1 activation function and reset to membrane potential upon spiking. notice that (a1(v cnv 2,,d (t)), a1(v cnv 1,,d (t)), a1(v cnv 0,,d (t))) is a length-3 thermometer code representation of a value in 0, 1, 2, 3, representing the th digit in the base-4 representation of vd(t1). for a set of qt-coded inputs, the wta system is realized by a cascade of (b1) feed-forward pruning networks where each of the pruning networks process only 3-bits of the qt codes and prune the inputs not equal to the bit-wise maximum of corresonding 3-bits thermometer codes from all inputs. now starting from the most significant bits, all the inputs smaller than the maximum will be pruned at different stages and only the winner(s) will survive at the output of the last cascade network. the membrane potential v wta,d of stage and disparity index d is given by, v wta,d (t) t w,d(t1) a1(v cnv ,b,d(t)) max ddw,d (t1)0 a1(v cnv ,b,d(t)) 3, (5) where w,d(t) a1(v wta 1,d (t)), 0, 1, if (6) note that the function w,d(t) represents the candidate status of the d-th input at the end of -th stage. initially all the 4the i variable indexing (vi d ) starts from the right of the thermometer code vd of (a1(v dot p,q,1), ...,a1(v dot p,q,k )) 0, 1k the dependence of vd and d on pixels p, q is implicit and is not shown to simplify notation. inputs are winning candidates (w0,d(t) 1) and the status changes after the input is pruned at any stage indicating it is out of the competition and the selection process continues with remaining candidates. as an illustration, winner is computed from the example set of numbers in table and the winner selection process is shown in table a left-right consistency check is then performed to verify that for each left-rectified pixel p matched to right-rectified pixel q, it is also the case that right-rectified pixel q gets matched to left-rectified pixel p. this is achieved using two parallel wta streams. stream calculates the winner disparities for left-to-right matching, and stream calculates the winner disparities of right-to-left matching. the outputs of each stream are represented by d retinotopic maps expressed in a fixed resolution (dvi,j,d(t), d 0, ..., d 1, v l,r), where events represent the retinotopic winner disparities for that stream. the streams are then merged to produce the disparity map d l,r i,j,d(t) a1(v l,r i,j,d (t)) where v l,ri,j,d (t) t dli,j,d(t 1) d r i,jd,d(t 1) a1(v spl (i,j,l,)(t t)) (7) where t is the propagation delay of the first layer splitter output events until the left-right consistency constraint merging takes place. this enforces that an output disparity is produced at time-stamp t and pixel (i, j) only for leftrectified pixel (i, j), where an event was produced at t t.", "section_index": 4}, {"content": "we evaluate the performance of the system on sequences of random dot stereograms (rds) representing a rotating synthetic 3d object (fig. 4a-f), and two real world sets of sequences, consisting of a fast rotating fan (fig. 4g-m) and a rotating toy butterfly (fig. 4n-u) captured using the davis stereo cameras. the synthetic dataset provides dense disparity estimates, which are difficult to acquire with the sparse event based cameras. the dataset is generated by assigning to each left sensor pixel a random event with a probability per polarity. similarly, each right sensor pixel is assigned a value by projecting it to the 3d scene and reprojecting the corresponding data-point to the left camera coordinate frame to find the closest pixel value. self-occluded pixels are assigned random values. for the non-synthetic datasets, a kinect is used to extract ground truth of the scene structure. this also entails a calibration process for transforming the undistorted kinect coordinate frame to the undistorted davis sensor coordinate frame. the fan sequence is useful for testing the ability of the algorithm to operate on rapidly moving objects. varying orientations of the revolving fan add continuously varying depth gradient to the dataset. ground truth is extracted in terms of the plane in 3d space representing the blades’ plane of rotation (fig. the butterfly sequence tests the ability of the algorithm to operate on nonrigid objects which are rapidly rotating in a circular plane approximately perpendicular to the y-axis. ground truth is extracted in terms of the coordinates of the circle spanned by the rotating butterfly (fig. nine fan sequences (3 distances orientations) and three butterfly sequences (3 distances) are used. the dataset, with kinect ground-truth, is at: url. on the synthetic dataset we measure the average absolute disparity error, and the average recall, which is defined as the fraction of pixels where a disparity measurement was found. on the non-synthetic data, performance is measured in terms of precision, which is defined as the median relative error xx x between each 3d coordinate x extracted in the davis frame using the neuromorphic algorithm, and the corresponding ground coordinate x in the aligned kinect coordinate frame. performance is also reported in terms of the recall, defined herein as the percentage of davis pixels containing events, where a disparity estimate was also extracted. we tested a suite of sixty stereo disparity networks generated with ranges of spatiotemporal scales, denoising parameters, kernel match thresholds, with/without left-right consistency constraints etc. power is measured using the same process described in we calculate the power consumed by an n-chip system by measuring power on a single truenorth chip model running on an ns1t board with a high event rate input generated by the fan sequence. this board has circuitry to measure the power consumed by a truenorth chip. we multiply the power value by n to extrapolate the power consumed by an n-chip system. measurements are reported at supply voltages of 0.8v, 1.0v. total chip power is the sum of passive power, computed by multiplying the idle power by the fraction of the chip’s cores under use, and active power computed by subtracting idle power from the total power measured when the system is accepting input events the rds is tested on a model using spatial windows, left-right consistency constraints, no morphological erosion/dilation after rectification, and disparity levels (0-30) plus a no-disparity’ indicator (often occurring due to self-occlusions). we also experiment with a postprocessing phase with erosion and dilation applied to output disparity maps in order to better regularize the output. average disparity error and recall before regularization is 0.19/0.66 and post-regularization is 0.04/0.63. we observe major improvements due to the regularization, often occurring in self-occluded regions. errors increase in slanted regions due to foreshortening effects. the left-right consistency constraint decreases false predictions in those regions. the evaluation on the non-synthetic dataset was done under the practical constraints of the availability of a limited number of ns1e boards on which non-simulated models could be run, as well as the need to process the full davis inputs at as high of a throughput as possible. the models that run on live davis input are operated at spike injection rate of up to 2,000hz (a new input every 1/2,000 seconds) and disparity map throughput of 400hz at a 0.5ms tick period (400 distinct disparity maps produced every second) across a cluster of truenorth chips. single chip passive/active power on a characteristic model and input is 34.4mw/35.8mw (0.8v) and 82.1mw/56.4mw (1.0v). running a model at the full 2,000hz throughput comes at the expense of an increased neuron count. by adding a multiplexing spiking network to the network, we are able to reuse each feature-extraction/wta circuit to process the disparities for different pixels, effectively decreasing the maximum disparity map throughput from 2,000hz to 400hz, requiring fewer chips to process the full image (9 truenorth chips). we tested the maximum disparity map throughput achievable, by executing a one-chip model on a cropped input, with no multiplexing (one disparity map ejected per tick) at a 0.5ms tick period, achieving the 2,000hz disparity map throughput. we tested sixty models on the truenorth simulator which provides a spike-forspike equivalent behavior to the chip. we achieved best relative errors of and on the fan and butterfly sequence respectively (fig. we also observe qualitatively good performance (fig. it is observed that the temporal scale has a higher effect on accuracy than spatial scale. left-right consistency constraints are typically present in the best performing fan-sequence models, but not so in the butterfly sequences. distance and orientation do not have a significant effect on performance. see supplementary materials for more details.", "section_index": 5}, {"content": "we have introduced an advanced neuromorphic 3d vision system uniting a pair of davis cameras with multiple truenorth processors, to create an end-to-end, scalable, event-based stereo system. by using a spiking neural network, with low-precision weights, we have shown that the system is capable of injecting event streams and ejecting disparity maps at high throughputs, low latencies, and low power. the system is highly parameterized and can operate with other event based sensors such as atis or dvs table compares our approach with the literature on event based disparity. comparative advantages are low power, multi-resolution disparity calculation, scalability to live sensor feed with large input sizes, and evaluation using synthetic as well as real world fast movements and depth gradients, in neuromorphic, non von-neumann hardware. the implemented neuromorphic stereo disparity system achieves these advantages, while consuming less power per pixel per disparity map compared to the stateof-the-art the homogeneous computational substrate provides the first example of a fully end-to-end low-power, high throughput fully event-based neuromorphic stereo system capable of running on live input event streams, using a fully graph-based computation model, where no frames, arrays or other such data-structures are used.", "section_index": 6}], "id": 1016}
{"text": [{"content": "the challenge of learning the causal structure underlying a certain phenomenon is undertaken by connecting the set of conditional independences (cis) readable from the observational data, on the one side, with the set of corresponding constraints implied over the graphical structure, on the other, which are tied through a graphical criterion known as d-separation (pearl, 1988). in this paper, we investigate the more general setting where multiple observational and experimental distributions are available. we start with the simple observation that the invariances given by cis/dseparation are just one special type of a broader set of constraints, which follow from the careful comparison of the different distributions available. remarkably, these new constraints are intrinsically connected with do-calculus (pearl, 1995) in the context of soft-interventions. we then introduce a novel notion of interventional equivalence class of causal graphs with latent variables based on these invariances, which associates each graphical structure with a set of interventional distributions that respect the do-calculus rules. given a collection of distributions, two causal graphs are called interventionally equivalent if they are associated with the same family of interventional distributions, where the elements of the family are indistinguishable using the invariances obtained from a direct application of the calculus rules. we introduce a graphical representation that can be used to determine if two causal graphs are interventionally equivalent. we provide a formal graphical characterization of this equivalence. finally, we extend the fci algorithm, which was originally designed to operate based on cis, to combine observational and interventional datasets, including new orientation rules particular to this setting.", "section_index": 0}, {"content": "explaining a complex system through their cause and effect relations is one of the fundamental challenges in science. data is collected and experiments are performed with the intent of understanding how a certain phenomenon comes about, or how the underlying system works, which could be social, biological, artificial, among others. the study of causal relations can be seen through the lens of learning and inference 16, the learning component is concerned with discovering the causal structure, which is the very subject of interest in many domains, since they can provide insight about equal contribution. 33rd conference on neural information processing systems (neurips 2019), vancouver, canada. how a complex system works and lead to better understanding about the phenomenon under investigation. the latter, inference, attempts to leverage the causal structure to compute quantitative claims about the effect of interventions and retrospective counterfactuals, which are critical to assign credit, understand blame and responsibility, and perform judgement about fairness in decision-making. one of the most popular languages used to encode the invariances needed to reason about causal relations, for both learning and inference, is based on graphical models, and appears under the rubric of causal graphs 16, 21, a causal graph is a directed acyclic graph (dag) with latent variables, where each edge encodes a causal relationship between its endpoints: x is a direct cause of y , i.e., x y , if, when the remaining factors are held constant, forcing x to take a specific value affects the realization of y , where x,y are random variables representing some relevant features of the system. the task of learning the causal structure entails a search over the space of causal graphs that are compatible with the observed data; the collection of these graphs forms what is called an equivalence class. the most popular mark imprinted on the data by the underlying causal structure that is used to delineate an equivalence class are conditional independence (ci) relations. these relations are the most basic type of probabilistic invariances used in the field and have been studied at large in the context of graphical models since, at least, (see also 5). while cis are powerful and have been the driving force behind some of the most prominent structural learning algorithms in the field 16, 21, including the pc, fci, these are constraints specific for one distribution. in this paper, we start by noting something very simple, albeit powerful, that happens when a combination of observational and experimental distributions are available: there are constraints over the graphical structure that emerge by comparing these different distributions, and which are not of ci-type2. remarkably, and unknown until our work, the converse of the causal calculus developed by pearl offers a systematic way of reading these constraints and tying them back to the underlying graphical structure. in reference to their connection to the do-calculus rules (or a generalization, as discussed later), we call these constraints the do-constraints. for concreteness, consider the graph in fig. 1(a), where the dashed-bidirected arrow represents hidden variables that generate variations of the two observed variables, x,y in this case. suppose the observational (conditional) distribution and an interventional distribution on x are available, which are written as p(yx), p(ydo(x)), respectively. suppose we contrast these two distributions and the test evaluating the expression p(ydo(x)) p(yx) comes out as false. this is called a do-see test since the experimental (or do) and observational (see) distributions are contrasted. based on the second rule of do-calculus, one can infer that there is an open backdoor path from x to y , where the edge adjacent to x on this path has an arrowhead into x. in our setting, we do not have access to the true graph, but we leverage this and the other do-constraints to reverse engineer the process and try to learn the structure. broadly speaking, do-constraints will play a critical role for learning, in the same way ci/d-separation plays in learning when only observational data is available. to the best of our knowledge, this type of constraints appeared first at the very definition of causal bayesian networks (cbns) in and then were leveraged to design efficient experiments to learn the causal graph in we assume throughout this work that interventions are soft. a soft intervention affects the mechanism that generates the variable, while keeping the causal connections intact. soft-interventions are widely employed in biology and medicine, where it is hard to change the underlying system, but possibly 2recall that a ci represents a constraint readable from one specific distribution saying that the value of z is irrelevant for computing the likelihood of y once we know the value of x, i.e., p(y x,z) p(y x),x,y,z. for our characterization, we utilize an extension of the causal calculus to soft interventions introduced in under soft-interventions, the do-see test can be written as checking if px(yx) p(yx), where px is the distribution obtained after a soft intervention on x. the second observation leveraged here follows from another realization by pearl that interventions can be represented explicitly in the graphical model he then introduced what we call f-nodes, which graphically encode the changes due to an intervention and the corresponding parametrization (see also 16, sec. this is important in our context since the do-calculus tests will be visible more explicitly in the graph. the graph obtained by adding f-nodes to the causal graph is called the augmented graph. the same construct was used more prominently in in the context of inference and identification. 1b, the existence of the backdoor path from x to y , as detected by rule of the calculus, can be captured by the statement fx is not d-separated from y given x. in the context of structure learning, similar constructions have been leveraged in the literature 13, we further make a specific assumption throughout the paper about the soft-interventions. we call it the controlled experiment setting, where each variable is intervened with the same mechanism change across different interventions. 1c, suppose we are given distributions from two controlled experiments px, px,z along with observational data. we can then use fz to capture the invariances between px,z and px. for example, if px,z(y) , px(y), for some y, we can read that fz y fx , fx,z accordingly, given a set of interventional distributions, we construct an augmented graph by introducing an f-node for every unique set difference between pairs of controlled intervention sets (more on that later on). without the controlled experiment assumption, our machinery can still be used if one knows which mechanism changes are identical and by constructing f-nodes to reflect and capture the mechanism difference across two interventions. for simplicity of presentation, however, we restrict ourselves to the controlled experiment setting and do not pursue this route explicitly. to encapsulate the distributional invariants directly induced by the causal calculus rules3, we call a set of interventional distributions i-markov to a graph, if these distributions respect the causal calculus rules relative to that graph. note that the notion of i-markov is first introduced in 9, for causally sufficient systems without the use of do-constraints4. for our characterization, we first extend the causal calculus rules to operate between arbitrary sets of interventions. we call two causal graphs d1,d2 i-markov equivalent if the set of distributions that are i-markov tod1 andd2 are the same. using the augmented graph, we identify a graphical condition that is necessary and sufficient for two cbns with latents to be i-markov equivalent. finally, we propose a sound algorithm for learning the augmented graph from interventional data. our contributions can be summarized as follows: we propose a characterization of i-markov equivalence between two causal graphs with latent variables for a given intervention set i that is based on a generalization of do-calculus rules to arbitrary subsets of interventions. we show a graphical characterization of i-markov equivalence of causal graphs with latents. we introduce a learning algorithm for inferring the graphical structure using a combination of observational and interventional data and utilizing the corresponding new constraints. this procedure comes with a new set of orientation rules. we formally show its soundness.", "section_index": 1}, {"content": "in this section, we introduce necessary concepts that we use throughout the paper. upper case letters denote variables and lower case letters denote an assignment. also, bold letters denote sets. causal bayesian network (cbn): let p(v) be a probability distribution over a set of variables v, and let px(v) denote the distribution resulting from the hard intervention do(x x), which sets x v to constants x. let p denote the set of all interventional distributions px(v), for all x v, including p(v). a directed acyclic graph (dag) over v is said to be a causal bayesian network compatible with p if and only if, for all x v, px(v) ivix p(vipai), for all v consistent with x, and where pai is the set of parents of vi 16, 1, pp. if so, we refer to the dag as causal. 3there may be constraints that can be obtained by applying the rules multiple times we do not consider here. 4in the causally sufficient case, name is in reference to both global and local markov conditions. however, in our work, the name stems from our observation that the do-constraints correspond to the global markov conditions in the augmented graph. given that a subset of the variables are unmeasured or latent,d(vl,e) represents the causal graph where v and l denote the measured and latent variables, respectively, and e denotes the edges. a dashed bi-directed edge is used instead of l , where l l, whenever l is a root node with exactly two children. the observed distribution p(v) is obtained by marginalizing l out. p(v) l itivl p(tipai) clearly, the joint distribution over v does not factorize relative to d in a typical fashion, since markovianity is no longer valid, but it does relative to both v and l. still, ci relations can be read from the graph using a graphical criterion known as d-separation. also, two causal graphs are called markov equivalent whenever they share the same set of conditional independences over v. soft interventions: another common type of intervention is soft, where the original conditional distributions of the intervened variables x are replaced with new ones, without completely eliminating the causal effect of the parents. accordingly, the interventional distribution px(v) becomes as follows, where p(xipai) , p(xipai) is the new conditional distribution set by the intervention: px(v) l ixix p(xipai) jt jx p(t jpa j) in this work, we assume that all the soft interventions are controlled. this means that for any two interventions i, j v where xi i j, we have pi(xipai) pj(xipai). ancestral graphs: we now introduce a graphical representation of equivalence classes of causal graphs with latent nodes. a mixed graph can contain directed and bi-directed edges. a is an ancestor of b if there is a directed path from a to b. a is a spouse of b if a b is present. if a is both a spouse and an ancestor of b, this creates an almost directed cycle. an inducing path relative to l is a path on which every non-endpoint node x l is a collider on the path (i.e., both edges incident to the node are into it) and every collider is an ancestor of an endpoint of the path. a mixed graph is ancestral if it does not contain a directed or almost directed cycle. it is maximal if there is no inducing path (relative to the empty set) between any two non-adjacent nodes. a maximal ancestral graph (mag) is a graph that is both ancestral and maximal given a causal graphd(v,l), a magmd over v can be constructed such that both the independence and the ancestral relations among variables in v are retained, see, for example, 27, p. a triple x,y,z is an unshielded triple if x and y are adjacent, y and z are adjacent, and x and z are not adjancent. if both edges are into y , then the triple is referred to as unshielded collider. ,w,z,y, is a discriminating path for z if (1) p includes at least three edges; (2) z is a non-endpoint node on p, and is adjacent to y on p; and (3) x is not adjacent to y , and every node between x and z is a collider on p and is a parent of y two mags are markov equivalent if and only if (1) they have the same adjacencies; (2) they have the same unshielded colliders; and (3) if a path p is a discriminating path for a vertex z in both graphs, then z is a collider on the path in one graph if and only if it is a collider on the path in the other. a pag, which represents a markov equivalence class of a mag, is learnable from the independence model over the observed variables, and the fci algorithm is a standard sound and complete method to learn such an object related work: learning causal graphs from a combination of observational and interventional data has been studied in the literature 3, 11, 7, 20, 8, 12, for causally sufficient systems, the notion and characterization of interventional markov equivalence has been introduced in 9, more recently, showed that the same characterization can be used for both hard and soft interventions. for causally insufficient systems, uses sat solvers to learn a summary graph over the observed variables given data from different experimental conditions. introduces an algorithm to pool experimental datasets together and runs a modification of fci to learn an augmented graph; however, they do not consider characterizing an equivalence class. notations: for random variables x,y,z, the ci relation x is independent of y conditioned on z is shown by x y z the d-separation statement node x is d-separated from y given z in graphd is shown by (x y z )d. i 2v is reserved for a set of interventions, where 2v is the power set of v. we show the symmetric difference by i4j b (i \\ j) (j \\ i). dx denotes the graph obtained fromd where all the incoming edges to the set of nodes in x are removed. similarly,dx denotes the removal of outgoing edges. we assume that there is no selection bias. a star on an endpoint of an edge is used as a wildcard to denote circle, arrowhead, or tail.", "section_index": 2}, {"content": "one of the most celebrated results in causal inference comes under the rubric of do-calculus (or causal calculus) 18, the calculus consists of a set of inference rules that allows one to create a map between distributions generated by a causal graph when certain graphical conditions hold in the graph. the calculus was developed in the context of hard interventions, and recent work presented a generalization of this result for soft interventions 4, which we state next: theorem (special case of thm. let d (v l,e) be a causal graph. then, the following holds for any strictly positive distribution consistent withd. rule (see-see): for any x v and disjoint y,z,w v px(yw, z) px(yw) if y z w ind. rule (do-see): for any disjoint x,y,z v and w v \\ (z y) px,z(yz,w) px(yz,w) if y z w indz rule (do-do): for any disjoint x,y,z v and w v \\ (z y) px,z(yw) px(yw) if y z w indz(w), where z(w) z are non-ancestors of w ind. the first rule of the calculus is a d-separation type of statement relative to a specific interventional distribution px, which says that y z w ind implies the corresponding conditional independence px(yw, z) px(yw). note that the converse of this rule is the work horse underlying most of the structure learning algorithms found in practice, which says that if some independence hold in p, this would imply a corresponding graphical separation (under faithfulness). in the case just mentioned, this would imply that y and z should be separated ind, meaning, they have neither a directed nor a bidirected arrow connecting them. from this understanding, we make a very simple, albeit powerful observation i.e., the converse of the other two rules should offer insights about the underlying graphical structure as well. to witness, consider the causal graph d x y, x cd y, and suppose we have the observational and interventional distributions p(y, x) and px(y, x), respectively. using the ci tests p(y, x) , p(y)p(x) and px(y, x) , px(y) px(x), we infer that the two variables are dependent (or not independent) and consequently d-connected in the graph, while no claim can be made about the causal relation between them. given the inequality px(y) , p(y), we infer that the condition for rule does not hold and y x indx hence, x must be a cause of y changing the value of x has a downstream effect on y similarly, given the inequality px(y x) , p(y x), the condition related to rule does not hold, and y x indx the implication in this case is that there is an unblockable backdoor path between x and y that is into x, i.e., a latent variable. alternatively, ifd x y, then px(y x) p(y x), under faithfulness, implies the absence of a latent variable by the converse of rule broadly speaking, rule allows one to infer causal relations between variables, and consequently directed edges in the causal graph. since the compared interventional distributions differ by a subset of interventions (z), we call this the do-do test. on the other hand, rule allows one to infer spurious relations between variables, and consequently latent variables in the causal graph5. the do-see naming of the test stems from the fact that we compare a distribution with an intervention on a subset z (do) versus another which only conditions on z (see). naturally, rule is the usual conditional independence test that allows one to detect that neither directed nor bidirected arrow exists. putting together these rules, we show in corollary a generalization of rules and note that rule appears when j i and i \\ j w; similarly, rule can be seen when j i and (i \\ j) w corollary (mixed do-do/do-see). let d (v l,e) be a causal graph. under the controlled intervention assumption, for any i, j v and disjoint y,w v, we have the following: pi(yw) pj(yw) if y k w \\wk indwk,r(w), where k b i4j, wk cw k, r b k\\wk, and r(w) r are non-ancestors of w ind. 5more precisely, rule allows us to detect inducing paths that are into both variables. in general, the proposed rule is a mixture of rules and as we could be conditioning in w on a subset of the symmetrical difference set i4j. for instance, consider the causal graph d c cd a b,c cd b and suppose we have the interventional distributions pa,b and pc,b. since b a,c in da,c , then pa,b(ba) pb,c(ba). this generalization will soon play a significant role in the characterization and learning of the interventional equivalence class.", "section_index": 3}, {"content": "in this section, the new do-constraints will be used to define the notion of interventional markov equivalence. then, we will characterize when two causal graphs are equivalent in accordance to the proposed definition. we start by defining the notion of interventional markov as shown below. consider the tuples of absolutely continuous probability distributions (pi)ii over a set of variables v. a tuple (pi)ii satisfies the i-markov property with respect to a graphd (vl,e) if the following holds for disjoint y,z,w v: (1) for i i: pi(yw, z) pi(yw) if y z w ind. (2) for i, j i: pi(yw) pj(yw) if y k w \\wk indwk,r(w), where k b i4j, wk cw k, r b k\\wk, and r(w) r are non-ancestors of w ind. the set of all tuples that satisfy the i-markov property with respect tod are denoted by pi (d,v). the two conditions used in the definition correspond to rule of theorem and that of corollary notice that the traditional markov definition only considers the first condition over the observational distribution p(v); a case included in the i-markov whenever i accordingly, two causal graphs are said to be i-markov equivalent if they license the same set of distribution tuples. this notion is formalized in the following definition. given two causal graphsd1 (vl1,e1) andd2 (vl2,e2), and an intervention set i 2v,d1 andd2 are called i-markov equivalent if pi (d1,v) pi (d2,v). one challenge with definition is that testing for the d-separation statement in condition (2) requires a mutilated graph where we cut some of the edges in d. this makes it harder to represent all the constraints imposed by a causal graph compactly. accordingly, we use the notion of an augmented graph that is introduced below (definition 3). in words, the construction of the augmented graph goes as follows. first, initialize the augmented graph to the input causal graph. then, for every distinct symmetric set difference between i, j i , denoted by si, introduce a new node fi and make it a parent to each node in si, i.e., fi s si. note that this type of construction has been used in the literature to model interventions 17, for example, for i , x, figure 2a presents the augmented graph corresponding to the causal graph, which is the induced subgraph over x,w,z,y. node fx is added in accordance with the symmetrical difference set ( \\ x) (x \\ ) x. definition (augmented graph). consider a causal graphd (v l,e) and an intervention set i 2v. the augmented graph of d with respect to i , denoted as augi (d), is the graph constructed as follows: augi (d) (v f ,e e) where f b fiik and e (fi, j)ik, jsi the significance of the augmented graph construction is illustrated by proposition 1, which provides criteria to test the d-separation statements in definition equivalently from the corresponding augmented graph of a causal graph. back to the example in figure 2a, the statement y x z in dx can be equivalently tested by the statement y fx z in the corresponding augmented graph. similarly, y x indx can be equivalently tested by y fx x in augi (d). consider a causal graphd (v l,e) and the corresponding augmented graph augi (d) (v l f ,e e) with respect to an intervention set i , where f fiik. let si be the set of nodes adjacent to fi,i k. we have the following equivalence relations. for disjoint y,z,w v: (y z w )d (y z w, fk )aug(d) (1) for disjoint y,w v, where wi bw si,r b si \\wi: (y si w \\wi )dwi ,r(w) (y fi w, fk\\i )aug(d) (2) in order to characterize causal graphs that are i-markov equivalent, we draw some insight from the markov equivalence of causal graphs with latents. ancestral graphs, and more specifically mags, were proposed as a representation to encode the d-separation statements of a causal graph among the measured variables while not explicitly encoding the latent nodes. the definition below (def. 4) introduces the augmented mag that is constructed over an augmented graph. since all the constraints in the i-markov definition can be tested by d-separation statements in the augmented graph, then an augmented mag preserves all those constraints. 2c and 2d present the augmented mags corresponding to the augmented graphs in figs. notice that fx and y are adjacent in both mags since they are not separable by any set in the augmented graphs. definition (augmented mag). given a causal graphd (v l,e) and an intervention set i , the augmented mag is the mag constructed over v from augi (d), i.e., mag(augi (d)). below, we derive a characterization for two causal graphs to be i-markov equivalent two causal graphs are i-markov equivalent if their corresponding augmented mags satisfy the three conditions given in theorem for example, the two augmented mags in figures 2c and 2d satisfy the three conditions, hence the original causal graphs are in the same i-markov equivalence class. two causal graphs d1 (v l1,e1) and d2 (v l2,e2) are i-markov equivalent for a set of controlled experiments i if and only if for m1 mag(augi(d1)) and m2 mag(augi (d2)): m1 andm2 have the same skeleton; m1 andm2 have the same unshielded colliders; if a path p is a discriminating path for a node y in bothm1 andm2, then y is a collider on the path in one graph if and only if it is a collider on the path in the other.", "section_index": 4}, {"content": "in this section, we develop an algorithm to learn the augmented graph from a combination of observational and interventional data, which consequently recovers the causal graph. however, similar to the observational case, it is typically impossible to completely determine the causal graph from the available measured data, especially when latents are present. then, the objective is to learn a class of augmented mags consistent with data. for this, we define an augmented pag as follows. given a causal graph d and an intervention set i , letm mag(augi(d)) and let m be the set of augmented mags corresponding to all the causal graphs that are i-markov equivalent tod. an augmented pag ford, denoted g pag(augi (d)), is a graph such that: g has the same adjacencies asm, and any member of m does; and every non-circle mark in g is an invariant mark in m. as with any learning algorithm, some faithfulness assumption is needed to infer graphical properties from the corresponding distributional constraints. hence, we assume that the given interventional distributions are c-faithful to the causal graphd as defined below. algorithm algorithm for learning augmented pag 1: function learnaugpag(i , (pi)ii ,v) 2: (f ,s, ) createaugmentednodes(i ,v) 3: v v f 4: phase i: learn adjacencies and seperating sets 5: form the complete graph g on v where between every pair of nodes there is an edge 6: for every pair x,y v do 7: if x f y f then 8: s eps et(x,y) , s epflag(x,y) true 9: else 10: (s eps et(x,y), s epflag) do-constraints((pi)ii , x,y,v,f , ) 11: if s epflag true then 12: remove the edge between x,y in g. 13: phase ii: learn unshielded colliders 14: for every unshielded triple x,z,y in g, orient it as x z y iff z s eps et(x,y) 15: phase iii: apply orientation rules 16: apply fci rules in together with the following additional rules until none applies. 17: rule 8: for any fk f , orient adjacent edges out of fk. 18: rule 9: for any fk f that is adjacent to a node y sk 19: if sk 1, orient x y as x y for x sk. algorithm creating f-nodes. 1: function createaugmentednodes(i ,v) 2: f ,s , k 0, : n 2v 2v 3: for all pairs i, j i , if i4j s do 4: set k k 1, set sk i4j, add fk to f , add sk to s, set (k) (i, j). consider a causal graphd (v l,e). a tuple of distributions (pi)ii p(d,v) is called c-faithful to graphd if the converse for each of the conditions given in definition holds. algorithm presents a modification of the fci algorithm to learn augmented pags. to explain the algorithm, we first describe fci which, given an independence model over the measured variables, proceeds in three phases 25: in phase i, the algorithm initializes a complete graph with circle edges (), then it removes the edge between any pair of nodes if a separating set between the pair exists and records the set. in phase ii, the algorithm identifies unshielded triples a, b,c and orients the edges into b if b is not in the separating set of a and c. finally, in phase iii, fci applies the orientation rules. only one of the rules uses separating sets while the rest use mag properties, and soundness and completeness of the previous phases the skeleton is correct and all the unshielded colliders are discovered. we note that fci looks for any separating sets, and not necessarily the minimal ones. we also observe that if two nodes x,y are separated given z in augi (d), they are also separated given z f since f are root nodes by construction, i.e., all the edges incident on f-nodes are out of them. algorithm follows a similar flow to that of the fci. in phase i, it learns the skeleton of the augmented pag. function createaugmentednodes() in alg. creates the f-nodes by computing the set s of unique symmetric difference sets from all pairs of interventions in i sigma () maps every f-node to a source pair of interventions, which is used later on to perform the do-tests. the algorithm starts by creating a complete graph of circle edges between v f then, it removes the edge between any two nodes x and y if a separating set exists. if the two nodes are f-nodes, then they are separated by the empty set by construction. otherwise, it calls the function do-constraints() in alg. to search for a separating set using the corresponding do-constraints. the function routine works as follows: if the two nodes are random variables (and not f-nodes), then an arbitrary distribution is chosen and we find a subset w that establishes conditional independence between x and y (rule of thm. else, one of the two nodes is an f-node; without loss of generality, we choose it to be x. the algorithm then looks for a subset w that satisfies the invariance of corollary 1, i.e., pi(yw) pj(yw). is similar to the fci counterpart. for the edge orientation phase, note that the augmented mag is a mag indeed, hence all the fci orientation rules still apply. therefore, phase iii algorithm find m-separation sets via calculus tests. 1: function do-constraints(i , (pi)ii , x,y,v,f , ) 2: s eps et , s epflag false 3: if x f y f then 4: pick i i arbitrarily. 5: for w v \\ f do 6: if pi(yw, x) pi(yw) then s eps et w f , s epflag true 7: else 8: suppose x f ,y f and x fi without loss of generality. 9: (i, j) (i) 10: for w v \\ (f y) do 11: if pi(yw) pj(yw) then s eps et w,f \\ fi, s epflag true uses the fci orientation rules along with the following two new ones. the algorithm keeps applying the rules until none applies anymore. rule (f-node edges): for any edge adjacent to an f node, orient the edge out of the f node. rule (inducing paths): if fk f is adjacent to a node y sk and sk 1, e.g., sk x, then orient x y out of x, i.e., x y the intuition for this rule is as follows: if fk is adjacent to a node y sk in g, then there is an inducing path p between fk and y in augi(d), where d is any causal graph in the equivalence class. since fk is a root node and by the properties of inducing paths, the subpath of p from x to y is an inducing path as well and x is an ancestor of y in augi (d). hence, the edge between x and y is out of x and into y in mag(augi (d)) and consequently in g. we give an example to illustrate the steps of the algorithm in figure 3, where i , x. figure 3a shows the augmented causal graph, i.e., augi (d), and figure 3b shows the corresponding augmented mag, i.e., mag(augi (d)). nodes fx and z are separable in augi (d) given the empty set and this can be tested by the do-constraint p(z) px(z). similarly, we can infer the separation of fx and w by the test p(w x) px(w x). figure 3c shows the graph obtained after applying the seven rules of the fci together with rule finally, by applying rule 9, we infer that the edge between x and y has a tail at x and we obtain the graph in figure 3d. the soudness of the algorithm is shown next. consider a set of interventional distributions (pi)ii c-faithful to a causal graph d (v l), where i is a set of controlled experiments. algorithm is sound, i.e., every adjacency and orientation is common for all mag(aug(d)) whered is i-markov equivalent tod.", "section_index": 5}, {"content": "we investigate the problem of learning the causal structure underlying a phenomenon of interest from a combination of observational and experimental data. we pursue this endeavor by noting that a generalization of the converse of pearl’s do-calculus (thm. 1) leads to new tests that can be evaluated against data. these tests, in turn, translate into constraints over the structure itself. we then define an interventional equivalence class based on such criteria (def. 1), and then derive a graphical characterization for the equivalence of two causal graphs (thm. finally, we develop an algorithm to learn an interventional equivalence class from data, which includes new orientation rules.", "section_index": 6}], "id": 1017}
{"text": [{"content": "recent years have seen increasingly complex question-answering on knowledge bases (kbqa) involving logical, quantitative, and comparative reasoning over kb subgraphs. neural program induction (npi) is a pragmatic approach toward modularizing the reasoning process by translating a complex natural language query into a multi-step executable program. while npi has been commonly trained with the gold’’ program or its sketch, for realistic kbqa applications such gold programs are expensive to obtain. there, practically only natural language queries and the corresponding answers can be provided for training. the resulting combinatorial explosion in program space, along with extremely sparse rewards, makes npi for kbqa ambitious and challenging. we present complex imperative program induction from terminal rewards (cipitr), an advanced neural programmer that mitigates reward sparsity with auxiliary rewards, and restricts the program space to semantically correct programs using high-level constraints, kb schema, and inferred answer type. cipitr solves complex kbqa considerably more accurately than key-value memory networks and neural symbolic machines (nsm). for moderately complex queries requiring 2to 5-step programs, cipitr scores at least higher f1 than the competing systems. on one of the hardest class of programs (comparative reasoning) with steps, cipitr outperforms nsm by a factor of and memory networks by times.1 now at hike messenger 1the nsm baseline in this work is a re-implemented version, as the original code was not available.", "section_index": 0}, {"content": "structured knowledge bases , or multi-hop , or complex queries such as how many countries have more rivers and lakes than brazil?’’ complex queries require a proper assembly of selected operators from a library of graph, set, logical, and arithmetic operations into a complex procedure, and is the subject of this paper. relatively simple query classes, in particular, in which answers are kb entities, can be served with feed-forward and seq2seq networks. however, such systems show copying or rote learning behavior when boolean or open numeric domains are involved. more complex queries need to be evaluated as an acyclic expression graph over nodes representing kb access, set, logical, and arithmetic operators a practical alternative to inferring a stateless expression graph is to generate an imperative sequential program to solve the query. each step of the program selects an atomic operator and a set of previously defined variables as arguments and writes the result to scratch memory, which can then be used in subsequent steps. such imperative programs are preferable to opaque, monolithic networks for their interpretability and generalization to diverse domains. another transactions of the association for computational linguistics, vol. action editor: scott wen-tau yih. submission batch: 8/2018; revision batch: 11/2018; final submission: 1/2019; published 4/2019. c association for computational linguistics. distributed under a cc-by license. motivation behind opting for the program induction paradigm for solving complex tasks, such as complex question answering, is modularizing the end-to-end complex reasoning process. with this approach it is now possible to first train separate modules for each of the atomic operations involved and then train a program induction model that learns to use these separately trained models and invoke the sub-modules in the correct fashion to solve the task. these sub-modules can even be task-agnostic generic models that can be pretrained with much more extensive training data, while the program induction model learns from examples pertaining to the specific task. this paradigm of program induction has been used for decades, with rule induction and probabilistic program induction techniques in lake et al. as well as for manipulating a physical environment program induction has also seen initial promise in translating simple natural language queries into programs executable in one or two hops over a kb to obtain answers in contrast, many of the complex queries from saha et al. (2018), such as the one in figure 1, require up to 10-step programs involving multiple relations and several arithmetic and logical operations. sample operations include genset: collecting t : (h, r, t) kb, computing setunion, counting set sizes (setcount), comparing numbers or sets, and so forth. these operations need to be executed in the correct order, with correct parameters, sharing information via intermediate results to arrive at the correct answer. note also that the actual gold program is not available for supervision and therefore the large space of possible translation actions at each step, coupled with a large number of steps needed to get any payoff, makes the reward very sparse. this renders complex kbqa in the absence of gold programs extremely challenging. main contributions we present complex imperative program induction from terminal rewards’’ (cipitr),2 an advanced neural program induction (npi) system that is able to answer complex logical, quantitative, and comparative queries by inducing programs of length up to 7, using atomic operators and variable types. this, to our knowledge, is the first npi system to be trained with only the gold answer as (very distant) supervision for inducing such complex programs. cipitr reduces the combinatorial program space to only semantically correct programs by (i) incorporating symbolic constraints guided by kb schema and inferred answer type, and (ii) adopting pragmatic programming techniques by decomposing the final goal into a hierarchy of sub-goals, thereby mitigating the sparse reward problem by considering additional auxiliary rewards in a generic, task-independent way. we evaluate cipitr on the following two challenging tasks: and (ii) multi-hop kbqa in one of the more 2the code and reinforcement learning environment of cipitr is made public inurl/ cipitr. popularly used kbqa data sets webquestionssp webquestionssp involves complex multi-hop inferencing, sometimes with additional constraints, as we will describe later. however, csqa poses a much greater challenge, with its more diverse classes of complex queries and almost 20-times larger scale. on a data set such as csqa, contemporary models like neural symbolic machines are also unable to perform the necessary complex multi-step inference. cipitr outperforms them both by a significant margin while avoiding exploration of unwanted program space or memorization of low-entropy answer distributions. on even moderately complex programs of length 25, cipitr scored at least higher f1 than both. on one of the hardest class of programs of around steps (i.e., comparative reasoning), cipitr outperformed nsm by a factor of and kvmnet by a factor of further, we empirically observe that among all the competing models, cipitr shows the best generalization across diverse program classes.", "section_index": 1}, {"content": "whereas most of the earlier efforts to handle complex kbqa did not involve writable memory, some recent systems used end-toend differentiable neural networks. one of the state-of-the-art neural models for kbqa, the keyvalue memory network kvmnet learns to answer questions by attending on the relevant kb subgraph stored in its memory. use a stateless’’ model where neural network based subroutines are assembled using syntactic parsing. recently, reed and de freitas and bosnjak et al. proposed the nsm framework in absence of the gold program, which translates the kb query to a structured program token-by-token. while being a natural approach for program induction, nsm has several inherent limitations preventing generalization towards longer programs that are critical for complex kbqa. subsequently, it was evaluated only on webquestionssp , that requires relatively simpler programs. we consider nsm as the primary and kvmnet as an additional baseline and show that cipitr significantly outperforms both, especially on the more complex query types.", "section_index": 2}, {"content": "the csqa data set contains 1.15m natural language questions and its corresponding gold answer from wikidata knowledge base. figure shows a sample query from the data set along with its true program-decomposed form, the latter not provided by csqa. csqa is particularly suited to study the complex program induction (cpi) challenge over other kbqa data sets because: it contains large-scale training data of question-answer pairs across diverse classes of complex queries, each requiring different inference tools over large kb sub-graphs. poor state-of-the-art performance of memory networks on it motivates the need for sweeping changes to the npi’s learning strategy. the massive size of the kb involved (13 million entities and million tuples) poses a scalability challenge for prior npi techniques. availability of kb metadata helps standardize comparisons across techniques (explained subsequently). we adapt csqa in two ways for the cpi problem. removal of extended conversations: to be consistent with the nsm work on kbqa, we discard qa pairs that depend on the previous dialogue context. this is possible as every query is annotated with information on whether it is self-contained or depends on the previous context. relevant statistics of the resulting data set are presented in table use of gold entity, type, and relation annotations to standardize comparisons: our focus being on the reasoning aspect of the kbqa problem, we use the gold annotations of canonical kb entities, types, and relations available in the data set along with the the queries, in order to remove a prominent source of confusion in comparing kbqa systems (i.e., all systems take as inputs the natural language query, with spans identified with kb ids of entities, types, relations, and integers). although annotation accuracy affects a complete kbqa system, our focus here is on complex, multi-step program generation with only final answer as the distant supervision, and not entity/type/relation linking. in figure we illustrate one of the most complex questions from the the webquestionssp data set and its semantic parsed version provided by human annotator. questions in the webquestionssp data set are answerable from the freebase kb and tyically require up to 2-hop inference chains, sometimes with additional requirements of satisfying specific constraints. these constraints can be temporal (e.g., governingpositionheldfrom) ornon-temporal (e.g., governmentofficeposition ortitle). the human-annotated semantic parse of the questions provide the exact structure of the subgraph and the inference process on it to reach the final answer. as in this work, we are focusing on inducing programs where the gold entity relation annotations are known; for this data set as well, we use the human-annotations to collect all the entities and relations in the oracle subgraph associated with the query. the npi model has to understand the role of these gold program inputs in question-answering and learn to induce a program to reflect the same inferencing.", "section_index": 3}, {"content": "this subsection introduces the different notations commonly used by our model. nine variable-types: (distinct from kb types) kb artifacts: ent(entity), rel(relation), type base data types: int, bool, none (empty argument type used for padding) composite data types: set (i.e., set of kb entities) or mapset and mapint (i.e., a mapping function from an entity to a set of kb entities or an integer) twenty operators: genset(ent, rel, type) set verify(ent, rel, ent) bool genmap set(type, rel, type) mapset mapcount(mapset) mapint set union/ints/diff(set, set) set mapunion/ints/diff(mapset, mapset) mapset setcount(set) int selectatleast/atmost/more/less/ equal/approx(map int, int) set selectmax/min(map int) ent noop() (i.e., no action taken) symbols and hyperparameters: (typical values) numop: number of operators (20) numvartypes: number of variable types (9) maxvar: maximum number of variables accommodated in memory for each type (3) m: maximum number of arguments for an operator (none padding for fewer arguments) (3) dkey dval: dimension of the key and value embeddings (dkey dval) (100, 300) np nv: number of operators and argument variables sampled per operator each time (4, 10) f with subscript: some feed-forward network embedding matrices: the model is trained with a vocabulary of operators and variable-types. in order to sample operators, two matrices mop key rnum opdkey and mop val rnum opdval are needed for encoding the operator’s key and value embedding. the key embedding is used for looking up and retrieving an entry from the operator vocabulary and the corresponding value embedding encodes the operator information. the variable type has only the value embedding mvtype val rnum opdval as no lookup is needed on it. operator prototype matrices: these matrices store the argument variable type information for the m arguments of every operator in mop arg 0, 1, , numvartypesnum opm and the output variable type created by it in mop out 0, 1, memory matrices: this is the query-specific scratch memory for storing new program variables as they get created by cipitr. for each variable type, we have separate key and value embedding matrices mvar key rnum var typemax vardkey and mvar val rnum var typemax vardval , respectively for looking up a variable in memory and accessing the information in it. in addition, we also have a variable attention matrix mvar att r num var typemax var which stores the attention vector over the variables declared of each type. cipitr consists of three components: the preprocessor takes the input query and the kb and performs the task of entity, relation, and type linking which acts as input to the program induction. it also pre-populates the variable memory matrices with any entity, relation, type, or integer variable directly extracted from the query. the programmer model takes as input the natural language question, the kb, and the pre-populated variable memory tables to generate a program (i.e., a sequence of operators invoked with past instantiated variables as their arguments and generating new variables in memory). the interpreter executes the generated program with the help of the kb and scratch memory and outputs the system answer. during training, the predicted answer is compared with the gold to obtain a reward, which is sent back to cipitr to update its model parameters through a reinforce to investigate robustness of cipitr to linkage errors may be of future interest. we describe some of the foundational modules invoked by the rest of cipitr. memory lookup: the memory lookup looks up scratch memory with a given probe, say x (of arbitrary dimension), and retrieves the memory entry having closest key embedding to x. it first passes x through a feed-forward layer to transform its dimension to key embedding dimensionxkey. then, by computing softmax over the matrix multiplication ofmx key and xkey, the distribution over the memory variables for lookup is obtained. xkey f(x), xdist softmax(mx keyxkey) feasibility sampling: to restrict the search space to meaningful programs, cipitr incorporates both high-level generic or task-specific constraints when sampling any action. the generic constraints can help it adopt more pragmatic programming styles like not repeating lines of code or avoiding syntactical errors. the task specific constraints ensure that the generated program is consistent as per the kb schema or on execution gives an answer of the desired variable type. to sample from the feasible subset using these constraints, the input sampling distribution, xdist, is elementwise transformed by a feasibility vector xfeas followed by a l1-normalization. along with the transformed distribution, the top-k entries xsampled is also returned. algorithm feasibility sampling input: xdist rn (where n is the size of the population set over which lookup needs to be done) xfeas 0, 1n (boolean feasibility vector) k (top-k sampled) procedure: feassampling (xdist, xfeas, k) xdist xdist xfeas (elementwise multiply) xdist l1-normalized(xdist) xsampled k-argmax(xdist) output: xdist, xsampled writing a new variable to memory: this operation takes a newly generated variable, say x, of type xtype and adds its key and value embedding to the row corresponding to xtype in the memory matrices. further, it updates the attention vector forxtype to provide maximum weight to the newest variable generated, thus, emulating a stack like behavior. algorithm write a new variable to memory input: xkey, xval the key and value embedding of x xtype is a scalar denoting type of variable x procedure: writevartomem(xkey, xval, xtype) i is the 1st empty slot in the row mx keyxtype, : mvar keyxtype, i xkey mvar valxtype, i xval mvar attxtype, : l1-normalized(mvar attxtype, : one-hot(i)) in figure 3, we sketch the cipitr components; in this section we describe them in the order they appear in the model. query encoder: the query is first parsed into a sequence of kb-entities and non-kb words. kb entities e are embedded with the concatenated vector transe, and non-kb words with 0,glove(). the final query representation is obtained from a gru encoder as q. npi core: the query representation q is fed at the initial timestep to an environment encoding rnn, which gives out the environment state et at every timestep. this, along with the value embedding uvalt1 of the last output variable generated by the npi engine, is fed at every timestep into another rnn that finally outputs the program state ht. ht is then fed into the successive modules of the program induction engine as described below. the outvargen’ algorithm describes how to obtain uvalt1. procedure: npi core(et1, ht1, uvalt1) et gru(et1, u val t1) ht gru(et, u val t1, ht1) output: et, ht operator sampler: it takes the program state ht, a boolean vector p feas t denoting operator feasibility, and the number of operators to sample np. it passes ht through the lookup operation followed by feasibility sampling to obtain the top-np operations (pt). argument variable sampler: for each sampled operator p, it takes: (i) program state ht, (ii) the list of variable types v typep of the m arguments obtained by looking up the operator prototype matrix mop arg, and (iii) a boolean vector v feasp that indicates the valid variable configurations for the m-tuple arguments of the operator p. for each of them arguments, a feed-forward network fvtype first transforms the program state ht to a vector in r max var. it is then element-wise multiplied with the current attention state over the variables in memory of that type. this provides the programstate-specific attention over variables vattp,j which is then passed through the lookup function to obtain the distribution over the variables in memory. next, feasibility sampling is applied over the joint distribution of its argument variables, comprised of the m individual distributions. this provides the top-nv tuples of m-variable instantiations vp. output variable generator: the new variable up of type u type p mop outp is generated by the procedure outvargen by invoking a sampled operator p with m variables vp,1 vp,m of type vtypep,1 v type p,m as arguments. this also requires generating the key and value embedding, which are both obtained by applying different feedforward layers over the concatenated representation of the value embedding of the operator mop valp, argument types (mvtype valvtypep,1 mvtype valvtypep,m ) and the instantiated variables (mvar valvtypep,1 , vp,1 mvar valv type p,m , vp,m). the newly generated variable is then written to memory using algorithm writevartomem. procedure: argvarsampler(ht, v typep , v feasp , nv) forj 1, 2, ,m do vattp,j softmax(m var attv typep,j ) fvtype(ht) vdistp,j lookup(vattp,j , fvar,mvar keyv type p,j ) v distp v dist p,0 vdistp,1 vdistp,m , joint distribution v distp , vp feassampling(v distp , v feasp , nv) output: vp end-to-end cipitr training: cipitr takes a natural language query and generates an output program in a number of steps. a program is composed of actions, which are operators applied over variables , cipitr uses a beam search to obtain multiple candidate programs to provide feedback to the model from a single training instance. algorithm shows the pseudocode of the program induction algorithm (with beam size b as for simplicity), which goes over t time steps, each time sampling np feasible operators conditional to the program state. then, for each of the np operators, it samples nv feasible algorithm cipitr pseudo-code (beam size1) query encoding: q gru(query) initialization: e1, h1 f(q), a for t 1, , t do pfeast feasibleop() pt operatorsampler(ht, pfeast , np) c for p pt do v typep v type p,1 , , vtypep,m mop argp v feasp feasiblevar(p) vp argvarsampler(ht, v typep , v feasp , nv) for v vp do c c (p, v, v typep ) (p, v, v typep ) argmax(c) ukeyp , u val p , u type p outvargen(p, v typep , v ) writevartomem(ukeyp , uvalp , utypep ) et1, ht1 npicore(et, ht) a.append((p, v )) output: a variable instantiations, resulting in a total of np nv candidates out of which b most-likely actions are sampled for the b beams and the corresponding newly generated variables written into memory. this way the algorithm progresses to finally output b candidate programs, each of which will feed the model back with some reward. finally, in order to learn from the discrete action samples, the reinforce objective we next describe several learning challenges that arise in the context of this overall architecture.", "section_index": 4}, {"content": "handling complex queries by expanding the operator set and generating longer programs blows up the program space to a huge size of (numop (maxvar) m)t this, in absence of gold programs, poses serious training challenges for the programmer. additionally, whereas the relatively simple nsm architecture could explore a large beam size (50100), the complex architecture of cipitr entailed by the cpi problem could only afford to operate with a smaller beam size ( 20), which further exacerbates the sparsity of the reward space. for example, for integer answers, only a single point in the integer space returns a positive reward, without any notion of partial reward. such a delayedindeed, terminal reward causes high variance, instability, and local minima issues. a problem as complex as ours requires not only generic constraints for producing semantically correct programs, but also incorporation of prior knowledge, if the model permits. we now describe how to guide cipitr more efficiently through such a challenging environment using both generic and task-specific constraints. phase change network: for complex real-word problems, the reinforcement learning community has proposed various task-abstractions to address the curse of dimensionality in exponential action spaces. hams, proposed by parr and russell (1998), is one such important form of abstraction aimed at restricting the realizable action sequences. inspired by hams, we decompose the program synthesis into phases having restricted action spaces. the first phase (retrieval phase) constitutes gathering the information from the preprocessed input variables only (i.e., kb entities, relations, types, integers). this restricts the feasible operator set to genset, genmapset, and verify. in the second phase (algorithm phase) the model is allowed to operate on all the generated variables in order to reach the answer. the programmer learns whether to switch from the first phase to the second at any timestep t, based on parameter t (t1 indicating change of phase, where 0) which is obtained as t 1max and iyyer et al. in contrast, here we further allow the model to learn when to switch from one stage to the next. note that this is a generic characteristic, as for every task, this kind of phase division is possible. generatingsemanticallycorrectprograms: other than the generic syntactical and semantic rules, the npi paradigm also allows us to leverage prior knowledge and to incorporate task-specific symbolic constraints in the program representation learning in an end-to-end differentiable way. enforcing kb consistency: operators used in the retrieval phase (described above) must honor the kb-imposed constraints, so as not to initialize variables that are inconsistent with respect to the kb. for example, a set variable assigned from genset is considered valid only when the ent, rel, type arguments to genset are consistent with the kb. biasing the last operator using answer type predictor: answer type prediction is a standard preprocessing step in question answering (li and roth, 2002). for this we use a rule-based predictor that has accuracy. the predicted answer type helps in directing the program search toward the correct answer type by biasing the sampling towards feasible operators that can produce the desired answer type. auxiliary reward strategy: jaccard scores of the executed program’s output and the gold answer set is used as reward. an invalid program gets a reward of further, to mitigate the sparsity of the extrinsic rewards, an additional auxiliary feedback is designed to reward the model on generating an answer of the predicted answer-type. a linear decay makes the effect of auxiliary reward vanish eventually. such a curriculum learning mechanism, while being particularly useful for the more complex queries, is still quite generic as it does not require any additional task-specific prior knowledge. beam management and action sampling pruning beams by target answer type: penalize beams that terminate with an answer type not matching the predicted answer type. length-based normalization of beam scores: to counteract the characteristic of beam search favoring shorter beams as more probable and to ensure the scoring is fair to the longer beams, we normalize the beam scores with respect to their length. penalizing beams for noop operators: another way of biasing the beams toward generating longer sequences, is by penalizing for the number of times a beam takes noop as the action. specifically, we reduce the beam score by a hyperparameter-controlled logarithmic factor of the number of noop actions taken till now. stochastic beam exploration with entropy annealing: to avoid early local minima where the model severely biases towards specific actions, we added techniques like (i) a stochastic version of beam search to sample operators in an -greedy fashion (ii) dropout, and (iii) entropy-based regularization of action distribution. sampling only feasible actions: sampling a feasible action requires first sampling a feasible operator and then its feasible variable arguments: the operator must be allowed in the current phase of the model’s program induction. valid variable instantiation: a feasible operator should be having at least one valid instantiation of its formal arguments with non-empty variable values that are also consistent with the kb. action repetition: an action (i.e., an operator invoked with a specific argument instantiation) should not be repeated at any time step. some operators disallow some arguments; for example, union or intersection of a set with itself.", "section_index": 5}, {"content": "we compare cipitr against baselines on complex kbqa and further identify the contributions of the ideas presented in section via ablation studies. for this work, we limit our effort on kbqa to the setting where the query is annotated with the gold kb-artifacts, which standardizes the input to the program induction for the competing models. we trained our model using the adam optimizer and tuned all hyperparameters on the validation set. some parameters are selectively turned on/ off after few training iterations, which is itself a hyperparameter (see table 1). we combined reward/ loss such as entropy annealing and auxiliary rewards using different weights detailed in table the key, value embedding dimensions are set to 100, we first evaluate our model on the more popularly used webquestionssp data set. though quite a few recent works on kbqa have evaluated their model on webquestionssp, the reported performance is always in a setting where the gold entities/relations are not known. they either internally handle the entity and relationlinking problem or outsource it to some external or in-house model, which itself might have been trained with additional data. additionally, the entity/relation linker outputs used by these models are also not made public, making it difficult to set up a fair ground for evaluating the program induction model, especially because we are interested in the program induction given the program inputs and handling the entity/relation linking is beyond the scope of this work. to avoid these issues, we use the human-annotated entity/relation linking data available along with the questions as input to the program induction model. consequently the performance reported here is not comparable to the previous works evaluated on this data set, as the query annotation is obtained here from an oracle linker. further, to gauge the proficiency of the proposed program induction model, we construct a rule-based model which is aware of the human annotated semantic parsed form of the querythat is, the inference chain of relations and the exact constraints that need to be additionally applied to reach the answer. the pseudocode below elaborates how the rule based model works on the human-annotated parse of the given query, taking as input the central entity, the inference chain, and associated constraints and their type. this procedure: rulebasedmodel(parse,kb) ent1 parsetopicentitymid’ rel1 parseinferentialchain’0 ans x (ent1, rel1, x) kb for c parseconstraints’ crel cnodepredicate’ cop coperator’ carg cargument’ if cargumenttype’ entity’ ans ans x (carg, crel, x) kb else ans xans x (x, c rel, y) kb, carg cop y if len(parseinferentialchain’) rel2 parseinferentialchain’1 ans xans y (x, rel2, y) kb output: ans inference rule, manually derived, can be written out in a program form, which on execution will give the final answer. on the other hand, the task of cipitr is to actually learn the program by looking at training examples of the query and corresponding answer. both the models need to induce the program using the gold entity/relation data. subsequently, the rule-based model is indeed a very strong competitor as it is generated by annotators having detailed knowledge about the kb. a comparative performance analysis of the proposed cipitr model, the rule-based model and the sparql executor is tabulated in table the main take-away from these results is that cipitr is indeed able to learn the rules behind the multi-step inference process simply from the distance supervision provided by the questionanswer pairs and even perform slightly better in some of the query classes. we now showcase the performance of the proposed models and related baselines on the csqa data set. kvmnet with decoder (as discussed in section 2), learns to attend on a kb subgraph in memory and decode the attention over memory-entries as their likelihood of being in the answer. further, it can also decode a vocabulary of non-kb words like integers or booleans. however, because of the inherent architectural constraints, it is not possible to incorporate most of the symbolic constraints presented in section in this model, other than kb-guided consistency and biasing towards answer-type. more importantly, recently the usage of these models have been criticized for numerical and boolean question answering as these deep networks can easily memorize answers without understanding’’ the logic behind the queries simply because of the skew in the answer distribution. in our case this effect is more pronounced as csqa evinces a curious skew in integer answers to count’’ queries. fifty-six percent of training and of test count-queries have single digit answers. ninety percent of training and of test count-queries have answers less than though this makes it unfair to compare npi models (that are oblivious to the answer vocabulary) with kvmnet on such queries, we still train a kvmnet version on a balanced resample of csqa, where, for only the count queries, the answer distribution over integers has been made uniform. nsm (2017) uses a key-variable memory and decodes the program as a sequence of operators and memory variables. as the nsm code was not available, we implemented it and further incorporated most of the six techniques presented in table however, constraints like action repetition, biasing last operator selection, and phase change cannot be incorporated in nsm while keeping the model generic, as it decodes the program token by token. in table we compare the f1 scores obtained by our system, cipitr, against the kvmnet and nsm baselines. for nsm and cipitr, we train seven models with different hyperparameters tuned on each of the seven question types. for the train and valid splits, a rule-based query type classifier with accuracy was used to bucket queries into the classes listed in table for each of these three systems, we also train and evaluate one single model over all question types. kvmnet does not have any beam search, the nsm model uses a beam size of 50, and cipitr uses only beams for exploring the program space. our manual inspection of these seven query categories show that simple and verify are simplest in nature requiring 1-line programs while logical is moderately difficult, with around lines of code. the query categories next in order of complexity are quantitative and quantitative count, needing a sequence of operations. the hardest types are comparative and comparative count, which translate to an average of lined programs. analysis: the experiments show that on the simple to moderately difficult (i.e., first three) query classes, cipitr’s performance at the top beam is up to times better than both the baselines. the superiority of cipitr over nsm is showcased better on the more complex classes where it outperforms the latter by times, with the biggest impact (by a factor of times) being on the comparative’’ questions. also, the better performance of cipitr over nsm over all category evinces the better generalizability of the abstract high-level program decomposition approach of the former. on the other hand, training the kvmnet model on the balanced data helps showcase the real performance of the model, where cipitr outperforms kvmnet significantly on most of the harder query classes. the only exception is the hardest class (comp, count with numerical answers) where the abrupt best performance’’ of kvmnet can be attributed to its rote learning abilities simply because of its knowledge of the answer vocabulary, which the program induction models are oblivious to, as they never see the actual answer. lastly, in our experimental configurations, whereas cipitr and nsm’s parameter-size is almost comparable, kvmnet’s is approximately larger. ablation study: to quantitatively analyze the utility of the features mentioned in section 5, we experiment with various ablations in table by turning off each feature, one at a time. we show the effect on the hardest question category (comparative’’) on which our proposed model achieved reasonable performance. we see in the table that each of the techniques helped the model significantly. some of them boosted f1 by times, while others proved to be instrumental to obtained large improvements in f1 score of over times. to summarize, cipitr has the following advantages, inducing programs more efficiently and pragmatically, as illustrated by the sample outputs in table 5: generating syntactically correct programs: because of the token-by-token decoding of the program, nsm cannot restrict its search to only syntactically correct programs, but rather only resorts to a post-filtering step during training. however, at test time, it could still generate programs with wrong syntax, as shown in table for example, for the logical question, it invokes a genset with a wrong argument type none and for the quantitative count question, it invokes the setunion operator on a non-set argument. on the other hand, cipitr, by design, can never generate a syntactically incorrect program because at every step it implicitly samples only feasible actions. generating semantically correct programs: cipitr is capable of incorporating different generic programming styles as well as problemspecific constraints, restricting its search space to only semantically correct programs. as shown in table 5, cipitr is able to generate at least meaningful programs having the desired answer-type or without repeating lines of code. on the other hand the nsmgenerated programs are often semantically wrong, for instance, both in the quantitative and quantitative count based questions, the type of the answer is itself wrong, rendering the program meaningless. this arises once again, owing to the token-by-token decoding of the program by nsm which makes it hard to incorporate high level rules to guide or constrain the search. efficient search-space exploration: owing to the different strategies used to explore the program space more intelligently, cipitr scales better to a wide variety of complex queries by using less than half of nsm’s beam size. we experimentally established that for programs of length these various techniques reduced the average program space from to 2,998 programs.", "section_index": 6}, {"content": "we presented cipitr, an advanced npi framework that significantly pushes the frontier of complex program induction in absence of gold programs. cipitr uses auxiliary rewarding techniques to mitigate the extreme reward sparsity and incorporates generic pragmatic programming styles to constrain the combinatorial program space to only semantically correct programs. as future directions of work, cipitr can be further improved to handle the hardest question types by making the search more strategic, and can be further generalized to a diverse set of goals when training on all question categories together. other potential directions of research could be toward learning to discover sub-goals to further decompose the most complex classes beyond just the two-level phase transition proposed here. additionally, further improvements are required to induce complex programs without availability of gold program input variables.", "section_index": 7}], "id": 1018}
{"text": [{"content": "we propose dual-ces a novel unsupervised, query-focused, multi-document extractive summarizer. dual-ces is designed to better handle the tradeoff between saliency and focus in summarization. to this end, dual-ces employs a two-step dual-cascade optimization approach with saliency-based pseudo-feedback distillation. overall, dual-ces significantly outperforms all other state-of-the-art unsupervised alternatives. dual-ces is even shown to be able to outperform strong supervised summarizers.", "section_index": 0}, {"content": "the vast amounts of textual data end users need to consume motivates the need for automatic summarization an automatic summarizer gets as an input one or more documents and possibly also a limit on summary length (e.g., maximum number of words). the summarizer then needs to produce a textual summary that captures the most salient (general and informative) content parts within input documents. moreover, the summarizer may also be required to satisfy a specific user information need, expressed by one or more queries. therefore, the summarizer will need to produce a focused summary which includes the most relevant information to that need. while both saliency and focus goals should be considered within a query-focused summarization setting, these goals may be actually conflicting with each other higher saliency usually comes at the expense of lower focus and vice-versa. moreover, such a tradeoff may directly depend on summary length. contact author: haggaiil.ibm.com to illustrate the effect of summary length on this tradeoff, using the duc dataset, figure reports the summarization quality which was obtained by the cross entropy summarizer (ces) a state of the art unsupervised query-focused multidocument extractive summarizer saliency was measured according to cosine similarity between the summary’s bigram representation and that of the input documents. focus was further measured relatively to how much the summary’s induced unigram model is concentrated around query-related words. as we can observe in figure 1, with the relaxation of the summary length limit, where a more lengthy summary is being allowed, saliency increases at the expense of focus. laying towards more saliency would result in a better coverage of general and more informative content. yet, this would result in the inclusion of less relevant content to the specific information need in mind. aiming at better handling the saliency versus focus tradeoff, in this work, we propose dual-ces an extended ces summarizer similar to ces, dual-ces is an unsupervised query-focused, multi-document, extractive summarizer. to this end, like ces, dual-ces utilizes the cross entropy method for selecting a subset of sentences extracted from input documents, whose combination is predicted to produce a good summary. yet, differently from ces, dual-ces does not attempt to address both saliency and focus goals in a single optimization step. instead, dual-ces implements a novel two-step dual-cascade optimization approach, which utilizes two sequential ces-like invocations. using such an approach, dual-ces tries to handle the tradeoff by gradually shifting from generating a long summary that is more salient in the first step to generating a short summary that is more focused in the second step. moreover, dualces utilizes the long summary that was generated in the first step for saliency-based pseudo-feedback distillation, which allows to generate a final focused summary with better saliency. dual-ces provides a fully unsupervised end-to-end query-focused multi-document extractive summarization solution. using an evaluation with the duc 2005, and benchmarks, we show that, dual-ces generates a focused (and shorter) summary which has much higher saliency (and hence a better tradeoff handling). overall, dual-ces provides a significantly better summarization quality compared to other alternative unsupervised summarizers; and in many cases, it even outperforms that of state-of-the art supervised summarizers.", "section_index": 1}, {"content": "in this work we employ an unsupervised learning approach for the task of query-based multi-document extractive summarization. many previous works have employed various unsupervised and/or supervised learning methods for the same task. some learning systems rank sentences based on their surface and/or graph level features 3, 15, others have used various sparse coding techniques for selecting a subset of sentences that minimizes a given documents reconstruction error 12, 26, 16, 9, or used a variational auto-encoder for sentence representation attention models incorporated within deep-learning summarization architectures have further been suggested for improving sentence ranking and selection 1, 12, such models try to simulate a human attentive reading behaviour. this allows to better account for context-sensitive features during summarization. compared to these works, we do not try to attend for sentence ranking or selection. alternatively, we distill informative hints from summarized documents, aiming to improve the saliency of produced focused summaries. finally, reinforcement learning methods have been recently considered 4, 6, 17, among such methods, the ces summarizer is the only one which is both query-sensitive and unsupervised. similar to ces, we also utilize the cross entropy (ce) method 21, a global policy search optimization framework, for solving the sentence subset selection problem. yet, differently from ces, we utilize the ce method twice, each time with a slightly-different summarization goal in mind (i.e., first saliency and then focus). moreover, we utilize the distilled saliency-based pseudo-feedback to improve the summarization policy search between such switched (dual) goals. to the best of our knowledge, this on its own, serves as a novel aspect of our work.", "section_index": 2}, {"content": "here we provide background details on our summarization task and the cross entropy method which we use for implementing dual-ces. we address the query-focused, multi-document summarization task. formally, let q denote some user information need for documents summarization, which may be expressed by one or more queries. let d denote a set of one or more matching documents to be summarized and lmax be the maximum allowed summary length (in words). we implement an extractive summarization approach. our goal is to produce a length-limited summary s by extracting salient content parts in d which are further relevant (focused) to q. following 6, we now cast the summarization task as a sentence subset selection problem. to this end, we produce summary s (with maximum length lmax) by choosing a subset of sentences s d which maximizes a given quality target q(sq,d). dual-ces is an unsupervised summarizer. similar to ces, it utilizes the cross entropy method for selecting the most promising subset of sentences in d. since we assume an unsupervised setting, no actual reference summaries are available for training nor can we directly optimize an actual quality target q(sq,d). instead, following 6, q(sq,d) is surrogated by several summary quality prediction measures qi(sq,d) (i 1, 2, each predictor qi(sq,d) is designed to estimate the level of saliency or focus of a given candidate summary s and is presumed to correlate (up to some extent) with actual summarization quality, e.g., rouge for simplicity, similar to ces, various predictions are assumed to be independent and are combined into a single optimization objective by taking their product, i.e. the ce-method provides a generic monte-carlo optimization framework for solving hard combinatorial problems previously, it was utilized for solving the sentence subset selection problem to this end, the ce-method gets as an input q(q,d), a constraint on maximum summary length l and an optional pseudo-reference summary sl, whose usage will be explained later on. let cem(q(q,d), l, sl) denote a single invocation of the ce-method. the result of such an invocation is a single length-feasible summary s which contains a subset of sentences selected from d which maximizes q(q,d). for example, ces is implemented by invoking cem(qces(q,d), lmax, ). we next briefly explain how the ce-method solves this problem. for a given sentence s d, let (s) denote the likelihood that it should be included in summary s. starting with a selection policy with the highest entropy (i.e. : 0(s) 0.5), the cemethod learns a selection policy () that maximizes q(q,d). to this end, () is incrementally learned using an importance sampling approach ., a sample of n sentence-subsets sj is generated according to the selection policy t1() which was learned in the previous iteration t the likelihood of picking a sentence s d at iteration t is estimated (via cross-entropy minimization) as follows: t(s) def n j1 q(sj q,d)tssj n j1 q(sj q,d)t (1) here, denotes the kronecker-delta (indicator) function and t denotes the (1 )-quantile ( (0, 1)) of the sample performances q(sj q,d) (j 1, 2, therefore, the likelihood of picking a sentence s d will increase when it is being included in more (subset) samples whose performance is above the current minimum required quality target value t. we further smooth t() as follows: t() t1() (1 )t(); with 0, upon its termination, the ce-method is expected to converge to the global optimal selection policy () we then produce a single summary s (). to enforce that only feasible summaries will be produced, following 6, we set q(sj q,d) whenever a sampled summary sj length exceeds the l word limit.", "section_index": 3}, {"content": "differently from ces, dual-ces does not attempt to maximize both saliency and focus goals in a single optimization step. instead, dual-ces implements a novel twostep dual-cascade optimization approach (see figure 2), which utilizes two ces-like invocations. both invocations consider the same sentences powerset solution space. yet, each such invocation utilizes a bit different set of summary quality predictors qi(sq,d), depending on whether the summarizer’s goal should lay towards higher summary saliency or focus. in the first step, dual-ces relaxes the summary length constraint, aiming at producing a longer and more salient summary. this summary is then treated as a pseudoeffective reference summary from which saliency-based pseudo-feedback is distilled. such pseudo-feedback is then utilized in the second step of the cascade for setting an additional auxiliary saliency-driven goal. yet, at the second step, similar to ces, the primary goal is actually to produce a focused summary (with maximum length limit lmax). overall, dual-ces is simply implemented as follows: cem(qfoc(q,d), lmax,cem(qsal(q,d), l, )). here, qsal(q,d) and qfoc(q,d) denote the saliency and focus summary quality objectives which are optimized during the cascade, respectively. both qsal(q,d) and qfoc(q,d) are implemented as a product of several basic predictors. l lmax denotes the relaxed summary length hyperparameter. we next elaborate the implementation details of dual-ces’s dual optimization steps. the purpose of the first step is to produce a single longer summary (with length l lmax) which will be used as a pseudo-reference for saliency-based feedback distillation. as illustrated in figure 1, with a longer summary length a more salient summary may be produced. this step is simply implemented by invoking the ce-method with cem(qsal(q,d), l, ). the target measure qsal(q,d) guides the optimization towards the production of a summary with the highest possible saliency. similar to ces, qsal(q,d) is calculated as the product of several summary quality predictors. overall, we use five different predictors, four of which were previously used in ces the additional predictor that we introduce is designed to drive the optimization even further towards higher saliency. next, we shortly describe each predictor. the symbol marks whether it was originally employed in ces this predictor estimates to what extent (candidate) summary s (generally) covers the document set d. here, we represent both s and d as term-frequency vectors, considering only bigrams, which commonly represent more important content units for a given text x, let cos(s, x) def sx sx the coverage predictor is then defined by qcov(sq,d) def cos(s,d). this predictor biases sentence selection towards sentences that appear earlier in their containing documents. it is calculated as qpos(sq,d) def s ss ( 1log(bpos(s)) ) , where pos(s) is the relative start position (in characters) of sentence s in its containing document and b is a position-bias hyperparameter (fixed to b 2, following 6). this predictor biases towards selection of summaries that are closer to the maximum permitted length. such summaries contain fewer and longer sentences, and therefore, tend to be more informative. let len(x) denote the length of text x (in number of words). here, x may either be a single sentence s d or a whole summary s. this predictor is then calculated as qlen(sq,d) def 1s len(s), where len(s) ss len(s). to target even higher saliency, we suggest a fourth predictor, inspired by the risk minimization framework to this end, we measure the kullback-leibler (kl) similarity between the two (unsmoothed) unigram language models induced from the centroid representation1 of s (s) and d (d), formally: qkl(sq,d) def exp ( w p(ws) log p(ws) p(wd) ) while producing a longer summary may result in higher saliency, as was further illustrated in figure 1, such a summary may be less focused. hence, to avoid such focusdrift, while we opt to optimize for higher saliency at this step, the target information 1such centroid representation is simply given by concatenating the text of sentences in s or documents in d. need q should be still considered. to this end, we add a predictor: qqf (sq,d) def wq p(ws), which acts as a query-anchor and measures to what extent summary s’s unigram model is devoted to the information need q. the input to the second step of the cascade consists of the same set of documents d, summary length constraint lmax and the pseudo-reference summary sl that was generated in the previous step. this step is simply implemented by invoking the cemethod with cem(qfoc(q,d), lmax, sl). here, the target measure qfoc(q,d) guides the optimization towards the production of a focused summary, while still keeping high saliency as much as possible. to achieve that, we use an additional focusdriven predictor which bias summary production towards higher focus. moreover, using the pseudo-reference summary sl we introduce an additional auxiliary saliencybased predictor, whose goal is to enhance the saliency of produced focused summary. overall, qfoc(q,d) is calculated as the product of the previous five summary quality predictors (predictors 15) and the two additional predictors, whose details are described next. this predictor estimates the relevancy of summary s to q. for that, we use two similarity measures. the first, following 6, measures the bhattacharyya similarity (coefficient) between the two (unsmoothed) unigram language models of q and s, i.e. the second measures the cosine simi- larity between q and s unigram term-frequency representations, i.e. the two similarity measures are then combined into a single measure using their geometric mean, i.e. : qsim(sq,d) def qsim1(sq,d) qsim2(sq,d). we further make use of the pseudo-reference summary sl, which was produced in the first step, and introduce an additional auxiliary saliency-based predictor. this predictor utilizes pseudo-feedback that is distilled from unique unigram words in sl. it is calculated as: qcov(sq,d) def wsl ws. following 10, 27, we only consider the top-100 most frequent unigrams in sl. intuitively speaking, sl usually will be longer (in words) than any candidate summary s that may be chosen in the second step; hence, sl is expected to be more salient than s. therefore, such a predictor is expected to drive the optimization to prefer those candidate summaries s that include as many salient words from sl, acting as if they were by themselves longer (and more salient) summaries (than those candidates that include less salient words from sl). apart from salient words in sl that are used as feedback, we note that, sentences in sl may also provide additional hints about other properties of informative sentences in d, which may potentially be selected to improve saliency. one such property is the relative start-positions of sentences in sl. to this end, we now assign b 1sl ssl pos(s) (i.e., the average start-position of feedback sentences in sl) as the value of the position-bias hyperparameter within qpos(sq,d) (predictor 2). we conclude this section with a suggestion of an extension to dual-ces that adaptively adjusts the value of hyperparameter l. to this end, we introduce a new learning parameter lt which defines the maximum length limit for summary production (sampling) that is allowed at iteration t of the ce-method. we now assume that summary lengths have a poisson(lt) distribution of word occurrences with mean lt. using importance sampling, this parameter is estimated at iteration t as follows: lt def n j1 len(sj) q(sj q,d)t n j1 q(sj q,d)t (2) similar to (), we further smooth lt as follows: lt def lt1(1)lt. here, 0, is the same smoothing hyperparameter which was used to smooth () and lt0 def l.", "section_index": 4}, {"content": "our evaluation is based on the document understanding conferences (duc) 2005, and benchmarks2. these benchmarks are commonly used for evaluating the query-based multi-document summarization task by all of our related works. given a topic statement, which is expressed by one or more questions, and a set of english documents, the main task is to produce a 250-word (i.e., lmax 250) topic-focused summary the number of topics per benchmark are 50, and in the duc 2005, and benchmarks, respectively. the number of documents to be summarized per topic is 32, and in the duc 2005, and benchmarks, respectively. each document was pre-segmented (by nist) into sentences. following 6, we use lucene’s english analysis3 for processing the text of topics and documents. we evaluated both dual-ces and its adaptive-length variant (hereinafter denoted dualces-a). to this end, on the first saliency-driven step, for dual-ces, we fixed the (strict) upper bound limit on summary length to l dual-ces-a, on the other hand, adaptively adjusts such length limit and was initialized with lt0 both variants were further set with a summary limit lmax for their second focus-driven step. we implemented both dual-ces and dual-ces-a in java (jre8). further following 6, to reduce ce-method’s runtime, we applied a preliminary step of sentence pruning, where only the top-150 sentences s d with the highest (unigram) bhattacharyya similarity to the topic’s queries were considered for summarization. similar to 6, the ce-method hyperparameters were fixed as follows: n 10, 000, and finally, to handle duc’s complex information needs, we closely followed 6, as follows. first, for each summarized topic, we calculated the query-focused predictions (i.e., qqf (q,d) and qsim(q,d)) per each one of its questions. to this end, each question was represented as a sub-query by concatenating the main topic’s text to the question’s text. each sub-query was further expanded with top-100 (unigram) wikipedia related-words we then obtained the topic query-sensitive predictions by summing up its various sub-queries’ predictions. the three duc benchmarks include four reference (ground-truth) human-written summaries per each topic we measured summarization quality using the rouge measure 14, which is the official one for this task to this end, we used the rouge toolkit with its standard parameters setting4. we report both recall and f-measure of rouge-1, rouge-2 and rouge-su4. rouge-1 and rouge-2 measure the overlap in unigrams and bigrams between the produced and the reference summaries, respectively. rouge-su4 measures the overlap in skip-grams separated by up to four words. finally, since dual-ces essentially depends on the ce-method which has a stochastic nature, its quality may depend on the specific seed that was used for random sampling. hence, following 6, to reduce sensitivity to random seed selection, per each summarization task (i.e., topic and documents pair), we run each dual-ces variant times (each time with a different random seed) and recorded its mean performance (and confidence interval). we compare the summary quality of dual-ces to the results that were previously reported for several competitive summarization baselines. these baselines include both supervised and unsupervised methods and apply various strategies for handling the 4rouge-1.5.5.pl -a -c -m -n -2 -u -p -l saliency versus focus tradeoff. to distinguish between both types of works, we mark supervised method names with a superscript the first line of baselines utilize various surface and graph level features, namely: bi-plsa 22, ctsum 24, hiersum 8, hybhsum 3, multimr 23, qode and submod-f the second line of baselines apply various sparse-coding or auto-encoding techniques, namely: docrebuild 16, ra-mds 11, spopt 26, and vaes-a the third line of baselines incorporate various attention models, namely: attsum 1, c-attention and crsumsf we further note that, some baselines, like docrebuild, spopt and c-attention, use hand-crafted rules for sentence compression. finally, we directly compare with two ces variants, which serve as direct alternatives to dual-ces. the first one, is the original ces summarizer, whose results are reported in the second one, denoted hereinafter ces, utilizes predictors 16, which are combined within a single optimized objective (by taking their product). this variant, therefore, allows to directly evaluate the contribution of our proposed dualcascade learning approach which is employed by the two dual-ces variants. the main results of our evaluation are reported in table (rouge-x f-measure) and table (rouge-x recall). the numbers reported for the various baselines are the best numbers reported in their respective works. unfortunately, not all baselines fully reported their results for all benchmarks and measures. whenever a report on a measure is missing, we further use the symbol ’-’. first we note that, among the various baseline methods that we have compared with, ces on its own, serves as the strongest baseline to outperform in most cases. overall, dual-ces provides better results compared to any other baseline (and specifically the unsupervised ones). specifically, on f-measure, dual-ces has achieved between and better rouge-2 and rouge-1, respectively. on recall, dualces has achieved between better rouge-1. on rouge-2, in the duc and benchmarks, dual-ces was about better, while it was slightly inferior to submod-f and crsumsf in the duc benchmark. yet, submod-f and crsumsf are actually supervised, while dual-ces is fully unsupervised. therefore, overall, dual-ces’s ability to reach (even to outperform in many cases) the quality of strong supervised counterparts actually only emphasizes more its potential. dual-ces significantly improves over the two ces variants in all benchmarks. on f-measure, dual-ces has achieved at least between and better rouge-2 and rouge-1, respectively. on recall, dual-ces has achieved at least between and better rouge-2 and rouge-1, respectively. by distilling saliency-based pseudo-feedback between step transitions, dual-ces manages to better utilize the ce-method for selecting a more promising subset of sentences. a case in point is the ces variant which is even inferior to ces. a simple combination of all predictors (except predictor which is unique to dual-ces since it requires a pseudo-reference summary) does not directly translates to a better tradeoff handling. this, therefore, serves as a strong empirical evidence of the importance of the dualcascade optimization approach implemented by dual-ces, which allows to produce focused summarizes with better saliency. the pseudo-feedback distillation approach employed between the two steps of dualces has some resemblance to attention models that are used by state-of-the-art deep learning summarization methods 1, 12, first we note that, dual-ces significantly improves over these attentive baselines on rouge-1. on rouge-2, dual-ces is significantly better than c-attention and attsum, while it provides (more or less) similar quality to crsumsf. closer analysis of the various attention strategies that are employed within these baselines, reveals that, while attsum only attends on a sentence representation level, c-attention and crsumsf further attend on a word level. such a more fine-granular attendance results in an improved saliency for the two latter. yet, while c-attention first attends on sentences then on words, crsumsf performs its attentions reversely. using dual-ces as a reference method for comparison, apparently, crsumsf attendance on salient words first and then on salient sentences based on such words seems as the better strategy. in a sense, similar to crsumsf, dual-ces also first attends on salient words which are distilled from the pseudo-feedback reference summary. dual-ces then utilizes such salient words for better selection of salient sentences within its second step of focused summary production. yet, compared to crsumsf and similar to c-attention, dual-ces’s saliency attention process is unsupervised. moreover, dual-ces further attends on salient sentence positions, which result in better tuning of the position-bias b hyperparameter. table reports the sensitivity of dual-ces (measured by rouge-x recall) to the value of hyperparameter l, using the duc benchmark. to this end, we ran dualces with an increasing l value. for further comparison, we also report in table the results of its adaptive-length version dual-ces-a. dual-ces-a is still initialized with lt0 and adaptively adjusts this hyperparameter. figure illustrates the (average) learning curve of its adaptive-length parameter lt. overall, dual-ces’s summarization quality remains quite stable, exhibiting low sensitivity to l. similar stability was further observed for the two other duc benchmarks. in addition, figure depicts an interesting empirical outcome: dual-ces-a converges (more or less) to the best hyperparameter l value (i.e., l in table 3). dual-ces-a, therefore, serves as a robust alternative for flexibly estimating such hyperparameter value during runtime. dual-ces-a can provide similar quality and may outperform dual-ces.", "section_index": 5}, {"content": "we proposed dual-ces, an unsupervised, query-focused, extractive multi-document summarizer. dual-ces was shown to better handle the tradeoff between saliency and focus, providing the best summarization quality compared to other alternative stateof-the-art unsupervised summarizers. moreover, in many cases, dual-ces even outperforms state-of-the-art supervised summarizers. as a future work, we would like to learn to distill from additional pseudo-feedback sources.", "section_index": 6}], "id": 1019}
{"text": [{"content": "we present a lightweight adaptable neural tts system with high quality output. the system is composed of three separate neural network blocks: prosody prediction, acoustic feature prediction and linear prediction coding net as a neural vocoder. this system can synthesize speech with close to natural quality while running times faster than real-time on a standard cpu. the modular setup of the system allows for simple adaptation to new voices with a small amount of data. we first demonstrate the ability of the system to produce high quality speech when trained on large, high quality datasets. following that, we demonstrate its adaptability by mimicking unseen voices using to minutes long datasets with lower recording quality. large scale mean opinion score quality and similarity tests are presented, showing that the system can adapt to unseen voices with quality gap of and similarity gap of compared to natural speech for male voices and quality gap of and similarity of gap of for female voices.", "section_index": 0}, {"content": "in recent years we are experiencing a dramatic improvement of the synthesized speech quality in tts systems, with the introduction of systems that are based on neural networks (nn). a major improvement in quality was achieved by using attention based models such as tacotron and by replacing vocoders with a nn based waveform generators such as wavenet a useful feature of systems with trainable models is the ability to adapt the tts to an unseen voice using a small amount of training data (from a few seconds to an hour of speech). this is usually done by training the system on a large number of speakers, and providing a speaker embedding vector as one of the system’s inputs. using this approach allows later retraining of only a subsets of the model parameters or prediction of the speaker embedding vector 3, 4, the drawback of this approach is that the resulting systems use large nn models. furthermore, a multi-speaker model usually needs much more trainable parameters than a single speaker model. this may lead to a computationally heavy and slow synthesis process even on a strong gpu. such requirements pose a severe problem for practical tts system that require very low latency for a dialog with a human. in our previous paper we introduced a nn based tts system with two trainable modules for prosody prediction and acoustic features prediction. this system used the world vocoder we demonstrated that this tts allows simple adaptation to new voices. this was carried out by retraining nn models that had already been trained using a large highquality voice, on a small amount of data from the new voice. although the quality of this system was better in many cases than similar concatenative tts, it was still limited by the quality of the world vocoder. recently, an efficient neural vocoder called lpcnet was introduced the lpcnet inference runs faster than realtime on a single cpu while producing a high quality speech output. lpcnet uses cepstrum representing spectral envelopes, pitch and pitch correlation as input features. this makes it a simple alternative to other vocoders, e.g. world, which work with similar features. in this paper we show that we can get a considerable quality improvement by modifying a tts system that produced the world vocoder parameters to predict parameters for lpcnet as in the previous work 6, we conduct multiple adaptation experiments, applied on multiple vctk voices and show that the new system has much better quality and similarity to the target voices but can still run much faster than real-time in a single-cpu mode.", "section_index": 1}, {"content": "an overview of our new tts system is presented in figure the system is a cascade of a rule-based front-end, a nn based prosody generator, a nn synthesizer and an lpcnet decoder. we adopted the front-end block which is used in the ibm watson tts engine and is described in detail in the front-end performs a grapheme-to-phoneme conversion, represents each word with a set of positional and categorical linguistic features and associates the features with the phonemes contained within the word. the prosody generator is described in section it emits a sequence of sub-phoneme elements, including duration, pitch and intensity values. each sub-phoneme element represents either a heading, a middle or a trailing part of a phoneme. the synthesizer is described in section it represents each sub-phoneme element by several consecutive frames according to the element’s duration and generates an acoustic feature vector for each frame. finally, an lpcnet block (section 2.3) is used to convert the stream of the acoustic feature vectors to a speech signal. the prosody generator, synthesizer and lpcnet blocks use neural-net models for generating their output. each block has its own model which is trained independently for each voice. hence, the system is modular and provides easy control, flexibility and adaptability at the component level. for each voice, the training and adaptation phases include the following data pre-processing steps: a grapheme-to-phoneme conversion using the frontend block. forced alignment of audio at the sub-phoneme level using proprietary acoustic modeling and speech recognition tools. extraction of textual features for prosody modeling using the front-end block. pitch detection for prosody modeling using a proprietary tool. cepstra and residual extraction using the lpcnet feature extraction tool. in the current work, the prosody generation and adaptation network follows the one presented in our previous work 6, where one can refer to for more details. it generates a 4- dimensional prosody vector per tts unit, comprising the unit’s log-duration, initial log-pitch, final log-pitch and logenergy. the tts units correspond to roughly 1/3 of a phone and result from forced-alignments with 3-state hidden markov models. the input features, derived from the tts front end, are comprised of 1-hot coded categorical features and standard positional features in this architecture the prosody adaptation to unseen speaker is based on a variational auto encoder (vae) utterance prosody embedding, averaged over all the speaker utterances 6, as presented on figure in the current work we used multi-speaker baseline models for prosody adaptation to unseen voices, as it resulted in better quality than the single speaker models. the synthesis process begins by resampling the phonetic data and pitch to 10msec frames based on their duration predicted by the prosody generator. the sub-phoneme labels are represented by element vectors, using a trainable embedding table. time dependencies and local context are extracted by convolution layers. the convolution is performed over time on the phonetic vector and the pitch curves independently with a window size of 0.32sec (forward and backward in time). a longer time dependent context is extracted by an lstm layer that merges the phonetic and pitch context. following this are fully connected layers with relu non-linearity. from the top layer we generate by linear transformations the speech parameters that the lpcnet requires as input: cepstral vector, pitch and a pitch correlation parameters with first and second derivatives for all (total of parameters). the final parameters which we use as input for the lpcnet are found by solving the maximum likelihood parameter generation (mlpg) equations we also apply a formant enhancement filter on the cepstral coefficients ck, k1n to compensate for the nn averaging and to improve the speech quality similar to the enhancement starts by multiplication of the high-order coefficients: c k c k kk ck kk (1) we choose and k2. this can cause changes to the energy of the output, so we have to normalize it. let ec be the energy of the signal derived from the coefficients c. to calculate ec we convert back from cepstrum to power spectrum and apply the inverse pre-emphasis filter. the energy is now the integration of this power spectrum. finally, to compensate for the energy change, we apply: c 0c0n log10( e c ec ) (2) the architecture of the synthesizer is shown in figure the size of the layers is: phonetic embedding: 32, phonetic convolution: 128, pitch convolution: 32, lstm: and full: the network is trained using an aligned corpus where the inputs are the frame based phonetic labels and pitch values, while the outputs are the corresponding lpcnet parameters. we use mse loss function on all output parameters. we first train two single-speaker models form large male and female datasets (see section 3.1). those are used as the base models for the adaptation experiments (section 3.2). to adapt the model to a smaller unseen voice, we first initialize the training with the weights of the base model of the same gender. then, the model is trained on a small target voice. a held-out validation set is used as a stop criterion for the adaptation to avoid over-fitting. the lpcnet decoder is a wavernn variant that uses a nn model to generate speech samples from equidistant-intime input of cepstrum, pitch and pitch correlation parameters. unlike other waveform generative models, such as wavenet and wavernn, the lpcnet uses its nn to predict the lpc residual (the vocal source signal) and then apply to it an lpc filter calculated from the cepstrum. this has the advantages of better control over the output of the spectral shape since it depends directly on the lpc filter shape. the model is also more robust to the predicted residual errors since any high frequency noise is also shaped by the lpc filter. in this work we used the code published by the mozilla team on github1 with some adjustments: we replaced the pitch and pitch correlation values with values that were produced by our tools in order to maintain data consistency over all blocks. we removed any data augmentation. we added validation score over held-out data to the training procedure. this score was used to select the best model and served as a training stop criteria. the lpcnet model was reported to perform well in speaker independent setting, when trained on multi-speaker datasets 8, however, we experimentally found that its performance further improves when retraining the initial multi-speaker same-gender model with the target voice specific data. the validation score (evaluated on of held out validation data) was used to avoid over-fitting.", "section_index": 2}, {"content": "speech samples from the following experiments are available online at url for the first experiment we built a male and a female highquality tts systems. we used two proprietary datasets that were originally created for building a product level concatenative tts system. the male dataset contains hours of speech and the female dataset contains hours of speech. both were produced by native us english speakers and 1url recorded in a professional studio. the audio was recorded sentence by sentence. for each of those voices we built the following single speaker tts systems: a world based system at 22khz as described in an lpcnet based system at 16khz as described in the previous section. tacotron2 based tts with wavenet decoder at 22khz we used each one of these systems to synthesize a set of held-out sentences and compared them by a mos test to the original recordings. because of the differences in sample rates, all of the samples were down-sampled to 16khz and normalized to the same energy. the tests were performed using the amazon mechanical turk (amt) platform with 50- anonymous and untrained subjects participating in several evaluation sessions, constructed so that each sentence is evaluated by distinct subjects. the quality mos deployed a points scale. the score for each system was calculated as the average over all its sentences. table shows the results of those tests. for the female voice the statistical significance of the difference between the lpcnet and the tacotron systems is small (i.e. we can see from these results that the lpcnet model has a huge impact on the quality compared to the world system. we can also see that even though the lpcnet system has much lower complexity than the tacotron2 like system, it gets close to it in quality. one should note relatively low mos scores for the original natural samples, which can be explained by the assumption that the listeners subjectively judged speaker pleasantness together with the speech quality and naturalness. hence, the evaluation should be based on relative comparison between the different systems and the original samples. in this experiment we selected male and female us english speakers from the vctk corpus. we created datasets out of each voice: the first contains the entire available data (19 minutes of audio with average of minutes), and the others two contain a random subset of the audio with total duration of and minutes. from each one of these datasets we created a single voice tts system. the networks for the acoustic features and the lpcnet where adapted from the corresponding, same gender networks that where trained in section the prosody network was adapted from a multi-speaker baseline model (that was originally trained on high-quality voices and vctk voices). in addition, we also built a world based tts for each voice by adapting the world based acoustic feature networks from the corresponding same gender networks of section using the full voice data. from each one of these systems we synthesized sentences using text that was excluded from all the datasets. we evaluated each systems quality with mos tests as in section for reference, the tests also included samples from the original vctk datasets. we noticed that the original samples usually do not contain full sentences but rather short phrases. this factor, combined with the fact that vctk comprises unprofessional speakers with varying voice pleasantness led, most probably, to the relatively low scores the original recordings received. the results of this test are summarized in table the statistical significance of the difference between the 5m and 10m lpcnet results is small. to measure the similarity of the synthesized voices to the original voices we performed additional subjective listening tests on amt. in these tests a subject is presented with a pair of samples that convey different text messages. the subject is asked to rate their voice similarity, using a 4-point scale adopted from the voice conversion challenge (vcc) 16, and utilized in our previous experiments we performed two tests: one with only male voices and the second with only female voices. for reference, in each test we also checked the similarity of pairs of natural speech samples from the same speaker and of pairs of natural speech from different speakers of the same gender. the results are summarized in table for each system we show the average score (on the scale 1-4) and the percentage of votes, which indicated that the two presented samples were from the same speaker (option or 4). the statistical significance is small for the differences between all the male lpcnet systems and also for the difference between the female lpcnet 5m and 20m systems. we can compare these results to those of the vcc hub task although the task setup and the listening tests conditions are a bit different we can see that our system mos and similarity score are comparable to those of the best vcc system (n10 with quality of and similarity of where the corresponding scores for the original speech are and 95). figure shows the results for each voice. to compensate for the variability between the voices, we have normalize the mos scores for each voice in the range from to the score of natural samples of this voice. the similarity scores for each voice, were normalized to the range between the scores of natural samples from different and same speakers. to clarify, the normalization ranges are different for each voice. the slowest block of this tts is the lpcnet. we found that it runs about times faster than real-time on a 2.8ghz i7 cpu (no gpu was used). when adding the rest of the blocks we found that we can synthesize about time faster than real-time on a cpu.", "section_index": 3}, {"content": "we have presented in this article a new tts system that addresses the challenging goals of producing high quality speech while operating at faster than real-time rate without an expensive gpu support. the system is built around three nn models for generating the prosody, acoustic features and the final speech signal. we tested this system using two proprietary tts voice datasets and demonstrated that our system produces high quality speech that is comparable to larger and much slower tacotron2 wavenet systems. the task of creating a high-quality tts system out of a smaller set of audio data is even more challenging. we have shown that our system can perform well even with datasets as small as 5-20 minutes of audio. we demonstrated that when we reduce the size of the training data, there is some graceful degradation to the quality, but we are still able to maintain good similarity to the original speaker. for future work, we plan to allow voice modifications by adding control over voice parameters such as pitch, breathiness and vocal tract.", "section_index": 4}], "id": 1020}
{"text": [{"content": "we propose the sobolev independence criterion (sic), an interpretable dependency measure between a high dimensional random variable x and a response variable y sic decomposes to the sum of feature importance scores and hence can be used for nonlinear feature selection. sic can be seen as a gradient regularized integral probability metric (ipm) between the joint distribution of the two random variables and the product of their marginals. we use sparsity inducing gradient penalties to promote input sparsity of the critic of the ipm. in the kernel version we show that sic can be cast as a convex optimization problem by introducing auxiliary variables that play an important role in feature selection as they are normalized feature importance scores. we then present a neural version of sic where the critic is parameterized as a homogeneous neural network, improving its representation power as well as its interpretability. we conduct experiments validating sic for feature selection in synthetic and real-world experiments. we show that sic enables reliable and interpretable discoveries, when used in conjunction with the holdout randomization test and knockoffs to control the false discovery rate. code is available at url.", "section_index": 0}, {"content": "feature selection is an important problem in statistics and machine learning for interpretable predictive modeling and scientific discoveries. our goal in this paper is to design a dependency measure that is interpretable and can be reliably used to control the false discovery rate in feature selection. the mutual information between two random variables x and y is the most commonly used dependency measure. the mutual information i(x;y ) is defined as the kullback-leibler divergence between the joint distribution pxy of x,y and the product of their marginals pxpy, i(x;y ) kl(pxy, pxpy). mutual information is however challenging to estimate from samples, which motivated the introduction of dependency measures based on other f -divergences or integral probability metrics than the kl divergence. for instance, the hilbert-schmidt independence criterion (hsic) uses the maximum mean discrepancy (mmd) to assess the dependency between two variables, i.e. hsic(x,y ) mmd(pxy, pxpy), which can be easily estimated from samples via kernel mean embeddings in a reproducing kernel hilbert space (rkhs) in this paper we introduce the sobolev independence criterion (sic), a form of gradient regularized integral probability metric (ipm) between the joint distribution and the product of marginals. sic relies on the statistics of the gradient of a witness function, or critic, for both (1) defining the ipm constraint and (2) finding the features that discriminate between the joint and the marginals. intuitively, the magnitude of the average gradient with respect to a feature gives an importance score for each feature. hence, promoting its sparsity is a natural constraint for feature selection. the paper is organized as follows: we show in section how sparsity-inducing gradient penalties can be used to define an interpretable dependency measure that we name sobolev independence criterion tom sercu is now with facebook ai research, and cicero dos santos with amazon aws ai. the work was done when they were at ibm research. 33rd conference on neural information processing systems (neurips 2019), vancouver, canada. we devise an equivalent computational-friendly formulation of sic in section 3, that gives rise to additional auxiliary variables j these naturally define normalized feature importance scores that can be used for feature selection. in section we study the case where the sic witness function f is restricted to an rkhs and show that it leads to an optimization problem that is jointly convex in f and the importance scores we show that in this case sic decomposes into the sum of feature scores, which is ideal for feature selection. in section we introduce a neural version of sic, which we show preserves the advantages in terms of interpretability when the witness function is parameterized as a homogeneous neural network, and which we show can be optimized using stochastic block coordinate descent. in section we show how sic and conditional generative models can be used to control the false discovery rate using the recently introduced holdout randomization test and knockoffs we validate sic and its fdr control on synthetic and real datasets in section", "section_index": 1}, {"content": "motivation: feature selection. we start by motivating gradient-sparsity regularization in sic as a mean of selecting the features that maintain maximum dependency between two randoms variable x (the input) and y (the response) defined on two spaces x rdx and y rdy (in the simplest case dy 1). let pxy be the joint distribution of (x,y ) and px, py be the marginals of x and y resp. let d be an integral probability metric associated with a function space f , i.e for two distributions p, q: d(p, q) sup ff expf(x) exqf(x). with p pxy and q pxpy this becomes a generalized definition of mutual information. instead of the usual kl divergence, the metric d with its witness function, or critic, f(x, y) measures the distance between the joint pxy and the product of marginals pxpy with this generalized definition of mutual information, the feature selection problem can be formalized as finding a sparse selector or gate w rdx such thatd(pw x,y, pw xpy) is maximal , i.e. supw,w0sd(pw x,y, pw xpy), where is a pointwise multiplication and w0 jwj this problem can be written in the following penalized form: (p) : sup w sup ff epxyf(w x, y) epxpyf(w x, y) w0 we can relabel f(x, y) f(w x, y) and write (p) as: supff epxy f(x, y)epxpy f(x, y), where f f f(x, y) f(w x, y)f f , w0 s. observe that we have: f xj wj f(w x,y) xj since wj is sparse the gradient of f is sparse on the support of pxy and pxpy. hence, we can reformulate the problem (p) as follows: (sic): sup ff epxyf(x, y) epxpyf(x, y) ps(f), where ps(f) is a penalty that controls the sparsity of the gradient of the witness function f on the support of the measures. controlling the nonlinear sparsity of the witness function in (sic) via its gradients is more general and powerful than the linear sparsity control suggested in the initial form (p), since it takes into account the nonlinear interactions with other variables. in the following section we formalize this intuition by theoretically examining sparsity-inducing gradient penalties sparsity inducing gradient penalties. gradient penalties have a long history in machine learning and signal processing. in image processing the total variation norm is used for instance as a regularizer to induce smoothness. splines in sobolev spaces 15, and manifold learning exploit gradient regularization to promote smoothness and regularity of the estimator. in the context of neural networks, gradient penalties were made possible through double back-propagation introduced in and were shown to promote robustness and better generalization. such smoothness penalties became popular in deep learning partly following the introduction of wgan-gp 17, and were used as regularizer for distance measures between distributions in connection to optimal transport theory let be a dominant measure of pxy and pxpy the most commonly used gradient penalties is l2(f) e(x,y) xf(x, y) while this penalty promotes smoothness, it does not control the desired sparsity as discussed in the previous section. we therefore elect to instead use the nonlinear sparsity penalty introduced in : 0(f) je(x,y) f(x,y)xj 0, and its relaxation : s(f) dx j1 e(x,y) f(x, y)xj as discussed in 14, e(x,y) f(x,y)xj implies that f is constant with respect to variable xj , if the function f is continuously differentiable and the support of is connected. these considerations motivate the following definition of the sobolev independence criterion (sic): sic(l1)2(pxy, pxpy) sup ff epxyf(x, y) epxpyf(x, y) (s(f)) ef2(x, y). note that we add a 1-like penalty (s(f) ) to ensure sparsity and an 2-like penalty (ef2(x, y)) to ensure stability. this is similar to practices with linear models such as elastic net. here we will consider pxpy (although we could also use (pxy pxpy)). then, given samples (xi, yi), i 1, , n from the joint probability distribution pxy and iid samples (xi, yi), i 1, , n from pxpy , sic can be estimated as follows: sic(l1)2(pxy, pxpy) sup ff n n i1 f(xi, yi) n n i1 f(xi, yi) ( s(f) )2 n n i1 f2(xi, yi), where s(f) dx j1 n n i1 f(xi,yi)xj throughout this paper we consider feature selection only on x since y is thought of as the response. nevertheless, in many other problems one can perform feature selection on x and y jointly, which can be simply achieved by also controlling the sparsity ofyf(x, y) in a similar way.", "section_index": 2}, {"content": "as it was just presented, the sic objective is a difficult function to optimize in practice. first of all, the expectation appears after the square root in the gradient penalties, resulting in a non-smooth term (since the derivative of square root is not continuous at 0). moreover, the fact that the expectation is inside the nonlinearity introduces a gradient estimation bias when the optimization of the sic objective is performed using stochastic gradient descent (i.e. we alleviate these problems (non-smoothness and biased expectation estimation) by making the expectation linear in the objective thanks to the introduction of auxiliary variables j that will end up playing an important role in this work. this is achieved thanks to a variational form of the square root that is derived from the following lemma (which was used for a similar purpose as ours when alleviating the non-smoothness of mixed norms encountered in multiple kernel learning and group sparsity norms): lemma (18,19). d, aj we have: (d j1 aj )2 inf d j1 aj j : , j d j1 j 1, optimum achieved at j aj/ j aj we alleviate first the issue of non smoothness of the square root by adding an (0, 1), and we define: s, dx j1 e(x,y) f(x,y)xj using lemma the nonlinear sparsity inducing gradient penalty can be written as : (s,(f)) inf dx j1 epxpy f(x,y)xj j : , j 0, dx j1 j 1, where the optimum is achieved for : j, jdx k1 k , where 2j epxpy f(x,y)xj we refer to j, as the normalized importance score of feature j. note that j is a distribution over the features and gives a natural ranking between the features. hence, substituting (s)(f) with s,(f) in its equivalent form we obtain the perturbed sic: sic(l1)2,(pxy, pxpy) infl(f, ) : f f , j , j 0, dx j1 j where l(f, ) (f, pxy, pxpy) dx j1 epxpy f(x,y)xj j 2epxpyf 2(x, y), and (f, pxy, pxpy) epxyf(x, y) epxpyf(x, y). finally, sic can be empirically estimated as sic(l1)2,(pxy, pxpy) infl(f, ) : f f , j , j 0, dx j1 j where l(f, ) (f, pxy, pxpy) dx j1 n n i1 f(xi,yi)xj j n n i1 f 2(xi, yi), and main the objective (f, pxy, pxpy) 1n n i1 f(xi, yi) n n i1 f(xi, yi). we can define similarly nonlinear group sparsity, if we would like our critic to depends on subsets of coordinates. ,k be an overlapping or non overlapping group : gs(f) k k1 jgk epxpy f(x,y)xj the -trick applies naturally.", "section_index": 3}, {"content": "we will now specify the function space f in sic and consider in this section critics of the form: f f f(x, y) u,(x, y) , u2 , where : x y rm is a fixed finite dimensional feature map. we define the mean embeddings of the joint distribution pxy and product of marginals pxpy as follow: (pxy) epxy (x, y), (pxpy) epxpy (x, y) rm. define the covariance embedding of pxpy as c(pxpy) epxpy (x, y) (x, y) rmm and finally define the gramian of derivatives embedding for coordinate j as dj(pxpy) epxpy (x,y) xj (x,y)xj r mm. we can write the constraint u2 as the penalty term u define l(u, ) u, (pxpy) (pxy) u, ( dx j1 dj(pxpy) j c(pxpy) im ) u observe that : sic(l1)2,(pxy, pxpy) infl(u, ) : u rm, j , j 0, dx j1 j we start by remarking that sic is a form of gradient regularized maximum mean discrepancy previous mmd work comparing joint and product of marginals did not use the concept of nonlinear sparsity. for example the hilbert-schmidt independence criterion (hsic) uses (x, y) (x) (y) with a constraint u2 cca and related kernel measures of dependence 20, use l22 constraints l 2(px) and l 2(py) on each function space separately. optimization properties of convex sic we analyze in this section the optimization properties of sic. theorem shows that the sic(l1)2, loss function is jointly strictly convex in (u, ) and hence admits a unique solution that solves a fixed point problem. theorem (existence of a solution, uniqueness, convexity and continuity). the following properties hold for the sic loss: 1) l(u, ) is differentiable and jointly convex in (u, ). l(u, ) is not continuous for , such that j for some j. 2) smoothing, perturbed sic: for (0, 1), l(u, ) l(u, ) dx j1 j is jointly strictly convex and has compact level sets on the probability simplex, and admits a unique minimizer (u, ). 3) the unique minimizer of l(u, ) is a solution of the following fixed point problem: u ( dx j1 dj(pxpy) j c(pxpy) im )1 ((pxy) (pxpy)), and j, u ,dj(pxpy)udx k1 u ,dk(pxpy)u the following theorem shows that a solution of the unperturbed sic problem can be obtained from the smoothed sic(l1)2, in the limit 0: theorem (from perturbed sic to sic). consider a sequence , as , and consider a sequence of minimizers (u , ) of l(u, ), and let (u , ) be the limit of this sequence, then (u, ) is a minimizer of l(u, ). interpretability of sic. the following corollary shows that sic can be written in terms of the importance scores of the features, since at optimum the main objective is proportional to the constraint term. it is to the best of our knowledge the first dependency criterion that decomposes in the sum of contributions of each coordinate, and hence it is an interpretable dependency measure. moreover, j are normalized importance scores of each feature j, and their ranking can be used to assess feature importance. corollary (interpretability of convex sic ). let (u, ) be the limit defined in theorem we have that sic(l1)2(pxy, pxpy) ( epxyf(x, y) epxpyf(x, y) ) dx j1 epxpy f(x, y) xj epxpyf,2(x, y) f2f moreover, epxpy f(x,y) xj js,l1(f) and dx j1 j the terms j can be seen as quantifying how much dependency as measured by sic can be explained by a coordinate j. ranking of j can be used to rank influence of coordinates. thanks to the joint convexity and the smoothness of the perturbed sic, we can solve convex empirical sic using alternating minimization on u and or block coordinate descent using first order methods such as gradient descent on u and mirror descent on that are known to be globally convergent in this case (see appendix a for more details).", "section_index": 4}, {"content": "while convex sic enjoys a lot of theoretical properties, a crucial short-coming is the need to choose a feature map that essentially goes back to the choice of a kernel in classical kernel methods. as an alternative, we propose to learn the feature map as a deep neural network. the architecture of the network can be problem dependent, but we focus here on a particular architecture: deep relu networks with biases removed. as we show below, using our sparsity inducing gradient penalties with such networks, results in input sparsity at the level of the witness function f of sic. this is desirable since it allows for an interpretable model, similar to the effect of lasso with linear models, our sparsity inducing gradient penalties result in a nonlinear self-explainable witness function f 23, with explicit sparse dependency on the inputs. deep relu networks with no biases, homogeneity and input sparsity via gradient penalties. we start by invoking the euler theorem for homogeneous functions: theorem (euler theorem for homogeneous functions). a continuously differentiable function f is defined as homogeneous of degree k if f(x) kf(x), r. the theorem states that f is homogeneous of degree k if and only if kf(x) xf(x), x dx j1 f(x) xj xj now consider deep relu networks with biases removed for any number of layers l: frelu f f(x, y) u,(x) , where (x, y) (wl (w2(w1x, y))), u rm, : rdxdy rm, where (t) max(t, 0),wj are linear weights. any f frelu is clearly homogeneous of degree as an immediate consequence of euler theorem we then have: f(x, y) xf(x, y), x yf(x, y), y. the first term is similar to a linear term in a linear model, the second term can be seen as a bias. using our sparsity-inducing gradient penalties with such networks guarantees that on average on the support of a dominant measure the gradients with respect to x are sparse. intuitively, the gradients wrt x act like the weight in linear models, and our sparsity inducing gradient penalty act like the regularization of lasso. the main advantage compared to lasso is that we have a highly nonlinear decision function, that has better capacity of capturing dependencies between x and y non-convex sic with stochastic block coordinate descent (bcd). we define the empirical non convex sic(l1)2 using this function space frelu as follows: sic(l1)2(pxy, pxpy) infl(f, ) : f frelu , j , j 0, dx j1 j 1, where (vec(w1) vec(wl), u) are the network parameters. algorithm in appendix b summarizes our stochastic bcd algorithm for training the neural sic. the algorithm consists of sgd updates to and mirror descent updates to when training neural sic, we can obtain different critics f and importance scores , by varying random seeds or hyper-parameters (architecture, batch size etc). inspired by importance scores in random forest, we define boosted sic as the arithmetic mean or the geometric mean of", "section_index": 5}, {"content": "controlling the false discovery rate (fdr) in feature selection is an important problem for reproducible discoveries. in a nutshell, for a feature selection problem given the ground-truth set of features s , and a feature selection method such as sic that gives a candidate set s, our goal is to maximize the tpr (true positive rate) or the power, and to keep the false discovery rate (fdr) under control. tpr and fdr are defined as follows: tpr : e i : i s s i : i s fdr : e i : i s\\s i : i s (1) we explore in this paper two methods that provably control the fdr: 1) the holdout randomization test (hrt) introduced in 8, that we specialize for sic in algorithm 4; 2) knockoffs introduced in that can be used with any basic feature selection method such as neural sic, and guarantees provable fdr control. we are interested in measuring the conditional dependency between a feature xj and the response variable y conditionally on the other features noted xj hence we have the following null hypothesis: h0 : xj y xj pxy pxj xjpyxjpxj in order to simulate the null hypothesis, we propose to use generative models for sampling from xj xj (see appendix d). the principle in hrt that we specify here for sic in algorithm (given in appendix b) is the following: instead of refitting sic under h0, we evaluate the mean of the witness function of sic on a holdout set sampled under h0 (using conditional generators for r rounds). the deviation of the mean of the witness function under h0 from its mean on a holdout from the real distribution gives us p-values. we use the benjamini-hochberg procedure on those p-values to achieve a target fdr. we apply hrt-sic on a shortlist of pre-selected features per their ranking of j knockoffs work by finding control variables called knockoffs x that mimic the behavior of the real features x and provably control the fdr we use here gaussian knockoffs and train sic on the concatenation of x, x, i.e we train sic(x; x, y ) and obtain that has now twice the dimension dx, i.e for each real feature j, there is the real importance score j and the knockoff importance score jdx knockoffs-sic consists in using the statistics wj j jdx and the knockoff filter to select features based on the sign of wj (see alg.", "section_index": 6}, {"content": "kernel/neural measure of dependencies. as discussed earlier sic can be seen as a sparse gradient regularized mmd 3, and relates to the sobolev discrepancy of 5, feature selection with mmd was introduced in and is based on backward elimination of features by recomputing mmd on the ablated vectors. sic has the advantage of fitting one critic that has interpretable feature scores. related to the mmd is the hilbert schmidt independence criterion (hsic) and other variants of kernel dependency measures introduced in 2, none of those criteria has a nonparametric sparsity constraint on its witness function that allows for explainability and feature selection. other neural measures of dependencies such as mine estimate the kl divergence using neural networks, or that of that estimates a proxy to the wasserstein distance using neural networks. interpretability, sparsity, saliency and sensitivity analysis. lasso and elastic net are interpretable linear models that exploit sparsity, but are limited to linear relationships. random forests have a heuristic for determining feature importance and are successful in practice as they can capture nonlinear relationships similar to sic. we believe sic can potentially leverage the deep learning toolkit for going beyond tabular data where random forests excel, to more structured data such as time series or graph data. finally, sic relates to saliency based post-hoc interpretation of deep models such as while those method use the gradient information for a post-hoc analysis, sic incorporates this information to guide the learning towards the important features. as discussed in section many recent works introduce deep networks with input sparsity control through a learned gate or a penalty on the weights of the network sic exploits a stronger notion of sparsity that leverages the relationship between the different covariates.", "section_index": 7}, {"content": "synthetic data validation. we first validate our methods and compare them to baseline models in simulation studies on synthetic datasets where the ground truth is available by construction. for this we generate the data according to a model y f(x) where the model f() and the noise define the specific synthetic dataset (see appendix f.1). in particular, the value of y only depends on a subset of features xi, i 1, , p through f(), and performance is quantified in terms of tpr and fdr in discovering them among the irrelevant features. we experiment with two datasets: a) complex multivariate synthetic data (sinexp), which is generated from a complex multivariate model proposed in sec 5.3, where ground truth features xi out of generate the output y through a non-linearity involving the product and composition of the cos, sin and exp functions (see appendix f.1). we therefore dub this dataset sinexp. to increase the difficulty even further, we introduce a pairwise correlation between all features of we show results for datasets of and samples repeated times comparing performance of our models with the one of two baselines: elastic net (en) and random forest (rf). we show results on the benchmark dataset proposed by 34, specifically the generalized liang dataset matching most of the setup from sec we provide dataset details and results in appendix f.1 (results in figure 2). tpr top fdr top tpr hrt fdr hrt tpr top fdr top tpr hrt fdr hrt po we r a nd f dr elastic net random forest tpr top fdr top tpr hrt fdr hrt tpr top fdr top tpr hrt fdr hrt mse sobolev penalty sic dataset sinexp, n125 samples dataset sinexp, n500 samples feature selection on drug response dataset. we consider as a real-world application the cancer cell line encyclopedia (ccle) dataset 36, described in appendix f.2. we study the result of using the normalized importance scores j from sic for (heuristic) feature selection, against features selected by elastic net. table shows the heldout mse of a predictor trained on selected features, averaged over runs (each run: new randomized 90/10 data split, nn initialization). the goal here is to quantify the predictiveness of features selected by sic on its own, without the full randomized testing machinery. the sic critic and regressor nn were respectively the bigcritic and regressornn described with training details in appendix f.3, while the random forest is trained with default hyper parameters from scikit-learn we can see that, with just j , informative features are selected for the downstream regression task, with performance comparable to those selected by elasticnet, which was trained explicitly for this task. the features selected with high j values and their overlap with the features selected by elasticnet are listed in appendix f.2 table hiv-1 drug resistance with knockoffs-sic. the second real-world dataset that we analyze is the hiv-1 drug resistance38, which consists in detecting mutations associated with resistance to a drug type. for our experiments we use all the three classes of drugs: protease inhibitors (pis), nucleoside reverse transcriptase inhibitors (nrtis), and non-nucleoside reverse transcriptase inhibitors (nnrtis). we use the pre-processing of each dataset (drug-class, drug-type) of the knockoff tutorial made available by the authors. concretely, we construct a dataset (x, x) of the concatenation of the real data and gaussian knockoffs 9, and fit sic(x, x, y ). as explained in section 6, we use in the knockoff filter the statistics wj j jdx , i.e. the difference of sic importance scores between each feature and its corresponding knockoff. for sic experiments, we use smallcritic architecture (see appendix f.3 for training details). we use boosted sic, by varying the batch sizes in n 10, 30, 50, and computing the geometric mean of produced by those three setups as the feature importance needed for knockoffs. results are summarized in table", "section_index": 8}, {"content": "we introduced in this paper the sobolev independence criterion (sic), a dependency measure that gives rise to feature importance which can be used for feature selection and interpretable decision making. we laid down the theoretical foundations of sic and showed how it can be used in conjunction with the holdout randomization test and knockoffs to control the fdr, enabling reliable discoveries. we demonstrated the merits of sic for feature selection in extensive synthetic and real-world experiments with controlled fdr.", "section_index": 9}], "id": 1021}
