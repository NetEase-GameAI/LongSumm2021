[{"document": "we describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language. our method does not assume any knowledge about the target language across eight european languages, our approach results in an average absolute improvement of over a state-of-the-art baseline, and over vanilla hidden markov models induced with the expectation maximization algorithm. supervised learning approaches have advanced the state-of-the-art on a variety of tasks in natural language processing, resulting in highly accurate systems. supervised part-of-speech however, supervised methods rely on labeled training data, which is time-consuming and expensive to generate. unsupervised learning approaches appear to be a natural solution to this problem, as they require only unannotated text for trainthis research was carried out during an internship at google research. unfortunately, the best completely unsupervised english pos tagger , making its practical usability questionable at best. to bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language study related but different multilingual grammar and tagger induction tasks, where it is assumed that no labeled data at all is available. our work is closest to that of yarowsky and ngai (2001), but differs in two important ways. first, we use a novel graph-based framework for projecting syntactic information across language boundaries. to this end, we construct a bilingual graph over word types to establish a connection between the two languages (3), and then use graph label propagation to project syntactic information from english to the foreign language (4). second, we treat the projected labels as features in an unsuper- 1for simplicity of exposition we refer to the resource-poor language as the foreign language. similarly, we use english as the resource-rich language, but any other language with labeled resources could be used instead. syntactic universals are a well studied concept in linguistics for multilingual grammar induction. because there might be some controversy about the exact definitions of such universals, this set of coarse-grained pos categories is defined operationally, by collapsing language (or treebank) specific distinctions to a set of categories that exists across all languages. these universal pos categories not only facilitate the transfer of pos information from one language to another, but also relieve us from using controversial evaluation metrics,2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed english labels. we evaluate our approach on eight european languages , and considerably bridges the gap to fully supervised pos tagging performance (96.6). we have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages. because we are interested in applying our techniques to languages for which no labeled resources are available, we paid particular attention to minimize the number of free parameters and used the same hyperparameters for all language pairs. our results suggest that it is possible to learn accurate pos taggers for languages which do not have any annotated data, but have translations into a resource-rich language. our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised pos tagging models.", "summary": " ", "id": 1000}, {"document": "recurrent neural networks (rnns) have had considerable success in classifying and predicting sequences. we demonstrate that rnns can be effectively used in order to encode sequences and provide effective representations. the methodology we use is based on fisher vectors, where the rnns are the generative probabilistic models and the partial derivatives are computed using backpropagation. state of the art results are obtained in two central but distant tasks, which both rely on sequences: video action recognition and image annotation. we also show a surprising transfer learning result from the task of image annotation to the task of video action recognition. fisher vectors have been shown to provide a significant performance gain on many different applications in the domain of computer vision 39, 33, 2, in the domain of video action recognition, fisher vectors and stacked fisher vectors have recently outperformed state-of-theart methods on multiple datasets 33, fisher vectors (fv) have also recently been applied to word embedding (e.g. word2vec 30) and have been shown to provide state of the art results on a variety of nlp tasks 24, as well as on image annotation and image search tasks in all of these contributions, the fv of a set of local descriptors is obtained as a sum of gradients of the loglikelihood of the descriptors in the set with respect to the parameters of a probabilistic mixture model that was fitted on a training set in an unsupervised manner. in spite of being richer than the mean vector pooling method, fisher vectors based on a probabilistic mixture model are invariant to order. this makes them less appealing for annotating, for example, video, in which the sequence of events determines much of the meaning. this work presents a novel approach for fv representation of sequences using a recurrent neural network (rnn). the rnn is trained to predict the next element of a sequence given the previous elements. conveniently, the gradients needed for the computation of the fv are extracted using the available backpropagation infrastructure. the new representation is sensitive to ordering and therefore mitigates the disadvantage of using the standard fisher vector representation. it is applied to two different and challenging tasks: video action recognition and image annotation by sentences. several recent works have proposed to use an rnn for sentence representation 44, 1, 31, the recurrent neural network fisher vector (rnn-fv) method differs from these works in that a sequence is represented by using derived gradient from the rnn as features, instead of using a hidden or an output layer of the rnn. the paper explores two different approaches for training the rnn for the image annotation and image search tasks. in the classification approach, the rnn is trained to predict the following word in the sentence. the regression approach tries to predict the embedding of the following word (i.e. treating it as a regression task). the large vocabulary size makes the regression approach more scalable and achieves better results than the classification approach. in the video action recognition task, the regression approach is the only variant being used, since the notion of a discrete word does not exist. the vgg convolutional neural network (cnn) is used to extract features from the frames of the video and the rnn is trained to predict the embedding of the next frame given the previous ones. similarly, c3d features of sequential video sub-volumes are used with the same training technique. although the image annotation and video action recognition tasks are quite different, a surprising boost in performance in the video action recognition task was achieved by using a transfer learning approach from the image annotation task. specifically, the vgg image embedding of a frame is projected using a linear transformation which was learned on matching images and sentences by the canonical correlation analysis (cca) algorithm the proposed rnn-fv method achieves state-of-theart results in action recognition on the hmdb51 and ucf101 datasets. in image annotation and image search tasks, the rnn-fv method is used for the representation of sentences and achieves state-of-the-art results on the flickr8k dataset and competitive results on other benchmarks. this paper introduces a novel fv representation for sequences that is derived from rnns. the proposed representation is sensitive to the element ordering in the sequence and provides a richer model than the additive bag model typically used for conventional fvs. the rnn-fv representation surpasses the state-of-theart results for video action recognition on two challenging datasets. when used for representing sentences, the rnnfv representation achieves state-of-the-art or competitive results on image annotation and image search tasks. since the length of the sentences in these tasks is usually short and, therefore, the ordering is less crucial, we believe that using the rnn-fv representation for tasks that use longer text will provide an even larger gap between the conventional fv and the rnn-fv. a transfer learning result from the image annotation task to the video action recognition task was shown. the con- ceptual distance between these two tasks makes this result both interesting and surprising. it supports a human development-like way of training, in which visual labeling is learned through natural language, as opposed to, e.g., associating bounding boxes with nouns. while such training was used in computer vision to learn related image to text tasks, and while recently zero-shot action recognition was shown 11, 55, nlp to video action recognition transfer was never shown to be as general as presented here.", "summary": " ", "id": 1001}, {"document": "currently, no large-scale training data is available for the task of scientific paper summarization. in this paper, we propose a novel method that automatically generates summaries for scientific papers, by utilizing videos of talks at scientific conferences. we hypothesize that such talks constitute a coherent and concise description of the papers\u2019 content, and can form the basis for good summaries. we collected papers and their corresponding videos, and created a dataset of paper summaries. a model trained on this dataset achieves similar performance as models trained on a dataset of summaries created manually. in addition, we validated the quality of our summaries by human experts. the rate of publications of scientific papers is increasing and it is almost impossible for re- searchers to keep up with relevant research. au- tomatic text summarization could help mitigate this problem. in general, there are two com- mon approaches to summarizing scientific papers: citations-based, based on a set of citation sen- tences (nakov et al., 2004; abu-jbara and radev, 2011; yasunaga et al., 2019), and content-based, based on the paper itself (collins et al., 2017; nikola nikolov and hahnloser, 2018). automatic summarization is studied exhaustively for the news domain (cheng and lapata, 2016; see et al., 2017), while summarization of scientific papers is less studied, mainly due to the lack of large-scale training data. the papers\u2019 length and complexity require substantial summarization effort from ex- perts. several methods were suggested to reduce these efforts (yasunaga et al., 2019; collins et al., 2017), still they are not scalable as they require human annotations. the authors contributed equally. recently, academic conferences started publishing videos of talks (e.g., acl1, emnlp1, icml2, and more). in such talks, the presenter (usually a co-author) must describe their paper coherently and concisely (since there is a time limit), provid- ing a good basis for generating summaries. based on this idea, in this paper, we propose a new method, named talksumm (acronym for talk- based summarization), to automatically generate extractive content-based summaries for scientific papers based on video talks. our approach uti- lizes the transcripts of video content of conference talks, and treat them as spoken summaries of pa- pers. then, using unsupervised alignment algo- rithms, we map the transcripts to the correspond- ing papers\u2019 text, and create extractive summaries. table gives an example of an alignment between 1vimeo.com/aclweb icml.cc/conferences/2017/videos a paper and its talk transcript (see table in the appendix for a complete example). summaries generated with our approach can then be used to train more complex and data- demanding summarization models. although our summaries may be noisy (as they are created auto- matically from transcripts), our dataset can easily grow in size as more conference videos are aggre- gated. moreover, our approach can generate sum- maries of various lengths. our main contributions are as follows: (1) we propose a new approach to automatically gener- ate summaries for scientific papers based on video talks; (2) we create a new dataset, that contains summaries for papers from several computer science conferences, that can be used as training data; (3) we show both automatic and human eval- uations for our approach. we make our dataset and related code publicly available3. to our knowl- edge, this is the first approach to automatically cre- ate extractive summaries for scientific papers by utilizing the videos of conference talks. we propose a novel automatic method to gener- ate training data for scientific papers summariza- tion, based on conference talks given by authors. we show that the a model trained on our dataset achieves competitive results compared to models trained on human generated summaries, and that the dataset quality satisfies human experts. in the future, we plan to study the effect of other video modalities on the alignment algorithm. we hope our method and dataset will unlock new opportu- nities for scientific paper summarization.", "summary": " ", "id": 1002}, {"document": "emotion detection from text has become a popular task due to the key role of emotions in human-machine interaction. current approaches represent text as a sparse bag-of-words vector. in this work, we propose a new approach that utilizes pre-trained, dense word embedding representations. we introduce an ensemble approach combining both sparse and dense representations. our experiments include ve datasets for emotion detection from di erent domains and show an average improvement of in macro average f1-score. emotions are an important element of human nature and detecting them in the textual messages written by users has many applications in information retrieval and human-computer interaction a common approach to emotion analysis and modeling is categorization, e.g., according to ekman\u2019s basic emotions; namely, anger, disgust, fear, happiness, sadness, and surprise approaches to categorical emotion classi cation based on text often employ supervised machine learning classi ers, which require labeled training data. currently, two types of datasets labeled with emotions are publicly available: manually labeled, and pseudo-labeled. manual annotation requires high cognitive capabilities of multiple human annotators per sample. as a result, the quality of these datasets is usually high. however, the task is tedious, time-consuming, and expensive 4, and thus, these datasets are usually small (in the order of thousands of annotated samples). manual annotations are usually applied to domain speci c datasets (e.g., news headlines). to overcome these limitations, pseudo-labeled datasets are gathered from social media platforms where social media posts are explicitly tagged by the author by using the hashtag symbol () or by adding emoticons this tagged data can be used to create large-scale training data labeled with emotions in a non-speci c domain as in given such a dataset (manually or pseudo labeled), it is then common to train a linear classi er based on bag-of-words (bow) 1url/ permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. copyrights for components of this work owned by others than acm must be honored. abstracting with credit is permitted. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. request permissions from permissionsacm.org. ictir\u201917, october 14, 2017, amsterdam, the netherlands. doi: 10.1145/3121050.3121093 representation: representing text samples as sparse vectors, where each vector entry corresponds to the presence of a speci c feature (such as n-grams, punctuation and other) as reported recently by 11, deep learning is a promising approach for solving nlp tasks including text classi cation. while the aforementioned approach utilizes bow representation and linear classi ers, neural network methods are based on dense vector representations of text samples (word embedding) and are nonlinear. such word embedding representation captures syntactic and semantic knowledge, which can improve the emotion detection task. for example, in such a representation the words awful and terrible are expected to have similar vector representations. generating high quality word vectors requires large-scale data and computing power. when generation is not an option, one can utilize pre-trained representations; the most popular pre-trained representations are based on word2vec and glove algorithms, and were trained on large corpora. inspired by the good results of the deep learning approach, we aim at using these techniques for emotion detection. pseudo-labeled large-scale data is suitable for deep learning techniques, however, it exhibits low accuracy when classifying domain speci c data, even after domain adaptation (using linear models). on the other hand, the manually annotated data is highly accurate, but it is not large enough to form an appropriate base for deep learning techniques, as these models tend to over t small size datasets. in order to utilize the high quality but small size datasets, we propose an ensemble approach that combines both a linear model based on bow, and a non-linear model based on the pre-trained word vectors. in addition, we propose a new method for realizing a sentence level representation from the single words vectors. to our knowledge, this is the rst research that shows how to utilize pre-trained word vectors to improve emotion detection from text in domain-speci c datasets. this work studied the use of pre-trained word vectors for emotion detection. we presented class, a novel method for representing a document as a dense vector based on the importance of the document\u2019s terms in respect to emotion classi cation. our results show that an ensemble that combines bow and embedded representations using our class method, outperforms previous approaches for domain-speci c datasets. in comparison to other deep-learning methods, our approach ts a small number of model parameters and requires little computing power. for future work we plan to investigate the use of deep learning models trained on domain adapted pseudo-labeled large-scale datasets. we also plan to investigate transfer learning for multidomain emotion detection.", "summary": " ", "id": 1003}, {"document": "providing customer support through social media channels is gaining increasing popularity. in such a context, automatic detection and analysis of the emotions expressed by customers is important, as is identification of the emotional techniques (e.g., apology, empathy, etc.) in the responses of customer service agents. result of such an analysis can help assess the quality of such a service, help and inform agents about desirable responses, and help develop automated service agents for social media interactions. in this paper, we show that, in addition to text based turn features, dialogue features can significantly improve detection of emotions in social media customer service dialogues and help predict emotional techniques used by customer service agents. an interesting use case for social media is customer support that can now take place over public social media channels. using this medium has its advantages as described, for example, in (demers, 2014): customers appreciate the simplicity and immediacy of social media conversations, the ability to reach real human beings, the transparency, and the feeling that someone listens to them. businesses also benefit from the publicity of giving good services almost in real-time, online, building an online community of customers and encouraging more brand mentions in social media. a recent study shows that one in five (23) customers in the u.s. say they have used social media for customer service in 2014, up from in obviously, companies hope that such 1url/ news/docs/2014x/2014-global-customer- uses are associated with a positive experience. yet there are limited tools for assessing this. in this paper, we analyze customer support dialogues using the twitter platform and show the utility of such analyses. the particular aspect of such dialogues that we concentrate on is emotions. emotions are a cardinal aspect of inter-personal communication: they are an implicit or explicit part of essentially any communication, and of particular importance in the setting of customer service, as they relate directly to customer satisfaction and experience (oliver, 2014). typical emotions expressed by customers in the context of social media service dialogues include anger and frustration, as well as gratitude and more (gelbrich, 2010). on the other hand, customer service agents also express emotions in service conversations, for example apology or empathy. however, it is important to note that emotions expressed by service agents are typically governed by company policies that specify which emotions should be expressed in which situation (rafaeli and sutton, 1987). this is why we talk in this paper about agent emotional techniques rather than agent emotions. consider, for example, the real (anonymized) twitter dialogue depicted in figure in this dialogue, customer disappointment is expressed in the first turn (\u2019bummer. /\u2019), followed by customer support empathy (\u2019uh oh!\u2019). then in the last two turns both customer and support express gratitude. the analysis of emotions being expressed in customer support conversations can take two applications: (1) to discern and compute quality of service indicators and (2) to provide real-time clues to customer service agents regarding the cus- service-barometer-us.pdf tomer emotion expressed in a conversation. a possible application here is recommending to customer service agents what should be their emotional response (for example, in each situation, should they apologize, should they thank the customer, etc.) another interesting trend in customer service, in addition to the use of social media described above, is the automation of various functions of customer interaction. several companies are developing text-based chat agents, typically accessible through corporate web sites, and partially automatized: in these platforms, a computer program handles simple conversations with customers, and more complicated dialogues are transferred to a human agent. such partially automated systems are also in use for social media dialogues. the automation in such systems helps save human resources and, with further development based on artificial intelligence, more automation in customer service chats is likely to appear. given the importance of emotions in service dialogues, such systems will benefit from the ability to detect (customer) emotions and will need to guide employees (and machines) regarding the right emotional technique in various situations (e.g., apologizing at the right point). thus, our goal, in this paper, is to show that the functionality of guiding employees regarding appropriate responses can be developed based on the analysis of textual dialogue data. we show first that it is possible to automatically detect emotions being expressed and, second that it is possible to predict the emotional technique that is likely to be used by a human agent in a given situation. this analysis reflects our ultimate goal: to enable a computer system to discern the emotions expressed by human customers, and to develop computerized tools that mimic the emotional technique used by a human customer service agent in a particular situation. we see the main contributions of this paper as follows: (1) to our knowledge, this is the first research focusing on automatic analysis of emotions expressed in customer service provided through social media. (2) this is the first research using unique dialogue features (e.g., emotions expressed in previous dialogue turns by the agent and customer, time between dialogue turns) to improve emotion detection. (3) this is the first research studying the prediction of the agent emotional techniques to be used in the response to customer turns. the rest of this paper is organized as follows. we start by reviewing the related work and a description of the data that we collected. then we formally define the methodology for detection and prediction of emotion expression in dialogues. finally, we describe our experiments, evaluate the various models, conclude and suggest future directions. in this work we studied emotions being expressed in customer service dialogues in the social media. specifically, we described two classification tasks, one for detecting customer emotions and the other for predicting the emotional technique used by support service agent. we have proposed two different models (svm dialogue and svm-hmm dialogue models) for these tasks. we studied the impact of dialogue features and dialogue history on the quality of the classification and showed improvement in performance for both models and both classification tasks. we also showed the robustness of our models across different data sources. as for future work we plan to work on several aspects: (1) in this work, we showed that it is possible to predict the emotional technique. in the future, we plan to run experiments in which the predicted emotional technique is actually applied in the context of new dialogues to measure the effect of such predictions on real support dialogues. (2) distinguish between dialogues that have positive outcomes (e.g., high customer satisfaction) and others.", "summary": " ", "id": 1004}, {"document": "few-shot learning (fsl) is a topic of rapidly growing interest. typically, in fsl a model is trained on a dataset consisting of many small tasks (meta-tasks) and learns to adapt to novel tasks that it will encounter during test time. this is also referred to as meta-learning. another topic closely related to meta-learning with a lot of interest in the community is neural architecture search (nas), automatically finding optimal architecture instead of engineering it manually. in this work we combine these two aspects of meta-learning. so far, meta-learning fsl methods have focused on optimizing parameters of pre-defined network architectures, in order to make them easily adaptable to novel tasks. moreover, it was observed that, in general, larger architectures perform better than smaller ones up to a certain saturation point (where they start to degrade due to over-fitting). however, little attention has been given to explicitly optimizing the architectures for fsl, nor to an adaptation of the architecture at test time to particular novel tasks. in this work, we propose to employ tools inspired by the differentiable neural architecture search (d-nas) literature in order to optimize the architecture for fsl without over-fitting. additionally, to make the architecture task adaptive, we propose the concept of metadapt controller\u2019 modules. these modules are added to the model and are meta-trained to predict the optimal network connections for a given novel task. using the proposed approach we observe state-of-the-art results on two popular few-shot benchmarks: miniimagenet and fc100. recently, there has been a lot of exciting progress in the field of few-shot learning in general, and in few-shot classification (fsc) in particular. a popular method for approaching fsc is meta-learning, or learning-to-learn. in meta-learning, the inputs to both train and test phases are not images, but instead a set of few-shot tasks, ti, each k-shot / n -way task containing a small amount k (usually 1-5) of labeled support images and some amount of unlabeled query images for each of the n categories of the task. the goal of meta-learning is to find a base model that is easily adapted to the specific task at hand, so that it will generalize well to tasks built from novel unseen categories and fulfill the goal of fsc (see section for further review). equal contributors corresponding authors: sivan doveh sivan.dovehibm.com and leonid karlinsky leonidkail.ibm.com ar x iv :1 2v cs .c v m ar many successful meta-learning based approaches have been developed for fsc 60,55,13,39,51,41,29 advancing its state-of-the-art. besides continuous improvements offered by the fsc methods, some general trends affecting the performance of fsc have become apparent. one of such major factors is the cnn backbone architecture at the basis of all the modern fsc methods. carefully reviewing and placing on a single chart the test accuracies of top-performing fsc approaches w.r.t. the backbone architecture employed reveals an interesting trend (figure 1). it is apparent that larger architectures increase fsc performance, up to a certain size, where performance seems to saturate or even degrade. this happens since bigger backbones carry higher risk of over-fitting. it seems the overall performance of the fsc techniques cannot continue to grow by simply expanding the backbone size. in light of the above, in this paper we set to explore methods for architecture search, their meta-adaptation and optimization for fsc. neural architecture search (nas) is a very active research field that has contributed significantly to overall improvement of the state of the art in supervised classification. some of the recent nas techniques, and in particular differentiable-nas (d-nas), such as darts 34, are capable of finding optimal (and transferable) architectures given a particular task using a single gpu in the course of 1-2 days. this is due to incorporating the architecture as an additional set of neural network parameters to be optimized, and solving this optimization using sgd. due to this use of additional architectural parameters, the training tends to over-fit. d-nas optimization techniques are especially designed to mitigate over-fitting, making them attractive to extreme situations with the greatest risk of overfitting, such as in the case of fsc. so far, d-nas techniques have been explored mainly in the context of large scale tasks, involving thousands of labeled examples for each class. very little work has been done on nas for few-shot. d-nas in particular, to the best of our knowledge, has not been applied to few-shot problems yet. meta-adaption of the architecture in task dependent manner to accommodate for novel tasks also has not been explored. in this work, we build our few-shot task-adaptive architecture search upon a technique from d-nas (darts 34). our goal is to learn a neural network where connections are controllable and adapt to the few-shot task with novel categories. similarly to darts, we have a neural network in the form of a directed acyclic graph (dag), where the nodes are the intermediate feature maps tensors, and edges are operations. each edge is a weighted sum of operations (with weights summing to 1), each operation is a different preset sequence of layers (convolution, pooling, batchnorm and non-linearity). the operations set includes the identity-operation and the zero-operation to either keep the representation untouched or cut the connection. to avoid over-fitting, a bi-level (two-fold) optimization is performed where first the operation layers\u2019 weights are trained on one fold of the data and then the connections\u2019 weights are trained on the other fold. however, unlike darts, our goal is not to learn a one time architecture to be used for all tasks. to be successful at fsc, we need to make our architecture task adaptive so it would be able to quickly rewire for each new target task. to this end, we employ a set of small neural networks, metadapt controllers, responsible for controlling the connections in the dag given the current task. the metadapt controllers adjust the weights of the different operations, such that if some operations are better for the current task they will get higher weights, thus, effectively modifying the architecture and adapting it to the task. to summarize, our contributions in this work are as follows: (1) we show that darts-like bi-level iterative optimization of layer weights and network connections performs well for few-shot classification without suffering from overfitting due to over-parameterization; (2) we show that adding small neural networks, metadapt controllers, that adapt the connections in the main network according to the given task further (and significantly) improves performance; (3) using the proposed method, we obtain improvements over fsc state-of-the-art on two popular fsc benchmarks: miniimagenet and fc100 in this work we have proposed metadapt, a few-shot learning approach that enables meta-learned network architecture that is adaptive to novel few-shot tasks. the proposed approach effectively applies tools from the neural architecture search (nas) literature, extended with the concept of metadapt controllers\u2019, in order to learn adaptive architectures. these tools help mitigate over-fitting to the extremely small data of the few-shot tasks and domain shift between the training set and the test set. we demonstrate that the proposed approach successfully improves state-of-the-art results on two popular few-shot benchmarks, miniimagenet and fc100, and carefully ablate the different optimization steps and design choices of the proposed approach. some interesting future work directions include extending the proposed approach to progressively searching the full network architecture (instead of just the last block), applying the approach to other few-shot tasks such as detection and segmentation, and researching into different variants of task-adaptivity including global connections modifiers and inter block adaptive wiring.", "summary": " ", "id": 1005}, {"document": "virtual agents are becoming a prominent channel of interaction in customer service. not all customer interactions are smooth, however, and some can become almost comically bad. in such instances, a human agent might need to step in and salvage the conversation. detecting bad conversations is important since disappointing customer service may threaten customer loyalty and impact revenue. in this paper, we outline an approach to detecting such egregious conversations, using behavioral cues from the user, patterns in agent responses, and useragent interaction. using logs of two commercial systems, we show that using these features improves the detection f1-score by around over using textual features alone. in addition, we show that those features are common across two quite different domains and, arguably, universal. automated conversational agents (chatbots) are becoming widely used for various tasks such as personal assistants or as customer service agents. recent studies project that of businesses plan to use chatbots by 20201, and that chatbots will power of customer service interactions by the year this increasing usage is mainly due to advances in artificial intelligence and natural language processing (hirschberg and manning, 2015) 1url 2url along with increasingly capable chat development environments, leading to improvements in conversational richness and robustness. still, chatbots may behave extremely badly, leading to conversations so off-the-mark that only a human agent could step in and salvage them. consequences of these failures may include loss of customer goodwill and associated revenue, and even exposure to litigation if the failures can be shown to include fraudulent claims. due to the increasing prevalence of chatbots, even a small fraction of such egregious3 conversations could be problematic for the companies deploying chatbots and the providers of chatbot services. in this paper we study detecting these egregious conversations that can arise in numerous ways. for example, incomplete or internally inconsistent training data can lead to false classification of user intent. bugs in dialog descriptions can lead to dead ends. failure to maintain adequate context can cause chatbots to miss anaphoric references. in the extreme case, malicious actors may provide heavily biased (e.g., the tay chatbot4) or even hacked misbehaviors. in this article, we focus on customer care systems. in such setting, a conversation usually becomes egregious due to a combination of the aforementioned problems. the resulting customer frustration may not surface in easily detectable ways such as the appearance of all caps, shouting to a speech recognizer, or the use of profanity or extreme punctuation. consequently, the chatbot will continue as if the conversation is proceeding well, usually 3defined by the dictionary as outstandingly bad. 4url leading to conversational breakdown. consider, for example, the anonymized but representative conversation depicted in figure here the customer aims to understand the details of a flight ticket. in the first two turns, the chatbot misses the customer\u2019s intentions, which leads to the customer asking are you a real person?. the customer then tries to explain what went wrong, but the chatbot has insufficient exposure to this sort of utterance to provide anything but the default response (i\u2019m not trained on that). the response seems to upset the customer and leads to a request for a human agent, which is rejected by the system (we don\u2019t currently have live agents). such rejection along with the previous responses could lead to customer frustration (amsel, 1992). being able to automatically detect such conversations, either in real time or through log analysis, could help to improve chatbot quality. if detected in real time, a human agent can be pulled in to salvage the conversation. as an aid to chatbot improvement, analysis of egregious conversations can often point to problems in training data or system logic that can be repaired. while it is possible to scan system logs by eye, the sheer volume of conversations may overwhelm the analyst or lead to random sampling that misses important failures. if, though, we can automatically detect the worst conversations (in our experience, typically under of the total), the focus can be on fixing the worst problems. our goal in this paper is to study conversational features that lead to egregious conversations. specifically, we consider customer inputs throughout a whole conversation, and detect cues such as rephrasing, the presence of heightened emotions, and queries about whether the chatbot is a human or requests to speak to an actual human. in addition, we analyze the chatbot responses, looking for repetitions (e.g. from loops that might be due to flow problems), and the presence of not trained responses. finally, we analyze the larger conversational context exploring, for example, where the presence of a not trained response might be especially problematic (e.g., in the presence of strong customer emotion). the main contributions of this paper are twofold: (1) this is the first research focusing on detecting egregious conversations in conversational agent (chatbot) setting and (2) this is the first research using unique agent, customer, and customer-agent interaction features to detect egregiousness. the rest of this paper is organized as follows. we review related work, then we formally define the methodology for detecting egregious conversations. we describe our data, experimental setting, and results. we then conclude and suggest future directions. in this paper, we have shown how it is possible to detect egregious conversations using a combination of customer utterances, agent responses, and customer-agent interactional features. as explained, the goal of this work is to give developers of automated agents tools to detect and then solve problems cre- ated by exceptionally bad conversations. in this context, future work includes collecting more data and using neural approaches (e.g., rnn, cnn) for analysis, validating our models on a range of domains beyond the two explored here. we also plan to extend the work to detect egregious conversations in real time (e.g., for escalating to a human operators), and create log analysis tools to analyze the root causes of egregious conversations and suggest possible remedies.", "summary": " ", "id": 1006}, {"document": "we present an analysis into the inner workings of convolutional neural networks (cnns) for processing text. cnns used for computer vision can be interpreted by projecting filters into image space, but for discrete sequence inputs cnns remain a mystery. we aim to understand the method by which the networks process and classify text. we examine common hypotheses to this problem: that filters, accompanied by global max-pooling, serve as ngram detectors. we show that filters may capture several different semantic classes of ngrams by using different activation patterns, and that global max-pooling induces behavior which separates important ngrams from the rest. finally, we show practical use cases derived from our findings in the form of model interpretability (explaining a trained model by deriving a concrete identity for each filter, bridging the gap between visualization tools in vision tasks and nlp) and prediction interpretability (explaining predictions). convolutional neural networks as well as other traditional natural language processing , even when considering relatively simple one-layer models (kim, 2014). as with other architectures of neural networks, explaining the learned functionality of cnns is still an active research area. the ability to interpret neural models can be used to increase trust in model predictions, analyze errors or improve the model the problem of interpretability in machine learning can be divided into two concrete tasks: given a trained model, model interpretability aims to supply a structured explanation which captures what the model has learned. given a trained model and a single example, prediction interpretability aims to explain how the model arrived at its prediction. these can be further divided into white-box and black-box techniques. while recent works have begun to supply the means of interpreting predictions , interpreting neural nlp models remains an under-explored area. accompanying their rising popularity, cnns have seen multiple advances in interpretability when used for computer vision tasks , which is likely different than the role it has when processing text. in this work, we examine and attempt to understand how cnns process text, and then use this information for the more practical goals of improving model-level and prediction-level explanations. we identify and refine current intuitions as to how cnns work. specifically, current common wisdom suggests that cnns classify text by working through the following steps (goldberg, 2016): 1) 1-dimensional convolving filters are used as ngram detectors, each filter specializing in a closely-related family of ngrams. 2) max-pooling over time extracts the relevant ngrams for making a decision. 3) the rest of the network classifies the text based on this information. we refine items and and show that: max-pooling induces a thresholding behavior, and values below a given threshold are ignored when (i.e. irrelevant to) making a prediction. specifically, we show an experiment for which of the pooled ngrams on average can be dropped with no loss of performance (section 4). filters are not homogeneous, i.e. a single filter can, and often does, detect multiple distinctly different families of ngrams (section 5.3). filters also detect negative items in ngrams they not only select for a family of ngrams but often actively suppress a related family of negated ngrams (section 5.4). we also show that the filters are trained to work with naturally-occurring ngrams, and can be easily misled (made to produce values substantially larger than their expected range) by selected nonnatural ngrams. these findings can be used for improving model-level and prediction-level interpretability (section 6). concretely: 1) we improve model interpretability by deriving a useful summary for each filter, highlighting the kinds of structures it is sensitive to. 2) we improve prediction interpretability by focusing on informative ngrams and taking into account also the negative cues. we have refined several common wisdom assumptions regarding the way in which cnns process and classify text. first, we have shown that maxpooling over time induces a thresholding behavior on the convolution layer\u2019s output, essentially separating between features that are relevant to the final classification and features that are not. we used this information to identify which ngrams are important to the classification. we also associate each filter with the class it contributes to. we decompose the ngram score into word-level scores by treating the convolution of a filter as a sum of word-level convolutions, allowing us to examine the word-level composition of the activation. specifically, by maximizing the word-level activations by iterating over the vocabulary, we observed that filters do not maximize activations at the word-level, but instead form slot activation patterns that give different types of ngrams similar activation strengths. this provides empirical evidence that filters are not homogeneous. by clustering high-scoring ngrams according to their slotactivation patterns we can identify the groups of linguistic patterns captured by a filter. we also show that filters sometimes opt to assign negative values to certain word activations in order to cause the ngrams which contain them to receive a low score despite having otherwise highly activating words. finally, we use these findings to suggest improvements to model-based and predictionbased interpretability of cnns for text.", "summary": " ", "id": 1007}, {"document": "we suggest a new idea of editorial network a mixed extractive-abstractive summarization approach, which is applied as a postprocessing step over a given sequence of extracted sentences. we further suggest an effective way for training the editor based on a novel soft-labeling approach. using the cnn/dailymail dataset we demonstrate the effectiveness of our approach compared to state-of-the-art extractive-only or abstractiveonly baselines. automatic text summarizers condense a given piece of text into a shorter version (the summary). this is done while trying to preserve the main essence of the original text and keeping the generated summary as readable as possible. existing summarization methods can be classified into two main types, either extractive or abstractive. extractive methods select and order text fragments (e.g., sentences) from the original text source. such methods are relatively simpler to develop and keep the extracted fragments untouched, allowing to preserve important parts, e.g., keyphrases, facts, opinions, etc. yet, extractive summaries tend to be less fluent, coherent and readable and may include superfluous text. abstractive methods apply natural language paraphrasing and/or compression on a given text. a common approach is based on the encoder-decoder , with the original text sequence being encoded while the summary is the decoded sequence. work was done during a summer internship in ibm research ai while such methods usually generate summaries with better readability, their quality declines over longer textual inputs, which may lead to a higher redundancy moreover, such methods are sensitive to vocabulary size, making them more difficult to train and generalize a common approach for handling long text sequences in abstractive settings is through attention mechanisms, which aim to imitate the attentive reading behaviour of humans two main types of attention methods may be utilized, either soft or hard. soft attention methods first locate salient text regions within the input text and then bias the abstraction process to prefer such regions during decoding on the other hand, hard attention methods perform abstraction only on text regions that were initially selected by some extraction process compared to previous works, whose final summary is either entirely extracted or generated using an abstractive process, in this work, we suggest a new idea of editorial network (editnet) a mixed extractive-abstractive summarization approach. a summary generated by editnet may include sentences that were either extracted, abstracted or of both types. moreover, per considered sentence, editnet may decide not to take either of these decisions and completely reject the sentence. using the cnn/dailymail dataset we demonstrate that, editnet\u2019s summarization quality is highly competitive to that obtained by both state-of-the-art abstractive-only and extractive-only baselines. we have proposed editnet a novel alternative summarization approach that instead of solely applying extraction or abstraction, mixes both together. moreover, editnet implements a novel sentence rejection decision, allowing to correct initial sentence selection decisions which are predicted to negatively effect summarization quality. as future work, we plan to evaluate other alternative extractor-abstractor configurations and try to train the network end-to-end. we further plan to explore reinforcement learning (rl) as an alternative decision making approach.", "summary": " ", "id": 1008}, {"document": "phonetic similarity algorithms identify words and phrases with similar pronunciation which are used in many natural language processing tasks. however, existing approaches are designed mainly for indo-european languages and fail to capture the unique properties of chinese pronunciation. in this paper, we propose a high dimensional encoded phonetic similarity algorithm for chinese, dimsim. the encodings are learned from annotated data to separately map initial and final phonemes into n-dimensional coordinates. pinyin phonetic similarities are then calculated by aggregating the similarities of initial, final and tone. dimsim demonstrates a 7.5x improvement on mean reciprocal rank over the state-of-theart phonetic similarity approaches. performing the mental gymnastics of transforming i\u2019m hear\u2019 to i\u2019m here,\u2019 or, i can\u2019t so buttons\u2019 to i can\u2019t sew buttons,\u2019 is familiar to anyone who has encountered autocorrected text messages, punny social media posts, or just friends with bad grammar. although at first glance it may seem that phonetic similarity can only be quantified for audible words, this problem is often present in purely textual spaces, such as social media posts or text messages. incorrect homophones and synophones, whether used in error or in jest, pose challenges for a wide range of nlp tasks, such as named entity identification, text normalization and spelling correction these tasks must therefore successfully transform incorrect words or phrases (hear\u2019,\u2019so\u2019) to their phonetically similar correct counterparts (\u2019here\u2019,\u2019sew\u2019), which in turn requires a robust representation of phonetic similarity between word pairs. a reli- able approach for generating phonetically similar words is equally crucial for chinese text unfortunately, most existing phonetic similarity algorithms such as soundex (archives and administration, 2007) and double metaphone (dm) philips (2000) are motivated by english and designed for indo-european languages. words are encoded to approximate phonetic presentations by ignoring vowels (except foremost ones), which is appropriate where phonetic transcription consists of a sequence of phonemes, such as for english. in contrast, the speech sound of a chinese character is represented by a single syllable in pinyin consisting of two or three parts: an initial (optional), a final or compound finals, and tone (table 1). as a result, phonetic similarity approaches designed for indo-european languages often fall short when applied to chinese text. note that we use pinyin as the phonetic representation because it is a widely accepted romanization system (san, 2007; iso, 2015) of chinese syllables, used to teach pronunciation of standard chinese. table shows two sentences from chinese microblogs, containing informal words derived from phonetic transcription. the dm and soundex encodings for 1chinese has five tones, represented on a 1-5 scale. near-homonyms of from table are shown in table since both dm and soundex ignore vowels and tones, words with dissimilar pronunciations are incorrectly assigned to the same encoding (e.g. and ), while true nearhomonyms are encoded much further apart (e.g. on the other hand, additional candidates with similar phonetic distances such as xin1fan2xi1fang1 for should be generated, for consumption by downstream applications such as text normalization. the example highlights the importance of considering all pinyin components and their characteristics when calculating chinese phonetic similarity one recent work (yao, 2015) manually assigns a single numerical number to encode and derive phonetic similarity. however, this single-encoding approach is inaccurate since the phonetic distances between pinyins are not captured well in a one dimensional space. figure illustrates the similarities between a subset of initials. initial groups z, c, zh, ch, z, zh and zh, ch are all similar, which cannot be captured using a one dimensional representation (e.g., an encoding of zh0,z1,c2,ch3 fails to identify the zh, ch pair as similar.) aline (kondrak, 2003) is another illustration of the challenge of manually assigning numerical values in order to accurately represent the complex relative phonetic similarity relationships across various languages. therefore, given the perceptual nature of the problem of phonetic similarity, it is critical to learn the distances based on as much empirical data as possible (kessler, 2005), rather than using a manually encoded metric. this paper presents dimsim, a learned ndimensional phonetic encoding for chinese along with a phonetic similarity algorithm, which uses the encoding to generate and rank phonetically similar words. to address the complexity of relative phonetic similarities in pinyin components, we propose a supervised learning approach to learn n dimensional encodings for finals and initials where n can be easily extended from one to two or higher dimensions. the learning model derives accurate encodings by jointly considering pinyin linguistic characteristics, such as place of articulation and pronunciation methods, as well as high quality annotated training data sets. we compare dimsim to double metaphone(dm), minimum edit distance(med) and aline demonstrating that dimsim outperforms these algorithms by 7.5x on mean reciprocal rank, 1.4x on precision and 1.5x on recall on a real-world dataset. an encoding for chinese pinyin leveraging chinese pronunciation characteristics. a simple and effective phonetic similarity algorithm to generate and rank phonetically similar chinese words. an implementation and a comprehensive evaluation showing the effectiveness of dimsim over the state-of-the-art algorithms. a package release of the implemented algorithm and a constructed dataset of chinese words with phonetic corrections.2 motivated by phonetic transcription as a widely observed phenomenon in chinese social media and informal language, we have designed an accurate phonetic similarity algorithm. dimsim generates phonetically similar candidate words based on learned encodings that capture the pronunciation characteristics of pinyin initial, final, and tone components. using a real world dataset, we demonstrate that dimsim effectively improves mrr by 7.5x , recall by 1.5x and precision by 1.4x over existing approaches. the original motivation for this work was to improve the quality of downstream nlp tasks, such as named entity identification, text normalization and spelling correction. these tasks all share a dependency on reliable phonetic similarity as an intermediate step, especially for languages such as chinese where incorrect homophones and synophones abound. we therefore plan to extend this line of work by applying dimsim to downstream applications, such as text normalization.", "summary": " ", "id": 1009}, {"document": "relation detection is a core component of many nlp applications including knowledge base question answering (kbqa). in this paper, we propose a hierarchical recurrent neural network enhanced by residual learning which detects kb relations given an input question. our method uses deep residual bidirectional lstms to compare questions and relation names via different levels of abstraction. additionally, we propose a simple kbqa system that integrates entity linking and our proposed relation detector to make the two components enhance each other. our experimental results show that our approach not only achieves outstanding relation detection performance, but more importantly, it helps our kbqa system achieve state-of-the-art accuracy for both single-relation (simplequestions) and multi-relation (webqsp) qa benchmarks. kb query, which can be executed to retrieve the answers from a kb. figure illustrates the process used to parse two sample questions in a kbqa system: ; and (b) a more complex case, where some constraints need to be handled for multiple entities in the question. the kbqa system in the figure performs two key tasks: (1) entity linking, which links n-grams in questions to kb entities, and (2) relation detection, which identifies the kb relation(s) a question refers to. the main focus of this work is to improve the relation detection subtask and further explore how it can contribute to the kbqa system. although general relation detection1 methods are well studied in the nlp community, such studies usually do not take the end task of kbqa into consideration. as a result, there is a significant gap between general relation detection studies and kb-specific relation detection. first, in most general relation detection tasks, the number of target relations is limited, normally smaller than in contrast, in kbqa even a small kb, like freebase2m , contains more than 6,000 relation types. second, relation detection for kbqa often becomes a zero-shot learning task, since some test instances may have unseen relations in the training data. for example, the simplequestions data set has of the golden test relations not observed in golden training tuples. third, as shown in figure 1, we need to predict a chain of relations instead of a single relation. this increases the number of target relation types and the sizes of candidate relation pools, further increasing the difficulty of kb relation detection. owing to these reasons, kb relation detection is significantly more challenging compared to general relation detection tasks. this paper improves kb relation detection to cope with the problems mentioned above. first, in order to deal with the unseen relations, we propose to break the relation names into word sequences for question-relation matching. second, noticing 1in the information extraction field such tasks are usually called relation extraction or relation classification. kb relation detection is a key step in kbqa and is significantly different from general relation extraction tasks. we propose a novel kb relation detection model, hr-bilstm, that performs hierarchical matching between questions and kb relations. our model outperforms the previous methods on kb relation detection tasks and allows our kbqa system to achieve state-of-the-arts. for future work, we will investigate the integration of our hr-bilstm into end-to-end systems. for example, our model could be integrated into the decoder in , to provide better sequence prediction. we will also investigate new emerging datasets like graphquestions (su et al.,", "summary": " ", "id": 1010}, {"document": "dictionaries and ontologies are foundational elements of systems extracting knowledge from unstructured text. however, as new content arrives keeping dictionaries up-to-date is a crucial operation. in this paper, we propose a human-in-the-loop (huml) dictionary expansion approach that employs a lightweight neural language model coupled with tight huml supervision to assist the user in building and maintaining a domain-specific dictionary from an input text corpus. the approach is based on the explore/exploit paradigm to effectively discover new instances (explore) from the text corpus as well as predict new unseen terms not currently in the corpus using the accepted dictionary entries (exploit). we evaluate our approach on a real-world scenario in the healthcare domain, in which we construct a dictionary of adverse drug reactions from user blogs as input text corpus. the evaluation shows that using our approach the user can easily extend the input dictionary, where tight human-in-the-loop integration results in a improvement in effectiveness. dictionary expansion is one area where close integration of humans into the discovery loop has been shown to enhance task performance substantially over more traditional post-adjudication. this is not surprising, as dictionary membership is often a fairly subjective judgment (e.g., should a fruit dictionary include tomatoes?) thus even with a system which finds similar terms (e.g., word2vec) guidance is important to keep the system focused on the subject matter expert\u2019s notion of lexicon. in this work we propose a feature agnostic approach for dictionary expansion based on lightweight neural language models, such as word2vec to prevent semantic drift during the dictionary expansion, we effectively include humanin-the-loop (huml). given an input text corpus and a set of seed examples, the proposed approach runs in two phases, explore and exploit, to identify new potential dictionary entries. the explore phase tries to identify similar instances to the dictionary entries that are present in the input text corpus, using term vectors from the neural language model to calculate a similarity score. the exploit phase tries to construct more complex multi-term phrases based on the instances already in the input dictionary. multi-term phrases are a challenge for word2vec style systems as they need to be known prior to model creation. to identify multi-term phrases, most commonly a simple phrase detection model is used, which is based on a term\u2019s co-occurrence score, i.e., terms that often appear together probably are part of the same phrase the phrase detection must be done before the model is built, and they remain unchanged after the model is built. however, depending on the domain and the task, the instances of interest evolve, or the example corpus may not be complete. for example, valid phrase combinations may simply not occur (e.g., acute joint pain may appear in the sample corpus, but for some reason chronic hip pain may not). however, these phrases are likely to occur in future texts from the same source, and thus are important to include in any entity extraction lexicon. in the exploit phase, the approach generates new phrases by analyzing the single terms of the instances in the input dictionary. we use two phrase generation algorithms: (i) modify the phrases by replacing single terms with similar terms from the text corpus, e.g., abnormal behavior can be modified to strange behavior; (ii) extend the instances with terms from the text corpus that are related to the terms in the instance, e.g., abnormal blood clotting problems is a an adverse drug reaction, which doesn\u2019t appear as such in a large text corpus, however the instances abnormal blood count, blood clotting and clotting problems appear several times in the corpus, which can be used to build the more complex instance. the approach allows us to construct new multi-term instances that don\u2019t appear as such in the text corpus, but there is enough statistical evidence in the corpus that such instances might be of interest for the user. combining the explore and exploit approaches in an unsupervised fashion (or an infrequently supervised fashion) is not particularly effective. it tends to generate many spurious results that the human subject matter expert needs to wade through. close supervision, however, results in a much more performant system. the evaluation shows that high promptness of the huml (tighter computer/human partnership) results in nearly perfect performance of the system, i.e., nearly all the candidates identified by the system are valid entries in the dictionary. more precisely, the experiments show that the system is more effective when receiving huml feedback after identified candidates, compared to receiving huml feedback after identified candidates, while both cases require equal amount of human effort. the rest of this paper is structured as follows. in section 2, we give an overview of related work. in section 3, we present our interactive dictionary expansion approach, followed by an evaluation in section 4; we conclude with a summary and an outlook on future work. this paper proposes an interactive dictionary expansion tool using a lightweight neural language model. our algorithm is iterative and purely statistical, hence does not require any feature extraction beyond tokenization. it incorporates human feedback to improve performance and control semantic drift at every iteration cycle. the experiments showed high importance of tight huml integration on discovery efficiency. in this work, we have considered only lightweight language models, which can be efficiently built and updated on large text corpora. in future work, we will analyze more complex language neural network models, such as recurrent neural networks (rnn), long short term memory networks (lstm), and bidirectional lstm, which might improve the search for similar and related terms, at the expense of higher training time. furthermore, future work will include an evaluation of the approach on multiple datasets covering different domains.", "summary": " ", "id": 1011}, {"document": "textual grounding is an important but challenging task for human-computer interaction, robotics and knowledge mining. existing algorithms generally formulate the task as selection from a set of bounding box proposals obtained from deep net based systems. in this work, we demonstrate that we can cast the problem of textual grounding into a unified framework that permits efficient search over all possible bounding boxes. hence, the method is able to consider significantly more proposals and doesn\u2019t rely on a successful first stage hypothesizing bounding box proposals. beyond, we demonstrate that the trained parameters of our model can be used as word-embeddings which capture spatial-image relationships and provide interpretability. lastly, at the time of submission, our approach outperformed the current state-of-the-art methods on the flickr 30k entities and the referitgame dataset by and respectively. grounding of textual phrases, i.e., finding bounding boxes in images which relate to textual phrases, is an important problem for human-computer interaction, robotics and mining of knowledge bases, three applications that are of increasing importance when considering autonomous systems, augmented and virtual reality environments. for example, we may want to guide an autonomous system by using phrases such as the bottle on your left,\u2019 or the plate in the top shelf.\u2019 while those phrases are easy to interpret for a human, they pose significant challenges for present day textual grounding algorithms, as interpretation of those phrases requires an understanding of objects and their relations. existing approaches for textual grounding, such as 38, take advantage of the cognitive performance improvements obtained from deep net features. more specifically, deep net models are designed to extract features from given bounding boxes and textual data, which are then compared to measure their fitness. to obtain suitable bounding boxes, many of the textual grounding frameworks, such as 38, 15, make use of region proposals. while being easy to obtain, automatic extraction of region proposals is limiting, because the performance of the visual grounding is inherently constrained by the quality of the proposal generation procedure. in this work we describe an interpretable mechanism which additionally alleviates any issues arising due to a limited number of region proposals. our approach is based on a number of image concepts\u2019 such as semantic segmentations, detections and priors for any number of objects of interest. based on those image concepts\u2019 which are represented as score maps, we formulate textual grounding as a search over all possible bounding boxes. we find the bounding box with highest accumulated score contained in its interior. the search for this box can be solved via an efficient branch and bound 31st conference on neural information processing systems (nips 2017), long beach, ca, usa. a woman in a green shirt is getting ready to throw her bowling ball down the lane... two women wearing hats covered in flowers are posing. young man wearing a hooded jacket sitting on snow in front of mountain area. second bike from right in front painting next to the two on theleft person all the way to the right figure 1: results on the test set for grounding of textual phrases using our branch and bound based algorithm. top row: flickr 30k entities dataset. bottom row: referitgame dataset (groundtruth box in green and predicted box in red). scheme akin to the seminal efficient subwindow search of lampert et al. the learned weights can additionally be used as word embeddings. we are not aware of any method that solves textual grounding in a manner similar to our approach and hope to inspire future research into the direction of deep nets combined with powerful inference algorithms. we evaluate our proposed approach on the challenging referitgame and the flickr 30k entities dataset 35, obtaining results like the ones visualized in fig. at the time of submission, our approach outperformed state-of-the-art techniques on the referitgame and flickr 30k entities dataset by and respectively using the iou metric. we also demonstrate that the trained parameters of our model can be used as a word-embedding which captures spatial-image relationships and provides interpretability. we demonstrated a mechanism for grounding of textual phrases which provides interpretability, is easy to extend, and permits globally optimal inference. in contrast to existing approaches which are generally based on a small set of bounding box proposals, we efficiently search over all possible bounding boxes. we think interpretability, i.e., linking of word and image concepts, is an important concept, particularly for textual grounding, which deserves more attention. acknowledgments: this material is based upon work supported in part by the national science foundation under grant no. this work is supported by nvidia corporation with the donation of a gpu. this work is supported in part by ibm-illinois center for cognitive computing systems research (c3sr) - a research collaboration as part of the ibm cognitive horizons network.", "summary": " ", "id": 1012}, {"document": "perceptual features (pfs) have been used with great success in tasks such as transfer learning, style transfer, and super-resolution. however, the efficacy of pfs as key source of information for learning generative models is not well studied. we investigate here the use of pfs in the context of learning implicit generative models through moment matching (mm). more specifically, we propose a new effective mm approach that learns implicit generative models by performing mean and covariance matching of features extracted from pretrained convnets. our proposed approach improves upon existing mm methods by: (1) breaking away from the problematic min/max game of adversarial learning; (2) avoiding online learning of kernel functions; and (3) being efficient with respect to both number of used moments and required minibatch size. our experimental results demonstrate that, due to the expressiveness of pfs from pretrained deep convnets, our method achieves stateof-the-art results for challenging benchmarks. the use of features from deep convolutional neural networks (dcnns) pretrained on imagenet has led to important advances in computer vision. dcnn features, usually called perceptual features (pfs), have been used in tasks such as transfer learning 40, 16, style transfer and super-resolution while there have been previous works on the use of pfs in the context of image generation and transformation 7, 17, exploration of pfs as key source of information for learning generative models is not well studied. particularly, the efficacy of pfs for implicit generative models trained through moment matching is an open question. moment matching approaches for generative modeling are based on the assumption that one can learn the data distribution by matching the moments of the model distribution to the empirical data distribution. two representative meth- equal contribution. ods of this family are based on maximum mean discrepancy by using a specially designed objective function. in this work we demonstrate that, by using pfs to perform moment matching, one can overcome some of the difficulties found in current moment matching approaches. more specifically, we propose a simple but effective moment matching method that: (1) breaks away from the problematic min/max game completely; (2) does not use online learning of kernel functions; and (3) is very efficient with regard to both number of used moments and required minibatch size. our proposed approach, named generative feature matching networks (gfmn), learns implicit generative models by performing mean and covariance matching of features extracted from all convolutional layers of pretrained deep convnets. some interesting properties of gfmns include: (a) the loss function is directly correlated to the generated image quality; (b) mode collapsing is not an issue; and (c) the same pretrained feature extractor can be used across different datasets. we perform an extensive number of experiments with different challenging datasets: cifar10, stl10, celeba and lsun. we demonstrate that our approach can achieve state-of-the-art results for challenging benchmarks such as cifar10 and stl10. moreover, we show that the same ar x iv :1 2v cs .c v a pr feature extractor is effective across different datasets. the main contributions of this work can be summarized as follows: (1) we propose a new effective moment matchingbased approach to train implicit generative models that does not use adversarial or online learning of kernel functions, provides stable training, and achieves state-of-the-art results; (2) we show theoretical results that demonstrate gfmn convergence under the assumption of the universality of perceptual features; (3) we propose an adam-based moving average method that allows effective training with small minibatches; (4) our extensive quantitative and qualitative experimental results demonstrate that pretrained autoencoders and dcnn classifiers can be effectively used as (cross-domain) feature extractors for gfmn training. convergence we summarize here the main definitions and theorems from regarding universality of kernels and feature maps. the following defines a universal kernel definition (universal kernel). given a kernel k defined on x x let z be any compact subset of x define the space of kernel sections: k(z) spanky, y z, where ky : x r, ky(x) k(x, y). let c(z) be the space of all continuous real valued functions defined on z a kernel is said universal if for any choice of z (compact subset of x ) k(z) is dense in c(z). in other words a kernel is universal if c(z) k(z). meaning if any continuous function can be expressed in the span of ky universal feature maps.we turn now for kernels defined by feature maps and how to characterize their universality. consider a continuous feature map : x w , where (w, , w) is a hilbert space; the kernel k has the following form: k(x, y) (x),(y)w (6) let y be an orthonormal basis of w define the following continuous function fy c(z) defined at x z: fy(x) (x), yw , and let: (y) spanfy, y y definition (universal feature map). a feature map is universal if (y) is dense in c(z), for all z compact subsets of x .i.e a feature map is universal if (y) c(z). the following theorem shows the relation between universality of a kernel defined by feature map and the universality of the feature map: theorem (26, thm 4, relation between k(z) and (y) ). for kernel defined by feature maps in (6) we have k(z) (y). a kernel of form (6) is universal if and only if its feature map is universal. hence the following theorem from 26: theorem (26). let s j , j i, where i is a countable set and j : x r continuous function. define the following kernel k(x, y) ji j(x)j(y). as we already discussed the moving average of v of the difference of features means t n n i1 e(xi) n n i1 e(g(zi, t)) between real and generated data at each time step t in the gradient descent up to time t , can be seen as a gradient descent in an online setting on the following cost : f min v t t1 ft(v) t t1 v t22 note that we are in the online setting since t is only known when t of the generator is updated. the sequence vt generated by ma (moving average) and by ama (adam moving average) is the sgd updates and adam updates respectively applied to the cost function ft. hence we can bound the regret of the sequence vmat and vamat using known results on sgd and adam. let d be the dimension of the encoding e. for ma, using classic regret bounds for gradient descents we obtain: rmat t t1 vmat t22 f o( dt ). for ama, using adam regrets bounds from let us define ramat t t1 vamat t22 f. we have: ramat o( t d i1 u t, i ) o d i1 t t1 (t,i vamat,i )2 c where u are defined in the adam updates as moving averages of second order moments of the gradients. the regret bound of ama is better than ma especially ifd i1 u t, i d and d i1 t t1 (t,i vamat,i )2 td. in this appendix, we present comparative results between gfmn with mean feature matching vs. gfmn with mean covariance feature matching. using the first and second moments to perform feature matching gives statistical advantage over using the first moment only. in table 5, we can see that for different feature extractors, performing mean covariance feature matching produces significantly better results in terms of both is and fid. have also demonstrated the advantages of using mean covariance matching in the context of gans. in tables and 7, and figure we detail the neural net architectures used in our experiments. in both dcgan-like generator and discriminator, an extra layer is added when using images of size in vgg19 architecture, after each convolution, we apply batch normalization and relu. the resnet generator is used for celeba128128 experiments and also for some experiments with cifar10 and stl10. for these two last datasets, the resnet generator has resblocks only, and the output size of the dense layer is both vgg19 and resnet18 networks are trained with sgd with fixed learning rate, momentum term, and weight decay set to we pick models with best top-1 accuracy on the validation set over epochs of training; for vgg19 (image size 3232), and for resnet18 (image size 3232). when training the classifiers we use random cropping and random horizontal flipping for data augmentation. when using vgg19 and resnet18 as feature extractors in gfmn, we use features from the output of each relu that follows a conv. layer, for a total of layers for vgg and for resnet18. in our experiments with autoencoders (ae) we pretrained them using either mean squared error (mse) or the laplacian pyramid loss 23, let e and d be the encoder and the decoder networks with parameters and , respectively. min , epdata xd(e(x;);)2 or the laplacian pyramid loss lap1(x, x ) j 22j lj(x) lj(x)1 where lj(x) is the j-th level of the laplacian pyramid representation of x. the laplacian pyramid loss provides better signal for learning high frequencies of images and overcome some of the blurriness issue known from using a simple mse loss. recently demonstrated that the lap1 loss produces better results than l2 loss for both autoencoders and generative models. we evaluate our models using two quantitative metrics: inception score (is) and frechet inception distance (fid) we followed the same procedure used in previous work to calculate is 36, 27, for each trained generator, we calculate the is for randomly generated images and repeat this procedure times (for a total of 50k generated images) and report the average and the standard deviation of the is. we compute fid using two sample sizes of generated images: 5k and 50k. in order to be consistent with previous works 27, and be able to directly compare our quantitative results with theirs, the fid is computed as follows: cifar10: the statistics for the real data are computed using the 50k training images. this (real data) statistics are used in the fid computation of both 5k and 50k samples of generated images. this is consistent with both miyato et al. procedure to compute fid for cifar10 experiments. stl10: when using 5k generated images, the statistics for the real data are computed using the set of 5k (labeled) training images. this is consistent with the fid fid computation is repeated times and the average is reported. there is very small variance in the fid results. extraction figure shows generated images from generators that were trained with a different number of layers employed to feature matching. in all the results in fig.7, the vgg19 network was used to perform feature extraction. we can see a significant improvement in image quality when more layers are used. better results are achieved when or more layers are used, which corroborates the quantitative results in sec. wgan-gp the objective of the experiments presented in this section is to evaluate if wgan-gp can benefit from dcnn classifiers pretrained on imagenet. in the experiments, we used a wgan-gp architecture where: (1) the discriminator is a vgg19 or a resnet18; (2) the discriminator is pretrained on imagenet; (3) the generator is pretrained on cifar10 through autoencoding. although we tried different hyperparameter combinations, we were not able to successfully train wgan-gp with vgg19 or resnet18 discriminators. indeed, the discriminator, being pretrained on imagenet, can quickly learn to distinguish between real and fake images. this limits the reliability of the gradient information from the discriminator, which in turn renders the training of a proper generator extremely challenging or even impossible. this is a well-known issue with gan training where the training of the generator and discriminator must strike a balance. this phenomenon is covered in section (illustrated in their figure 2) as one motivation for work like wassertein gans. if a discriminator can distinguish perfectly between real and fake early on, the generator cannot learn properly and the min/max game becomes unbalanced, having no good discriminator gradients for the generator to learn from, producing degenerate models. figure shows some examples of images generated by the unsuccessfully trained models. in this appendix, we present a comparison between the simple moving average (ma) and adam moving average (ama) for the case where vgg19 imagenet classifier is used as a feature extractor. this experiment uses a minibatch size of that ama has a very positive effect in the quality of generated images. gfmn trained with ma produces various images with some sort of crossing line artifacts. figure shows a visual comparison between images generated by gfmn (figs. 10a and 10b) and generative moment matching networks (gmmn) (figs. gmmn generated images were obtained from li et al. in this experiment, both gmmn and gfmn use a dcgan-like architecture in the generator. images generated by gfmn have significantly better quality compared to the ones generated by gmmn, which corroborates the quantitative results in sec. in this appendix, we present a comparison in image quality for autoencoder features vs. vgg19 features for the celeba dataset. we show results for both simple moving average (ma) and adam moving average (ama), for both cases we use a minibatch size of 11, we show generated images from gfmn trained with either vgg19 features (top row) or autoencoder (ae) features (bottom row). we show images generated by gfmn models trained with simple moving average (ma) and adam moving average (ama). we can note in the images that, although vgg19 features are from a cross-domain classifier, they lead to much better generation quality than ae features, specially for the ma case.", "summary": " ", "id": 1013}, {"document": "recommendation for e-commerce with a mix of durable and nondurable goods has characteristics that distinguish it from the well-studied media recommendation problem. the demand for items is a combined effect of form utility and time utility, i.e., a product must both be intrinsically appealing to a consumer and the time must be right for purchase. in particular for durable goods, time utility is a function of inter-purchase duration within product category because consumers are unlikely to purchase two items in the same category in close temporal succession. moreover, purchase data, in contrast to rating data, is implicit with non-purchases not necessarily indicating dislike. together, these issues give rise to the positive-unlabeled demand-aware recommendation problem that we pose via joint low-rank tensor completion and product category inter-purchase duration vector estimation. we further relax this problem and propose a highly scalable alternating minimization approach with which we can solve problems with millions of users and millions of items in a single thread. we also show superior prediction accuracies on multiple real-world datasets. e-commerce recommender systems aim to present items with high utility to the consumers utility may be decomposed into form utility: the item is desired as it is manifested, and time utility: the item is desired at the given point in time 28; recommender systems should take both types of utility into account. economists define items to be either durable goods or nondurable goods based on how long they are intended to last before being replaced a key characteristic of durable goods is the long duration of time between successive purchases within item categories whereas this duration for nondurable goods is much shorter, or even negligible. thus, durable and nondurable goods have differing time utility characteristics which lead to differing demand characteristics. although we have witnessed great success of collaborative filtering in media recommendation, we should be careful when expanding its application to general e-commerce recommendation involving both durable and nondurable goods due to the following reasons: since media such as movies and music are nondurable goods, most users are quite receptive to buying or renting them in rapid succession. however, users only purchase durable goods when the time is right. for instance, most users will not buy televisions the day after they have already bought one. therefore, recommending an item for which a user has no immediate demand can hurt user experience and waste an opportunity to drive sales. now at tencent ai lab, bellevue, wa, usa 31st conference on neural information processing systems (nips 2017), long beach, ca, usa. a key assumption made by matrix factorization- and completion-based collaborative filtering algorithms is that the underlying rating matrix is of low-rank since only a few factors typically contribute to an individual\u2019s form utility however, a user\u2019s demand is not only driven by form utility, but is the combined effect of both form utility and time utility. hence, even if the underlying form utility matrix is of low-rank, the overall purchase intention matrix is likely to be of high-rank,2 and thus cannot be directly recovered by existing approaches. an additional challenge faced by many real-world recommender systems is the one-sided sampling of implicit feedback 15, unlike the netflix-like setting that provides both positive and negative feedback (high and low ratings), no negative feedback is available in many e-commerce systems. for example, a user might not purchase an item because she does not derive utility from it, or just because she was simply unaware of it or plans to buy it in the future. in this sense, the labeled training data only draws from the positive class, and the unlabeled data is a mixture of positive and negative samples, a problem usually referred to as positive-unlabeled (pu) learning to address these issues, we study the problem of demand-aware recommendation. given purchase triplets (user, item, time) and item categories, the objective is to make recommendations based on users\u2019 overall predicted combination of form utility and time utility. we denote purchases by the sparse binary tensor p to model implicit feedback, we assume that p is obtained by thresholding an underlying real-valued utility tensor to a binary tensor y and then revealing a subset of y\u2019s positive entries. the key to demand-aware recommendation is defining an appropriate utility measure for all (user, item, time) triplets. to this end, we quantify purchase intention as a combined effect of form utility and time utility. specifically, we model a user\u2019s time utility for an item by comparing the time t since her most recent purchase within the item\u2019s category and the item category\u2019s underlying inter-purchase duration d; the larger the value of d t, the less likely she needs this item. in contrast, d t may indicate that the item needs to be replaced, and she may be open to related recommendations. therefore, the function h max(0, d t) may be employed to measure the time utility factor for a (user, item) pair. then the purchase intention for a (user, item, time) triplet is given by x h, where x denotes the user\u2019s form utility. this observation allows us to cast demand-aware recommendation as the problem of learning users\u2019 form utility tensor x and items\u2019 inter-purchase durations vector d given the binary tensor p although the learning problem can be naturally formulated as a tensor nuclear norm minimization problem, the high computational cost significantly limits its application to large-scale recommendation problems. to address this limitation, we first relax the problem to a matrix optimization problem with a label-dependent loss. we note that the problem after relaxation is still non-trivial to solve since it is a highly non-smooth problem with nested hinge losses. more severely, the optimization problem involves mnl entries, where m, n, and l are the number of users, items, and time slots, respectively. thus a naive optimization algorithm will take at least o(mnl) time, and is intractable for largescale recommendation problems. to overcome this limitation, we develop an efficient alternating minimization algorithm and show that its time complexity is only approximately proportional to the number of nonzero elements in the purchase records tensor p since p is usually very sparse, our algorithm is extremely efficient and can solve problems with millions of users and items. compared to existing recommender systems, our work has the following contributions and advantages: (i) to the best of our knowledge, this is the first work that makes demand-aware recommendation by considering inter-purchase durations for durable and nondurable goods; (ii) the proposed algorithm is able to simultaneously infer items\u2019 inter-purchase durations and users\u2019 real-time purchase intentions, which can help e-retailers make more informed decisions on inventory planning and marketing strategy; (iii) by effectively exploiting sparsity, the proposed algorithm is extremely efficient and able to handle large-scale recommendation problems. in this paper, we examine the problem of demand-aware recommendation in settings when interpurchase duration within item categories affects users\u2019 purchase intention in combination with intrinsic properties of the items themselves. we formulate it as a tensor nuclear norm minimization problem that seeks to jointly learn the form utility tensor and a vector of inter-purchase durations, and propose a scalable optimization algorithm with a tractable time complexity. our empirical studies show that the proposed approach can yield perfect recovery of duration vectors in noiseless settings; it is robust to noise and scalable as analyzed theoretically. on two real-world datasets, tmall and amazon review, we show that our algorithm outperforms six state-of-the-art recommendation algorithms on the tasks of category, item, and purchase time predictions.", "summary": " ", "id": 1014}, {"document": "we present a neural response generation model that generates responses conditioned on a target personality. the model learns high level features based on the target personality, and uses them to update its hidden state. our model achieves performance improvements in both perplexity and bleu scores over a baseline sequence-to-sequence model, and is validated by human judges. automated conversational agents are becoming popular for various tasks, such as personal assistants, shopping assistants, or as customer service agents. automated agents benefit from adapting their personality according to the task at hand thus, it is desirable for automated agents to be capable of generating responses that express a target personality. personality is defined as a set of traits which represent durable characteristics of a person. many models of personality exist while the most common one is the big five model (digman, 1990) , including: openness, conscientiousness, extraversion, agreeableness, and neuroticism. these traits were correlated with linguistic choices including lexicon and syntax (mairesse and walker, 2007). in this paper we study how to encode personality traits as part of neural response generation for conversational agents. our approach builds upon a sequence-to-sequence by adding an additional layer that represents the target set of personality traits, and a hidden layer that learns high-level personality based features. the response is then generated conditioned on these features. specifically, we focus on conversational agents for customer service; in this context, many studies examined the effect of specific personality traits of human agents on service performance. results indicate that conscientiousness figure shows examples of customer utterances, followed by two automatically generated responses. the first response (in each example), is generated by a standard seq2seq response generation system that ignores personality modeling and in effect generates the consensus response of the humans represented in the training data. the second response is generated by our system, and is aimed to generate data for an agent that expresses a high level of a specific trait. in example 1, the agreeableness-agent is more compassionate (expresses empathy) and is more cooperative (asks questions). in example 2, the conscientiousness-agent is more thoughtful (will check the issue). we experimented with a dataset of 87.5k real customer-agent utterance pairs from social media. we find that leveraging personality encoding improves relative performance up to in bleu score, compared to a baseline seq2seq model. to our knowledge, this work is the first to train a neural response generation model that encodes target personality traits. we have presented a personality-based response generation model and tested it in customer care tasks, outperforming baseline seq2seq model. in future work, we would like to generate responses adapted to the personality traits of the customer as well, and to apply our model to other tasks such as education systems.", "summary": " ", "id": 1015}, {"document": "we introduce a stereo correspondence system implemented fully on event-based digital hardware, using a fully graph-based non von-neumann computation model, where no frames, arrays, or any other such data-structures are used. this is the first time that an end-to-end stereo pipeline from image acquisition and rectification, multi-scale spatiotemporal stereo correspondence, winner-take-all, to disparity regularization is implemented fully on event-based hardware. using a cluster of truenorth neurosynaptic processors, we demonstrate their ability to process bilateral event-based inputs streamed live by dynamic vision sensors (dvs), at up to 2,000 disparity maps per second, producing high fidelity disparities which are in turn used to reconstruct, at low power, the depth of events produced from rapidly changing scenes. experiments on real-world sequences demonstrate the ability of the system to take full advantage of the asynchronous and sparse nature of dvs sensors for low power depth reconstruction, in environments where conventional frame-based cameras connected to synchronous processors would be inefficient for rapidly moving objects. system evaluation on event-based sequences demonstrates a improvement in terms of power per pixel per disparity map compared to the closest stateof-the-art, and maximum latencies of up to 11ms from spike injection to disparity map ejection. sparsity and parallel asynchronous computation are two key principles of information processing in the brain. they allow to solve complex tasks using a tiny fraction of the energy consumed by stored-program computers while the successful artificial neural networks may not operate the same way as the brain, both of them utilize highly parallel and hierarchical architectures that gradually abstract input data to more meaningful concepts 8, 51, however, event-based computation has not been equally adopted work done as an intern at ibm research - almaden. cognitive anteater robotics lab (carl), university of california, irvine another barrier for sparse computation are traditional sensors, such as frame-based cameras, which provide regular inputs. for autonomous vehicles, drones, and satellites, energy consumption is a challenge event-based processing dramatically reduces power consumption by computing only what is new while omitting unchanged input parts. recently developed event-based cameras such as dynamic vision sensor (dvs) 37, and atis 50, inspired by the biological retina, encode pixel illumination changes as events. these sensors solve two major drawbacks of frame-based cameras. first, temporal resolution of frame-based applications is limited by the camera frame rate, usually frames per second. event-based cameras can generate events at microsecond resolution. second, consecutive frames in videos are usually highly redundant, which waste downstream data transfer, computing resources and power. since events are sparse, event-based cameras lead to better downstream resource usage. moreover, eventbased cameras have high dynamic range ( db), which is useful for real world variations in lighting conditions. to achieve the low energy and high temporal resolution benefits of event-based inputs, computations must be performed asynchronously. to benefit from sparse and asynchronous computation, neuromorphic processors have been developed 44, 24, 30, 9, these processors represent input events as spikes and process them in parallel using a large neuron population. they are stimulus-driven and the propagation delay of an event through the neuron layers is usually a few milliseconds, suitable for many real-time applications. for example, the truenorth neuromorphic chip has been used for high throughput convolutional neural networks (cnns) 22, character recognition 53, optic flow 11, saliency 3, and gesture recognition depth perception is an important task for autonomous mobile agents to navigate in the real world. the speed and low power requirements of these applications can be effectively met using event-based sensors. event-based stereo provides additional advantages over other depth estimation methods that increase accuracy and save energy, such as high temporal resolution, high dynamic range, and robustness to interference with other agents. several methods have been proposed to solve event- based stereo correspondence. most global methods 40, 17, 49, are derived from the marr and poggio cooperative stereo algorithm the algorithm assumes depth continuity and often event-based implementations are not tested with objects tilted in depth. local methods can be parallelized and find corresponding events using either local features over a spatiotemporal window or event-to-event features 13, 58, 52, 32, however, most approaches use non-event-based hardware, such as cpu or dsp. we propose a fully neuromorphic event-based stereo disparity algorithm. a live-feed version of the system running on nine truenorth chips is shown to calculate disparity maps per second, and the ability to increase this up to 2,000 disparities per second (subject to certain trade-offs) is demonstrated, for use with high speed event cameras, such as dvs. the main advantages of the proposed method, compared to the related work 17, 49, 45, 52, 57, are simultaneous end-to-end neuromorphic disparity calculation, low power, high throughput, low latency (9-11 ms), and linear scalability to multiple neuromorphic processors for larger input sizes. compared to frame-based computation, in the asynchronous, event-based computation supported by truenorth, at each time cycle, in general only neurons that have input spikes are computed, and only spike events are communicated. when the data in a cycle is sparse, as is the case with a dvs sensor, most neurons would not compute for most of the time, resulting in low active power this processing differs from traditional architectures that use frame-buffers and other conventional data structures; where same memory fetching and computation is repeated for each pixel every frame, independent of scene activity. the proposed event-based disparity method is implemented using a stereo pair of davis sensors (a version of dvs) and nine truenorth ns1e boards however, the method is applicable to other spiking neuromorphic architectures, and it is also tested offline on larger models using a truenorth simulator. input rectification, spatiotemporal scaling, feature matching, search for best matches, morphological erosion and dilation, and bidirectional consistency check are all performed on truenorth, for a fully neuromorphic disparity solution. with respect to the most relevant state-of-the-art approach 17, our method uses less power per pixel per disparity map. we also release the event-based stereo dataset used, which includes kinect-based registered ground-truth. we have introduced an advanced neuromorphic 3d vision system uniting a pair of davis cameras with multiple truenorth processors, to create an end-to-end, scalable, event-based stereo system. by using a spiking neural network, with low-precision weights, we have shown that the system is capable of injecting event streams and ejecting disparity maps at high throughputs, low latencies, and low power. the system is highly parameterized and can operate with other event based sensors such as atis or dvs table compares our approach with the literature on event based disparity. comparative advantages are low power, multi-resolution disparity calculation, scalability to live sensor feed with large input sizes, and evaluation using synthetic as well as real world fast movements and depth gradients, in neuromorphic, non von-neumann hardware. the implemented neuromorphic stereo disparity system achieves these advantages, while consuming less power per pixel per disparity map compared to the stateof-the-art the homogeneous computational substrate provides the first example of a fully end-to-end low-power, high throughput fully event-based neuromorphic stereo system capable of running on live input event streams, using a fully graph-based computation model, where no frames, arrays or other such data-structures are used.", "summary": " ", "id": 1016}, {"document": "the challenge of learning the causal structure underlying a certain phenomenon is undertaken by connecting the set of conditional independences (cis) readable from the observational data, on the one side, with the set of corresponding constraints implied over the graphical structure, on the other, which are tied through a graphical criterion known as d-separation (pearl, 1988). in this paper, we investigate the more general setting where multiple observational and experimental distributions are available. we start with the simple observation that the invariances given by cis/dseparation are just one special type of a broader set of constraints, which follow from the careful comparison of the different distributions available. remarkably, these new constraints are intrinsically connected with do-calculus (pearl, 1995) in the context of soft-interventions. we then introduce a novel notion of interventional equivalence class of causal graphs with latent variables based on these invariances, which associates each graphical structure with a set of interventional distributions that respect the do-calculus rules. given a collection of distributions, two causal graphs are called interventionally equivalent if they are associated with the same family of interventional distributions, where the elements of the family are indistinguishable using the invariances obtained from a direct application of the calculus rules. we introduce a graphical representation that can be used to determine if two causal graphs are interventionally equivalent. we provide a formal graphical characterization of this equivalence. finally, we extend the fci algorithm, which was originally designed to operate based on cis, to combine observational and interventional datasets, including new orientation rules particular to this setting. explaining a complex system through their cause and effect relations is one of the fundamental challenges in science. data is collected and experiments are performed with the intent of understanding how a certain phenomenon comes about, or how the underlying system works, which could be social, biological, artificial, among others. the study of causal relations can be seen through the lens of learning and inference 16, the learning component is concerned with discovering the causal structure, which is the very subject of interest in many domains, since they can provide insight about equal contribution. 33rd conference on neural information processing systems (neurips 2019), vancouver, canada. how a complex system works and lead to better understanding about the phenomenon under investigation. the latter, inference, attempts to leverage the causal structure to compute quantitative claims about the effect of interventions and retrospective counterfactuals, which are critical to assign credit, understand blame and responsibility, and perform judgement about fairness in decision-making. one of the most popular languages used to encode the invariances needed to reason about causal relations, for both learning and inference, is based on graphical models, and appears under the rubric of causal graphs 16, 21, a causal graph is a directed acyclic graph (dag) with latent variables, where each edge encodes a causal relationship between its endpoints: x is a direct cause of y , i.e., x y , if, when the remaining factors are held constant, forcing x to take a specific value affects the realization of y , where x,y are random variables representing some relevant features of the system. the task of learning the causal structure entails a search over the space of causal graphs that are compatible with the observed data; the collection of these graphs forms what is called an equivalence class. the most popular mark imprinted on the data by the underlying causal structure that is used to delineate an equivalence class are conditional independence (ci) relations. these relations are the most basic type of probabilistic invariances used in the field and have been studied at large in the context of graphical models since, at least, (see also 5). while cis are powerful and have been the driving force behind some of the most prominent structural learning algorithms in the field 16, 21, including the pc, fci, these are constraints specific for one distribution. in this paper, we start by noting something very simple, albeit powerful, that happens when a combination of observational and experimental distributions are available: there are constraints over the graphical structure that emerge by comparing these different distributions, and which are not of ci-type2. remarkably, and unknown until our work, the converse of the causal calculus developed by pearl offers a systematic way of reading these constraints and tying them back to the underlying graphical structure. in reference to their connection to the do-calculus rules (or a generalization, as discussed later), we call these constraints the do-constraints. for concreteness, consider the graph in fig. 1(a), where the dashed-bidirected arrow represents hidden variables that generate variations of the two observed variables, x,y in this case. suppose the observational (conditional) distribution and an interventional distribution on x are available, which are written as p(yx), p(ydo(x)), respectively. suppose we contrast these two distributions and the test evaluating the expression p(ydo(x)) p(yx) comes out as false. this is called a do-see test since the experimental (or do) and observational (see) distributions are contrasted. based on the second rule of do-calculus, one can infer that there is an open backdoor path from x to y , where the edge adjacent to x on this path has an arrowhead into x. in our setting, we do not have access to the true graph, but we leverage this and the other do-constraints to reverse engineer the process and try to learn the structure. broadly speaking, do-constraints will play a critical role for learning, in the same way ci/d-separation plays in learning when only observational data is available. to the best of our knowledge, this type of constraints appeared first at the very definition of causal bayesian networks (cbns) in and then were leveraged to design efficient experiments to learn the causal graph in we assume throughout this work that interventions are soft. a soft intervention affects the mechanism that generates the variable, while keeping the causal connections intact. soft-interventions are widely employed in biology and medicine, where it is hard to change the underlying system, but possibly 2recall that a ci represents a constraint readable from one specific distribution saying that the value of z is irrelevant for computing the likelihood of y once we know the value of x, i.e., p(y x,z) p(y x),x,y,z. for our characterization, we utilize an extension of the causal calculus to soft interventions introduced in under soft-interventions, the do-see test can be written as checking if px(yx) p(yx), where px is the distribution obtained after a soft intervention on x. the second observation leveraged here follows from another realization by pearl that interventions can be represented explicitly in the graphical model he then introduced what we call f-nodes, which graphically encode the changes due to an intervention and the corresponding parametrization (see also 16, sec. this is important in our context since the do-calculus tests will be visible more explicitly in the graph. the graph obtained by adding f-nodes to the causal graph is called the augmented graph. the same construct was used more prominently in in the context of inference and identification. 1b, the existence of the backdoor path from x to y , as detected by rule of the calculus, can be captured by the statement fx is not d-separated from y given x. in the context of structure learning, similar constructions have been leveraged in the literature 13, we further make a specific assumption throughout the paper about the soft-interventions. we call it the controlled experiment setting, where each variable is intervened with the same mechanism change across different interventions. 1c, suppose we are given distributions from two controlled experiments px, px,z along with observational data. we can then use fz to capture the invariances between px,z and px. for example, if px,z(y) , px(y), for some y, we can read that fz y fx , fx,z accordingly, given a set of interventional distributions, we construct an augmented graph by introducing an f-node for every unique set difference between pairs of controlled intervention sets (more on that later on). without the controlled experiment assumption, our machinery can still be used if one knows which mechanism changes are identical and by constructing f-nodes to reflect and capture the mechanism difference across two interventions. for simplicity of presentation, however, we restrict ourselves to the controlled experiment setting and do not pursue this route explicitly. to encapsulate the distributional invariants directly induced by the causal calculus rules3, we call a set of interventional distributions i-markov to a graph, if these distributions respect the causal calculus rules relative to that graph. note that the notion of i-markov is first introduced in 9, for causally sufficient systems without the use of do-constraints4. for our characterization, we first extend the causal calculus rules to operate between arbitrary sets of interventions. we call two causal graphs d1,d2 i-markov equivalent if the set of distributions that are i-markov tod1 andd2 are the same. using the augmented graph, we identify a graphical condition that is necessary and sufficient for two cbns with latents to be i-markov equivalent. finally, we propose a sound algorithm for learning the augmented graph from interventional data. our contributions can be summarized as follows: we propose a characterization of i-markov equivalence between two causal graphs with latent variables for a given intervention set i that is based on a generalization of do-calculus rules to arbitrary subsets of interventions. we show a graphical characterization of i-markov equivalence of causal graphs with latents. we introduce a learning algorithm for inferring the graphical structure using a combination of observational and interventional data and utilizing the corresponding new constraints. this procedure comes with a new set of orientation rules. we formally show its soundness. we investigate the problem of learning the causal structure underlying a phenomenon of interest from a combination of observational and experimental data. we pursue this endeavor by noting that a generalization of the converse of pearl\u2019s do-calculus (thm. 1) leads to new tests that can be evaluated against data. these tests, in turn, translate into constraints over the structure itself. we then define an interventional equivalence class based on such criteria (def. 1), and then derive a graphical characterization for the equivalence of two causal graphs (thm. finally, we develop an algorithm to learn an interventional equivalence class from data, which includes new orientation rules.", "summary": " ", "id": 1017}, {"document": "recent years have seen increasingly complex question-answering on knowledge bases (kbqa) involving logical, quantitative, and comparative reasoning over kb subgraphs. neural program induction (npi) is a pragmatic approach toward modularizing the reasoning process by translating a complex natural language query into a multi-step executable program. while npi has been commonly trained with the gold\u2019\u2019 program or its sketch, for realistic kbqa applications such gold programs are expensive to obtain. there, practically only natural language queries and the corresponding answers can be provided for training. the resulting combinatorial explosion in program space, along with extremely sparse rewards, makes npi for kbqa ambitious and challenging. we present complex imperative program induction from terminal rewards (cipitr), an advanced neural programmer that mitigates reward sparsity with auxiliary rewards, and restricts the program space to semantically correct programs using high-level constraints, kb schema, and inferred answer type. cipitr solves complex kbqa considerably more accurately than key-value memory networks and neural symbolic machines (nsm). for moderately complex queries requiring 2to 5-step programs, cipitr scores at least higher f1 than the competing systems. on one of the hardest class of programs (comparative reasoning) with steps, cipitr outperforms nsm by a factor of and memory networks by times.1 now at hike messenger 1the nsm baseline in this work is a re-implemented version, as the original code was not available. structured knowledge bases , or multi-hop , or complex queries such as how many countries have more rivers and lakes than brazil?\u2019\u2019 complex queries require a proper assembly of selected operators from a library of graph, set, logical, and arithmetic operations into a complex procedure, and is the subject of this paper. relatively simple query classes, in particular, in which answers are kb entities, can be served with feed-forward and seq2seq networks. however, such systems show copying or rote learning behavior when boolean or open numeric domains are involved. more complex queries need to be evaluated as an acyclic expression graph over nodes representing kb access, set, logical, and arithmetic operators a practical alternative to inferring a stateless expression graph is to generate an imperative sequential program to solve the query. each step of the program selects an atomic operator and a set of previously defined variables as arguments and writes the result to scratch memory, which can then be used in subsequent steps. such imperative programs are preferable to opaque, monolithic networks for their interpretability and generalization to diverse domains. another transactions of the association for computational linguistics, vol. action editor: scott wen-tau yih. submission batch: 8/2018; revision batch: 11/2018; final submission: 1/2019; published 4/2019. c association for computational linguistics. distributed under a cc-by license. motivation behind opting for the program induction paradigm for solving complex tasks, such as complex question answering, is modularizing the end-to-end complex reasoning process. with this approach it is now possible to first train separate modules for each of the atomic operations involved and then train a program induction model that learns to use these separately trained models and invoke the sub-modules in the correct fashion to solve the task. these sub-modules can even be task-agnostic generic models that can be pretrained with much more extensive training data, while the program induction model learns from examples pertaining to the specific task. this paradigm of program induction has been used for decades, with rule induction and probabilistic program induction techniques in lake et al. as well as for manipulating a physical environment program induction has also seen initial promise in translating simple natural language queries into programs executable in one or two hops over a kb to obtain answers in contrast, many of the complex queries from saha et al. (2018), such as the one in figure 1, require up to 10-step programs involving multiple relations and several arithmetic and logical operations. sample operations include genset: collecting t : (h, r, t) kb, computing setunion, counting set sizes (setcount), comparing numbers or sets, and so forth. these operations need to be executed in the correct order, with correct parameters, sharing information via intermediate results to arrive at the correct answer. note also that the actual gold program is not available for supervision and therefore the large space of possible translation actions at each step, coupled with a large number of steps needed to get any payoff, makes the reward very sparse. this renders complex kbqa in the absence of gold programs extremely challenging. main contributions we present complex imperative program induction from terminal rewards\u2019\u2019 (cipitr),2 an advanced neural program induction (npi) system that is able to answer complex logical, quantitative, and comparative queries by inducing programs of length up to 7, using atomic operators and variable types. this, to our knowledge, is the first npi system to be trained with only the gold answer as (very distant) supervision for inducing such complex programs. cipitr reduces the combinatorial program space to only semantically correct programs by (i) incorporating symbolic constraints guided by kb schema and inferred answer type, and (ii) adopting pragmatic programming techniques by decomposing the final goal into a hierarchy of sub-goals, thereby mitigating the sparse reward problem by considering additional auxiliary rewards in a generic, task-independent way. we evaluate cipitr on the following two challenging tasks: and (ii) multi-hop kbqa in one of the more 2the code and reinforcement learning environment of cipitr is made public inurl/ cipitr. popularly used kbqa data sets webquestionssp webquestionssp involves complex multi-hop inferencing, sometimes with additional constraints, as we will describe later. however, csqa poses a much greater challenge, with its more diverse classes of complex queries and almost 20-times larger scale. on a data set such as csqa, contemporary models like neural symbolic machines are also unable to perform the necessary complex multi-step inference. cipitr outperforms them both by a significant margin while avoiding exploration of unwanted program space or memorization of low-entropy answer distributions. on even moderately complex programs of length 25, cipitr scored at least higher f1 than both. on one of the hardest class of programs of around steps (i.e., comparative reasoning), cipitr outperformed nsm by a factor of and kvmnet by a factor of further, we empirically observe that among all the competing models, cipitr shows the best generalization across diverse program classes. we presented cipitr, an advanced npi framework that significantly pushes the frontier of complex program induction in absence of gold programs. cipitr uses auxiliary rewarding techniques to mitigate the extreme reward sparsity and incorporates generic pragmatic programming styles to constrain the combinatorial program space to only semantically correct programs. as future directions of work, cipitr can be further improved to handle the hardest question types by making the search more strategic, and can be further generalized to a diverse set of goals when training on all question categories together. other potential directions of research could be toward learning to discover sub-goals to further decompose the most complex classes beyond just the two-level phase transition proposed here. additionally, further improvements are required to induce complex programs without availability of gold program input variables.", "summary": " ", "id": 1018}, {"document": "we propose dual-ces a novel unsupervised, query-focused, multi-document extractive summarizer. dual-ces is designed to better handle the tradeoff between saliency and focus in summarization. to this end, dual-ces employs a two-step dual-cascade optimization approach with saliency-based pseudo-feedback distillation. overall, dual-ces significantly outperforms all other state-of-the-art unsupervised alternatives. dual-ces is even shown to be able to outperform strong supervised summarizers. the vast amounts of textual data end users need to consume motivates the need for automatic summarization an automatic summarizer gets as an input one or more documents and possibly also a limit on summary length (e.g., maximum number of words). the summarizer then needs to produce a textual summary that captures the most salient (general and informative) content parts within input documents. moreover, the summarizer may also be required to satisfy a specific user information need, expressed by one or more queries. therefore, the summarizer will need to produce a focused summary which includes the most relevant information to that need. while both saliency and focus goals should be considered within a query-focused summarization setting, these goals may be actually conflicting with each other higher saliency usually comes at the expense of lower focus and vice-versa. moreover, such a tradeoff may directly depend on summary length. contact author: haggaiil.ibm.com to illustrate the effect of summary length on this tradeoff, using the duc dataset, figure reports the summarization quality which was obtained by the cross entropy summarizer (ces) a state of the art unsupervised query-focused multidocument extractive summarizer saliency was measured according to cosine similarity between the summary\u2019s bigram representation and that of the input documents. focus was further measured relatively to how much the summary\u2019s induced unigram model is concentrated around query-related words. as we can observe in figure 1, with the relaxation of the summary length limit, where a more lengthy summary is being allowed, saliency increases at the expense of focus. laying towards more saliency would result in a better coverage of general and more informative content. yet, this would result in the inclusion of less relevant content to the specific information need in mind. aiming at better handling the saliency versus focus tradeoff, in this work, we propose dual-ces an extended ces summarizer similar to ces, dual-ces is an unsupervised query-focused, multi-document, extractive summarizer. to this end, like ces, dual-ces utilizes the cross entropy method for selecting a subset of sentences extracted from input documents, whose combination is predicted to produce a good summary. yet, differently from ces, dual-ces does not attempt to address both saliency and focus goals in a single optimization step. instead, dual-ces implements a novel two-step dual-cascade optimization approach, which utilizes two sequential ces-like invocations. using such an approach, dual-ces tries to handle the tradeoff by gradually shifting from generating a long summary that is more salient in the first step to generating a short summary that is more focused in the second step. moreover, dualces utilizes the long summary that was generated in the first step for saliency-based pseudo-feedback distillation, which allows to generate a final focused summary with better saliency. dual-ces provides a fully unsupervised end-to-end query-focused multi-document extractive summarization solution. using an evaluation with the duc 2005, and benchmarks, we show that, dual-ces generates a focused (and shorter) summary which has much higher saliency (and hence a better tradeoff handling). overall, dual-ces provides a significantly better summarization quality compared to other alternative unsupervised summarizers; and in many cases, it even outperforms that of state-of-the art supervised summarizers. we proposed dual-ces, an unsupervised, query-focused, extractive multi-document summarizer. dual-ces was shown to better handle the tradeoff between saliency and focus, providing the best summarization quality compared to other alternative stateof-the-art unsupervised summarizers. moreover, in many cases, dual-ces even outperforms state-of-the-art supervised summarizers. as a future work, we would like to learn to distill from additional pseudo-feedback sources.", "summary": " ", "id": 1019}, {"document": "we present a lightweight adaptable neural tts system with high quality output. the system is composed of three separate neural network blocks: prosody prediction, acoustic feature prediction and linear prediction coding net as a neural vocoder. this system can synthesize speech with close to natural quality while running times faster than real-time on a standard cpu. the modular setup of the system allows for simple adaptation to new voices with a small amount of data. we first demonstrate the ability of the system to produce high quality speech when trained on large, high quality datasets. following that, we demonstrate its adaptability by mimicking unseen voices using to minutes long datasets with lower recording quality. large scale mean opinion score quality and similarity tests are presented, showing that the system can adapt to unseen voices with quality gap of and similarity gap of compared to natural speech for male voices and quality gap of and similarity of gap of for female voices. in recent years we are experiencing a dramatic improvement of the synthesized speech quality in tts systems, with the introduction of systems that are based on neural networks (nn). a major improvement in quality was achieved by using attention based models such as tacotron and by replacing vocoders with a nn based waveform generators such as wavenet a useful feature of systems with trainable models is the ability to adapt the tts to an unseen voice using a small amount of training data (from a few seconds to an hour of speech). this is usually done by training the system on a large number of speakers, and providing a speaker embedding vector as one of the system\u2019s inputs. using this approach allows later retraining of only a subsets of the model parameters or prediction of the speaker embedding vector 3, 4, the drawback of this approach is that the resulting systems use large nn models. furthermore, a multi-speaker model usually needs much more trainable parameters than a single speaker model. this may lead to a computationally heavy and slow synthesis process even on a strong gpu. such requirements pose a severe problem for practical tts system that require very low latency for a dialog with a human. in our previous paper we introduced a nn based tts system with two trainable modules for prosody prediction and acoustic features prediction. this system used the world vocoder we demonstrated that this tts allows simple adaptation to new voices. this was carried out by retraining nn models that had already been trained using a large highquality voice, on a small amount of data from the new voice. although the quality of this system was better in many cases than similar concatenative tts, it was still limited by the quality of the world vocoder. recently, an efficient neural vocoder called lpcnet was introduced the lpcnet inference runs faster than realtime on a single cpu while producing a high quality speech output. lpcnet uses cepstrum representing spectral envelopes, pitch and pitch correlation as input features. this makes it a simple alternative to other vocoders, e.g. world, which work with similar features. in this paper we show that we can get a considerable quality improvement by modifying a tts system that produced the world vocoder parameters to predict parameters for lpcnet as in the previous work 6, we conduct multiple adaptation experiments, applied on multiple vctk voices and show that the new system has much better quality and similarity to the target voices but can still run much faster than real-time in a single-cpu mode. we have presented in this article a new tts system that addresses the challenging goals of producing high quality speech while operating at faster than real-time rate without an expensive gpu support. the system is built around three nn models for generating the prosody, acoustic features and the final speech signal. we tested this system using two proprietary tts voice datasets and demonstrated that our system produces high quality speech that is comparable to larger and much slower tacotron2 wavenet systems. the task of creating a high-quality tts system out of a smaller set of audio data is even more challenging. we have shown that our system can perform well even with datasets as small as 5-20 minutes of audio. we demonstrated that when we reduce the size of the training data, there is some graceful degradation to the quality, but we are still able to maintain good similarity to the original speaker. for future work, we plan to allow voice modifications by adding control over voice parameters such as pitch, breathiness and vocal tract.", "summary": " ", "id": 1020}, {"document": "we propose the sobolev independence criterion (sic), an interpretable dependency measure between a high dimensional random variable x and a response variable y sic decomposes to the sum of feature importance scores and hence can be used for nonlinear feature selection. sic can be seen as a gradient regularized integral probability metric (ipm) between the joint distribution of the two random variables and the product of their marginals. we use sparsity inducing gradient penalties to promote input sparsity of the critic of the ipm. in the kernel version we show that sic can be cast as a convex optimization problem by introducing auxiliary variables that play an important role in feature selection as they are normalized feature importance scores. we then present a neural version of sic where the critic is parameterized as a homogeneous neural network, improving its representation power as well as its interpretability. we conduct experiments validating sic for feature selection in synthetic and real-world experiments. we show that sic enables reliable and interpretable discoveries, when used in conjunction with the holdout randomization test and knockoffs to control the false discovery rate. code is available at url. feature selection is an important problem in statistics and machine learning for interpretable predictive modeling and scientific discoveries. our goal in this paper is to design a dependency measure that is interpretable and can be reliably used to control the false discovery rate in feature selection. the mutual information between two random variables x and y is the most commonly used dependency measure. the mutual information i(x;y ) is defined as the kullback-leibler divergence between the joint distribution pxy of x,y and the product of their marginals pxpy, i(x;y ) kl(pxy, pxpy). mutual information is however challenging to estimate from samples, which motivated the introduction of dependency measures based on other f -divergences or integral probability metrics than the kl divergence. for instance, the hilbert-schmidt independence criterion (hsic) uses the maximum mean discrepancy (mmd) to assess the dependency between two variables, i.e. hsic(x,y ) mmd(pxy, pxpy), which can be easily estimated from samples via kernel mean embeddings in a reproducing kernel hilbert space (rkhs) in this paper we introduce the sobolev independence criterion (sic), a form of gradient regularized integral probability metric (ipm) between the joint distribution and the product of marginals. sic relies on the statistics of the gradient of a witness function, or critic, for both (1) defining the ipm constraint and (2) finding the features that discriminate between the joint and the marginals. intuitively, the magnitude of the average gradient with respect to a feature gives an importance score for each feature. hence, promoting its sparsity is a natural constraint for feature selection. the paper is organized as follows: we show in section how sparsity-inducing gradient penalties can be used to define an interpretable dependency measure that we name sobolev independence criterion tom sercu is now with facebook ai research, and cicero dos santos with amazon aws ai. the work was done when they were at ibm research. 33rd conference on neural information processing systems (neurips 2019), vancouver, canada. we devise an equivalent computational-friendly formulation of sic in section 3, that gives rise to additional auxiliary variables j these naturally define normalized feature importance scores that can be used for feature selection. in section we study the case where the sic witness function f is restricted to an rkhs and show that it leads to an optimization problem that is jointly convex in f and the importance scores we show that in this case sic decomposes into the sum of feature scores, which is ideal for feature selection. in section we introduce a neural version of sic, which we show preserves the advantages in terms of interpretability when the witness function is parameterized as a homogeneous neural network, and which we show can be optimized using stochastic block coordinate descent. in section we show how sic and conditional generative models can be used to control the false discovery rate using the recently introduced holdout randomization test and knockoffs we validate sic and its fdr control on synthetic and real datasets in section we introduced in this paper the sobolev independence criterion (sic), a dependency measure that gives rise to feature importance which can be used for feature selection and interpretable decision making. we laid down the theoretical foundations of sic and showed how it can be used in conjunction with the holdout randomization test and knockoffs to control the fdr, enabling reliable discoveries. we demonstrated the merits of sic for feature selection in extensive synthetic and real-world experiments with controlled fdr.", "summary": " ", "id": 1021}]