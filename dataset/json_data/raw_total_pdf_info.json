[
  {
    "blog_id": "glove-global-vectors-for-word-representation",
    "summary": [
      "GloVe: Global Vectors for Word Representation \u2013 Pennington et al. 2014  Yesterday we looked at some of the amazing properties of word vectors with word2vec .",
      "Pennington et al. argue that the online scanning approach used by word2vec is suboptimal since it doesn\u2019t fully exploit statistical information regarding word co-occurrences.",
      "They demonstrate a Global Vectors (GloVe) model which combines the benefits of the word2vec skip-gram model when it comes to word analogy tasks, with the benefits of matrix factorization methods that can exploit global statistical information.",
      "The GloVe model\u2026  \u2026 produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task.",
      "It also outperforms related models on similarity tasks and named entity recognition.",
      "The source code for the model, as well as trained word vectors can be found at  [url]"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://www-nlp.stanford.edu/pubs/glove.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 22546727
  },
  {
    "blog_id": "1810.02334",
    "summary": [
      "What is stopping us from applying meta-learning to new tasks?",
      "Where do the tasks come from?",
      "Designing task distribution is laborious.",
      "We should automatically learn tasks!",
      "Unsupervised Learning via Meta-Learning: The idea is to use a distance metric in an out-of-the-box unsupervised embedding space created by BiGAN/ALI or DeepCluster to construct tasks in an unsupervised way.",
      "If you cluster points to randomly define classes (e.g. random k-means) you can then sample tasks of 2 or 3 classes and use them to train a model.",
      "Where does the extra information come from?",
      "The metric space used for k-means asserts specific distances.",
      "The intuition why this works is that it is useful model initialization for downstream tasks.",
      "This summary was written with the help of Chelsea Finn."
    ],
    "author_id": "joecohen",
    "pdf_url": "http://arxiv.org/pdf/1810.02334v2",
    "author_full_name": "Joseph Cohen",
    "source_website": "https://www.shortscience.org/user?name=joecohen",
    "id": 97592875
  },
  {
    "blog_id": "niculaeb17",
    "summary": [
      "The idea in this paper is to develop a version of attention that will incorporate similarity in neighboring bins.",
      "This aligned with the work  [ref]  which presented a different approach to deal with consistency between classes of predictions.",
      "In this work the closed form softmax function is replaced by a small optimization problem with this regularizer:  $$ +\\lambda \\sum_{i=1}^{d-1} |y_{i+1}-y_i|$$  Because of this, many of the neighboring probabilities are exactly the same resulting in attention that can be seen as blocks.",
      "[url]"
    ],
    "author_id": "joecohen",
    "pdf_url": "http://papers.nips.cc/paper/6926-a-regularized-framework-for-sparse-and-structured-neural-attention.pdf",
    "author_full_name": "Joseph Cohen",
    "source_website": "https://www.shortscience.org/user?name=joecohen",
    "id": 74374635
  },
  {
    "blog_id": "diplomat-using-delegations-to-protect-community-repositories",
    "summary": [
      "Diplomat: Using Delegations to Protect Community Repositories \u2013 Kuppusamy et al. 2016  Community repositories, such as Docker Hub, Python Package Index (PyPI), RubyGems, and SourceForge provide an easy way for a developer to disseminate software\u2026 [they] are immensely popular and collectively serve more than a billion packages per year.",
      "Unfortunately, the popularity of these repositories also makes them an attractive target to hackers\u2026 Major repositories run by Adobe, Apache, Debian, Fedora, FreeBSD, Gentoo, GitHub, GNU Savannah, Linux, Microsoft, npm, Opera, PHP, RedHat, RubyGems, SourceForge, and WordPress have all been compromised at least once.",
      "This is a topic of immediate importance.",
      "Diplomat is a practical security system for community repositories that combines immediate project registration (adding new projects happens all the time with popular repositories) and compromise-resilience.",
      "Diplomat source code and standards documents are freely available at  [url]"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://isis.poly.edu/%7Ejcappos/papers/kuppusamy_nsdi_16.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 34524227
  },
  {
    "blog_id": "musketeer-part-ii-one-for-all-and-all-for-one",
    "summary": [
      "Musketeer: all for one, one for all in data processing systems \u2013 Gog et al. 2015  Musketeer gives you portability of data processing workflows across across data processing systems.",
      "It can even analyse your workflow and recommend the best system to run it on, as well as combining systems for different parts of the workflow.",
      "This is important since, as we saw yesterday in part  1 , no one system is universally best across all workload sizes and varieties.",
      "It  works by introducing a DAG-based intermediate representation (IR) and a translator from existing workflow specifications into this IR (all for one).",
      "The IR is then analysed and optimised and efficient code is generated for the target back-end system (one for all).",
      "You\u2019ll find the paper at the link above, and the open source project at  [url]"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://www.cl.cam.ac.uk/research/srg/netos/camsas/pubs/eurosys15-musketeer.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 5262509
  },
  {
    "blog_id": "1411.5752",
    "summary": [
      "So the hypervector is just a big vector created from a network:  `\"We concatenate features from some or all of the feature maps in the network into one long vector for every location which we call the hypercolumn at that location.",
      "As an example, using pool2 (256 channels), conv4 (384 channels) and fc7 (4096 channels) from the architecture of [28] would lead to a 4736 dimensional vector.",
      "\"`  So how exactly do we construct the vector?",
      "!",
      "[]( [url]"
    ],
    "author_id": "joecohen",
    "pdf_url": "http://arxiv.org/pdf/1411.5752v2",
    "author_full_name": "Joseph Cohen",
    "source_website": "https://www.shortscience.org/user?name=joecohen",
    "id": 96599676
  },
  {
    "blog_id": "lempitskyz10",
    "summary": [
      "They introduce the concept of counting in images by predicting a density map.",
      "Their training only requires dot annotations on the center of objects.",
      "Each dot is expanded to a gaussian to form a density.",
      "A model is trained to predict this density and then the total count is recovered by integrating over the resulting density map.",
      "They create a function to produce the density based on quantized dense SIFT features  [ref]  from every pixel in the image.",
      "A simple version of the definition of $F$ is shown below.",
      "Each pixel becomes an $x_p$ vector which is used to train and model to implement the function $F$.",
      "$$\\forall p \\in I, \\hspace{10pt } F(p|w) = wx_p $$  The obtained quantized dense SIFT features using the [VLFEAT]( [url]"
    ],
    "author_id": "joecohen",
    "pdf_url": "http://papers.nips.cc/paper/4043-learning-to-count-objects-in-images.pdf",
    "author_full_name": "Joseph Cohen",
    "source_website": "https://www.shortscience.org/user?name=joecohen",
    "id": 33015723
  },
  {
    "blog_id": "6245692b276cd0b6dcbaf43e4211db",
    "summary": [
      "The paper explains how to apply dropout to LSTMs and how it could reduce overfitting in tasks like language modelling, speech recognition, image caption generation and machine translation.",
      "Dropout  Regularisation method that drops out (or temporarily removes) units in a neural network.",
      "the network, along with all its incoming and outgoing connections  Conventional dropout does not work well with RNNs as the recurrence amplifies the noise and hurts learning.",
      "Regularization  The paper proposes to apply dropout to only the non-recurrent connections.",
      "The dropout operator would corrupt information carried by some units (and not all) forcing them to perform intermediate computations more robustly.",
      "The information is corrupted L+1 times where L is the number of layers and is independent of timestamps traversed by the information.",
      "Observation  In the context of language modelling, image caption generation, speech recognition and machine translation, dropout enables training larger networks and reduces the testing error in terms of perplexity and frame accuracy."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1409.2329",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 80897282
  },
  {
    "blog_id": "mariets15a",
    "summary": [
      "The goal is to compress a neural network based on figuring out the most significant neurons.",
      "They sample from Determinantal Point Process (DPP) in order to find set of neurons that have the most dissimilar activations and then project remaining neurons to them in order to reduce number of neurons overall.",
      "DPPs compute the probability of volume of dissimilarity over volume of all neurons:  $$P(\\text{subset } Y) = \\frac{det(L_Y)}{det(L+I)}$$   More dissimilarity means higher probability.",
      "A simple sample of the neurons outputs are taken given the training set."
    ],
    "author_id": "joecohen",
    "pdf_url": "http://arxiv.org/pdf/1511.05077",
    "author_full_name": "Joseph Cohen",
    "source_website": "https://www.shortscience.org/user?name=joecohen",
    "id": 963444
  },
  {
    "blog_id": "freund1999",
    "summary": [
      "This extends perceptrons  [ref]  but uses what is known as the Hinge loss (aka SVM loss):  $$J_i(w) = max(0,\\gamma -y\\_i f(x\\_i))$$  Where $\\gamma$ is the margin.",
      "$J_i(w)$ is the error given some weight $w$ parameters.",
      "$x_i$ and $y_i$ are a training example and correct label.",
      "$f(x_i)$ is the perceptron function we are trying learn the best weights for."
    ],
    "author_id": "joecohen",
    "pdf_url": "http://cseweb.ucsd.edu/~yfreund/papers/LargeMarginsUsingPerceptron.pdf",
    "author_full_name": "Joseph Cohen",
    "source_website": "https://www.shortscience.org/user?name=joecohen",
    "id": 6469050
  },
  {
    "blog_id": "1704.07820",
    "summary": [
      "In this work they take a different approach to the GAN model  [ref] .",
      "In the traditionally GAN model a neural network is trained to up-sample from random noise in a feed forward fashion to generate samples from the data distribution.",
      "This work instead iteratively permutes an image of random noise similar to Artistic Style Transfer  [ref] .",
      "The image is permuted in order to fool a set of discriminators.",
      "To obtain the set of discriminators each is trained starting from random noise until some max $t$ step.",
      "1.",
      "At first a discriminator is trained to discriminate between the true data and random noise .",
      "2.",
      "Images is then permuted using gradients which aim to fool the discriminator and included in the data distribution as a negative example.",
      "3.",
      "The discriminator is trained on the true data + random noise + fake data from the previous steps  The images generated at each step are shown below:   [url]"
    ],
    "author_id": "joecohen",
    "pdf_url": "http://arxiv.org/pdf/1704.07820v1",
    "author_full_name": "Joseph Cohen",
    "source_website": "https://www.shortscience.org/user?name=joecohen",
    "id": 80004741
  },
  {
    "blog_id": "home-a-household-multimodal-environment",
    "summary": [
      "Environment for learning using modalities like vision, audio, semantics, physics and interaction with objects and other agents.",
      "Motivation  Humans learn by interacting with their surroundings (environment).",
      "Similarly training an agent in an interactive multi-model environment (virtual embodiment) could be useful for a learning agent.",
      "Characteristics  Open-source and Open-AI gym compatible  Built on top of 45000 3D house layouts from SUNCG dataset.",
      "Provides both 3D visual and audio recording.",
      "Semantic image segmentation and langauge description of objects.",
      "Components  Rendering Engine  Implemented using Panda 3D game engine.",
      "Renders RGB+depth scenes based on textures, multi-source lightings and shadows.",
      "Acoustic Engine  Implemented using EVERT  Supports multiple microphones, sound sources, sound absorption based on material, atmospheric conditions etc.",
      "Semantics Engine  Provides a short textual description for each object, along with information like color, category, material size, location etc.",
      "Physics Engine  Implemented using Bullet3 Engine  Supports physical interaction, external forces like gravity and position and velocity information for multiple agents.",
      "Potential Applications  Visual Question Answering  Conversational Agents  Training an agent to follow instructions  Multi-agent communication"
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1711.11017",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 99295545
  },
  {
    "blog_id": "identity_mappings_in_deep_residual_networks",
    "summary": [
      "What  The authors reevaluate the original residual design of neural networks.",
      "They compare various architectures of residual units and actually find one that works quite a bit better.",
      "How  The new variation starts the transformation branch of each residual unit with BN and a ReLU.",
      "It removes BN and ReLU after the last convolution.",
      "As a result, the information from previous layers can flow completely unaltered through the shortcut branch of each residual unit.",
      "The image below shows some variations (of the position of BN and ReLU) that they tested.",
      "The new and better design is on the right:  They also tried various alternative designs for the shortcut connections.",
      "However, all of these designs performed worse than the original one.",
      "Only one (d) came close under certain conditions.",
      "Therefore, the recommendation is to stick with the old/original design.",
      "Results  Significantly faster training for very deep residual networks (1001 layers).",
      "Better regularization due to the placement of BN.",
      "CIFAR-10 and CIFAR-100 results, old vs. new design:"
    ],
    "author_id": "ALEJU",
    "pdf_url": "http://arxiv.org/pdf/1603.05027v2",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 4216862
  },
  {
    "blog_id": "1706.02515",
    "summary": [
      "\"Using the \"SELU\" activation function, you get better results than any other activation function, and you don't have to do batch normalization.",
      "The \"SELU\" activation function is:  if x<0, 1.051\\*(1.673\\*e^x-1.673) if x>0, 1.051\\*x\" Source: narfon2, reddit   ``` import numpy as np  def selu(x):     alpha = 1.6732632423543772848170429916717     scale = 1.0507009873554804934193349852946     return scale*np.where(x>=0.0, x, alpha*np.exp(x)-alpha) ``` Source: CaseOfTuesday, reddit  Discussion here:  [url]"
    ],
    "author_id": "joecohen",
    "pdf_url": "http://arxiv.org/pdf/1706.02515v1",
    "author_full_name": "Joseph Cohen",
    "source_website": "https://www.shortscience.org/user?name=joecohen",
    "id": 8240954
  },
  {
    "blog_id": "convexified-cnns",
    "summary": [
      "In this paper, the authors proposed a method for convexifying convolutional neural networks to train them without backpropagation.",
      "Furthermore, this relaxation to the convex setting allows for theoretical proofs of bounds on the generalization error.",
      "Succinctly, they propose to use RKHS and the kernel trick to lift the data into a high-dimensional space that is expressive enough to capture certain nonlinear activation functions.",
      "Hence, on experiments on MNIST and CIFAR-10, they show that they can outperform smaller CNNs by \u201cconvexifying\u201d them.",
      "They note that their method doesn\u2019t work with max pooling or very deep CNNs with lots of bells and whistles.",
      "This is a thought-provoking paper.",
      "I like how the authors pursued a theoretically interesting question, even though there isn\u2019t much practical use yet for this.",
      "I don\u2019t have personal experience writing theory papers, but I imagine that this is a good(?)",
      "representation of how they often go in ML.",
      "The research is driven by an interesting theoretical question, not a practical application that needs solving/SOTA results."
    ],
    "author_id": "pemami",
    "pdf_url": "https://arxiv.org/pdf/1609.01000",
    "author_full_name": "Patrick Emami",
    "source_website": "https://pemami4911.github.io/index.html",
    "id": 20550713
  },
  {
    "blog_id": "claverars0aa18",
    "summary": [
      "In terms of model based RL, learning dynamics models is imperfect, which often leads to the learned policy overfitting to the learned dynamics model, doing well in the learned simulator but not in the real world.",
      "Key solution idea: No need to try to learn one accurate simulator.",
      "We can learn an ensemble of models that together will sufficiently represent the space.",
      "If we learn an ensemble of models (to be used as many learned simulators) we can denoise estimates of performance.",
      "In a meta-learning sense these simulations become the tasks.",
      "The real world is then just yet another task, to which the policy could adapt quickly.",
      "One experimental observation is that at the start of training there is a lot of variation between learned simulators, and then the simulations come together over training, which might also point to this approach providing improved exploration.",
      "This summary was written with the help of Pieter Abbeel."
    ],
    "author_id": "joecohen",
    "pdf_url": "http://proceedings.mlr.press/v87/clavera18a/clavera18a.pdf",
    "author_full_name": "Joseph Cohen",
    "source_website": "https://www.shortscience.org/user?name=joecohen",
    "id": 31543932
  },
  {
    "blog_id": "glorotb10",
    "summary": [
      "The weights at each layer $W$ are initialized based on the number of connections they have.",
      "Each $w \\in W$  is drawn from a Gaussian distribution with mean $\\mu = 0$ with the variance as follows.",
      "$$\\text{Var}(W) = \\frac{2}{n_\\text{in}+ n_\\text{out}}$$  Where $n_\\text{in}$ is the number of neurons in the previous layer from the feedforward direction and $n_\\text{out}$ is the number of neurons from the previous layer from the backprop direction.",
      "Reference: [Andy Jones's Blog]( [url]"
    ],
    "author_id": "joecohen",
    "pdf_url": "http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf",
    "author_full_name": "Joseph Cohen",
    "source_website": "https://www.shortscience.org/user?name=joecohen",
    "id": 70140653
  },
  {
    "blog_id": "4d803bc021f579d4aa3b24cec5b994",
    "summary": [
      "Task of translating natural language queries into regular expressions without using domain specific knowledge.",
      "Proposes a methodology for collecting a large corpus of regular expressions to natural language pairs.",
      "Reports performance gain of 19.6% over state-of-the-art models.",
      "Architecture  LSTM based sequence to sequence neural network (with attention)  Six layers  One-word embedding layer  Two encoder layers  Two decoder layers  One dense output layer.",
      "Attention over encoder layer.",
      "Dropout with the probability of 0.25.",
      "20 epochs, minibatch size of 32 and learning rate of 1 (with decay rate of 0.5)  Dataset Generation  Created a public dataset - NL-RX - with 10K pair of (regular expression, natural language)  Two step generate-and-paraphrase approach  Generate step  Use handcrafted grammar to translate regular expressions to natural language.",
      "Paraphrase step  Crowdsourcing the task of translating the rigid descriptions into more natural expressions.",
      "Results  Evaluation Metric  Functional equality check (called DFA-Equal) as same regular expression could be written in many ways.",
      "Proposed architecture outperforms both the baselines - Nearest Neighbor classifier using Bag of Words (BoWNN) and Semantic-Unify"
    ],
    "author_id": "shugan",
    "pdf_url": "http://arxiv.org/pdf/1608.03000v1",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 37775721
  },
  {
    "blog_id": "dbsherlock-a-performance-diagnostic-tool-for-transactional-databases",
    "summary": [
      "DBSherlock: A performance diagnostic tool for transactional databases Yoon et al. SIGMOD \u201916  \u2026tens of thousands of concurrent transactions competing for the same resources (e.g.",
      "CPU, disk I/O, memory) can create highly non-linear and counter-intuitive effects on database performance.",
      "If you\u2019re a DBA responsible for figuring out what\u2019s going on, this presents quite a challenge.",
      "You might be awash in stats and graphs (MySQL maintains 260 statistics and variables for example), but still sorely lacking the big picture\u2026 \u201cas a consquence, highly-skilled and highly-paid DBAs (a scarce resource themselves) spend many hours diagnosing performance problems through different conjectures and manually inspecting various queries and log files, until the root cause is found.\u201d  DBSherlock (available at  [url]"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://web.eecs.umich.edu/~mozafari/php/data/uploads/sigmod_2016.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 414696
  },
  {
    "blog_id": "915921d7d0ac5cfd0e379025acfb9f",
    "summary": [
      "The paper proposes a general and end-to-end approach for sequence learning that uses two deep LSTMs, one to map input sequence to vector space and another to map vector to the output sequence.",
      "For sequence learning, Deep Neural Networks (DNNs) requires the dimensionality of input and output sequences be known and fixed.",
      "This limitation is overcome by using the two LSTMs.",
      "Model  Recurrent Neural Networks (RNNs) generalizes feed forward neural networks to sequences.",
      "Given a sequence of inputs (x1, x2...xt), RNN computes a sequence of outputs (y1, y2...yt') by iterating over the following equation:  ht = sigm(Whxxt + Whhht-1) yt = Wyhht  To map variable length sequences, the input is mapped to a fixed size vector using an RNN and this fixed size vector is mapped to output sequence using another RNN.",
      "Given the long-term dependencies between the two sequences, LSTMs are preferred over RNNs.",
      "LSTMs estimate the conditional probability p(output sequence | input sequence) by first mapping the input sequence to a fixed dimensional representation and then computing the probability of output with a standard LST-LM formulation.",
      "Differences between the model and standard LSTMs  The model uses two LSTMs (one for input sequence and another for output sequence), thereby increasing the number of model parameters at negligible computing cost.",
      "Model uses Deep LSTMs (4 layers).",
      "The words in the input sequences are reversed to introduce short-term dependencies and to reduce the \"minimal time lag\".",
      "By reversing the word order, the first few words in the source sentence (input sentence) are much closer to first few words in the target sentence (output sentence) thereby making it easier for LSTM to \"establish\" communication between input and output sentences.",
      "Experiments  WMT'14 English to French dataset containing 12 million sentences consisting of 348 million French words and 304 million English words.",
      "Model tested on translation task and on the task of re-scoring the n-best results of baseline approach.",
      "Deep LSTMs trained in sentence pairs by maximizing the log probability of a correct translation T, given the source sentence S  The training objective is to maximize this log probability, averaged over all the pairs in the training set.",
      "Most likely translation is found by performing a simple, left-to-right beam search for translation.",
      "A hard constraint is enforced on the norm of the gradient to avoid the exploding gradient problem.",
      "Min batches are selected to have sentences of similar lengths to reduce training time.",
      "Model performs better when reversed sentences are used for training.",
      "While the model does not beat the state-of-the-art, it is the first pure neural translation system to outperform a phrase-based SMT baseline.",
      "The model performs well on long sentences as well with only a minor degradation for the largest sentences.",
      "The paper prepares ground for the application of sequence-to-sequence based learning models in other domains by demonstrating how a simple and relatively unoptimised neural model could outperform a mature SMT system on translation tasks."
    ],
    "author_id": "shugan",
    "pdf_url": "https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 30789903
  },
  {
    "blog_id": "starspace-embed-all-the-things",
    "summary": [
      "The paper describes a general purpose neural embedding model where different type of entities (described in terms of discrete features) are embedded in a common vector space.",
      "A similarity function is learnt to compare these entities in a meaningful way and score their similarity.",
      "The definition of the similarity function could depend on the downstream task where the embeddings are used.",
      "Link to the implementation  Approach  Each entity is described as a set of discrete features.",
      "For example, for the recommendation use case, the users may be described as a bag-of-words of movies they have liked.",
      "For the search use case, the document may be described as a bag-of-words of words they are made up of.",
      "Given a dataset and a task at hand, generate a set of positive samples E = (a, b) such that a is the input to the task (from the dataset) and b is the expected label(answer/entity) for the given task.",
      "Similarly, generate another set of negative samples E - = (a, bi-) such that bi- is one of the incorrect label(answer/entity) for the given task.",
      "The incorrect entity can be sampled randomly from the set of candidate entities.",
      "Multiple incorrect samples could be generated for each positive example.",
      "These incorrect samples are indexed using i.",
      "For example, in case of supervised learning problem like document classification, a would be one of the documents (probably described in terms of words), b is the correct label and bi-) is one of the randomly sampled label from set of all the labels (excluding the correct label).",
      "In case of collaborative filtering, a would be the user (either described as a discrete entity like a userid or in terms of items purchased so far), b is the next item the user purchases and bi-) is one of the randomly sampled item from the set of all the items.",
      "A similarity function is chosen to compare the representation of entities of type a and b.",
      "The paper considered cosine similarity and inner product and observed that cosine similarity works better for the case with a large number of entities.",
      "A loss function compares the similarity between positive pairs (a, b) and (a, bi-).",
      "The paper considered margin ranking loss and negative log loss of softmax and reported that margin ranking loss works better.",
      "The norm of embeddings is capped at 1.",
      "Observations  The same model architecture is applied to a variety of tasks including multi-class classification, multi-label classification, collaborative filtering, content-based recommendation, link prediction, information retrieval, word embeddings and sentence embeddings.",
      "The model provides a strong baseline on all the tasks and performs at par with much more complicated and task-specific networks."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1709.03856",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 24950879
  },
  {
    "blog_id": "snapshot_ensembles",
    "summary": [
      "What  They suggest a method to generate ensembles of models requiring less total training time.",
      "The method is based on \"saving\" intermediary versions of models.",
      "How  They save every M epochs an intermediary version of the model (i.e. they save the parameters).",
      "Then they combine the last n models to an ensemble.",
      "To make each version more dissimilar from the other ones, they cycle the learning rate using a cosine function.",
      "That means that they increase the learning rate significantly, then keep it at the high level for a short time, then decrease it fast, then keep it at a low level for a short time.",
      "Formula for the (cycled) learning rate:  alpha_0 is the initial learning rate,  T the total number of training iterations  M the number of training iterations per cycle (after each cycle, the learning rate is increased)  Visualization of the cycled learning rate:  (Note that after the third cycle the model is already at nearly optimal accuracy, despite having suffered two times from increasing the learning rate.",
      "With a better learning rate schedule it might have been able to reach the optimum in 2-3 cycles.",
      "So it is kinda unhonest to say that this ensembling method adds no training it, it just probably adds less than \"normal\" ensembling from scratch.)",
      "(Also note here that the blue learning rate schedule that they are comparing against is probably far away from being optimal.)",
      "They argue that cycling the learning rate is also useful to \"jump\" between local minima.",
      "I.e. the model reaches a local minima, then escapes it using a high learning rate, then descends into a new local minima using the lowered learning rate.",
      "Then the ensemble would consist of models in different local minima.",
      "(Note here though that the current state of science is that there aren't really local minima in deep NNs, only saddle points.)",
      "(Also note that this means that the ensembled models probably are often fairly similar.",
      "A proper ensemble consisting only of models trained from scratch might perform better.)",
      "Results  Using roughly the 3 last snapshots for the ensemble seems to be the best compromise (at alpha_0=0.1).",
      "Using too many snaphots can worsen the results.",
      "Using alpha_0=0.2 seems to be a better choice than alpha_0=0.1.",
      "They argue that the high learning rate between cycles leads to more diverse local minima.",
      "Only running one learning rate cycle and collecting the ensemble models from that leads to worse results (as opposed to running multiple cycles).",
      "The following visualization shows the effect of using a single cycle vs. multiple (in relations to the training iterations).",
      "They perform overall better than models without ensembling.",
      "True ensembles reach quite a bit better accuracy still.",
      "Running models with interpolated snaphots (e.g. set each weight to 30% of snapshot 1 and 70% of snaphot 5), the test accuracy is improved if the snapshots are close to each other (e.g.",
      "snaphot 4 and 5).",
      "This indicates that the parameters change more and more with each cycle."
    ],
    "author_id": "ALEJU",
    "pdf_url": "https://arxiv.org/pdf/1704.00109",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 59485457
  },
  {
    "blog_id": "ten_years_of_pedestrian_detection_what_have_we_learned",
    "summary": [
      "What  They compare the results of various models for pedestrian detection.",
      "The various models were developed over the course of ~10 years (2003-2014).",
      "They analyze which factors seemed to improve the results.",
      "They derive new models for pedestrian detection from that.",
      "Comparison: Datasets  Available datasets  INRIA: Small dataset.",
      "Diverse images.",
      "ETH: Video dataset.",
      "Stereo images.",
      "TUD-Brussels: Video dataset.",
      "Daimler: No color channel.",
      "Daimler stereo: Stereo images.",
      "Caltech-USA: Most often used.",
      "Large dataset.",
      "KITTI: Often used.",
      "Large dataset.",
      "Stereo images.",
      "All datasets except KITTI are part of the \"unified evaluation toolbox\" that allows authors to easily test on all of these datasets.",
      "The evaluation started initially with per-window (FPPW) and later changed to per-image (FPPI), because per-window skewed the results.",
      "Common evaluation metrics:  MR: Log-average miss-rate (lower is better)  AUC: Area under the precision-recall curve (higher is better)  Comparison: Methods  Families  They identified three families of methods: Deformable Parts Models, Deep Neural Networks, Decision Forests.",
      "Decision Forests was the most popular family.",
      "No specific family seemed to perform better than other families.",
      "There was no evidence that non-linearity in kernels was needed (given sophisticated features).",
      "Additional data  Adding (coarse) optical flow data to each image seemed to consistently improve results.",
      "There was some indication that adding stereo data to each image improves the results.",
      "Context  For sliding window detectors, adding context from around the window seemed to improve the results.",
      "E.g. context can indicate whether there were detections next to the window as people tend to walk in groups.",
      "Deformable parts  They saw no evidence that deformable part models outperformed other models.",
      "Multi-Scale models  Training separate models for each sliding window scale seemed to improve results slightly.",
      "Deep architectures  They saw no evidence that deep neural networks outperformed other models.",
      "(Note: Paper is from 2014, might have changed already?)",
      "Features  Best performance was usually achieved with simple HOG+LUV features, i.e. by converting each window into:  6 channels of gradient orientations  1 channel of gradient magnitude  3 channels of LUV color space  Some models use significantly more channels for gradient orientations, but there was no evidence that this was necessary to achieve good accuracy.",
      "However, using more different features (and more sophisticated ones) seemed to improve results.",
      "Their new model:  They choose Decisions Forests as their model framework (2048 level-2 trees, i.e. 3 thresholds per tree).",
      "They use features from the Integral Channels Features framework .",
      "(Basically just a mixture of common/simple features per window.)",
      "They add optical flow as a feature.",
      "They add context around the window as a feature.",
      "(A second detector that detects windows containing two persons.)",
      "Their model significantly improves upon the state of the art (from 34 to 22% MR on Caltech dataset).",
      "Overview of models developed over the years, starting with Viola Jones (VJ) and ending with their suggested model (Katamari-v1).",
      "(DF = Decision Forest, DPM = Deformable Parts Model, DN = Deep Neural Network; I = Inria Dataset, C = Caltech Dataset)"
    ],
    "author_id": "ALEJU",
    "pdf_url": "https://arxiv.org/pdf/1411.4304",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 39382584
  },
  {
    "blog_id": "hierarchical-drl-intrinsic-motivation",
    "summary": [
      "The first two sections of this paper provide a decent overview of recent advances in the hierarchical and intrinsic RL literature.",
      "The authors propose a learning framework for achieving complex goals in the face of sparse rewards.",
      "Drawing from Singh, et al. , a separation between intrinsic and extrinsic learning is derived, such that external reward signals are generated from the environment and internal reward signals are generated by an internal critic.",
      "There is a meta-controller that learns an approximation of the optimal goal policy $\\pi_{g} (g | s )$ and a controller that learns an approximation of the optimal action policy $\\pi_{a g} (a|g,s)$.",
      "The meta-controller operates on a slower time-scale than the controller; it is concerned with selecting the optimal goal for the controller to work towards.",
      "The internal critic provides incremental feedback for the controller, and the meta-controller receives external feedback from the environment.",
      "Each controller is represented as a Deep Q-Network, and the usual DRL learning tricks are applied.",
      "The framework is tested in two settings, a stochastic decision making problem and the Atari game Montezuma\u2019s Revenge.",
      "The hierarchical agent significantly outperforms DQN on Montezuma\u2019s Revenge (DQN gets 0 points).",
      "My Notes  The main contribution of this paper is the use of deep Q-networks for hierarchical/intrinsically motivated RL.",
      "However, the theory of intrinsically motivated RL and hierarchical RL with sub-policies for learning incremental behaviors under sparse extrinsic feedback is not novel; hence, the overall impact of the paper is substantially reduced.",
      "DQN is used as a baseline in the results/figures; but the reason for using such a questionable baseline is unclear.",
      "The authors even mention that Gorila DQN achieves a better average reward of 4.16; it would be better to see a more recent DRL algorithm used as a baseline.",
      "The set of goals, the intrinsic critic, and the external reward all still need to be hand-crafted for every learning problem- not a flaw of the research, since the motivation for this work was to handle sparse rewards.",
      "The algorithm relies doubly on epsilon-greedy exploration; an epsilon parameter is annealed for both policies.",
      "Even with the exploration decay schedule, the asymptotic variance of the total reward for the Montezuma experiment is pretty large.",
      "To solve Montezuma\u2019s Revenge, they implemented a \u201ccustom object detector\u201d to identify objects in the game such as the ladder and key.",
      "However, the authors only mention it in passing.",
      "Also, the details of the internal critic for the Montezuma agent are unclear.",
      "It appears that they defined certain relations such as \u201cagent reaches ladder\u201d and \u201cagent reaches key\u201d and used these for intrinsic rewards, but it is unclear exactly how they went about doing it.",
      "They claim that their method does not require explicit encoding of the relations between objects as well.",
      "Overall, the idea of using hierarchical DQN to learn a temporal abstraction is a promising one, and it should be explored more.",
      "Unsupervised discovery of sub-tasks/goals is still an open problem in RL as well.",
      "An interesting avenue to look into as mentioned by the authors is the use of evolutionary methods to search the space of reward functions ."
    ],
    "author_id": "pemami",
    "pdf_url": "http://arxiv.org/pdf/1604.06057v2.pdf",
    "author_full_name": "Patrick Emami",
    "source_website": "https://pemami4911.github.io/index.html",
    "id": 94730041
  },
  {
    "blog_id": "smooth-loss-functions-for-deep-top-k-classification",
    "summary": [
      "For top-k classification tasks, cross entropy is widely used as the learning objective even though it is the optimal metric only in the limit of infinite data.",
      "The paper introduces a family of smoothed loss functions that are specially designed for top-k optimization.",
      "Paper  Code  Idea  Inspired by the multi-loss SVMs, a surrogate loss (lk) is introduced that creates a margin between the ground truth and the kth largest score.",
      "Here s denotes the output of the classifier model to be learnt, y is the ground truth label, s[p] denotes the kth largest element of s and s\\p denotes the vector s without pth element.",
      "This lk loss has two limitations:  It is continous but not differentiable in s.  Its weak derivatives have at most 2-nonzero elements.",
      "The loss can be reformulated by adding and subtracting the k-1 largest scores of s\\y and sy and by introducing a temperature parameter \u03c4.",
      "Properties of Lk\u03c4  For any \u03c4 > 0, Lk\u03c4 is infinite-differentiable and has non-sparse gradients.",
      "Under mild conditions, Lk\u03c4 apporachs lk (in a pointwise sense) as \u03c4 approaches to 0++.",
      "It is an upper bound on the actual loss (up to a constant factor).",
      "It is a generalization of the cross-entropy loss for different values of k, and \u03c4 and higher margins.",
      "Computational Challenges  nCk number of terms needs to be evaluated for computing the loss for one sample (n is number of classes).",
      "Loss Lk\u03c4 can be expressed in terms of elementary symmetric polynomials \u03c3i(e) (sum of all products of i distinct elements of vector e).",
      "Thus the challenge is to compute \u03c3k efficiently.",
      "Forward Computation  Compute \u03c3k(e) where e is a n-dimensional vector and k\u00ab\u00a0n and e[i]!=0 for all i.  \u03c3i(e) can be computed using the coefficients of the polynomial (X+e1)(X+e2)\u2026(X+en) by divide and conquer approach with polynomial multiplication.",
      "With some more optimizations (eg log(n) levels of recursion and each level being parallelized on a GPU), the resulting algorithms scale well with n on a GPU.",
      "Operations are performed in the log-space using the log-sum-exp trick to achieve numerical stability in single floating point precision.",
      "Backward computation  The backward pass uses optimizations like computing derivative of \u03c3j with respect to ei in a recursive manner.",
      "Appendix of the paper describes these techniques in detail.",
      "Experiments  Experiments are performed on CIFAR-100 (with noise) and Imagenet.",
      "For CIFAR-100 with noise, the labels are randomized with probability p (within the same top-level class).",
      "The proposed loss function is very robust to both noise and reduction in the amount of training dataset as compared to cross-entropy loss function for both top-k and top-1 performance."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1812.00420",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 26955055
  },
  {
    "blog_id": "to-tune-or-not-to-tune-adapting-pretrained-representations-to-diverse-tasks",
    "summary": [
      "   The paper provides useful empirical advice for adapting pretrained language models for a given target task.",
      "Pre-trained models considered  ELMo  BERT  Tasks considered  Named Entity Recognition (NER) - CoNLL 2003 dataset  Sentiment Analysis (SA) - Stanford Sentiment Treebank (SST-2) dataset  Natural Language Inference (NLI) - MultiNLI and Sentences Involving Compositional Knowledge (SICK-E) dataset  Paraphrase Detection (PD) - Microsoft Research Paraphrase Corpus (MRPC)  Semantic Textual Similarity (STS) - Semantic Textual Similarity Benchmark (STS-B) and SICK-R  The last 3 tasks (NLI, PD, STS) are defined for sentence pairs.",
      "Adaptation Strategies  Feature Extraction  The pretrained model is only used for extracting features and its weights are kept fixed.",
      "For both ELMo and BERT, the contextual representation of the words from all the layers are extracted.",
      "A weighted combination of these layers is used as an input to the task-specific model.",
      "Task-specific models  NER - BiLSTM with CRF layer  SA - bi-attentive classification network  NLI, PD, STS - Enhanced Sequential Inference Model (ESIM)  Fine-tuning  The pretrained model is finetuned on the target task.",
      "Task-specific models for ELMO  NER - CRF on top of LSTM states  SA - Max-pool over the language model states followed by a softmax layer  NLI, PD, STS - cross sentence bi-attention between the language model states followed by pooling and softmax layer.",
      "Task-specific models for BERT  NER - Extract representation of the first-word piece of each token followed by the softmax layer  SA, NLI, PD, STS - standard BERT training  Main observations  Feature extraction and fine-tuning have comparable performance in most cases unless the two tasks are highly similar(fine-tuning is better) or highly dissimilar (feature extraction is better).",
      "For ELMo, feature extraction consistently outperforms fine-tuning for the sentence pair tasks (NLI, PD, STS).",
      "The reverse trend is observed for BERT with fine-tuning being better on sentence pair tasks.",
      "Adding extra parameters is helpful for feature extraction but not fine-tuning.",
      "ELMo fine-tuning requires careful tuning and other tricks like triangular learning rates, gradual unfreezing and discriminative fine-tuning.",
      "For the tasks considered, there is no correlation observed between the distance of the source and target domains and adaptation performance.",
      "Training a diagnostic classifier (on the intermediate representations) suggests that fine-tuning improves the performance of the classifier at all the intermediate layers (which is sort of expected).",
      "In terms of mutual information estimates, fine-tuned representations have a much higher mutual information as compared to the feature extraction based representations.",
      "Knowledge for single sentence tasks seems to be mostly concentrated in the last layers while for pair classification tasks, the knowledge seems gradually build un in the intermediate layers, all the way up to the last layer."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1903.05987",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 16973018
  },
  {
    "blog_id": "maclaurinda15",
    "summary": [
      "This is another \"learning the learning rate\" paper, which predates (and might have inspired) the \"Speed learning on the fly\" paper I recently wrote notes about (see  [ref] ).",
      "In this paper, they consider the off-line training scenario, and propose to do gradient descent on the learning rate by unrolling the *complete* training procedure and treating it all as a function to optimize, with respect to the learning rate.",
      "This way, they can optimize directly the validation set loss.",
      "The paper in fact goes much further and can tune many other hyper-parameters of the gradient descent procedure: momentum, weight initialization distribution parameters, regularization and input preprocessing.",
      "#### My two cents  This is one of my favorite papers of this year.",
      "While the method of unrolling several steps of gradient descent (100 iterations in the paper) makes it somewhat impractical for large networks (which is probably why they considered 3-layer networks with only 50 hidden units per layer), it provides an incredibly interesting window on what are good hyper-parameter choices for neural networks.",
      "Note that, to substantially reduce the memory requirements of the method, the authors had to be quite creative and smart about how to encode changes in the network's weight changes.",
      "There are tons of interesting experiments, which I encourage the reader to go check out (see section 3).",
      "One experiment on training the learning rates, separately for each iteration (i.e. learning a learning rate schedule), for each layer and for either weights or biases (800 hyper-parameters total) shows that a good schedule is one where the top layer first learns quickly (large learning), then the bottom layer starts training faster, and finally the learning rates of all layers is decayed towards zero.",
      "Note that some of the experiments presented actually optimized the training error, instead of the validation set error.",
      "Another looked at finding optimal scales for the weight initialization.",
      "Interestingly, the values found weren't that far from an often prescribed scale of $1 / \\sqrt{N}$, where $N$ is the number of units in the previous layer.",
      "The experiment on \"training the training set\", i.e. generating the 10 examples (one per class) that would minimize the validation set loss of a network trained on these examples is a pretty cool idea (it essentially learns prototypical images of the digits from 0 to 9 on MNIST).",
      "Another experiment tried to optimize a multitask regularization matrix, in order to encourage forms of soft-weight-tying across tasks.",
      "Note that approaches like the one in this paper make tools for automatic differentiation incredibly valuable.",
      "Python autograd, the author's automatic differentiation Python library  [url]"
    ],
    "author_id": "hlarochelle",
    "pdf_url": "http://proceedings.mlr.press/v37/maclaurin15.pdf",
    "author_full_name": "Hugo Larochelle",
    "source_website": "https://www.shortscience.org/user?name=hlarochelle",
    "id": 72788236
  },
  {
    "blog_id": "2e665b27696ce0436c79174a136410",
    "summary": [
      "The paper presents a novel open vocabulary NMT(Neural Machine Translation) system that translates mostly at word level and falls back to character level models for rare words.",
      "Advantages:  Faster and easier to train as compared to character models.",
      "Does not produce unknown words in the translations which need to be removed using unk replacement techniques.",
      "Unk Replacement Technique  Most NMT operate on constrained vocabulary and represent unknown words with unk token.",
      "A post-processing step replaces unk tokens with actual words using alignment information.",
      "Disadvantages:  These systems treat words as independent entities while they are morphologically related.",
      "Difficult to capture things like name translation.",
      "Proposed Architecture  Word-level NMT  Deep LSTM encoder-decoder.",
      "Global attention mechanism and bilinear attention scoring function.",
      "Similar to regular NMT system except in the way unknown words are handled.",
      "Character-level NMT  Deep LSTM model used to generate on-the-fly representation of rare words (using final hidden state from the top layer).",
      "Advantages:  Simplified architecture.",
      "Efficiency through precomputation - representations for rare sources words can be computed at once before each mini-batch.",
      "The model can be trained easily in an end-to-end fashion.",
      "Hidden-state Initialization  For source representation, layers of the LSTM are initialized with zero hidden states and cell values.",
      "For target representation, the same strategy is followed except for the hidden state of the first layer where one of the following approaches are used:  same-path target generation approach  Use the context vector just before softmax (of word-level NMT).",
      "seperate-path target generation approach  Learn a new weight matrix W that will be used to generate the context vector.",
      "Training Objective  J = Jw + \u03b1Jc  J - total loss  Jw - loss in a regular word-level NMT  \u03b1Jc - loss in the character-level NMT  Word Character Generation Strategy  The final hidden state from character-level decoder could be interpreted as the representation of unk token but this approach would not be efficient.",
      "Instead, unk is fed to the word-level decoder as it is so as to decouple the execution for the character-level model as soon the word-level model finishes.",
      "During testing, a beam search decoder is run at the word level to find the best translation using the word NMT alone.",
      "Next, a character-level encoder is used to generate the words in place of unk to minimise the combined loss.",
      "Experiments  Data  WMT\u201915 translation task from English into Czech with newstest2013 (3000 sentences) as dev set and newstest2015 (2656 sentences) as a test set.",
      "Metrics  Case-sensitive NIST BLEU.",
      "chrF3  Models  Purely word based  Purely character based  Hybrid (proposed model)  Observations  Hybrid model surpasses all the other systems (neural/non-neural) and establishes a new state-of-the-art result for English-Czech translation in WMT\u201915 with 19.9 BLEU.",
      "Character-level models, when used as a replacement for the standard unk replacement technique in NMT, yields an improvement of up to +7.9 BLEU points.",
      "Attention is very important for character-based models as the non-attentional character models perform poorly.",
      "Character models with shorter time-step backpropagation perform inferior as compared to ones with longer backpropagation.",
      "Separate-path strategy outperforms same-path strategy.",
      "Rare word embeddings  Obtain representations for rare words.",
      "Compare the Spearman correlation between similarity scores assigned by humans and by the model.",
      "Outperforms the recursive neural network model (which also uses a morphological analyser) on this task."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1604.00788",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 17409856
  },
  {
    "blog_id": "neural-relational-inference-for-interacting-systems",
    "summary": [
      "The paper presents Neural Relational Inference (NRI) model which can infer underlying interactions in a dynamical system in an unsupervised manner, using just the observational data in terms of the trajectories.",
      "For instance, consider a simulated system where the particles are connected to each other by springs.",
      "The observational data does not explicitly specify which particles are connected to each other and only contains information like position and velocity of each particle at different timesteps.",
      "The task is to explicitly infer the interaction structure (in this example, which pair of particles are connected to each other) while learning the dynamical model of the system itself.",
      "Link to the implementation  Model  The model consists of an encoder that encodes the given trajectories into an interaction graph and a decoder that decodes the dynamical model given the interaction graph.",
      "The model starts by assuming that a full connected interaction graph exists between the objects in the system.",
      "For this latent graph z, zi, j denotes the (discrete) edge type between object vi and vj with the assumption that there are K edge types.",
      "The object vi has a feature vector xit associated with it at time t. This feature vector captures information like location and velocity.",
      "Encoder  A Graph Neural Network (GNN) acts on the fully connected latent graph z, performs message passing from node to node via edges and predicts the discrete label for each edge.",
      "The GNN architecture may itself use MLPs or ConvNets and returns a factorised distribution over the edge types q\u03c6(z|x).",
      "Decoder  The decoder is another GNN (with separate params for each edge type) that predicts the future dynamics of the system and returns p\u03b8(x|z).",
      "The overall model is a VAE that optimizes the ELBO given as:  Eq\u03c6(z|x)[log p\u03b8(x|z)] \u2212 KL[q\u03c6(z|x)||p\u03b8(z)]  p\u03b8(x) is the prior which is assumed to be uniform distribution over the edge types.",
      "Instead of predicting the dynamics of the system for just the next timestep, the paper chooses to use the prediction multiple steps (10) in the future.",
      "This ensures that the interactions can have a significant effect on the dynamics of the system.",
      "In some cases, like real humans playing a physical sport, the dynamics of the system need not be Markovian and a recurrent decoder is used to model the time dependence.",
      "Pipeline  Given the dynamical system, run the encoder to obtain q\u03c6(z|x).",
      "Sample zi, j from q\u03c6(z|x).",
      "Run the decoder to predict the future dynamics for the next T timesteps.",
      "Optimise the ELBO loss.",
      "Note that since the latent variables (edge labels) are discrete in this case, the sampling is done from a continuous approximation of the discrete distribution and reparameterization trick is applied over this discrete approximation to get the (biased) gradients.",
      "Observations  Experiments are performed using simulated systems like particles connected to springs, phase coupled oscillators and charged particles and using real-world data like CMU Motion Capture database and NBA tracking data.",
      "The NRI system effectively predicts the dynamics of the systems and is able to reconstruct the ground truth interaction graph (for simulated systems)."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1802.04687",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 69694249
  },
  {
    "blog_id": "8d8de0034a350d97738bbedadc9373",
    "summary": [
      "Given a video, can a machine learning system detect the arrow of time and distinguish whether the video is running forward or backwards.",
      "Datasets  Youtube Dataset  180 short videos (6-10 seconds) are manually selected using keywords like \"dance\", \"stream trains\" etc.",
      "155 forward videos and 25 reverse videos - highly imbalanced dataset!!",
      "Tennis Ball Dataset  Recorded 13 HD videos of tennis ball rolling and colliding on the floor.",
      "Methods  Baseline  Spatial-temporal Oriented Energy (SOE) as off the shelf feature extractor.",
      "Split videos into 2x2 spatial subregions and concatenate SOE features to obtain final features.",
      "These final features are fed to linear SVM classifier and the performance varies from 48% to 60%.",
      "One reason for poor performance could be the difficulty in generalising motion over different sub-regions.",
      "Statistical Flow Method  Idea is to capture local regions of motion in a video to examine what type of motion is a good feature for detecting the arrow of time.",
      "Flow Words are object-motion descriptors based on SIFTlike descriptors and capture motion occurring in small patches of videos.",
      "These descriptors are motion quantized to obtain a discrete set of flow words.",
      "The entire video sequence can be encoded as a bag of flow-word descriptors which becomes the features for the learning system.",
      "Training  For each video, 4 descriptor histograms were extracted:  (A): the native direction of the video  (B): this video mirrored in the left-right direction  (C): the original video time-flipped  (D): the time-flipped left-right-mirrored version  Train an SVM using the 4 histograms and combine their scores as A + B - C - D expecting a positive result for forwarding clips and negative for backwards clips.",
      "Result  Performance varies from 75% to 90%  Motion-Causation Method  Idea is to capture motion causing other motions as it is more common for one motion to cause multiple motions instead of multiple motions collapsing into one motion.",
      "The system looks at the regions in the video from frame to frame with the expectation that, in the forwards-time direction, there would be more occurrences of one region splitting in two than of two regions joining to become one.",
      "Result  Performance varies from 70% to 73%.",
      "Though it underperforms as compared to the flow-word method, it can complement that method as Motion-causation considers the spatial location of motions while flow-word method considers motion in each frame separately.",
      "AR Method  Idea is to model the problem as that of inferring casual direction in cause-effect models.",
      "The assumption is that some image motions will be modelled as AR models with additive non-Gaussian noise.",
      "In such a scenario, noise added at some point in time, is independent of the past values of the time series but not of future values.",
      "This allows independence tests to be performed for determining the direction of time.",
      "Result  There is a tradeoff between the accuracy achieved by the system versus the number of videos it can classify (depending on the value of delta for p-test).",
      "Comment  The paper poses a new and interesting research problem but uses a very small dataset which makes the results inconclusive in my opinion."
    ],
    "author_id": "shugan",
    "pdf_url": "https://www.robots.ox.ac.uk/~vgg/publications/2014/Pickup14/pickup14.pdf",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 86423785
  },
  {
    "blog_id": "7f0d5c1dfe60ce5da0dd8241e506ea",
    "summary": [
      "The paper presents NewsQA, a machine comprehension dataset of 119,633 natural language questions obtained from 12,744 CNN articles.",
      "Issues With Existing Datasets  Too small - eg MCTest  USe synthetically generated questions - eg BookTest Dataset  SQuAD is similar to NewsQA but is not as challenging and diverse as NewsQA.",
      "Desired Characteristics Of A Machine Comprehension Dataset  Answers of arbitrary length instead of candidate answers to choose from.",
      "Some questions should have no correct answer in the document.",
      "Lexical and syntactic divergence between questions and answers.",
      "Questions should require reasoning beyond simple word and context matching.",
      "Collection Methodology  Article Curation  Retrieve and sample articles from CNN.",
      "Partition data into a training set (90%), a development set (5%), and a test set (5%).",
      "Question Sourcing  Questioners see a news article's headline and its summary and use that to formulate questions about the article.",
      "Answer Sourcing  Answerers receive the questions along with the full article.",
      "They either mark the answer in the article or reject the question as nonsensical or select null answer if the article contains insufficient information.",
      "Validation  Another set of crowd workers sees the full article, a question and the set of unique answers to that question.",
      "They either choose the best answer among the candidates or reject all the answers.",
      "Data Analysis  Answer Types  Linguistically diverse answer set with following distribution:  common noun phrases (22.2%), clause phrase (18.3%), person (14.8%), numeric (9.8%), and other (11.2%) types.",
      "Reasoning Types  Type of reasoning required, in ascending order of difficulty, along with approx.",
      "percentage of questions:  Word Matching (32.7%)  Paraphrasing (27%)  Inference (13.2%)  Synthesis (20.7%)  Ambiguous/Insufficient (6.4%)  Baseline Models  match-LSTM  LSTM network encodes the article and the question as sequences of hidden states.",
      "mLSTM network compares the article encodings with the question encodings.",
      "A Pointer Network uses the hidden states of the mLSTM to select the boundaries of the answer span.",
      "Bilinear Annotation Re-encoding Boundary (BARB) Model  Encode all the words in the articles and the question using GloVe embeddings and further into contextual states using GRU.",
      "Compare the document and the question encodings using C bilinear transformations to obtain the tensor of annotation scores.",
      "Take the maximum over the question-token dimension to obtain annotation over document word dimension.",
      "For each document word, input the document encodings, annotation vector and binary feature (indicating whether the document appears in the question) to the re-encoding RNN and obtain encodings for the boundary-pointing stage.",
      "Use convolutional networks to determine the boundaries of answer span (similar to edge detection).",
      ".",
      "Observations  Gap between human and machine performance on NewsQA is much higher than that for SQuAD probably because of longer sentences in NewsQA.",
      "This suggests that NewsQA is a far more challenging dataset than SQuAD and presents a large scope for improvement for machine comprehension tasks.",
      "Questions requiring inference and synthesis are more challenging for the model as compared to other kinds of questions.",
      "Interestingly, BARB outperforms human annotators on SQuAD in terms of answering ambiguous questions or those with incomplete information."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1611.09830",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 8087311
  },
  {
    "blog_id": "improving-information-extraction-by-acquiring-external-evidence-with-reinforcement-learning",
    "summary": [
      "Information Extraction  - Given a query to be answered and an external search engine, information extraction entails the task of issuing search queries, extracting information from new sources and reconciling the extracted values till we are sufficiently confident about the extracted values.",
      "The paper proposes the use of Reinforcement Learning (RL) to solve this task.",
      "Implementation  Key Aspect  Use of Reinforcement Learning to resolve the ambiguity inherent in the textual documents.",
      "Given a query, the RL agent would use template statement to formulate the queries (to be performed on the black box search engine).",
      "It would further resolve and combine the result for the query from the set of retrieved documents.",
      "Datasets  Database of Mass Shootings in the United States.",
      "Food Shield database of illegal food adulteration.",
      "Framework  Information extraction task is modelled as a Markov Decision Process (MDP) <S, A, T, R>  S - Set of all possible states  The state consists of:  Extractor\u2019s confidence in predicted entity values.",
      "Context from which values are extracted.",
      "Similarity between the new document (extracted just now from the search engine) and the original document accompanying the given query.",
      "A - Set of all possible actions  Reconciliation decision - d  Accept all entities values.",
      "Reject all entities values.",
      "Stop the current episode.",
      "Query choice - q  Choose the next query from a set of automatically generated alternatives.",
      "R - Rewards  Maximise the final extraction accuracy while minimising the number of queries.",
      "Q - Queries  Generated using a template.",
      "The query is searched on a search engine and the top k links are retrieved.",
      "Transition  Start with a single source article xi and extract the initial set of entities.",
      "At each timestep, the agent is given the state (s) on basis of which it chooses the action (d, q).",
      "The episode stops whenever the action is a stop action.",
      "Deep Q Network is used.",
      "Parameters are learned using SGD and RMSProp.",
      "Experimental Setup  Extraction Model  Max Entropy Classifier is used as the base extraction system.",
      "First, all the words in the document are tagged as one of the entity types and the mode of these values is used to obtain the set of extracted entities.",
      "Baseline  Basic Extractors  Aggregation System which either chooses the entity value with the highest confidence or takes a majority vote over all extracted values.",
      "Meta-Classifier which operates over the same input state space and produces the same set of reconciliation decisions as the DQN.",
      "Oracle Extractor which is computed assuming perfect reconciliation and query decisions on the top of the Maxnet base extractor.",
      "RL Models  RL Basic - Only reconciliation decision.",
      "RL Query - Only query decision with a fixed reconciliation strategy.",
      "RL Extract - the full system with both reconciliation and query decision.",
      "Result  RL Extract obtains substantial gains eg up to 11% over Maxnet.",
      "Simple aggregation schemes do not handle the task well.",
      "In terms of reward structure, providing rewards after each step works better than a single delayed reward."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1603.07954",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 66099843
  },
  {
    "blog_id": "blumhp15",
    "summary": [
      "This paper starts by introducing a trick to reduce the variance of stochastic gradient variational Bayes (SGVB) estimators.",
      "In neural networks, SGVB consists in learning a variational (e.g. diagonal Gaussian) posterior over the weights and biases of neural networks, through a procedure that (for the most part) alternates between adding (Gaussian) noise to the model's parameters and then performing a model update with backprop.",
      "The authors present a local reparameterization trick, which exploits the fact that the Gaussian noise added into the weights could instead be added directly into the pre-activation (i.e. before the activation fonction) vectors during forward propagation.",
      "This is due to the fact that computing the pre-activation is a linear operation, thus noise at that level is also Gaussian.",
      "The advantage of doing so is that, in the context of minibatch training, one can efficiently then add independent noise to the pre-activation vectors for each example of the minibatch.",
      "The nature of the local reparameterization trick implies that this is equivalent to using one corrupted version of the weights for each example in the minibatch, something that wouldn't be practical computationally otherwise.",
      "This is in fact why, in normal SGVB, previous work would normally use a single corrupted version of the weights for all the minibatch.",
      "The authors demonstrate that using the local reparameterization trick yields stochastic gradients with lower variance, which should improve the speed of convergence.",
      "Then, the authors demonstrate that the Gaussian version of dropout (one that uses multiplicative Gaussian noise, instead of 0-1 masking noise) can be seen as the local reparameterization trick version of a SGVB objective, with some specific prior and variational posterior.",
      "In this SGVB view of Gaussian dropout, the dropout rate is an hyper-parameter of this prior, which can now be tuned by optimizing the variational lower bound of SGVB.",
      "In other words, we now have a method to also train the dropout rate!",
      "Moreover, it becomes possible to tune an individual dropout rate parameter for each layer, or even each parameter of the model.",
      "Experiments on MNIST confirm that tuning that parameter works and allows to reach good performance of various network sizes, compared to using a default dropout rate.",
      "##### My two cents  This is another thought provoking connection between Bayesian learning and dropout.",
      "Indeed, while Deep GPs have allowed to make a Bayesian connection with regular (binary) dropout learning  [ref] , this paper sheds light on a neat Bayesian connection for the Gaussian version of dropout.",
      "This is great, because it suggests that Gaussian dropout training is another legit way of modeling uncertainty in the parameters of neural networks.",
      "It's also nice that that connection also yielded a method for tuning the dropout rate automatically.",
      "I hope future work (by the authors or by others) can evaluate the quality of the corresponding variational posterior in terms of estimating uncertainty in the network and, in particular, in obtaining calibrated output probabilities.",
      "Little detail: I couldn't figure out whether the authors tuned a single dropout rate for the whole network, or used many rates, for instance one per parameter, as they suggest can be done."
    ],
    "author_id": "hlarochelle",
    "pdf_url": "http://papers.nips.cc/paper/5666-variational-dropout-and-the-local-reparameterization-trick.pdf",
    "author_full_name": "Hugo Larochelle",
    "source_website": "https://www.shortscience.org/user?name=hlarochelle",
    "id": 85853580
  },
  {
    "blog_id": "unsupervised_image-to-image_translation_networks",
    "summary": [
      "What  They present a method to learn mapping functions that transform images from one style to another style.",
      "(E.g. photos from daylight to nighttime.)",
      "Their method only requires example images for both styles (i.e. class labels per image).",
      "How  Architecture  Their method is based on VAEs (i.e. autoencoders) and GANs.",
      "Their architecture is kinda similar to an autoencoder.",
      "For an image style A, an encoder E first transform an image to a vector representation z.",
      "Then a generator G transforms z into an image.",
      "There are two encoders (E1, E2), one per image style (A, B).",
      "There are two generators (G1, G2), one per image style (A, B).",
      "There are two discriminators (D1, D2), one per generator (and therefore style).",
      "An image can be changed in style from A to B using e.g. G2(E1(I_A)).",
      "The weights of the encoders are mostly tied/shared.",
      "Only the last layers are not-shared.",
      "The weights of the generators are mostly tied/shared.",
      "Only the last layers are not-shared.",
      "They use 3 convs + 4 residual blocks for the encoders and 4 residual blocks + 3 transposes convs for the generators.",
      "They use normal convs for the discriminators.",
      "Nonlinearities are LeakyReLUs.",
      "The encoders are VAEs and trained with common VAE-losses (i.e. lower bound optimization).",
      "However, they only predict mean values per component in z, not variances.",
      "The variances are all 1.",
      "Visualization of the architecture:  Loss  Their loss consists of three components:  VAE-loss: Reconstruction loss (absolute distance) and KL term on z (to keep it close to the standard normal distribution).",
      "Most weight is put on the reconstruction loss.",
      "GAN-loss: Standard as in other GANs, i.e. cross-entropy.",
      "Cycle-Consistency-loss: For an image I_A, it is expected to look the same after switching back and forth between image styles, i.e. I_A = G1(E2( G2(E1(I_A)) )) (switch from style A to B, then from B to A).",
      "The cycle consistency loss uses a reconstruction loss and two KL-terms (one for the first E(.)",
      "and one for the second).",
      "Results  When testing on the (satellite) map dataset:  Weight sharing between encoders and between generators improved accuracy.",
      "The cycle consistency loss improved accuracy.",
      "Using 4-6 layers (as opposed to just 3) in the discriminator improved accuracy.",
      "Translations that added details (e.g. night to day) were harder for the model.",
      "After training, the features from each discriminator seem to be quite good for the respective dataset (i.e. unsupervised learned features).",
      "Example translations:"
    ],
    "author_id": "ALEJU",
    "pdf_url": "https://arxiv.org/pdf/1703.00848",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 45043151
  },
  {
    "blog_id": "an-empirical-investigation-of-catastrophic-forgetting-in-gradient-based-neural-networks",
    "summary": [
      "Catastrophic Forgetting refers to the phenomenon where when a learning system is trained on two tasks in succession, it may forget how to perform the first task.",
      "The paper investigates this behaviour for different learning activations in presence and absence of dropout.",
      "Link to the implementation  Experiment Formulation  For each experiment, two tasks are defined - \u201cold\u201d task and \u201cnew\u201d task.",
      "The network is first trained on the \u201cold\u201d task until the validation set error has not improved for the last 100 epochs.",
      "The \u201cbest\u201d performing model is then trained for the \u201cnew\u201d task until the combined error on the \u201cold\u201d and the \u201cnew\u201d validation datasets has not improved in the last 100 epochs.",
      "All the tasks used the same model architecture - 2 hidden layers followed by a softmax layer.",
      "Following activations were tested:  Sigmoid  ReLU  Hard Local Winner Takes It All  Maxout  Models were trained using SGD with or without dropout.",
      "For each combination of the model, activation and the training mechanism, a random hyper param search was performed with set of 25 hyperparams.",
      "The authors took care to keep the hyperparams and other settings consistent and comparable across different experiments.",
      "Deviations, wherever applicable, and their reasons were documented.",
      "Observations  In terms of the relationship between the \u201cold\u201d and the \u201cnew\u201d tasks, three kinds of settings are considered:  The tasks are very very similar but the input is processed in a different format.",
      "For this setting, MNIST dataset was used with a different permutation of pixels for the \u201cold\u201d and the \u201cnew\u201d task.",
      "The tasks are similar but not exactly the same.",
      "For this setting, the task was to predict sentiments of reviews across 2 different product categories.",
      "In the last setting, 2 dissimilar tasks were used.",
      "One task was to predict sentiment of reviews and another task was to perform classification over MNIST dataset (reduced to 2 classes).",
      "Using Dropout improved the overall validation performance for all the models for all the tasks.",
      "Using Dropout also increase the size of the optimal model across all the activations indicating that maybe the increased size of the model could explain the increased resistance to forgetting.",
      "It would have been interesting to check if dropout always selected the largest model possible given the set of the hyperparams.",
      "On the dissimilar task, dropout improved the performance while reducing the model size so it might have other properties as well that helps to prevent forgetting.",
      "As compared to the choice of training technique, the activation function has a less consistent effect on resistance to forgetting.",
      "The paper recommends performing cross-validation for the choice of the activation function.",
      "If that is not feasible, maxout activation function with dropout could be used."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1312.6211",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 7120002
  },
  {
    "blog_id": "fractalnet_ultra-deep_networks_without_residuals",
    "summary": [
      "What  They describe an architecture for deep CNNs that contains short and long paths.",
      "(Short = few convolutions between input and output, long = many convolutions between input and output)  They achieve comparable accuracy to residual networks, without using residuals.",
      "How  Basic principle:  They start with two branches.",
      "The left branch contains one convolutional layer, the right branch contains a subnetwork.",
      "That subnetwork again contains a left branch (one convolutional layer) and a right branch (a subnetwork).",
      "This creates a recursion.",
      "At the last step of the recursion they simply insert two convolutional layers as the subnetwork.",
      "Each pair of branches (left and right) is merged using a pair-wise mean.",
      "(Result: One of the branches can be skipped or removed and the result after the merge will still be sound.)",
      "Their recursive expansion rule (left) and architecture (middle and right) visualized:  Blocks:  Each of the recursively generated networks is one block.",
      "They chain five blocks in total to create the network that they use for their experiments.",
      "After each block they add a max pooling layer.",
      "Their first block uses 64 filters per convolutional layer, the second one 128, followed by 256, 512 and again 512.",
      "Drop-path:  They randomly dropout whole convolutional layers between merge-layers.",
      "They define two methods for that:  Local drop-path: Drops each input to each merge layer with a fixed probability, but at least one always survives.",
      "(See image, first three examples.)",
      "Global drop-path: Drops convolutional layers so that only a single columns (and thereby path) in the whole network survives.",
      "(See image, right.)",
      "Visualization:  Results  They test on CIFAR-10, CIFAR-100 and SVHN with no or mild (crops, flips) augmentation.",
      "They add dropout at the start of each block (probabilities: 0%, 10%, 20%, 30%, 40%).",
      "They use for 50% of the batches local drop-path at 15% and for the other 50% global drop-path.",
      "They achieve comparable accuracy to ResNets (a bit behind them actually).",
      "Note: The best ResNet that they compare to is \"ResNet with Identity Mappings\".",
      "They don't compare to Wide ResNets, even though they perform best.",
      "If they use image augmentations, dropout and drop-path don't seem to provide much benefit (only small improvement).",
      "If they extract the deepest column and test on that one alone, they achieve nearly the same performance as with the whole network.",
      "They derive from that, that their fractal architecture is actually only really used to help that deepest column to learn anything.",
      "(Without shorter paths it would just learn nothing due to vanishing gradients.)"
    ],
    "author_id": "ALEJU",
    "pdf_url": "http://arxiv.org/pdf/1605.07648v1",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 42045072
  },
  {
    "blog_id": "kronecker-recurrent-units",
    "summary": [
      "Recurrent Neural Networks have two key issues:  Over parameterization which increases the time for training and inference.",
      "Ill conditioned recurrent weight matrix which makes training difficult due to vanishing or exploding gradients.",
      "The paper presents a flexible RNN model called as KRU (Kronecker Recurrent Units) which overcomes the above problems by using a Kronecker factored recurrent matrix and soft unitary constraints on the factors.",
      "Related Work  Existing solutions for overparameterization  Low-rank decomposition.",
      "Training a neural network on the soft targets predicted by a big pre-trained network.",
      "Low-bit precision training.",
      "Hashing.",
      "Existing solutions for vanishing and exploding gradients  Gating mechanism like in LSTMs.",
      "Gradient Clipping.",
      "Orthogonal Weight Initialization.",
      "Parameterizing recurrent weight matrix.",
      "KRU  Uses a Kronecker factored recurrent matrix which enables controlling the number of parameters and number of factor matrices.",
      "Vanishing and exploding gradients are taken care of by using a soft unitary constraint.",
      "Why not use strict unitary constraint:  Restricts the search space and makes learning process unstable.",
      "Makes forgetting (irrelevant) information difficult.",
      "Relaxing the strict constraint has shown to improve the convergence speed and generalization performance.",
      "KRU can be easily plugged into RNNs, LSTMs and other variants.",
      "The recurrent matrix W is paramterized as a kronecker product of F matrices W0, \u2026, WF-1 where each Wf is a complex matrix of shape Pf x Qf and the product of all Pf and producto of all Qf are both equal to N.  Why is W a complex matrix?",
      "In the real space, the set of all unitary matrices have the determinant as 1 or -1.",
      "Given that determinant is a continuous function, the unitary set in the real space is disconnected.",
      "The unitary set in the complex space is connected as its determinants are points on the unit circle.",
      "Soft Unitary Constraint  A soft unitary constraint is introduced in the form of regularization term  \u00a0  WfHWf - I  \u00a0  2 (per kronecker factored recurrent matrix).",
      "If each of the Kronecker factors is unitary, the resulting matrix W would also be unitary.",
      "It is computationally inefficient to apply this constraint over the recurrent matrix W itself as the complexity of the regularizer is given as O(N3).",
      "Use of Kronecker factorisation makes it computationally feasible to use this regulariser.",
      "Experiment  The Kronecker recurrent model is compared against the existing recurrent models for multiple tasks including copy memory, adding memory, pixel-by-pixel MNIST, char level language models, polyphonic music modelling, and framewise phoneme classification.",
      "For most of the task, KRU model produces results comparable to the best performing models despite using fewer parameters.",
      "Using soft unitary constraints in KRU provides a principled alternative to gradient clipping (a common heuristic to avoid exploding gradients).",
      "Further, recent theoretical results suggest the gradient descent converges to a global optimizer of linear recurrent networks even if the learning problem is non-convex provided that the spectral norm of the recurrent matrix is bound by 1.",
      "The key take away from the paper is that state should be high dimensional so that high capacity network can be used for encoding and decoding the input and output.",
      "The recurrent dynamics should be implemented via a low capacity model.s per task."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1705.10142",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 65123124
  },
  {
    "blog_id": "pixel_recurrent_neural_networks",
    "summary": [
      "Note: This paper felt rather hard to read.",
      "The summary might not have hit exactly what the authors tried to explain.",
      "What  The authors describe multiple architectures that can model the distributions of images.",
      "These networks can be used to generate new images or to complete existing ones.",
      "The networks are mostly based on RNNs.",
      "How  They define three architectures:  Row LSTM:  Predicts a pixel value based on all previous pixels in the image.",
      "It applies 1D convolutions (with kernel size 3) to the current and previous rows of the image.",
      "It uses the convolution results as features to predict a pixel value.",
      "Diagonal BiLSTM:  Predicts a pixel value based on all previous pixels in the image.",
      "Instead of applying convolutions in a row-wise fashion, they apply them to the diagonals towards the top left and top right of the pixel.",
      "Diagonal convolutions can be applied by padding the n-th row with n-1 pixels from the left (diagonal towards top left) or from the right (diagonal towards the top right), then apply a 3x1 column convolution.",
      "PixelCNN:  Applies convolutions to the region around a pixel to predict its values.",
      "Uses masks to zero out pixels that follow after the target pixel.",
      "They use no pooling layers.",
      "While for the LSTMs each pixel is conditioned on all previous pixels, the dependency range of the CNN is bounded.",
      "They use up to 12 LSTM layers.",
      "They use residual connections between their LSTM layers.",
      "All architectures predict pixel values as a softmax over 255 distinct values (per channel).",
      "According to the authors that leads to better results than just using one continuous output (i.e. sigmoid) per channel.",
      "They also try a multi-scale approach: First, one network generates a small image.",
      "Then a second networks generates the full scale image while being conditioned on the small image.",
      "Results  The softmax layers learn reasonable distributions.",
      "E.g. neighboring colors end up with similar probabilities.",
      "Values 0 and 255 tend to have higher probabilities than others, especially for the very first pixel.",
      "In the 12-layer LSTM row model, residual and skip connections seem to have roughly the same effect on the network's results.",
      "Using both yields a tiny improvement over just using one of the techniques alone.",
      "They achieve a slightly better result on MNIST than DRAW did.",
      "Their negative log likelihood results for CIFAR-10 improve upon previous models.",
      "The diagonal BiLSTM model performs best, followed by the row LSTM model, followed by PixelCNN.",
      "Their generated images for CIFAR-10 and Imagenet capture real local spatial dependencies.",
      "The multi-scale model produces better looking results.",
      "The images do not appear blurry.",
      "Overall they still look very unreal.",
      "Generated ImageNet 64x64 images.",
      "Completing partially occluded images."
    ],
    "author_id": "ALEJU",
    "pdf_url": "http://arxiv.org/pdf/1601.06759",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 47723293
  },
  {
    "blog_id": "learning-to-prove-theorems-via-interacting-with-proof-assistants",
    "summary": [
      "Learning to prove theorems via interacting with proof assistants Yang & Deng, ICML\u201919  Something a little different to end the week: deep learning meets theorem proving!",
      "It\u2019s been a while since we gave formal methods some love on The Morning Paper, and this paper piqued my interest.",
      "You\u2019ve probably heard of Coq , a proof management system backed by about 30 years of research and developed out of INRIA.",
      "Here\u2019s how the Coq home page introduces it:  Coq is a formal proof management system.",
      "It provides a formal language to write mathematical definitions, executable algorithms and theorems together with an environment for semi-interactive development of machine-checked proofs.",
      "Certified programs can then be extracted to languages like OCaml, Haskell, and Scheme.",
      "In fully automated theorem proving (ATP), after providing a suitable formalism and a theorem, the goal is to be able to push a button and have the system prove that the theorem is true (or false!).",
      "The state-of-the-art in ATP is still some way behind human experts though it two key areas: the scale of systems it can tackle, and the interpretability of the generated proofs.",
      "What a typical theorem prover does\u2026 is to prove by resolution refutation: it converts the premises and the negation of the theorem into first-order clauses in conjunctive normal form.",
      "It then keeps generating new clauses by applying the resolution rule until an empty clause emerges, yielding a proof consisting of a long sequence of CNFs and resolutions.",
      "Enter interactive theorem proving (ITP).",
      "In ITP a human expert guides the structure of the proof, telling the system which proof tactics to use at each step:  The tactics capture high-level proof techniques such as induction, leaving low-level details to the software referred to as proof assistants.",
      "( Enlarge )  It would be nice if we could learn from all the ITP proofs that humans have generated, and use those learnings to improve the proof steps that a system can take automatically:  \u2026 human experts have written a large amount of ITP code, which provides an opportunity to develop machine learning systems to imitate humans for interacting with the proof assistant.",
      "Such a combination would get us closer to fully automated human-like proofs.",
      "To pull all this together we need three things:  A large ITP dataset we can learn from  A learning environment for training and evaluating auto-ITP agents.",
      "Agents start with a set of premises and a theorem to prove, and then they interact with the proof assistant by issuing a sequence of tactics.",
      "A way to learn and generate new tactics which can be used in the construction of those proofs.",
      "The first two requirements are satisfied by CoqGym, and ASTactic handles tactic generation.",
      "CoqGym  CoqGym (  [url]"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://proceedings.mlr.press/v97/yang19a/yang19a.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 24510148
  },
  {
    "blog_id": "relational-reinforcement-learning",
    "summary": [
      "Relational Reinforcement Learning (RRL) paradigm uses relational state (and action) space and policy representation to leverage the generalization capability of relational learning for reinforcement learning.",
      "The paper shows that effectiveness of RRL - in terms of generalization, sample efficiency and interplay - using box-world and StarCraft II minigames.",
      ".",
      "Architecture  The main idea is to use neural network models that operate on structured representations and perform relational reasoning via iterated, message-passing style methods.",
      "Use of non-local computations using a shared function (in terms of pairwise interactions between entities) provides a better inductive bias.",
      "Multi-head dot product attention mechanism is used to model the pairwise interactions (with one or more attention blocks).",
      "Iterative computations can be used to capture higher-order interactions between entities.",
      "Entity extraction is based on the assumption that entities are things located at a particular point in space.",
      "A CNN is used to parse the pixel space observation into k feature maps of size nxn.",
      "The (x, y) coordinates are concatenated to each k-dimensional pixel feature-vector to indicate the pixel\u2019s position in the map.",
      "The resulting n2 x k matrix acts as the entity matrix.",
      "Actor-critic architecture (using distributed agent IMPALA) is used.",
      "Environment  Box-World  12 x 12-pixel room with keys and boxes placed randomly.",
      "Agent can move in 4 directions.",
      "The task is to collect gems by unlocking boxes (which may contain keys to unlock other boxes).",
      "Each level has a unique sequence in which boxes need to be opened as opening the wrong box could make the level unsolvable.",
      "Difficulty of a level can be controlled using: (i) Number of boxes in the path to the goal.",
      "(ii) The number of distractor branches, (iii)  Length of distractor branches.",
      "StarCraft II minigames  9 mini games designed as specific scenarios in the Starcraft game are used.",
      "Results  Box-World  RRL agents solve over 98% of the levels while the RL agent solves less than 95% of the levels.",
      "Visualising the attention scores indicate that:  keys attend to locks they can unlock.",
      "all objects attend to agent\u2019s location.",
      "agent and gem attend to each other (and themselves).",
      "Generalization capacity is tested in two ways:  Performance on levels that require opening a larger sequence of boxes than it is trained on.",
      "Performance on levels that require key-lock combinations not seen during training.",
      "In both the scenarios, the RRL agent significantly outperforms the RL agent.",
      "StarCraft  RLL agent achieves better or equal results that the RL agent in all but one game.",
      "For testing generalization, the agent, that was trained for controlling two marines, was transferred on the task which requires it to control 5 marines.",
      "These results are not conclusive given the high variability."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1806.01830",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 11636296
  },
  {
    "blog_id": "babyai-first-steps-towards-grounded-language-learning-with-a-human-in-the-loop",
    "summary": [
      "BabyAI is a research platform to investigate and support the feasibility of including humans in the loop for grounded language learning.",
      "The setup is a series of levels (of increasing difficulty) to train the agent to acquire a synthetic language (Baby Language) which is a proper subset of English language.",
      "Motivation  BabyAI platform provides support for curriculum learning and interactive learning as part of its human-in-the-loop training setup.",
      "Curriculum learning is incorporated by having a curriculum of levels of increasing difficulty.",
      "Interactive learning is supported by including a heuristic expert which can provide new demonstrations on the fly to the learning agent.",
      "The heuristic expert can be thought of as the human-in-the-loop which can guide the agent through the learning process.",
      "One downside of human-in-the-loop is the poor sample complexity of the learning agent.",
      "The heuristic agent can be used to estimate the sample  efficiency.",
      "Contribution  BabyAI research platform for grounded language learning with a simulated human-in-the-loop.",
      "Baseline results for performance and sample efficiency for the different tasks.",
      "BabyAI Platform  Environment  MiniGrid - A partially observable 2D grid-world environment.",
      "Entities - Agent, ball, box, door, keys  Actions - pick, drop or move objects, unlock doors etc.",
      "Baby Language  Synthetic Language (a proper subset of English) - Used to give instructions to the agent  Support for verifying if the task (and the subtasks) are completed or not  Levels  A level is an instruction-following task.",
      "Formally, a level is a distribution of missions - a combination of initial state of the environment and an instruction (in Baby Language)  Motivated by curriculum learning, the authors create a series of tasks (with increasing difficulty).",
      "A subset of skills (competencies) is required for solving each task.",
      "The platform takes into account this constraint when creating a level.",
      "Heuristic Expert  The platform supports a Heuristic expert that simulates the role of a human teacher and knows how to solve each task.",
      "For any level, it can suggest actions or generate demonstrations (given the state of the environment).",
      "Experiment  An imitation learning baseline is trained for each level.",
      "Data requirement for each level and the benefits of curriculum learning and imitation learning are investigated (in terms of sample efficiency).",
      "Model Architecture  GRU to encode the sentence, CNN to encode the input observation  FiLM layer to combine the two representations  LSTM to encode the per-timestep FiLM encoding (timesteps in the environment)  Two model variants are considered:  Large Model - Bidirectional GRU + attention + large hidden state  Small Model - Unidirectional GRU + No attention + small hidden state  Heuristic expert used to generate trajectory and the models are trained by imitation learning (to be used as baselines)  Results  The key takeaway is that the current deep learning approaches are extremely sample inefficient when learning a compositional language.",
      "Data efficiency of RL methods is much worse than that of imitation learning methods showing that the current imitation learning and reinforcement learning methods scale and generalize poorly.",
      "Curriculum-based pretraining and interactive learning was found to be useful in only some cases."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1810.08272",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 76817909
  },
  {
    "blog_id": "prioritizing-experience-replay",
    "summary": [
      "Uniform sampling from replay memories is not an efficient way to learn.",
      "Rather, using a clever prioritization scheme to label the experiences in replay memory, learning can be carried out much faster and more effectively.",
      "However, certain biases are introduced by this non-uniform sampling; hence, weighted importance sampling must be employed in order to correct for this.",
      "It is shown through experimentation with the Atari Learning Environment that prioritized sampling with Double DQN significantly outperforms the previous state-of-the-art Atari results.",
      "Evidence  Implemented Double DQN with main changes being the addition of prioritized experience replay sampling and importance-sampling  Tested on Atari Learning Environment  Strengths  Lots of insight about the repercussions of this research and plenty of discussion on extensions  Notes  The magnitude of the TD-error indicates how unexpected a certain transition was  The TD-error can be a poor estimate about the amount an agent can learn from a transition when rewards are noisy  Problems with greedily selecting experiences:  High-error transitions are replayed too frequently  Low-error transitions are almost entirely ignored  Expensive to update entire replay memory, so errors are only updated for transitions that are replayed  Lack of diversity leads to over-fitting  A stochastic sampling method is introduced which finds a balance between greedy prioritization and random sampling (current method)  Two variants of  were studied, where $P$ is the probability of sampling transition $i$, $p_i > 0$ is the priority of transition $i$, and the exponent $\\alpha$ determines how much prioritization is used, with $\\alpha = 0$ the uniform case  Variant 1: proportional prioritization, where $p_i = | \\delta_i| + \\epsilon$ is used and $\\epsilon$ is a small positive constant that prevents the edge-case of transitions not being revisited once their error is zero.",
      "$\\delta$ is the TD-error  Variant 2: rank-based prioritization, with $p_i = \\frac{1}{rank(i)}$ where $rank(i)$ is the rank of transition $i$ when the replay memory is sorted according to $\\delta_i$  Key insight The estimation of the expected value of the total discounted reward with stochastic updates requires that the updates correspond to the same distribution as the expectation.",
      "Prioritized replay introduces a bias that changes this distribution uncontrollably.",
      "This can be corrected by using importance-sampling (IS) weights $ w_i = (\\frac{1}{N} \\frac{1}{P(i)})^{\\beta} $ that fully compensate for the non-uniform probabilities $P(i)$ if $\\beta = 1$.",
      "These weights are folded into the Q-learning update by using $w_i \\times \\delta_i$, which is normalized by $\\frac{1}{\\max_i w_i}$  IS is annealed from $\\beta_0$ to 1, which means its affect is felt more strongly at the end of the stochastic process; this is because the unbiased nature of the updates in RL is most important near convergence  IS also reduces the gradient magnitudes which is good for optimization; allows the algorithm to follow the curvature of highly non-linear optimization landscapes because the Taylor expansion (gradient descent) is constantly re-approximated"
    ],
    "author_id": "pemami",
    "pdf_url": "http://arxiv.org/pdf/1511.05952.pdf",
    "author_full_name": "Patrick Emami",
    "source_website": "https://pemami4911.github.io/index.html",
    "id": 42731227
  },
  {
    "blog_id": "one-shot-learning-with-memory-augmented-neural-networks",
    "summary": [
      "The paper demonstrates that Memory Augmented Neural Networks (MANN) are suitable for one-shot learning by introducing a new method for accessing an external memory.",
      "This method focuses on memory content while earlier methods additionally used memory location based focusing mechanisms.",
      "Here, MANN refers to neural networks that have an external memory.",
      "This includes Neural Turning Machines (NTMs) and excludes LSTMs.",
      "Meta-Learning  In meta-learning, a learner is learning at two levels.",
      "The learner is shown a sequence of tasks D1, D2, \u2026, DT.",
      "When it is training on one of the datasets (say DT), it learns to solve the current dataset.",
      "At the same time, the learner tries to incorporate knowledge about how task structure changes across different datasets (second level of learning).",
      "MANN + Meta Learning  Following are the desirable characteristics for a scalable, combined architecture:  Memory representation should be both stable and element-wise accessible.",
      "Number of model parameters should not be tied to the size of the memory.",
      "Task Setup  In standard learning, the goal is to reduce error on some dataset D. In meta-learning, the goal is to reduce the error across a distribution of datasets p(D).",
      "Each dataset is presented to the model in the form (x1, null), (x1, y0), \u2026, (xt+1, yt) where yt is the correct label (or value) corresponding to the inpuit xt.",
      "Further, the data labels are shuffled from dataset to dataset.",
      "The model must learn to hold the data samples in memory till the appropriate candidate labels are presented in the next step.",
      "The idea is that a model that meta learns would learn to map data representation to correct labels regardless of the actual context of data representation or the label.",
      "The paper uses NTM as the MANN with one modification.",
      "In the original formulation, the memories were addressed by both context and location.",
      "Location-based addressing is not optimal for the current setup where information encoding is not independent of the sequence.",
      "A new access module - LRUA - Least Recent Used Access - is used to write to memory.",
      "LRUA is purely content-based and writes to either least used memory location (to preserve recent information) or most recently used memory location (to overwrite recent information with more relevant information).",
      "This is decided on the basis of interpolation between previous read weights and weights scaled according to the usage weight.",
      "Datasets  Omniglot (classification)  Sampled functions from Gaussian Processes  Results  For the omniglot dataset, the model was trained with various combinations of randomly chosen classes with randomly chosen labels.",
      "As baselines, following models were considered:  Regular NTM  LSTM  Feedforward RNN  Nearest Neighbour Classifier  Since each episode (dataset created by the combination of classes) contains unique classes (with their own unique labels) it is important to clear the memory across different episodes.",
      "For the regression task, the data was generated from a GP prior with a fixed set of hyper-parameters which resulted in different functions.",
      "For both the tasks, the MANN architecture outperforms the LSTM architecture baseline NTMs."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1605.06065",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 75389943
  },
  {
    "blog_id": "making-the-v-in-vqa-matter-elevating-the-role-of-image-understanding-in-visual-question-answering",
    "summary": [
      "Problem Statement  Standard VQA models benefit from the inherent bias in the structure of the world and the language of the question.",
      "For example, if the question starts with \u201cDo you see a \u2026\u201d, it is more likely to be \u201cyes\u201d than \u201cno\u201d.",
      "To truly assess the capability of any VQA system, we need to have evaluation tasks that require the use of both the visual and the language modality.",
      "The authors present a balanced version of VQA dataset where each question in the dataset is associated with a pair of similar images such that the same question would give different answers on the two images.",
      "The proposed data collection procedure enables the authors to develop a novel interpretable model which, given an image and a question, identifies an image that is similar to the original image but has a different answer to the same question thereby building trust for the system.",
      "Dataset Collection  Given an (image, question, answer) triplet (I, Q, A) from the VQA dataset, a human worker (on AMT) is asked to identify an image I\u2019 which is similar to I but for which the answer to question Q is A\u2019 (different from A).",
      "To facilitate the search for I\u2019, the worker is shown 24 nearest-neighbor images of I (based on VGGNet features) and is asked to choose the most similar image to I, for which Q makes sense and answer for Q is different than A.",
      "In case none of the 24 images qualifies, the worker may select \u201cnot possible\u201d.",
      "In the second round, the workers were asked to answer Q for I\u2019.",
      "This 2-stage protocol results in a significantly more balanced dataset than the previous dataset.",
      "Observation  State-of-the-art models trained on unbalanced VQA dataset perform significantly worse on the new, balanced dataset indicating that those models benefitted from the language bias in the older dataset.",
      "Training on balanced dataset improves performance on the unbalanced dataset.",
      "Further, the VQA model, trained on the balanced dataset, learns to differentiate between otherwise similar images.",
      "Counter-example Explanations  Given an image and a question, the model not only answers the question, it also provides an image (from the k nearest neighbours of I, based on VGGNet features) which is similar to the input image but for which the model would have given different answer for the same image.",
      "Supervising signal is provided by the data collection procedure where humans pick the image I\u2019 from the same set of candidate images.",
      "For each image in the candidate set, compute the inner product of question-image embedding and answer embedding.",
      "The K inner product values are passed through a fully connected layer to generate K scores.",
      "Trained with pairwise hinge ranking loss so that the score of the human picked image is higher than the score of all other images by a margin of M (hyperparameter).",
      "The proposed explanation model achieves a recall@5 of 43.49%"
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1612.00837",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 11684235
  },
  {
    "blog_id": "quantifying-generalization-in-reinforcement-learning",
    "summary": [
      "The paper introduces a new, procedurally generated environment called as CoinRun that is designed to benchmark the generalization capabilities of RL algorithms.",
      "The paper reports that deep convolutional architectures and techniques like L2 regularization, batch norm, etc (which were proposed in the context of generalization in supervised learning) are also useful for RL.",
      "CoinRun Environment  CoinRun is made of multiple levels.",
      "In each level, the agent spawns on the far left side and needs to collect a single coin that lies on the far right side.",
      "There are many obstacles in between and colliding with an obstacle leads to agent\u2019s death.",
      "Each episode extends for a maximum for 1000 steps.",
      "CoinRun is designed such that given sufficient training time and levels, a near-optimal policy can be learned for all the levels.",
      "Generalization  Generalization can be measure by training an agent on a given set of training tasks and evaluating on an unseen set of test tasks.",
      "9 agents are trained to play CoinRun, on different training sets (each with a different number of levels).",
      "The first 8 agents are trained on sets of size 100 to 16000 levels while the last agent is trained on an unbounded set of levels.",
      "Training a model on an unbounded set of levels provides a good proxy for the train-to-test generalization performance.",
      "Evaluating Architectures  Two convolutional architectures (of different sizes) are compared:  Nature-CNN: The CNN architecture used in the Deep Q Network .",
      "This is the smaller network among the two models.",
      "IMPALA-CNN: The CNN architecture used in the Imapla architecture .",
      "IMPALA-CNN agent always outperforms the Nature-CNN agent indicating that larger architecture has more capacity for generalization.",
      "But increasing the network size beyond a limit gives diminishing returns.",
      "Evaluating Regularization  While both L2 regularization and Dropout helps to improve generalization, L2 regularization is more impactful.",
      "A domain randomization/data augmentation approach is tested where rectangular regions of different sizes are masked and assigned a random color.",
      "This approach seems to improve performance.",
      "Batch Normalization helps to improve performance as well.",
      "Environment stochasticity is introduced by using sticky actions while policy stochasticity is introduced by controlling the entropy bonus.",
      "Both these forms of stochasticity boost performance.",
      "While combining different regularization methods help, the gains are only marginally better than using just 1 regularization approach.",
      "This suggests that these different approaches induce similar generalization properties.",
      "Additional Environments  Two additional environments are also considered to verify the high degree of overfitting observed in the CoinRun environment:  CoinRun-Platforms:  Unlike CoinRun, each episode can have multiple coins and the time limit is 0increased to 1000 steps.",
      "Levels are larger as well so the agent might need to backtrack their steps.",
      "RandomMazes:  Partially observed environment with square mazes of dimensions 3x3 to 25x25.",
      "Timelimit of 500 steps  Overfitting is observed for both these environments as well."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1812.02341",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 43280550
  },
  {
    "blog_id": "387256f2bb08f39509600f9d7db498",
    "summary": [
      "Automated Theorem Proving (ATP) - Attempting to prove mathematical theorems automatically.",
      "Bottlenecks in ATP:  Autoformalization - Semantic or formal parsing of informal proofs.",
      "Automated Reasoning - Reasoning about already formalised proofs.",
      "Paper evaluates the effectiveness of neural sequence models for premise selection (related to automated reasoning) without using hand engineered features.",
      "Premise Selection  Given a large set of premises P, an ATP system A with given resource limits, and a new conjecture C, predict those premises from P that will most likely lead to an automatically constructed proof of C by A  Dataset  Mizar Mathematical Library (MML) used as the formal corpus.",
      "The premise length varies from 5 to 84299 characters and over 60% if the words occur fewer than 10 times (rare word problem).",
      "Approach  The model predicts the probability that a given axiom is useful for proving a given conjecture.",
      "Conjecture and axiom sequences are separately embedded into fixed length real vectors, then concatenated and passed to a third network with few fully connected layers and logistic loss.",
      "The two embedding networks and the joint predictor path are trained jointly.",
      "Stage 1: Character-level Models  Treat premises on character level using an 80-dimensional one hot encoding vector.",
      "Architectures for embedding:  pure recurrent LSTM and GRU Network  CNN (with max pooling)  Recurrent-convolutional network that shortens the sequence using convolutional layer before feeding it to LSTM.",
      "Stage 2: Word-level Models  MML dataset contains both implicit and explicit definitions.",
      "To avoid manually encoding the implicit definitions, the entire statement defining an identifier is embedded and the definition embeddings are used as word level embeddings.",
      "This is better than recursively expanding and embedding the word definition as the definition chains can be very deep.",
      "Once word level embeddings are obtained, the architecture from Character-level models can be reused.",
      "Experiments  Metrics  For each conjecture, the model ranks the possible premises.",
      "Primary metric is the number of conjectures proved from top-k premises.",
      "Average Max Relative Rank (AMMR) is  more sophisticated measure based on the motivation that conjectures are easier to prove if all their dependencies occur earlier in ranking.",
      "Since it is very costly to rank all axioms for a conjecture, an approximation is made and a fixed number of random false dependencies are used for evaluating AMMR.",
      "Network Training  Asynchronous distributed stochastic gradient descent using Adam optimizer.",
      "Clipped vs Non-clipped Gradients.",
      "Max Sequence length of 2048 for character-level sequences and 500 for word-level sequences.",
      "Results  Best Selection Pipeline - Stage 1 character-level CNN which produces word-level embeddings for the next stage.",
      "Best models use simple CNNs followed by max pooling and two-stage definition-based def-CNN outperforms naive word-CNN (where word embeddings are learnt in a single pass).",
      "This comment has been minimized.",
      "Sign in to view  Copy link  Quote reply  Autoformalization commented  Mar 18, 2017  Thank you for your  interest to the concept \"autoformalization\" (some times spelled also as \"Autoformalisation\".",
      "Just thought that you might  be interested how it was described when I coined the term about 30 years ago ...   [url]"
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1606.04442",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 93460473
  },
  {
    "blog_id": "burdags15",
    "summary": [
      "This paper proposes to train a neural network generative model by optimizing an importance sampling (IS) weighted estimate of the log probability under the model.",
      "The authors show that the case of an estimate based on a single sample actually corresponds to the learning objective of variational autoencoders (VAE).",
      "Importantly, they exploit this connection by showing that, similarly to VAE, a gradient can be passed through the approximate posterior (the IS proposal) samples, thus yielding an importance weighted autoencoder (IWAE).",
      "The authors also show that, by using more samples, this objective, which is a lower bound of the actual log-likelihood, becomes an increasingly tighter approximation to the log-likelihood.",
      "In other words, the IWAE is expected to better optimize the real log-likelihood of the neural network, compared to VAE.",
      "The experiments presented show that the model achieves competitive performance on a version of the binarized MNIST benchmark and on the Omniglot dataset.",
      "#### My two cents  This is a really neat contribution!",
      "While simple (both conceptually and algorithmically), it really seems to be an important step forward for the VAE framework.",
      "I really like the theoretical result showing that IWAE provides a better approximation to the real log-likelihood, it's quite neat and provides an excellent motivation for the method.",
      "The results on binarized MNIST are certainly impressive.",
      "Unfortunately, it appears that the training setup isn't actually comparable to the majority of published results on this dataset.",
      "Indeed, it seems that they didn't use the stochastic but *fixed* binarization of the inputs that other publications on this benchmark have used (since my paper on NADE with Iain Murray, we've made available that fixed training set for everyone to use, along with fixed validation and test sets as well).",
      "I believe instead they've re-sampled the binarization for each minibatch, effectively creating a setup with a somewhat larger training set than usual.",
      "It's unfortunate that this is the case, since it makes this result effectively impossible to compare directly with previous work.",
      "I'm being picky on this issue only because I'm super interested in this problem (that is of generative modeling with neural networks) and this little issue is pretty much the only thing that stops this paper from being a slam dunk.",
      "Hopefully the authors (or perhaps someone interested in reimplementing IWAE) can clarify this question eventually.",
      "Otherwise, it seems quite clear to me that IWAE is an improvement over VAE.",
      "The experiments of section 5.2, showing that fine-tuning a VAE model with IWAE training improves performance, while fine-tuning a IWAE model using VAE actually makes things worse, is further demonstration that IWAE is indeed a good idea."
    ],
    "author_id": "hlarochelle",
    "pdf_url": "http://arxiv.org/pdf/1509.00519",
    "author_full_name": "Hugo Larochelle",
    "source_website": "https://www.shortscience.org/user?name=hlarochelle",
    "id": 15765418
  },
  {
    "blog_id": "d464e7d0ea4c7c6ed5189ac4e44095",
    "summary": [
      "The paper presents persona-based models which add coherency to the response generated by sequence-to-senquence models like Neural Conversational Model .",
      "Persona  Defined as the character that an agent performs during conversational interactions.",
      "Combination of identity, language, behaviour and interaction style.",
      "May be adapted during the conversation itself.",
      "Neural Conversational Model fails to maintain a consistent persona throughout the conversation resulting in incoherent responses.",
      "Persona Based Models  Speaker Model  Models the personality of only the respondent.",
      "Represents each speaker as a vector (embedding) which encodes speaker-specific information (eg age, gender, etc)  The model learns to cluster users along these traits using the data alone.",
      "The vector v, corresponding to the given speaker, is used along with the context vector and the target representation generated in the previous timestamp to generate the current output representation.",
      "v is learnt along with other model parameters.",
      "Since the model learns the representation for each speaker, it can infer answers to certain questions about a given speaker even if the question has not been explicitly answered in the context of the given user (using the answers for similar users).",
      "Speaker-Addressee Model  Models the personality of both the speaker and addressee.",
      "Associate a representation Vi, j to capture the style of user i towards user j.  Vi, j = tanh(W1 \u00b7 vi + W2 \u00b7 v2)  Use Vi, j as we used v in the speaker model.",
      "Speaker-Addressee model can derive generalization capabilities from speaker embeddings.",
      "For example, even if two speakers have never engaged in a conversation, the conversations between speakers similar to the two given speakers can be to capture the associated representation.",
      "Decoding and Reranking  Generate N-best lists with beam size B = 200 and Max length = 20 (for generated candidates).",
      "At each time step, examine all B \u00d7 B possible next-word candidates, and add all hypothesis ending with an EOS token to the N-best list.",
      "Rerank the generated N-best list using the scoring function from Li et al to avoid generic and commonplace responses.",
      "Datasets  Twitter Persona Dataset  Dataset of tweet sequences having frequent (at least 60) engagements from Twitter FireHose.",
      "Twitter Sordoni Dataset  Similar to Twitter Persona Dataset but with more references per message (up to 10).",
      "Television Transcripts  Since this dataset alone was very small to train an open domain dialogue model, a standard SEQ2SEQ model is first trained using OpenSubtitles dataset and further tuned to the transcripts dataset.",
      "Experiments  The proposed models yields performance improvements in both perplexity and BLEU scores over baseline SEQ2SEQ models.",
      "Similar gains observed in speaker consistency as measured by human judges.",
      "Open Questions  There is no evaluation of what does the speaker embeddings map to.",
      "The paper mentions that the embeddings should be able to capture the aspects like age, gender etc but these embeddings have not been explored in the paper."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1603.06155",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 88532041
  },
  {
    "blog_id": "combining_mrfs_and_cnns_for_image_synthesis",
    "summary": [
      "What  They describe a method that applies the style of a source image to a target image.",
      "Example: Let a normal photo look like a van Gogh painting.",
      "Example: Let a normal car look more like a specific luxury car.",
      "Their method builds upon the well known artistic style paper and uses a new MRF prior.",
      "The prior leads to locally more plausible patterns (e.g. less artifacts).",
      "How  They reuse the content loss from the artistic style paper .",
      "The content loss was calculated by feed the source and target image through a network (here: VGG19) and then estimating the squared error of the euclidean distance between one or more hidden layer activations.",
      "They use layer relu4_2 for the distance measurement.",
      "They replace the original style loss with a MRF based style loss.",
      "Step 1: Extract from the source image k x k sized overlapping patches.",
      "Step 2: Perform step (1) analogously for the target image.",
      "Step 3: Feed the source image patches through a pretrained network (here: VGG19) and select the representations r_s from specific hidden layers (here: relu3_1, relu4_1).",
      "Step 4: Perform step (3) analogously for the target image.",
      "(Result: r_t)  Step 5: For each patch of r_s find the best matching patch in r_t (based on normalized cross correlation).",
      "Step 6: Calculate the sum of squared errors (based on euclidean distances) of each patch in r_s and its best match (according to step 5).",
      "They add a regularizer loss.",
      "The loss encourages smooth transitions in the synthesized image (i.e. few edges, corners).",
      "It is based on the raw pixel values of the last synthesized image.",
      "For each pixel in the synthesized image, they calculate the squared x-gradient and the squared y-gradient and then add both.",
      "They use the sum of all those values as their loss (i.e. regularizer loss = <sum over all pixels> x-gradient^2 + y-gradient^2).",
      "Their whole optimization problem is then roughly image = argmin_image MRF-style-loss + alpha1 * content-loss + alpha2 * regularizer-loss.",
      "In practice, they start their synthesis with a low resolution image and then progressively increase the resolution (each time performing some iterations of optimization).",
      "In practice, they sample patches from the style image under several different rotations and scalings.",
      "Results  In comparison to the original artistic style paper:  Less artifacts.",
      "Their method tends to preserve style better, but content worse.",
      "Can handle photorealistic style transfer better, so long as the images are similar enough.",
      "If no good matches between patches can be found, their method performs worse.",
      "Non-photorealistic example images.",
      "Their method vs. the one from the original artistic style paper.",
      "Photorealistic example images.",
      "Their method vs. the one from the original artistic style paper."
    ],
    "author_id": "ALEJU",
    "pdf_url": "http://arxiv.org/pdf/1601.04589",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 4728464
  },
  {
    "blog_id": "veritas-shared-verifiable-databases-and-tables-in-the-cloud",
    "summary": [
      "Veritas: shared verifiable databases and tables in the cloud Allen et al., CIDR\u201919  Two (or more) parties want to transact based on the sharing of information (e.g. current offers).",
      "In order to have trust in the system and provide a foundation for resolving disputes, we\u2019d like a tamperproof and immutable audit log of all shared data and actions, such that an independent auditor can reconstruct the state of the system at any point in time.",
      "Enter the blockchain?!",
      "Not so fast say Allen et al., blockchain technology as we know it today is \u2018 one step forward, two steps back \u2019 ;).",
      "Today, for gaining immutability and auditability with new blockchain platforms, we give up decades of research in data management\u2014 and hardened, enterprise-ready code that implements these ideas.",
      "We\u2019d still like to be able to use SQL for example.",
      "We want transaction throughput much closer to a traditional database, and we want to take advantage of query optimisation and sophisticated query processing engines.",
      "We could try adding database like features to blockchain systems, but that looks to be a long road:  There are now a gazillion start-ups that are adding these basic database features to blockchains, but it will take years if not decades to catch up.",
      "How about trying it the other way round then?",
      "Start with a mature database system, and add a sprinkling of blockchain?",
      "Instead of adding database capabilities to blockchains, we propose to address the problem from the opposite approach: we add trust and auditability to existing database management systems.",
      "The key notions in the paper are verifiable databases and verifiable tables.",
      "A verifiable database has all the features of a regular database, but in addition it supports tamper-evident collaboration across mutually untrusted entities.",
      "The idea of a shared verifiable table goes one step further: integrating a special table directly into the existing databases of the transacting parties.",
      "The same instance of the table is visible to all parties, and all activities are written to a tamper-proof log.",
      "There is an N:1 relationship between shared tables and tamper-proof logs.",
      "Verifiable databases (and tables) provide a set of cryptographic guarantees:  each party can verify the actions (updates) of all other parties and provide proof of its own actions  all parties can verify that the state of the shared database (or table) and its responses to queries is consistent with the prior actions of legitimate actors  unauthorized parties (hackers or operators with administrative privileges) cannot tamper with the state of the verifiable database (table) without being detected by the verification mechanism  So we\u2019re looking at a permissioned system supporting a set of verifiers.",
      "The assumption in this work is that verifiers have access to the full log.",
      "Confidentiality is an orthogonal concern that could be addressed by frameworks such as Coco , Quorum , Spice , or [Corda](  [url]"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://cidrdb.org/cidr2019/papers/p111-gehrke-cidr19.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 88548359
  },
  {
    "blog_id": "deep-symbolic-rl",
    "summary": [
      "It is desirable for an agent to symbolically reason about its environment, in order to expediate the process of learning optimal behaviors.",
      "However, \u201cclassic\u201d symbolic AI suffers from the symbol grounding problem; symbolic elements have traditionally been hand-crafted, and hence, are brittle.",
      "On the other hand, Deep Learning can be used to automatically learn ~optimal \u201csymbols\u201d, upon which a reinforcement learning agent could learn behaviors motivated by these learned symbols.",
      "By forcing a Deep RL agent to operate in a symbolic domain, the decisions made by the agent are naturally more interpretable.",
      "The aspects of AI that this work focuses on are closely related to those proposed in the manifesto written by Lake et al. .",
      "There are also nods to Douglas Hofstadter\u2019s work on analogy as being the main driving force behind intelligence, as well as Marcus Hutter\u2019s Universal Artificial Intelligence work.",
      "Conceptual abstraction - agents can use abstractions to make analogies and hence learn optimal behaviors faster  Compositional structure - probabilistic first-order logic provides the underlying framework for a representation medium that is compositional  Common sense priors - priors from simple object tracking!",
      "Persistence, kinematics (constant velocity models), etc.",
      "Causal reasoning - the proposed architecture attempts to discover causal structure in the environment to accelerate learning by explicitly maintaining sets of causal rules  A Q-learning agent is designed for their toy example.",
      "The state space consists of the different interactions between the learned symbolic abstractions of the environment.",
      "A tracking process is carried out separately from the agent for keeping track of the different symbolic abstractions.",
      "Main result: on random toy problem instances, DQN is not able to better than chance\u2026 Hypothesis: DQN relies on the fact that you should be able to internally learn a model for $p(s_{t+1}|s_{t},a_{t})$ after going through a lot of examples.",
      "When this distribution is non-stationary, it can\u2019t!",
      "However, the proposed architecture doesn\u2019t seem to care about this.",
      "It instead directly is learning about object interactions.",
      "Methodology  Their environment is a simple B&W grid upon which shapes (O\u2019s, X\u2019s, and +\u2019s) can be randomly positioned.",
      "Low-level symbol generation: Uses a convolutional autoencoder to do simple dimensionality reduction and learn relevant features.",
      "Then, they do a form of unsupervised clustering by finding the maximally activated feature corresponding to each pixel, then thresholding these values.",
      "Once they have a sparse list of salient pixels, they form a spectrum with the activations across all features, and define a distance metric on the spectra via the sum of squared distances.",
      "This allows them to cluster pixels (objects) into certain categories.",
      "Representation building: notions of spatial proximity, type-transitions between frames (including birth and death), neighborhood (number of neighbors), relative distances and positions  Reinforcement Learning: Agent is the \u2018+\u2019, separate Q for interactions between different pairs of object types.",
      "State space describes different possible relations between two objects types.",
      "Seek to learn how to interact via (U, D, L, R) actions."
    ],
    "author_id": "pemami",
    "pdf_url": "https://arxiv.org/pdf/1609.05518v2",
    "author_full_name": "Patrick Emami",
    "source_website": "https://pemami4911.github.io/index.html",
    "id": 11836666
  },
  {
    "blog_id": "poincare-embeddings-for-learning-hierarchical-representations",
    "summary": [
      "Much of the work in representation leaning uses Euclidean vector spaces to embed datapoints (like words, nodes, entities etc).",
      "This approach is not effective when data has a (latent) hierarchical structure.",
      "The paper proposes to compute the embeddings in the hyperbolic space so as to preserve both the similarity and structure information.",
      "Hyperbolic Geometry  Hyperbolic spaces are spaces with a constant negative curvature while Euclidean spaces have zero curvature.",
      "The hyperbolic disc area and circle length increase exponentially with the radius r while in Euclidean space, it increases quadratically and linearly respectively.",
      "This makes the hyperbolic space more suitable for embedding tree-like structures where the number of nodes increases as we move away from the root.",
      "Hyperbolic spaces can be thought of as the continuous version of trees and trees can be thought of as the discrete version of hyperbolic spaces.",
      "Poincare Embeddings  Poincare model is one of the several possible models of the hyperbolic space and is considered here as it is more amenable to gradient-based optimisation.",
      "Distance between 2 pints change smoothly and is symmetric.",
      "Thus the hierarchical organisation only depends on the distance from the origin which makes the model applicable in settings where the hierarchical structure needs to be inferred from the data.",
      "Eventually the norm of a point represents its hierarchy and distance between the points represents similarity.",
      "Optimization  RSGD (Riemannian SGD) method is used.",
      "Riemannian gradients can be computed from the Euclidean gradients by rescaling with the inverse of the Poincare ball metric tensor.",
      "The embeddings are constrained to be within the Poincare ball by projection operation which normalizes the magnitude of embeddings to be 1.",
      "Training Details  Initializing the embeddings close to 0 (by sampling uniformly from (-0.001, 0.001)) helps.",
      "The model is trained for an initial burn-out period of 10 epochs with 0.1 times the learning rate so as to find a better initial angular layout.",
      "Evaluation  Embedding taxonomy for wordnet task  Setup  Reconstruction  Link Prediction  The input data is a collection of a pair of words (u, v) which are related to each other.",
      "For each word pair, 10 negative samples of the form (u, v\u2019) are sampled and the training procedure uses a soft ranking loss that aims to bring the related objects closer together.",
      "Network Embedding  Baselines  Euclidean Embeddings  Translational Embedding where a relation vector corresponding to the edge type is also learnt.",
      "Datasets  ASTROPH  CONDMAT  GRQC  HEPPH  Lexical Entailment  * Hyperlex - Gold standard to evaluate how well the semantics models capture lexical entailment on a scale of [0, 10].",
      "* The key takeaway is that for all the datasets/setups, hyperbolic embeddings give a performance benefit when the embedding dimension is small.",
      "Challenges  Hyperbolic embeddings are not suitable for all the datasets.",
      "Eg if the dataset is not tree-like or has cycles.",
      "Hyperbolic embeddings are difficult to optimize as each operation needs to be modified to be usable in the hyperbolic space."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1705.08039.pdf",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 32248697
  },
  {
    "blog_id": "legoos-a-disseminated-distributed-os-for-hardware-resource-disaggregation",
    "summary": [
      "LegoOS: a disseminated, distributed OS for hardware resource disaggregation Shan et al., OSDI\u201918  One of the interesting trends in hardware is the proliferation and importance of dedicated accelerators as general purposes CPUs stopped benefitting from Moore\u2019s law.",
      "At the same time we\u2019ve seen networking getting faster and faster, causing us to rethink some of the trade-offs between local I/O and network access.",
      "The monolithic server as the unit of packaging for collections of such devices is starting to look less attractive:  It leads to inefficient resource utilisation, since CPU and memory for a job have to be allocated from the same machine.",
      "This can lead to eviction even when utilisation is overall low (e.g. 50%).",
      "It is difficult to add, move, remove, or reconfigure hardware components after they have been installed in a server, leading to long up-front planning cycles for hardware rollouts at odds with the fast-moving rate of change in requirements.",
      "It creates a coarse failure domain \u2013 when any hardware component within a server fails, the whole server is often unusable.",
      "It doesn\u2019t work well with heterogeneous devices and their rollout: e.g. GPGPUs, TPUs, DPUs, FGPAs, NVM, and NVMe-based SSDs.",
      "To fully support the growing heterogeneity in hardware and to provide elasticity and flexibility at the hardware level, we should break the monolithic server model.",
      "We envision a hardware resource disaggregation model where hardware resources in traditional servers are disseminated into network-attached hardware components.",
      "Each component has a controller and a network interface, can operate on its own, and is an independent, failure-isolated entity.",
      "Hardware resource disaggregation is enabled by three hardware trends:  Rapidly growing networks speeds (e.g. 200 Gbps InfiniBand will be only 2-4x slower than the main memory bus in bandwidth).",
      "\u201cWith the main memory bus facing a bandwidth wall, future network bandwith (at line rate) is even projected to exceed local DRAM bandwidth\u201d (just think about how many design assumptions that turns on their heads!).",
      "Network technologies such as Intel OmniPath, RDMA, and NVMe over Fabrics enable hardware devices to access the network directly without the need to attached any process.",
      "Hardware devices are incorporating more processing power, making it possible to offload more application and OS functionality to them.",
      "From a hardware perspective this seems to open up a bunch of exciting possibilities.",
      "But what on earth does an operating system look like in such a world?",
      "That\u2019s the question this paper sets out to address, with the design of LegoOS.",
      "LegoOS distributes operating system functions across a collection of loosely-coupled monitors, each of which runs and manages a hardware component.",
      "The initial implementation goes after the big three: processing, memory, and storage.",
      "Yes, that does mean that processor and memory are separated over the network!",
      "The biggest challenge and our focus in building LegoOS is the separation of processor and memory and their management.",
      "Modern processors and OSes assume all hardware memory units including main memory, page tables, and TLB are local.",
      "Simply moving all memory hardware and memory management software to across the network will not work.",
      "LegoOS is available at  [url]"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://www.usenix.org/system/files/osdi18-shan.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 89767636
  },
  {
    "blog_id": "learning_to_navigate_in_complex_environments",
    "summary": [
      "What  They describe a model based on the A3C algorithm.",
      "The model is optimized so that it learns fast to navigate through mazes/games.",
      "Most of the modification is based on adding auxiliary losses (depth prediction and \"have I been here before?\").",
      "How  Their model is based on the A3C algorithm and works mostly in the same way.",
      "They describe four variations of the model:  FF A3C: Standard A3C based agent with a simple CNN.",
      "LSTM A3C: Standard A3C based agent with a simple CNN+LSTM.",
      "Nav A3C: Like \"LSTM A3C\", but uses two stacked LSTMs and gets three additional inputs: last received reward, last performed action, current velocity vector (lateral + rotation).",
      "Nav A3C +D1D2L: Like \"Nav A3C\", but additionally predicts depth information and loop information (see below).",
      "Visualizations:  All of these models predict (at least) a policy (pi(a_t|s_t)) and value function (V(s_t)).",
      "Depth prediction  They let the model predict the depth map of the currently visible scene/screen.",
      "They could also feed the depth map as an additional input into the model, but argue that it helps more with understanding and learning the navigation task to let the model predict it.",
      "They try prediction both via regression and classification.",
      "(Where classification uses non-uniform bins, so that there are more bins for far away depths.)",
      "Loop closure prediction  They let the model predict whether it  has been at (roughly) the current position t timesteps in the past  AND also moved sufficiently far away from that position between now and t timesteps in the past (in order to not make the loss too simple)  Results  They use a maze game from DeepMind Labs for training and testing.",
      "In that maze, the agent can accelerate forward/backward/sideways/rotational.",
      "The agent must find a goal within a short amount of time to gain 10 reward.",
      "Occasionally there are other reward objects giving +1 or +2.",
      "They use the following maze types:  Static maze: Goal locations are always the same, but the agent is placed at a random location after finding them.",
      "Dynamic maze / random goal maze: Goal locations vary per episode.",
      "I-Maze: Static maze layout.",
      "Goal is at a random one of four locations per episode.",
      "Observations:  Predicting depth via regression works significantly worse than via classification.",
      "FF A3C (raw feed-forward CNN without RNN-memory) can sometimes still navigate quite well.",
      "Memory is apparently not always required for navigation, despite ambiguity.",
      "Adding the last action, last reward and velocity as an input seems to usually help the agent, but not always.",
      "Adding the auxiliary losses helps significantly with learning.",
      "Learning curves:  Legend: -L = with loop closure prediction, -D1 = with depth prediction after first LSTM layer, -D2 = with depth prediction after second LSTM layer."
    ],
    "author_id": "ALEJU",
    "pdf_url": "https://arxiv.org/pdf/1611.03673",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 46416317
  },
  {
    "blog_id": "memory-transformations",
    "summary": [
      "The research question the authors answered was whether by shifting from an episodic to a \u201cschematic\u201d, or gist-like, memory system, a reinforcement learning agent could learn to achieve its goals in a dynamic environment.",
      "The authors focused on 2D navigation tasks where the reward locations constantly changed, such that new reward locations were correlated in the short-term but where independent and sampled from a stable distribution in the long-term.",
      "I found it interesting that the authors claimed the real world is like this, and consequentially they staked a lot of the significance of their work on this fact.",
      "The main conclusion they came to was that given the existence of a stable long-term distribution for reward location (or whatever random variable the agent is concerned with estimating a distribution for), the optimal strategy for an agent is to shift from utilizing episodic to schematic memories slowly.",
      "The authors implemented their agent using a novel neural network architecture that consisted of, in general, an episodic memory system, a schematic memory system, a critic to generate a TD-error.",
      "The episodic memory system was:  a spatial encoder which took in the (x,y)-pair of the current location of the agent,  an autoencoder implemented as a 3-layer recurrent network  a network of place field units  The output of the spatial encoder fed into the autoencoder, and the output of this fed into the place cells.",
      "\u201cRetrieving\u201d memory from the place cells was implemented as a fully-connected sigmoid layer.",
      "The use of place field units was quite interesting; the idea behind this was to learn to associate activation patterns of place cells with specific locations within the environment where rewards were recently found.",
      "The schematic memory was implemented as a Restricted Boltzman Machine.",
      "The first layer was a direct projection of the place cells from the episodic network.",
      "The ultimate goal of the RBM was to learn a general statistical model of the reward locations.",
      "It was trained in an offline manner (i.e., while the agent was \u201cat rest\u201d between trials) by using random activity in the spatial encoder, and propagating that through to the RBM.",
      "This was curious, but apparently since they also had added a TD-prediction error to the episodic system via a critic, this was more biologically plausible than iid sampling from the episodic memory.",
      "The agent has a parameter that controls how much it mixes its episodic and schematic memories; the resultant \u201cmixed\u201d memory then influences action-selection.",
      "Future Directions  How would this compare with an LSTM- literally, a Long Short-Term Memory neural network?",
      "Can an LSTM learn to adapt to environments with with both short-term and long-term statistical patterns like this?",
      "We\u2019re seeing a shift towards more complex RL environments that this could be applied to; for example, 3D navigation tasks where there are multiple goals that could potentially move over time.",
      "Perhaps this could also be applied to modeling of the dynamic behavior of other agents in a multi-agent setting?",
      "Cool use of unsupervised learning to enhance RL!"
    ],
    "author_id": "pemami",
    "pdf_url": "https://www.jneurosci.org/content/jneuro/36/48/12228.full.pdf",
    "author_full_name": "Patrick Emami",
    "source_website": "https://pemami4911.github.io/index.html",
    "id": 74261188
  },
  {
    "blog_id": "dueling-networks",
    "summary": [
      "For many states, it is unnecessary to estimate the action value for each action.",
      "This is a problem with methods that attempt to favor exploration over exploitation too much, because often times there will be a large number of actions that have little to no value for a given state.",
      "The Q-Network in this novel architecture is decomposed into two separate streams; a value stream and an advantage stream.",
      "Feature learning is carried out by a number of convolutional and pooling layers.",
      "The activations of the last of these layers are sent to both separate streams.",
      "Each stream contains a number of fully-connected layers.",
      "The final layer combines the output of the two streams, and the output of the network is a set of Q values, one for each action.",
      "The aggregator for the two outputs of the advantage and value streams is:  $\\beta$ refers to the parameters specific to the value network, and the alpha refers to the parameters specific to the advantage network.",
      "The advantage of the dueling network over standard Q-Networks is especially prominent when the number of actions is large.",
      "For standard Q-Networks, when the variation between actions is small, the Q-Network effectively has to learn the same value for all actions while each update only modifies the Q value of one action.",
      "Evidence  The dueling network architecture outperformed the Double-DQN results in 50 out of 57 learned Atari games.",
      "Strengths  The paper does a good job of making its main contribution (a novel neural network architecture) clear at the beginning.",
      "The experimental algorithm for Dueling Networks employs other state-of-the-art advances in DRL (such as Double-DQN) which helps show the correlation between research being carried on in this field.",
      "Interesting related works  Increasing the action gap (Bellemare et al., 2016)  Notes  The value function V measures the importance of being in a particular state s. The Q-function measures the importance about the value of choosing each possible action when in this state.",
      "The advantage function, $A^{\\pi}(s,a) = Q^{\\pi}(s,a) - V^{\\pi}(s)$, subtracts the value of the state from the Q-function to obtain a relative measure of the importance of each action  A deep Q-network is a non-linear function approximator for the Q function having the form $Q(s,a;\\theta)$ with parameters theta  We optimize the following sequence of loss functions:  with the target of the loss function, $y_i^{DQN}$, given by the reward signal plus the discounted maximal Q value provided by the target Q-Network  A target Q-Network that uses parameter freezing for a certain number of iterations is used to stabilize the algorithm  The specific gradient is  Experience replay is used; use prioritized experience replay instead!",
      "(Don\u2019t just sample uniformly from memory)  To avoid over-optimistic value estimates (van Hasselt, 2010), use Double Q-Learning.",
      "Originally, the max operator uses the same values to both select and evaluate an action.",
      "Instead, use the following target:"
    ],
    "author_id": "pemami",
    "pdf_url": "http://arxiv.org/pdf/1511.06581",
    "author_full_name": "Patrick Emami",
    "source_website": "https://pemami4911.github.io/index.html",
    "id": 33427271
  },
  {
    "blog_id": "moment-based-quantile-sketches-for-efficient-high-cardinality-aggregation-queries",
    "summary": [
      "Moment-based quantile sketches for efficient high cardinality aggregation queries Gan et al., VLDB\u201918  Today we\u2019re temporarily pausing our tour through some of the OSDI\u201918 papers in order to look at a great sketch-based data structure for quantile queries over high-cardinality aggregates.",
      "That\u2019s a bit of a mouthful so let\u2019s jump straight into an example of the problem at hand.",
      "Say you have telemetry data from millions of heterogenous mobile devices running your app.",
      "Each device tracks multiple metrics such as request latency and memory usage, and is associated with dimensional metadata (categorical variables) such as application version and hardware model.",
      "In applications such as A/B testing, exploratory data analysis, and operations monitoring, analysts perform aggregation queries to understand how specific user cohorts, device types, and feature flags are behaving.",
      "We want to be able to ask questions like \u201cwhat\u2019s the 99%-ile latency over the last two weeks for v8.2 of the app?\u201d  SELECT percentile(latency, 99) FROM requests WHERE time > date_sub(curdate(), 2 WEEK) AND app_version = \"v8.2\"  As well as threshold queries such as \u201cwhat combinations of app version and hardware platform have a 99th percentile latency exceeding 100ms?\u201d  SELECT app_version, hw_model, PERCENTILE(latency, 99) as p99 FROM requests GROUP BY app_version, hw_model HAVING p99 > 100  Instead of starting from raw data every time when answering this type of query, OLAP engines can reduce query time and memory usage by maintaining a data cube of pre-aggregated summaries for each tuple of dimension values.",
      "The ultimate query performance then depends on just how quickly we can merge those summaries to compute quantile roll-ups over the requested dimensions.",
      "Let\u2019s take a very simple example.",
      "Suppose I have two dimensions, letter (with values A and B), and colour (with values red and green), and I have request latency data from log messages including these attributes.",
      "Then I will have four summary sketches, one accumulating latency values for (A, red) one for (A, green), one for (B, red) and one for (B, green).",
      "If a query wants to know the P99 latency for \u2018red\u2019 requests, we can add together the (A, red) and (B, red) sketches to get a complete sketch for red.",
      "In this paper, we enable interactive quantile queries over high-cardinality aggregates by introducing a compact and efficiently mergeable quantile sketch and associated quantile estimation routines.",
      "The data structure than makes all this possible is called a moments sketch (named after the method of moments statistical technique).",
      "It\u2019s easy to construct, but a bit more difficult to interpret.",
      "It\u2019s worth the effort though, as the evaluation shows:  The moments sketch supports 15-50x faster query times that comparably accurate summaries on quantile aggregations  The moments sketch gives good accuracy across a range of real-world datasets using less than 200 bytes of storage  Integration of the moments sketch in Druid provides 7x faster quantile queries than the default quantile summary in Druid workloads.",
      "There\u2019s a Java implementation available at  [url]"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://www.bailis.org/papers/moments-vldb2018.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 4784102
  },
  {
    "blog_id": "set-transformer-a-framework-for-attention-based-permutation-invariant-neural-networks",
    "summary": [
      "Consider problems where the input to the model is a set.",
      "In such problems (referred to as the set-input problems), the model should be invariant to the permutation of the data points.",
      "In \u201cset pooling\u201d methods ( 1 , 2 ), each data point (in the input set) is encoded using a feed-forward network and the resulting set of encoded representations are pooled using the \u201csum\u201d operator.",
      "This approach can be shown to be bot permutation-invariant and a universal function approximator.",
      "The paper proposes an attention-based network module, called as the Set Transformer, which can model the interactions between the elements of an input set while being permutation invariant.",
      "Transformer  An attention function Attn(Q, K, V) = (QKT)V is used to map queries Q to output using key-value pairs K, V.  In case of multi-head attention, the key, query, and value are projected into h different vectors and attention is applied on all these vectors.",
      "The output is a linear transformation of the concatenation of all the vectors.",
      "Set Transformer  3 modules are introduced: MAB, SAB and ISAB.",
      "Multihead Attention Block (MAB) is a module very similar to to the encoder in the Transformer, without the positional encoding and dropout.",
      "Set Attention Block (SAB) is a module that takes as input a set and performs self-attention between the elements of the set to produce another set of the same size ie SAB(X) = MAB(X, X).",
      "The time complexity of the SAB operation is O(n2) where n is the number of elements in the set.",
      "It can be reduced to O(m*n) by using Induced Set Attention Blocks (ISAB) with m induced point vectors (denoted as I).",
      "ISABm = MAB(X, MAB(I, X)).",
      "ISAB can be seen as performing a low-rank projection of inputs.",
      "These modules can be used to model the interactions between data points in any given set.",
      "Pooling by Multihead Attention (PMA)  Aggregation is performed by applying multi-head attention on a set of k seed vectors.",
      "The interaction between the k outputs (from PMA) can be modeled by applying another SAB.",
      "Thus the entire network is a stack of SABs and ISABs.",
      "Both the modules are permutation invariant and so is any network obtained by stacking them.",
      "Experiments  Datasets include:  Predicting the maximum value from a set.",
      "Counting unique (Omniglot) characters from an image.",
      "Clustering with a mixture of Gaussians (synthetic points and CIFAR 100).",
      "Set Anomaly detection (celebA).",
      "Generally, increasing m (the number of inducing datapoints) improve performance, to some extent.",
      "This is somewhat expected.",
      "The paper considers various ablations of the proposed approach (like disabling attention in the encoder or pooling layer) and shows that attention mechanism is needed during both the stages.",
      "The work has two main benefits over prior work:  Reducing the O(n2) complexity to O(m*n) complexity.",
      "Using self-attention mechanism for both encodings the inputs and for aggregating the encoded representations."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1810.00825",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 69976012
  },
  {
    "blog_id": "net2net-accelerating-learning-via-knowledge-transfer",
    "summary": [
      "Notes  The paper presents a simple yet effective approach for transferring knowledge from a trained neural network (referred to as the teacher network) to a large, untrained neural network (referred to as the student network).",
      "The key idea is to use a function-preserving transformation that guarantees that for any given input, the output from the teacher network and the newly created student network would be the same.",
      "Link to an implementation  The approach works as follows - Let us say that the teacher network was represented by the transformation y = f(x, \u03b8) where \u03b8 refer to the parameters of the network.",
      "The task is to choose a new set of parameters \u03b8\u2019 for the student network g(x, \u03b8\u2019) such that for all x, f(x, \u03b8) = g(x, \u03b8\u2019)  To start, we can assume that f and g are composed of standard linear layers.",
      "Layer i and i+1 are represented by weights Wmxni and Wnxpi+1  We want to grow layer i to have q output units (where q > n) and layer i+1 to have q input units.",
      "The new weight matrix would be Umxqi and Uqxpi+1  The first q columns (rows) of Wi (Wi+1) would be copied as it is into Ui(Ui+1).",
      "For filling the remaining n-q slots, columns (rows) would be sampled randomly from Wi (Wi+1).",
      "Finally, each layer in Ui is scaled by dividing by the corresponding replication factor to ensure that the output value of function remains unchanged by the operation.",
      "Since convolutions can be seen as multiplication by a double block circulant matrix, the approach can be readily extended for convolutional networks.",
      "The benefits of using this approach are the following:  The newly created student network performs at least as good as the teacher network.",
      "Any changes to the network are guaranteed to be an improvement.",
      "It is safe to optimize all the parameters in the network.",
      "The variant discussed above is called the Net2WiderNet variant.",
      "There is another variant calledNet2DeeperNet that enables the network to grow in depth.",
      "In that case, a new matrix, U, initialized as the identity matrix, is added to the network.",
      "Note that unlike the Net2WiderNet, this approach would not work with arbitrary activation function between the layers.",
      "Strengths  The model can accelerate the training of neural networks, especially during development cycle when the designers try out different models.",
      "The approach could potentially be used in life-long learning systems where the model is trained over a stream of data and needs to grow over time.",
      "Limitations  The function preserving transformations need to be worked out manually.",
      "Extra care needs to be taken when operations like concatenation or batch norm are present."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1511.05641",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 1540690
  },
  {
    "blog_id": "assessing-generalization-in-deep-reinforcement-learning",
    "summary": [
      "The paper presents a benchmark and experimental protocol (environments, metrics, baselines, training/testing setup) to evaluate RL algorithms for generalization.",
      "Several RL algorithms are evaluated and the key takeaway is that the \u201cvanilla\u201d RL algorithms can generalize better than the RL algorithms that are specifically designed to generalize, given enough diversity in the distribution of the training environments.",
      "The focus is on evaluating generalization to environmental changes that affect the system dynamics (and not the goal or rewards).",
      "Two generalization regimes are considered:  Interpolation - parameters of the test environment are similar to the parameters of the training environment.",
      "Extrapolation - parameters of the test environment are different from the parameters of the training environment.",
      "Following algorithms are considered as part of the benchmark:  \u201cVanilla\u201d RL algorithms - A2C, PPO  RL algorithms that are designed to generalize:  EPOpt - Learn a (robust) policy that maximizes the expected reward over the most difficult distribution of environments (ones with the worst expected reward).",
      "RL2 - Learn an (adaptive) policy that can adapt to the current environment/task by considering the trajectory and not just the state transition sequence.",
      "These specially designed RL algorithms can be optimized using either A2C or PPO leading to combinations like EPOpt-A2C or EPOpt-PPO etc.",
      "The models are either composed of feedforward networks completely or feedforward + recurrent networks.",
      "Environments  CartPole, MountainCar, Acrobot, and Pendulum from OpenAI Gym.",
      "HalfCheetah and Hopper from OpenAI Roboschool.",
      "Three versions of each environment are considered:  Deterministic: Environment parameters are fixed.",
      "This case corresponds to the standard environment setup in classical RL.",
      "Random: Environment parameters are sampled randomly.",
      "This case corresponds to sampling from a distribution of environments.",
      "Extreme: Environment parameters are sampled from their extreme values.",
      "This case corresponds to the edge-case environments which would not be encountered during training generally.",
      "Performance Metrics  Average total reward per episode.",
      "Success percentage: Percentage of episodes where a certain goal (or reward) is obtained.",
      "Evaluation Metrics/Setups  Default: success percentage when training and evaluating the deterministic version of the environment.",
      "Interpolation: success percentage when training and evaluating on the random version of the environment.",
      "Extrapolation: the geometric mean of the success percentage of following three versions:  Train on deterministic and evaluate on the random version.",
      "Train on deterministic and evaluate on extreme version.",
      "Train on random and evaluate on the extreme version.",
      "Observations  Extrapolation is harder than interpolation.",
      "Increasing the diversity in the training environments improves the interpolation generalization of vanilla RL methods.",
      "EPOpt improves generalization only for continuous control environments and only with PPO.",
      "RL2 is difficult to train on the environments considered and did not provide a clear advantage in terms of generalization.",
      "EPOpt-PPO outperforms PPO on only 3 environments and EPOpt-A2C does not"
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1810.12282",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 91812941
  },
  {
    "blog_id": "mohamedr15",
    "summary": [
      "This paper presents a variational approach to the maximisation of mutual information in the context of a reinforcement learning agent.",
      "Mutual information in this context can provide a learning signal to the agent that is \"intrinsically motivated\", because it relies solely on the agent's state/beliefs and does not require from the (\"outside\") user an explicit definition of rewards.",
      "Specifically, the learning objective, for a current state s, is the mutual information between the sequence of K actions a proposed by an exploration distribution $w(a|s)$ and the final state s' of the agent after performing these actions.",
      "To understand what the properties of this objective, it is useful to consider the form of this mutual information as a difference of conditional entropies:  $$I(a,s'|s) = H(a|s) - H(a|s',s)$$  Where $I(.|.",
      ")$ is the (conditional) mutual information and $H(.|.",
      ")$ is the (conditional) entropy.",
      "This objective thus asks that the agent find an exploration distribution that explores as much as possible (i.e. has high $H(a|s)$ entropy) but is such that these actions have predictable consequences (i.e.",
      "lead to predictable state s' so that $H(a|s',s)$ is low).",
      "So one could think of the agent as trying to learn to have control of as much of the environment as possible, thus this objective has also been coined as \"empowerment\".",
      "The main contribution of this work is to show how to train, on a large scale (i.e. larger state space and action space) with this objective, using neural networks.",
      "They build on a variational lower bound on the mutual information and then derive from it a stochastic variational training algorithm for it.",
      "The procedure has 3 components: the exploration distribution $w(a|s)$, the environment $p(s'|s,a)$ (can be thought as an encoder, but which isn't modeled and is only interacted with/sampled from) and the planning model $p(a|s',s)$ (which is modeled and can be thought of as a decoder).",
      "The main technical contribution is in how to update the exploration distribution (see section 4.2.2 for the technical details).",
      "This approach exploits neural networks of various forms.",
      "Neural autoregressive generative models are also used as models for the exploration distribution as well as the decoder or planning distribution.",
      "Interestingly, the framework allows to also learn the state representation s as a function of some \"raw\" representation x of states.",
      "For raw states corresponding to images (e.g. the pixels of the screen image in a game), CNNs are used."
    ],
    "author_id": "hlarochelle",
    "pdf_url": "http://papers.nips.cc/paper/5668-variational-information-maximisation-for-intrinsically-motivated-reinforcement-learning.pdf",
    "author_full_name": "Hugo Larochelle",
    "source_website": "https://www.shortscience.org/user?name=hlarochelle",
    "id": 63848470
  },
  {
    "blog_id": "c96d7dd0488d0d00bd7078889dd6f6",
    "summary": [
      "Semantic parsing is the problem of mapping natural language utterances into logical forms that can be executed on a Knowledge Base (KB).",
      "The paper presents a new approach to semantic parsing that uses paraphrasing to leverage the large amount of text which is not covered by the KB.",
      "Approach  Given an input utterance x, construct a set of logical forms Zx using a different kind of logical form templates.",
      "For each logical form z in Zx, generate a small set Cz, of canonical utterances using Freebase description of the type, entity and property involved in the logical form.",
      "Note: Both the steps above are performed with a small, simple set of deterministic rules which the authors found sufficient for their datasets.",
      "For each z in Z and each c in Cz, use a paraphrase model to score pairs (c, z) given x.",
      "The paraphrase model has two parts:  Association Model  For each pair of (x, c), the model goes through all spans of x and c and identifies pairs of potential paraphrases (associations).",
      "To determine the associations, the model uses  Phrase pairs from a phrase table, constructed using Paralex corpus.",
      "Linguistic features like lemma, POS tag and Wordnet derivations.",
      "During training, the model learns to weight the associations appropriately.",
      "Vector Space Model  Assign vector representations to x and c by averaging over the word2vec representations corresponding to the different words in these utterances.",
      "Estimate paraphrase score for (x, c) via weighted combination of their vector representations.",
      "The two paraphrase models are complementary to each other in terms of the information they capture.",
      "Training  Dataset  WEBQUESTIONS dataset - 5810 question answer pairs.",
      "FREE917 dataset - 917 questions (annoted with logical form).",
      "Learning  Given the question-answer pair (xi, yi), the objective function minimizes the log-likelihood of the correct answer along with the restriction of L1 regularization.",
      "Results  The proposed model improves the accuracy on WEBQUESTIONS by 12% and matches the best results on FREE917.",
      "Removing the association model results in a much larger degradation of performance as compared to removing the VS model.",
      "Error analysis suggests that the model:  can not handle temporal relations.",
      "suffers from ambiguity in entity recognition.",
      "counts multiple associations multiple times and assigns inflated scores to such associations.",
      "Comments  The core idea of using paraphrasing for semantic parsing seems promising and would further benefit from advanced models like skip-thought vectors which provide a more natural vector representation for the sentences, thereby helping to reduce the dependence on handcrafted features.",
      "This comment has been minimized.",
      "Sign in to view  Copy link  Quote reply  BrijeshKaria commented  Jan 20, 2018  Is there any good implementation of this available as open source?",
      "I am trying sempre which is based on this same concept.",
      "I am however stuck there with lack of good documentation about how to create paraphrase models for custom domain."
    ],
    "author_id": "shugan",
    "pdf_url": "http://nlp.stanford.edu/pubs/berant14paraphrasing.pdf",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 94646863
  },
  {
    "blog_id": "the-google-file-system",
    "summary": [
      "The Google File System \u2013 Ghemawat, Gobioff & Leung, 2003  Here\u2019s a paper with a lot to answer for!",
      "Back in 2003 Ghemawat et al reported that  We have designed and implemented the Google File System, a scalable distributed file system for large distributed data-intensive applications.",
      "It provides fault-tolerance while running on inexpensive commodity hardware, and it delivers high aggregate performance to a large number of clients.",
      "Google\u2019s workloads and use of commodity hardware led to the following observations:  Component failures are the norm rather than the exception  The files Google worked with were huge by traditional standards \u2013 multi-GB files were very common.",
      "Managing a large number of small (Kb) files became unwieldy causing block size and I/O to be revisited.",
      "Most Google workloads only mutated files by appending to them.",
      "Random writes were practically non-existent.",
      "Files were often only read sequentially  Google were able to co-design their applications and the API to their file system (i.e. abandon the requirement for POSIX compliance) for better flexibility  The result was the Google File System, designed to store a modest (few million) number of large files, with workloads consisting of large streaming reads and small random reads, large sequential writes that append data to files, and semantics for supporting multiple clients concurrently appending to the same file.",
      "In addition:  High sustained bandwidth is more important than low latency.",
      "The implementation details probably sound very familiar: a master and multiple chunkservers, with files divided into replicated chunks of 64MB.",
      "We keep the overall system highly available with two simple yet effective strategies: fast recovery and replication.",
      "We always talk a lot about replication, it\u2019s good to remember the fast-recovery part of the equation too.",
      "Also of note in the paper is the empasis on providing good diagnostic tools.",
      "In summary:  The Google File System demonstrates the qualities essential for supporting large-scale data processing workloads on commodity hardware.",
      "While some design decisions are specific to our unique setting, many apply to data processing tasks of a similar magnitude and cost consciousness.",
      "GFS of course went on to inspire Hadoop\u2019s HDFS, and the rest is history.",
      "It\u2019s good to go back and look at the workload assumptions that inspired GFS, as a sanity check that your use-cases match.",
      "Deploying HDFS just because 67% of other companies install HDFS is not a good strategy!",
      "It\u2019s also interesting to wonder how things would have played out if Google themselves had decided to open source GFS.",
      "Google of course subsequently went on to introduce Caffeine and  Colossus (aka GFS 2) as they needed to respond in more of a real-time than a batch mode.",
      "HDFS evolved to\u2026.",
      "?"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en/us/archive/gfs-sosp2003.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 56472821
  },
  {
    "blog_id": "large-scale-analysis-of-style-injection-by-relative-path-overwrite",
    "summary": [
      "Large-scale analysis of style injection by relative path overwrite Arshad et al., WWW\u201918  (If you don\u2019t have ACM Digital Library access, the paper can be accessed either by following the link above directly from The Morning Paper blog site, or from the WWW 2018 proceedings page).",
      "We\u2019ve all been fairly well trained to have good awareness of cross-site scripting (XSS) attacks.",
      "Less obvious, and also less well known, is that a similar attack is possible using style sheet injection.",
      "A good name for these attacks might be SSS: same-site style attacks.",
      "Even though style injection may appear less serious a threat than script injection, it has been shown that it enables a range of attacks, including secret exfiltration\u2026 Our work shows that around 9% of the sites in the Alexa top 10,000 contain at least one vulnerable page, out of which more than one third can be exploited.",
      "I\u2019m going to break today\u2019s write-up down into four parts:  How on earth do you do secret exfiltration with a stylesheet?",
      "Injecting stylesheet content using Relative Path Overwite (RPO)  Finding RPO vulnerabilities in the wild  How can you defend against RPO attacks?",
      "Secret exfiltration via stylesheets  Style sheet injection belongs to a family of attacks known as \u2018scriptless\u2019 attacks.",
      "While CSS is intended for controlling styling and layout, it does also contain some context-sensitive features that can be used to extract and exfiltrate data.",
      "Suppose a page contains some sensitive data you\u2019d like to get your hands on.",
      "The first thing you need to do is make that visible if it was otherwise hidden (CSS attribute accessors and content properties will help with this).",
      "Once the content is visible, you can apply style directives to it, such as fonts\u2026  Custom attacker-supplied fonts can change the size of the secret text depending on its value.",
      "Animation features can be used to cycle through a number of fonts in order to test different combinations.",
      "Media queries or the appearance of scrollbars can be used to implement conditional style, and data exfiltration by loading a different URL for each condition from the attacker\u2019s server.",
      "Taken together, Heiderich et al. demonstrate that these techniques allow an attacker to steal credit card numbers or CSRF tokens without script execution.",
      "There are other attacks too, this is just one example.",
      "Helping the attacker is the fact that the CSS standard mandates browsers be forgiving when parsing CSS, skipping over parts they don\u2019t understand.",
      "Against the attacker though, is the fact that modern browsers won\u2019t load documents with non-CSS content types or syntax errors as stylesheets, if they come from a different domain than the including page.",
      "The Relative Path Overwrite (RPO) attack vector  If both the including page and the included stylesheet come from the same domain though, it\u2019s game on.",
      "Relative Path Overwrite vulnerabilities allow an attacker to engineer this scenario.",
      "Consider a web page hosted at  [url]"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3178876.3186090?download=true",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 45186835
  },
  {
    "blog_id": "how-transferable-are-features-in-deep-neural-networks",
    "summary": [
      "When neural networks are trained on images, they tend to learn the same kind of features for the first layer (corresponding to Gabor filters or colour blobs).",
      "The first layer features are \u201cgeneral\u201d irrespective of the task/optimizer etc.",
      "The final layer features tend to be \u201cspecific\u201d in the sense that they strongly depend on the task.",
      "The paper studies the transition of generalization property across layers in the network.",
      "This could be useful in the domain of transfer learning where features are reused across tasks.",
      "Setup  Degree of generality of a set of features, learned on task A, is defined as the extent to which these features can be used for another task B.  Randomly split 1000 ImageNet classes into 2 groups (corresponding to tasks A and B).",
      "Each group has 500 classes and half the total number of examples.",
      "Two 8-layer convolutional networks are trained on the two datasets and labelled as baseA and baseB respectively.",
      "Now choose a layer numbered n from {1, 2\u20267}.",
      "For each layer n, train the following two networks:  Selffer Network BnB  Copy (and freeze) first n layers from baseB.",
      "The remaining layers are initialized randomly and trained on B.",
      "This serves as the control group.",
      "Transfer Network AnB  Copy (and freeze) first n layers from baseA.",
      "The remaining layers are initialized randomly and trained on B.",
      "This corresponds to transferring features from A to B.",
      "If AnB performs well, nth layer features are \u201cgeneral\u201d.",
      "In another setting, the transferred layers are also fine-tuned (BnB+ and AnB+).",
      "ImageNet dataset contains a hierarchy of classes which allow for creating the datasets A and B with high and low similarity.",
      "Observation  Dataset A and B are similar  For n = {1, 2}, the performance of the BnB model is same as baseB model.",
      "For n = {3, 4, 5, 6}, the performance of BnB model is worse.",
      "This indicates the presence of \u201cfragile co-adaption\u201d features on successive layers where features interact with each other in a complex way and can not be easily separated across layers.",
      "This is more prominent across middle layers and less across the first and the last layers.",
      "For model AnB, the performance of baseB for n = {1, 2}.",
      "Beyond that, the performance begins to drop.",
      "Transfer learning of features followed by fine-tuning gives better results than training the network from scratch.",
      "Dataset A and B are dissimilar  Effectiveness of feature transfer decreases as the two tasks become less similar.",
      "Random Weights  Instead of using transferred weights in BnB and BnA, the first n layers were initialized randomly.",
      "The performance falls for layer 1 and 2.",
      "It further drops to near-random level for layers 3 and beyond.",
      "Another interesting insight is that even for dissimilar tasks, transferring features is better than using random features."
    ],
    "author_id": "shugan",
    "pdf_url": "http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks.pdf",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 53721399
  },
  {
    "blog_id": "semantic-image-seg-with-deep-g-crfs",
    "summary": [
      "Chandra, Kokkinos, 2016  Semantic Image Segmentation  Image Example  Put simply, the task is to cluster pixels in an image together and to assign all pixels in a cluster a meaningful label.",
      "Labels are generally \u201chigh-level\u201d concepts, such as \u201chorse\u201d, \u201cdog\u201d, \u201cboat\u201d, etc.",
      "Here is the VOC PASCAL 2012 dataset used in this paper.",
      "Discussion Q\u2019s  Why G-CRFs?",
      "Joint distributions are hard, so exploit factorizable (exponential-family) probability distributions and conditional independence.",
      "This results in a graph-like structure.",
      "If the label assigned to each pixel is a random variable, then an conditional random field suggests that there\u2019s a dependence between a pixel\u2019s label and the label\u2019s of other nearby pixels.",
      "This seems intuitive.",
      "Author\u2019s argue that continuous Gaussian outputs are unimodal conditioned on the data, so given an image, one solution dominates the posterior.",
      "The log-likelihood is a quadratic, which allows the approximate inference task to be cast a quadratic optimization in an energy-minimization setting.",
      "Deep G-CRF architecture  The basic idea is to set the outputs of a convolutional neural network to be the energy of a segmentation hypothesis, in the form of  .",
      "The network predicts what the $A$ (pairwise) and $B$ (unary) terms in the energy function are for an image.",
      "$\\lambda$ is set manually to enforce positive-definiteness.",
      "Then a Quadratic Optimization + softmax module gives the final per-class scores for each pixel in the image (See Fig.",
      "1) by solving for $x$.",
      "Originally, $A$ is a $(P \\times L) \\times (P \\times L)$ matrix, where $L$ is the number of class labels and $P$ is number of pixels.",
      "Shared pair-wise terms  $A$ is no longer dependent on the number of class labels; only care about pixel interactions independent on what the label is.",
      "Now, the inference equation $(A + \\lambda I) x = B$ is reduced to a system of $L + 1$ equations for $A$ of dim $P \\times P$.",
      "Conjugate Gradient method  Need to solve $x = A^{-1}b$.",
      "The $A$ matrix is very sparse, since it only deals with 4, 8, or 12-connected neighborhoods.",
      "CG is the current recommended approach for solving $Ax = b$ when $A$ is sparse why?",
      "When $A$ is dense, it is recommended to factorize A and then use backsubstitution.",
      "see here  Experiments  Try out the above + fusing information across 3 different resolutions.",
      "metric - Intersection over Union  baselines - Multiresolution DeepLab-LargeFOV, CRF-RNN  QO network - Baseline extended to have a binary and unary stream.",
      "QO module can be shared by all resolutions, or replicated three times for each scale  75.5% mean IOU on VOC PASCAL 2012 for this approach.",
      "Without more details and tests of significance, hard to say whether this method is really more effective than prev SOTA.",
      "It seems to do about ~1-2% IOU better than the baselines.",
      "Also seems to be much faster."
    ],
    "author_id": "pemami",
    "pdf_url": "https://arxiv.org/pdf/1603.08358",
    "author_full_name": "Patrick Emami",
    "source_website": "https://pemami4911.github.io/index.html",
    "id": 21542239
  },
  {
    "blog_id": "5748b6339ceff26420ceecfc79d58d",
    "summary": [
      "The paper explores the strengths and weaknesses of different evaluation metrics for end-to-end dialogue systems(in unsupervised setting).",
      "Evaluation Metrics Considered  Word Based Similarity Metric  BLEU  Analyses the co-occurrences of n-grams in the ground truth and the proposed responses.",
      "BLEU-N: N-gram precision for the entire dataset.",
      "Brevity penalty added to avoid bias towards short sentences.",
      "METEOR  Create explicit alignment between candidate and target response (using Wordnet, stemmed token etc).",
      "Compute the harmonic mean of precision and recall between proposed and ground truth.",
      "ROGUE  F-measure based on Longest Common Subsequence (LCS) between candidate and target response.",
      "Embedding Based Metric  Greedy Matching  Each token in actual response is greedily matched with each token in predicted response based on cosine similarity of word embedding (and vice-versa).",
      "Total score is averaged over all words.",
      "Embedding Average  Calculate sentence level embedding by averaging word level embeddings  Compare sentence level embeddings between candidate and target sentences.",
      "Vector Extrema  For each dimension in the word vector, take the most extreme value amongst all word vectors in the sentence, and use that value in the sentence-level embedding.",
      "Idea is that by taking the maxima along each dimension, we can ignore the common words (which will be pulled towards the origin in the vector space).",
      "Dialogue Models Considered  Retrieval Models  TF-IDF  Compute the TF-IDF vectors for each context and response in the corpus.",
      "C-TFIDF computes the cosine similarity between an input context and all other contexts in the corpus and returns the response with the highest score.",
      "R-TFIDF computes the cosine similarity between the input context and each response directly.",
      "Dual Encoder  Two RNNs which respectively compute the vector representation of the input context and response.",
      "Then calculate the probability that given response is the ground truth response given the context.",
      "Generative Models  LSTM language model  LSTM model trained to predict the next word in the (context, response) pair.",
      "Given a context, model encodes it with the LSTM and generates a response using a greedy beam search procedure.",
      "Hierarchical Recurrent Encoder-Decoder (HRED)  Uses a hierarchy of encoders.",
      "Each utterance in the context passes through an \u2018utterance-level\u2019 encoder and the output of these encoders is passed through another 'context-level' decoder.",
      "Better handling of long-term dependencies as compared to the conventional Encoder-Decoder.",
      "Observations  Human survey to determine the correlation between human judgement on the quality of responses, and the score assigned by each metric.",
      "Metrics (especially BLEU-4 and BLEU-3) correlate poorly with human evaluation.",
      "Best performing metric:  Using word-overlaps - BLEU-2 score  Using word embeddings - vector average  Embedding-based metrics would benefit from a weighting of word saliency.",
      "BLEU could still be a good evaluation metric in constrained tasks like mapping dialogue acts to natural language sentences."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1603.08023",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 99878837
  },
  {
    "blog_id": "deep-exploration",
    "summary": [
      "This paper presents a novel approach to replace the classic epsilon-greedy exploration strategy.",
      "The main idea is to encourage deep exploration by creating a new Deep Q-Learning architecture that supports selecting actions from randomized Q-functions that are trained on bootstrapped data.",
      "This is a quick look at the proposed architecture.",
      "Bootstrapped DQN architecture  Each head represents a different Q-function that is trained on a subset of the data.",
      "The shared network learns a joint feature representation across all the data; it can be thought of as a data-dependent dropout.",
      "For DRL, samples stored in a replay buffer contain a flag marking which of the K Q-functions it came from.",
      "The algorithm speeds up learning compared to other exploration tactics for DRL since it encourages deep exploration; at the beginning of each episode, a different Q-function is randomly sampled from a uniform distribution and it is used until the end of that episode.",
      "Another key component of the Bootstrapped DQN algorithm is the bootstrap mask.",
      "The mask decides, for each Q-value function, whether or not it should train upon the experience generated at step t. Each individual experience is given a randomly sampled mask m ~ M, where M is Bernoulli, Poission, etc.",
      "Then, when training the network on a minibatch sampled from the replay buffer, the mask m decides whether or not a specific Q-value function is to be trained upon that experience.",
      "The authors show that the effect of this on the learning process is akin to dropout.",
      "This heuristic, plus the randomized Q-value functions, help Bootstrapped DQN deal with learning from noisy data and exploring complex state/action spaces efficiently.",
      "Strengths  The authors based their idea on sound statistical principles and conducted numerous experiments to back up their claims.",
      "Their results show that Bootstrapped DQN can learn faster (but not necessarily with higher long-term rewards) than state-of-the-art DQN.",
      "The authors also compare their work with Stadie, Levine, and Abeel\u2019s paper on Incentivizing Exploration in RL.",
      "See my previous post for details .",
      "The authors show that Bootstrapped DQN outperforms Stadie\u2019s methods, as Stadie\u2019s methods attempt the more ambitious task of learning a model of the task dynamics and using how well the agent has learned said model to inform the exploration.",
      "Weaknesses  The paper is a bit hard to follow at times, and you have to go all the way to the appendix to get a good understanding of how the entire algorithm comes together and works.",
      "The step-by-step algorithm description could be more complete (there are steps of the training process left out, albeit they are not unique to Bootstrap DQN) and should not be hidden down in the appendix.",
      "This should probably be in Section 3.",
      "The MDP examples in Section 5 were not explained well; it feels like it doesn\u2019t contribute too much to the overall impact of the paper."
    ],
    "author_id": "pemami",
    "pdf_url": "http://arxiv.org/pdf/1602.04621v3.pdf",
    "author_full_name": "Patrick Emami",
    "source_website": "https://pemami4911.github.io/index.html",
    "id": 5674972
  },
  {
    "blog_id": "multi-view_face_detection_using_deep_convolutional_neural_networks",
    "summary": [
      "What  They propose a CNN-based approach to detect faces in a wide range of orientations using a single model.",
      "However, since the training set is skewed, the network is more confident about up-right faces.",
      "The model does not require additional components such as segmentation, bounding-box regression, segmentation, or SVM classifiers  How  Data augmentation: to increase the number of positive samples (24K face annotations), the authors used randomly sampled sub-windows of the images with IOU > 50% and also randomly flipped these images.",
      "In total, there were 20K positive and 20M negative training samples.",
      "CNN Architecture: 5 convolutional layers followed by 3 fully-connected.",
      "The fully-connected layers were converted to convolutional layers.",
      "Non-Maximal Suppression is applied to merge predicted bounding boxes.",
      "Training: the CNN was trained using Caffe Library in the AFLW dataset with the following parameters:  Fine-tuning with AlexNet model  Input image size = 227x227  Batch size = 128 (32+, 96-)  Stride = 32  Test: the model was evaluated on PASCAL FACE, AFW, and FDDB dataset.",
      "Running time: since the fully-connected layers were converted to convolutional layers, the input image in running time may be of any size, obtaining a heat map as output.",
      "To detect faces of different sizes though, the image is scaled up/down and new heatmaps are obtained.",
      "The authors found that rescaling image 3 times per octave gives reasonable good performance.",
      "The authors realized that the model is more confident about up-right faces than rotated/occluded ones.",
      "This trend is because the lack of good training examples to represent such faces in the training process.",
      "Better results can be achieved by using better sampling strategies and more sophisticated data augmentation techniques.",
      "The authors tested different strategies for NMS and the effect of bounding-box regression for improving face detection.",
      "They NMS-avg had better performance compared to NMS-max in terms of average precision.",
      "On the other hand, adding a bounding-box regressor degraded the performance for both NMS strategies due to the mismatch between annotations of the training set and the test set.",
      "This mismatch is mostly for side-view faces.",
      "Results:  In comparison to R-CNN, the proposed face detector had significantly better performance independent of the NMS strategy.",
      "The authors believe the inferior performance of R-CNN due to the loss of recall since selective search may miss some of the face regions; and loss in localization since bounding-box regression is not perfect and may not be able to fully align the segmentation bounding-boxes, provided by selective search, with the ground truth.",
      "In comparison to other state-of-art methods like structural model, TSM and cascade-based methods the DDFD achieve similar or better results.",
      "However, this comparison is not completely fair since the most of methods use extra information of pose annotation or information about facial landmarks during the training."
    ],
    "author_id": "ALEJU",
    "pdf_url": "https://arxiv.org/pdf/1502.02766",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 97980035
  },
  {
    "blog_id": "distilling-the-knowledge-in-a-neural-network",
    "summary": [
      "In machine learning, it is common to train a single large model (with a large number of parameters) or ensemble of multiple smaller models using the same dataset.",
      "While such large models help to improve the performance of the system, they also make it difficult and computationally expensive to deploy the system.",
      "The paper proposes to transfer the knowledge from such \u201ccumbersome\u201d models into a single, \u201csimpler\u201d model which is more suitable for deployment.",
      "This transfer of knowledge is referred to as \u201cdistillation\u201d.",
      "Idea  Train the cumbersome model using the given training data in the usual way.",
      "Train the simpler, distilled model using the class probabilities (from the cumbersome model) as the soft target.",
      "Thus, the simpler model is trained to generalise the same way as the cumbersome model.",
      "If the soft targets have high entropy, they provide much more information than the hard targets and the gradient (between training examples) would vary lesser.",
      "One approach is to minimise the L2 difference between logits produced by the cumbersome model and the simpler model.",
      "This approach was pursued by Bucilu\u01ce et al. The paper proposes a more general solution which they name \u201cdistillation\u201d.",
      "The temperature of the final softmax is increased till the cumbersome model produces a set of soft targets (from the final softmax layer).",
      "These soft targets are then used to train the simpler model.",
      "It also shows that the proposed approach is, in fact, a more general case of the first approach.",
      "Approach  In the simplest setting, the cumbersome model is first trained with a high value of temperature and then the same temperature value is used to train the simpler model.",
      "The temperature is set to 1 when making predictions using the simpler model.",
      "It helps to add an auxiliary objective function which corresponds to the cross-entropy loss with the correct labels.",
      "The second objective function should be given a much lower weight though.",
      "Further, the magnitude of the soft targets needs to be scaled by multiplying with the square of temperature.",
      "Experiment  The paper reports favourable results for distillation task for the following domains:  Image Classification (on MNIST dataset)  An extra experiment is performed where the simpler model is not shown any images of \u201c3\u201d but the model fails for only 133 cases out of 1010 cases involving \u201c3\u201d.",
      "Automatic Speech Recognition (ASR)  An extra experiment is performed where the baseline model is trained using both hard targets and soft targets alternatively.",
      "Further, only 3% of the total dataset is used.",
      "The model using hard targets overfits and has poor test accuracy while the model using soft targets does not overfit and gets much better test accuracy.",
      "This shows the regularizing effect of soft targets.",
      "Training ensemble specialists for very large datasets (JFT dataset - an internal dataset at Google)  The experiment shows that while training a single large model would take a lot of time, the performance of the model can be improved by learning a small number of specialised networks (which are faster to train).",
      "Though it is yet to be shown that the knowledge of such specialist models can be distilled back into a single model."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1503.02531",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 7415123
  },
  {
    "blog_id": "virtual_to_real_rl_for_ad",
    "summary": [
      "What  They suggest a method to train a model for self-driving cars in a (virtual) game, while letting each frame look like a real one.",
      "This allows to learn driving policies via reinforcement learning in a virtual environment, yet (later on) use them in real cars.",
      "How  Basics  The method is based on GANs, similar to something like CycleGAN.",
      "The basic idea is to transform each game image/frame into a (semantic) segmentation map.",
      "Then the segmentation map is transformed into a realistic looking image.",
      "Both steps use GANs.",
      "The model is then trained on the realistic looking images, instead of the game frames.",
      "(Why not just train it on the segmentation maps...?)",
      "They argue that the segmentation maps can be viewed as the semantic representation between both (fake & real) images.",
      "(Similar to how machine translation models often convert each sentence to a vector representing the semantics before generating the translated sentence.)",
      "Visualization of the architecture:  Loss  They use conditional GANs.",
      "The generator gets the frame image x and a noise vector z and has to generate a segmentation map s.  The discriminator gets the frame image x and a segmentation map s and has to tell whether s is real or fake.",
      "A second pair of generator and discriminator is then used to turn s into real images.",
      "They use the standard GAN loss.",
      "They add an L1 loss to \"suppress blurring\".",
      "(???",
      "GANs shouldn't generate blurry images.",
      "This sounds more like they train G to predict s using the L1 loss.",
      "They then end up with blurry images, so they add the GAN loss to make them sharp.)",
      "Full loss:  Agent  They use the A3C algorithm for training.",
      "(12 threads)  Their reward function incentivizes fast speeds with the car being close to the road's center.",
      "They punish collisions.",
      "Reward function:  v_t is the speed in m/s  alpha is the angle in rad  beta = 0.006  gamma = -0.025  They predict 9 actions: left/straight/right, each with accelerate/brake/nothing.",
      "Game  They train on the game \"TORCS\".",
      "(I guess that game provides segmentation maps for each game frame?)",
      "Results  They train their model in TORCS on track X and evaluate on Y.",
      "They achieve slightly better scores than a competing model trained on several tracks (A, B, C, D, ..., but not on X).",
      "A model trained directly on X peforms significantly better.",
      "They test their model on a dataset associated with the NVIDIA self-driving paper.",
      "They reach 43% correct, while the supervised method reaches 53%.",
      "A competing model \"B-RL\" reaches 28% (reinforcement learned, but only on game images).",
      "Example translations from game to real:"
    ],
    "author_id": "ALEJU",
    "pdf_url": "https://arxiv.org/pdf/1704.03952",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 26473847
  },
  {
    "blog_id": "dilated_recurrent_neural_networks",
    "summary": [
      "What  They add a dilation factor to recurrent neural networks (e.g. LSTMs), similar to dilation in convolutions.",
      "This enables learning of long-term dependencies, prevents vanishing/exploding gradients and makes the networks sometimes more parallelizable.",
      "How  Dilation  Dilation d here simply means, that each timestep gets input from the d-th previous timestep.",
      "With d=1, this is identical to a normal reccurent network.",
      "With d=2 there is a gap of 1 between each timestep.",
      "They suggest to let the dilation exponentially increase per layer, e.g. 1, 2, 4, ...  Visualization:  Using dilation can make it possible to execute some steps of the RNN in parallel.",
      "The following visualization shows that:  The dilation may start at a value higher than 1.",
      "This however should be compensated before generating the output vector.",
      "To do that, output of the last layer at timestep t and t-1 has to be used.",
      "Visualization:  Memory capacity  They show that the memory capacity of dilated RNNs is better than in skip RNNs, i.e. the average path length in the network is shorter.",
      "Results  Copy-Task  This task involves copying of inputs.",
      "I.e. the network gets some integers at the start, then T={500, 1000} timesteps pass, then it has to output the input values.",
      "They use 9 layers with a dilation of up to 256.",
      "(This means that it is really almost raw copying of data for the network.)",
      "Dilated RNNs perform best here, followed by dilated GRUs and dilated LSTMs.",
      "All non-dilated networks resort to random guessing (i.e. fail to learn anything).",
      "MNIST  They predict classes on MNIST, where each image is turned into a 784-element vector.",
      "All networks, including competitors, can handle that task.",
      "They make the task harder by padding the vectors to 1000 and 2000 elements length.",
      "Then only their dilated networks, dilated (1d-)CNNs and RNNs with skip connections can handle the task.",
      "Their dilated RNNs learn faster the more layers they have.",
      "With 2 layers they learn nothing.",
      "With few layers they can sometimes also have major swings in accuracy during training.",
      "When increasing the minimum dilation, they find that they can drop layers and still achieve almost the same accuracy, leading to much faster training (wall-clock time).",
      "Training with minimum dilation 2 leads to same accuracy at half the training time.",
      "Language modelling  They test their models on Penn Treebank character predictions.",
      "Their models get beaten by LayerNorm HM-LSTM, HyperNetworks, Zoneout.",
      "They argue though that their models achieve the highest scores among models that do not use any normalization.",
      "Speaker identification from raw waveform  They train on VCTK.",
      "Their dilated models achieve much higher accuracy than non-dilated ones and can compete with models using MFCC features."
    ],
    "author_id": "ALEJU",
    "pdf_url": "https://arxiv.org/pdf/1710.02224",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 15643187
  },
  {
    "blog_id": "e6213906ec2642f27b1aca3a6201c6",
    "summary": [
      "The paper describes a \"compositional\" training approach for vector space models, corresponding to Knowledge Bases (KBs).",
      "The new approach improves the system's ability to answer path queries and impute missing information for the KBs.",
      "Task  Given a KB, knowledge graph, G, is defined as the set of triplets (s, r, t) where s, t \u2208 Entities and r \u2208 Relations.",
      "A path query q consists of an initial entity, s, followed by a sequence of relations, p, to be traversed.",
      "The answer to the query is the set of all the entities that can be reached from s by traversing p.  Knowledge base completion (KBC) is the task of predicting if an edge (s, r, t) belongs in the graph.",
      "Compositionalization  Given a triplet (s, r, t), define score(s/r, t) as the liklihood of s being connected to t via r.  In general, score(s/r, t) = M(Tr(xs), xt) for some membership operator M and some traversal operator T.  Given a dataset of form (q, t) where q is the path query and t is the answer to the path query,  Minimize the max-margin objective 1 - margin(q, t, t')  margin(q, t, t') = score(q, t) - score(q, t')  This objective function is better that the existing objectives which only train on queries of length 1 (single-edge training).",
      "Candidates Models  TransE  score(s/r, t) = -|| x<sub>s</sub> + w><sub>r</sub> - x<sub>t</sub>||<sub>2</sub><sup>2</sup>  Bilinear-Diag  Similar to TransE, but with multiplicative interactions between entity and relation vectors.",
      "Datasets  Single-Edge Query datasets:  Freebase  WordNet  Path Query Dataset  Given a base knowledge graph, generate path queries of different lengths by performing random walks on the graph.",
      "Results  Evalution Metric  Mean Quantile - For a query q, the quantile of a correct answer t is the fraction of incorrect answers ranked after t.  hit at 10 - Percentage of correct answers ranked among top 10 results.",
      "Compositional training improves path querying performance across all models and metrics on both the datasets.",
      "TransE(COMP) is the best model in terms of mean quantile.",
      "Performance improves for both induction and deduction based queries.",
      "Analysis  Why does compositional training improve path query answering?",
      "Cascading nature of errors along the path - For a given edge (s, r, t) on the path, the single-edge training encourages xt to be closer to xs, only to the extent that margin is 1 and does not push them any closer.",
      "The remaining discrepancy gets added as noise at each step of the traversal.",
      "Why does compositional training improve knowledge base completion?",
      "Paths in a knowledge graph are an important feature for predicting the existence of single edges and training on paths should provide some form of structural regularisation which should reduce cascading errors."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1506.01094",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 31183232
  },
  {
    "blog_id": "stacked_hourglass_networks_for_human_pose_estimation",
    "summary": [
      "What  They suggest a new model architecture for human pose estimation (i.e. \"lay a skeleton over a person\").",
      "Their architecture is based progressive pooling followed by progressive upsampling, creating an hourglass form.",
      "Input are images showing a person's body.",
      "Outputs are K heatmaps (for K body joints), with each heatmap showing the likely position of a single joint on the person (e.g. \"akle\", \"wrist\", \"left hand\", ...).",
      "How  Basic building block  They use residuals as their basic building block.",
      "Each residual has three layers: One 1x1 convolution for dimensionality reduction (from 256 to 128 channels), a 3x3 convolution, a 1x1 convolution for dimensionality increase (back to 256).",
      "Visualized:  Architecture  Their architecture starts with one standard 7x7 convolutions that has strides of (2, 2).",
      "They use MaxPooling (2x2, strides of (2, 2)) to downsample the images/feature maps.",
      "They use Nearest Neighbour upsampling (factor 2) to upsample the images/feature maps.",
      "After every pooling step they add three of their basic building blocks.",
      "Before each pooling step they branch off the current feature map as a minor branch and apply three basic building blocks to it.",
      "Then they add it back to the main branch after that one has been upsampeled again to the original size.",
      "The feature maps between each basic building block have (usually) 256 channels.",
      "Their HourGlass ends in two 1x1 convolutions that create the heatmaps.",
      "They stack two of their HourGlass networks after each other.",
      "Between them they place an intermediate loss.",
      "That way, the second network can learn to improve the predictions of the first network.",
      "Architecture visualized:  Heatmaps  The output generated by the network are heatmaps, one per joint.",
      "Each ground truth heatmap has a small gaussian peak at the correct position of a joint, everything else has value 0.",
      "If a joint isn't visible, the ground truth heatmap for that joint is all zeros.",
      "Other stuff  They use batch normalization.",
      "Activation functions are ReLUs.",
      "They use RMSprob as their optimizer.",
      "Implemented in Torch.",
      "Results  They train and test on FLIC (only one HourGlass) and MPII (two stacked HourGlass networks).",
      "Training is done with augmentations (horizontal flip, up to 30 degress rotation, scaling, no translation to keep the body of interest in the center of the image).",
      "Evaluation is done via PCK@0.2 (i.e. percentage of predicted keypoints that are within 0.2 head sizes of their ground truth annotation (head size of the specific body)).",
      "Results on FLIC are at >95%.",
      "Results on MPII are between 80.6% (ankle) and 97.6% (head).",
      "Average is 89.4%.",
      "Using two stacked HourGlass networks performs around 3% better than one HourGlass network (even when adjusting for parameters).",
      "Training time was 5 days on a Titan X (9xx generation)."
    ],
    "author_id": "ALEJU",
    "pdf_url": "https://arxiv.org/pdf/1603.06937",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 88049354
  },
  {
    "blog_id": "ollivierc15",
    "summary": [
      "This paper suggests a method (NoBackTrack) for training recurrent neural networks in an online way, i.e. without having to do backprop through time.",
      "One way of understanding the method is that it applies the [forward method for automatic differentiation](//en.wikipedia.org/wiki/Automatic_differentiation#Forward_accumulation), but since it requires maintaining a large Jacobian matrix (nb.",
      "of hidden units times nb.",
      "of parameters), they propose a way of obtaining a stochastic (but unbiased!)",
      "estimate of that matrix.",
      "Moreover, the method is improved by using Kalman filtering on that estimate, effectively smoothing the estimate over time.",
      "#### My two cents  Online training of RNNs is a big, unsolved problem.",
      "The current approach people use is to truncate backprop to only a few steps in the past, which is more of a heuristic.",
      "This paper makes progress towards a more principled approach.",
      "I really like the \"rank-one trick\" of Equation 7, really cute!",
      "And it is quite central to this method too, so good job on connecting those dots!",
      "The authors present this work as being preliminary, and indeed they do not compare with truncated backprop.",
      "I really hope they do in a future version of this work.",
      "Also, I don't think I buy their argument that the \"theory of stochastic gradient descent applies\".",
      "Here's the reason.",
      "So the method tracks the Jacobian of the hidden state wrt the parameter, which they note $G(t)$.",
      "It is update into $G(t+1)$, using a recursion which is based on the chain rule.",
      "However, between computing $G(t)$ and $G(t+1)$, a gradient step is performed during training.",
      "This means that $G(t)$ is now slightly stale, and corresponds to the gradient with respect to old value of the parameters, not the current value.",
      "As far as I understand, this implies that $G(t+1)$ (more specifically, its stochastic estimate as proposed in this paper) isn't unbiased anymore.",
      "So, unless I'm missing something (which I might!",
      "), I don't think we can invoke the theory of SGD as they suggest.",
      "But frankly, that last issue seems pretty unavoidable in the online setting.",
      "I suspect this will never be solved, and future research will have to somehow have to design learning algorithms that are robust to this issue (or develop new theory that shows it isn't one).",
      "So overall, kudos to the authors, and I'm really looking forward to read more about where this research goes!"
    ],
    "author_id": "hlarochelle",
    "pdf_url": "http://arxiv.org/pdf/1507.07680",
    "author_full_name": "Hugo Larochelle",
    "source_website": "https://www.shortscience.org/user?name=hlarochelle",
    "id": 43687503
  },
  {
    "blog_id": "a136088f58d24f7b08056ec8b97595",
    "summary": [
      "The paper introduces:  Sentiment Sentiment Treebank - A dataset containing 215,154 phrases with fine-grained sentiment labels (5 classes).",
      "Recursive Neural Tensor Network - Model to learn these fine-grained sentiment labels.",
      "Sentiment Sentiment Treebank  Corpus of 11,855 sentences with fully labelled parse trees.",
      "Can be used to analyse the compositional effects of sentiment in language.",
      "Start with movie reviews dataset , normalise and parse sentences, and label using crowdsourcing on Amazon Mechanical Turk.",
      "Observations  Majority of shorter phrases are neutral while longer phrases have stronger sentiments.",
      "A 5-class classification is sufficient to capture the variability of sentiments.",
      "Recursive Neural Models  Parse a given n-gram into a binary tree and represent each word (corresponding to leaves in the tree) using a d-dimensional vector.",
      "Compute parent vectors using a bottom-up approach using different composition functions.",
      "To start with, word vectors are initialized randomly from a uniform distribution.",
      "For classification task, use the compositions word vectors as input for the softmax.",
      "Different models differ in terms of how word vectors are combined together as shown in the figure.",
      "RNN: Recursive Neural Network  Uses the equation shown in the figure  f = tanh  W is the weight matrix to be learnt.",
      "Con  Input vectors interact only implicitly, via the nonlinear tanh function.",
      "MV-RNN: Matrix-Vector RNN  Represent every word and phrase as both a vector and a matrix.",
      "Matrix for each word is initialized as identity matrix plus a small Gaussian noise.",
      "For the parse tree as  the equation used is  W and WM are both learnt.",
      "Con  Number of parameters depend on the size of vocabulary and could be very large.",
      "RNTN: Recursive Neural Tensor Network  Equations  V is the tensor that defines multiple bilinear forms.",
      "Each slice of the tensor V can be interpreted as capturing a specific type of composition.",
      "Observations  Models compared with  Naive Bayes - NB  SVMs  Naive Bayes with bag of bigram features - biNB  Average neural word vectors (ignoring word order) - vecAvg  Task  Fine-grained Sentiment For All Phrases  RNTN > MV-RNN > RNN > other models.",
      "Full Sentence Binary Sentiment  RNTN pushes the state of the art on short phrases to 85.4%  Contrastive Conjunction  Sentences of the form X but Y  RNTN > MV-RNN > RNN > SVM  RNTN outperforms other models in special cases like where a positive sentence is negated or where a negative sentence is negated to make it less negative (not positive though).",
      "This suggests that RNTN could capture the effect of negative words in both positive and negative sentiment sentences.",
      "Notes  The optimal word vector size reported by the paper was between 25 and 35 and these word vectors were trained as part of sentiment tagging process.",
      "It would be interesting to see how are these results affected by using word vectors from say Glove which may or may not be fine tuned for the sentiment labelling."
    ],
    "author_id": "shugan",
    "pdf_url": "http://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 47469397
  },
  {
    "blog_id": "yolo",
    "summary": [
      "What  They suggest a model (\"YOLO\") to detect bounding boxes in images.",
      "In comparison to Faster R-CNN, this model is faster but less accurate.",
      "How  Architecture  Input are images with a resolution of 448x448.",
      "Output are S*S*(B*5 + C) values (per image).",
      "S is the grid size (default value: 7).",
      "Each image is split up into S*S cells.",
      "B is the number of \"tested\" bounding box shapes at each cell (default value: 2).",
      "So at each cell, the network might try one large and one small bounding box.",
      "The network predicts additionally for each such tested bounding box 5 values.",
      "These cover the exact position (x, y) and scale (height, width) of the bounding box as well as a confidence value.",
      "They allow the network to fine tune the bounding box shape and reject it, e.g. if there is no object in the grid cell.",
      "The confidence value is zero if there is no object in the grid cell and otherwise matches the IoU between predicted and true bounding box.",
      "C is the number of classes in the dataset (e.g. 20 in Pascal VOC).",
      "For each grid cell, the model decides once to which of the C objects the cell belongs.",
      "Rough overview of their outputs:  In contrast to Faster R-CNN, their model does not use a separate region proposal network (RPN).",
      "Per bounding box they actually predict the square root of height and width instead of the raw values.",
      "That is supposed to result in similar errors/losses for small and big bounding boxes.",
      "They use a total of 24 convolutional layers and 2 fully connected layers.",
      "Some of these convolutional layers are 1x1-convs that halve the number of channels (followed by 3x3s that double them again).",
      "Overview of the architecture:  They use Leaky ReLUs (alpha=0.1) throughout the network.",
      "The last layer uses linear activations (apparently even for the class prediction...!?).",
      "Similarly to Faster R-CNN, they use a non maximum suppression that drops predicted bounding boxes if they are too similar to other predictions.",
      "Training  They pretrain their network on ImageNet, then finetune on Pascal VOC.",
      "Loss  They use sum-squared losses (apparently even for the classification, i.e. the C values).",
      "They dont propagate classification loss (for C) for grid cells that don't contain an object.",
      "For each grid grid cell they \"test\" B example shapes of bounding boxes (see above).",
      "Among these B shapes, they only propagate the bounding box losses (regarding x, y, width, height, confidence) for the shape that has highest IoU with a ground truth bounding box.",
      "Most grid cells don't contain a bounding box.",
      "Their confidence values will all be zero, potentialle dominating the total loss.",
      "To prevent that, the weighting of the confidence values in the loss function is reduced relative to the regression components (x, y, height, width).",
      "Results  The coarse grid and B=2 setting lead to some problems.",
      "Namely, small objects are missed and bounding boxes can end up being dropped if they are too close to other bounding boxes.",
      "The model also has problems with unusual bounding box shapes.",
      "Overall their accuracy is about 10 percentage points lower than Faster R-CNN with VGG16 (63.4% vs 73.2%, measured in mAP on Pascal VOC 2007).",
      "They achieve 45fps (22ms/image), compared to 7fps (142ms/image) with Faster R-CNN + VGG16.",
      "Overview of results on Pascal VOC 2012:  They also suggest a faster variation of their model which reached 145fps (7ms/image) at a further drop of 10 percentage points mAP (to 52.7%).",
      "A significant part of their error seems to come from badly placed or sized bounding boxes (e.g. too wide or too much to the right).",
      "They mistake background less often for objects than Fast R-CNN.",
      "They test combining both models with each other and can improve Fast R-CNN's accuracy by about 2.5 percentage points mAP.",
      "They test their model on paintings/artwork (Picasso and People-Art datasets) and notice that it generalizes fairly well to that domain.",
      "Example results (notice the paintings at the top):"
    ],
    "author_id": "ALEJU",
    "pdf_url": "https://arxiv.org/pdf/1506.02640",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 72922440
  },
  {
    "blog_id": "41ebd5ba0e0fc0f49d7836e30891a7",
    "summary": [
      "Problem: Building an expressive, tractable and scalable image model which can be used in downstream tasks like image generation, reconstruction, compression etc.",
      "Model  Scan the image, one row at a time and one pixel at a time (within each row).",
      "Given the scanned content, predict the distribution over the possible values for the next pixel.",
      "Joint distribution over the pixel values is factorised into a product of conditional distributions thus causing the problem as a sequence problem.",
      "Parameters used in prediction are shared across all the pixel positions.",
      "Since each pixel is jointly determined by 3 values (3 colour channels), each channel may be conditioned on other channels as well.",
      "Pixel as discrete value  The conditional distributions are multinomial (with channel variable taking 1 of 256 discrete values).",
      "This discrete representation is simpler and easier to learn.",
      "Pixel RNN  Row LSTM  Undirectional layer that processed image row by row.",
      "Uses one-dimensional convolution (kernel of size kx1, k>=3).",
      "Refer image 2 in the paper .",
      "Weight sharing in convolution ensures translation invariance of computed feature along each row.",
      "For LSTM, the input-to-state component is computed for the entire 2-d input map and then is masked to include only the valid context.",
      "For equations related to state-to-state component, refer to equation 3 in the paper  Diagonal BiLSTM  Bidirectional layer that processes the image in the diagonal fashion.",
      "Input map skewed by offsetting each row of the image by one position with respect to the previous row.",
      "Refer image 3 in the paper  For both directions, the input-to-state component is a 1 x 1 convolution while the state-to-state recurrent component is computed with column wise convolution using kernel size 2x1.",
      "Kernel size of 2x1 processes minimal information yielding a highly non-linear computation.",
      "Output map is skewed back by removing the offset positions.",
      "To prevent layers from seeing further pixels, the right output map is shifted down by one row and added to left output map.",
      "Residual Connections  Residual connections (or skip connections) are used to increase convergence speed and to propagate signals more explicitly.",
      "Refer image 4 in the paper  Masked Convolutions  Masks are used to enforce certain restrictions on the connections in the network (eg when predicting values for R channel, values of B channel can not be used).",
      "Mask A is applied to first convolution layer and restricts connections to only those neighbouring pixels and colour channels that have already been seen.",
      "Mask B is applied to all subsequent input-to-state convolution transactions and allows connections from a colour channel to itself.",
      "Refer image 4 in the paper  PixelCNN  Uses multiple convolution layers that preserve spatial resolution.",
      "Makes receptive field large but not unbounded.",
      "Mask used to avoid seeing the future context.",
      "Faster that PixelRNN at training or evaluation time (as convolutions can be parallelized easily).",
      "Multi-Scale PixelRNN  Composed of one unconditional PixelRNN and multiple conditional PixelRNNs.",
      "Unconditional network generates a smaller s x s image which is fed as input to the conditional PixelRNN.",
      "(n is a multiple of s)  Conditional PixelRNN is a standard PixelRNN with layers biased with an upsampled version of the s x s image.",
      "For upsampling, a convolution network with deconvolution layers constructs an enlarged feature map of size c x n x n.  For biasing, the c x n x n map is mapped to 4hxnxn map (using 1x1 unmasked convolution) and added to input-to-state map.",
      "Training and Evaluation  Pixel values are dequantized using real-valued noise and log likelihood of continuous and discrete models are compared.",
      "Update rule - RMSProp  Batch size - 16 for MNIST and CIFAR 10 and 32(or 64) for IMAGENET.",
      "Residual connections are as effective as Skip connections, in fact, the 2 can be used together as well.",
      "PixelRNN outperforms other models for Binary MNIST and CIFAR10.",
      "For CIFAR10, Diagonal BiLSTM > Row LSTM > PixelCNN.",
      "This is also the order of receptive field for the 3 architectures and the observation underlines the importance of having a large receptive field.",
      "The paper also provides new benchmarks for generative image modelling on IMAGENET dataset."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1601.06759",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 35247822
  },
  {
    "blog_id": "ssd",
    "summary": [
      "What  They suggest a new bounding box detector.",
      "Their detector works without an RPN and RoI-Pooling, making it very fast (almost 60fps).",
      "Their detector works at multiple scales, making it better at detecting small and large objects.",
      "They achieve scores similar to Faster R-CNN.",
      "How  Architecture  Similar to Faster R-CNN, they use a base network (modified version of VGG16) to transform images to feature maps.",
      "They do not use an RPN.",
      "They predict via convolutions for each location in the feature maps:  (a) one confidence value per class (high confidence indicates that there is a bounding box of that class at the given location)  (b) x/y offsets that indicate where exactly the center of the bounding box is (e.g. a bit to the left or top of the feature map cell's center)  (c) height/width values that reflect the (logarithm of) the height/width of the bounding box  Similar to Faster R-CNN, they also use the concept of anchor boxes.",
      "So they generate the described values not only once per location, but several times for several anchor boxes (they use six anchor boxes).",
      "Each anchor box has different height/width and optionally scale.",
      "Visualization of the predictions and anchor boxes:  They generate these predictions not only for the final feature map, but also for various feature maps in between (e.g. before pooling layers).",
      "This makes it easier for the network to detect small (as well as large) bounding boxes (multi-scale detection).",
      "Visualization of the multi-scale architecture:  Training  Ground truth bounding boxes have to be matched with anchor boxes (at multiple scales) to determine correct outputs.",
      "To do this, anchor boxes and ground truth bounding boxes are matched if their jaccard overlap is 0.5 or higher.",
      "Any unmatched ground truth bounding box is matched to the anchor box with highest jaccard overlap.",
      "Note that this means that a ground truth bounding box can be assigned to multiple anchor boxes (in Faster R-CNN it is always only one).",
      "The loss function is similar to Faster R-CNN, i.e. a mixture of confidence loss (classification) and location loss (regression).",
      "They use softmax with crossentropy for the confidence loss and smooth L1 loss for the location.",
      "Similar to Faster R-CNN, they perform hard negative mining.",
      "Instead of training every anchor box at every scale they only train the ones with the highest loss (per example image).",
      "While doing that, they also pick the anchor boxes to be trained so that 3 in 4 boxes are negative examples (and 1 in 4 positive).",
      "Data Augmentation: They sample patches from images using a wide range of possible sizes and aspect ratios.",
      "They also horizontally flip images, perform cropping and padding and perform some photo-metric distortions.",
      "Non-Maximum-Suppression (NMS)  Upon inference, they remove all bounding boxes that have a confidence below 0.01.",
      "They then apply NMS, removing bounding boxes if there is already a similar one (measured by jaccard overlap of 0.45 or more).",
      "Results  Pascal VOC 2007  They achieve around 1-3 points mAP better results than Faster R-CNN.",
      "Despite the multi-scale method, the model's performance is still significantly worse for small objects than for large ones.",
      "Adding data augmentation significantly improved the results compared to no data augmentation (around 6 points mAP).",
      "Using more than one anchor box also had noticeable effects on the results (around 2 mAP or more).",
      "Using multiple feature maps to predict outputs (multi-scale architecture) significantly improves the results (around 10 mAP).",
      "Though adding very coarse (high-level) feature maps seems to rather hurt than help.",
      "Pascal VOC 2012  Around 4 mAP better results than Faster R-CNN.",
      "COCO  Between 1 and 4 mAP better results than Faster R-CNN.",
      "Times  At a batch size of 1, SSD runs at about 46 fps at input resolution 300x300 (74.3 mAP on Pascal VOC) and 19 fps at input resolution 512x512 (76.8 mAP on Pascal VOC)."
    ],
    "author_id": "ALEJU",
    "pdf_url": "https://arxiv.org/pdf/1512.02325",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 3953515
  },
  {
    "blog_id": "discretized-streams-fault-tolerant-stream-computing-at-scale",
    "summary": [
      "Discretized Streams: Fault Tolerant Stream Computing at Scale \u2013 Zaharia et al. 2013  This is the Spark Streaming paper, and it sets out very clearly the problem that Discretized Streams were designed to solve: dealing effectively with faults and stragglers when processing streams in large clusters.",
      "This is hard to do in the traditional continuous operator model in which long-running operators receive and process each update in turn, maintaining their own internal state along the way.",
      "Specifically, given the continuous operator model, systems perform recovery through two approaches: replication, where there are two copies of each node, or upstream backup, where nodes buffer sent messages and replay them to a new copy of a failed node.",
      "Neither approach is attractive in large clusters: replication costs 2\u00d7 the hardware, while upstream backup takes a long time to recover, as the whole system must wait for a new node to serially rebuild the failed node\u2019s state by rerunning data through an operator.",
      "In addition, neither approach handles stragglers: in upstream backup, a straggler must be treated as a failure, incurring a costly recovery step, while replicated systems use synchronization protocols like Flux to coordinate replicas, so a straggler will slow down both replicas.",
      "Discretized Streams (D-Streams) work differently: instead of managing long-lived operators they structure a streaming computation as a series of stateless deterministic batch computations.",
      "Unlike traditional batch systems that write intermediate state to disk though, D-Streams keep data in memory using RDDs:  we use a data structure called Resilient Distributed Datasets (RDDs), which keeps data in memory and can recover it without replication by tracking the lineage graph of operations that were used to build it.",
      "With RDDs, we show that we can attain sub-second end-to-end latencies.",
      "We believe that this is sufficient for many real-world big data applications, where the timescale of the events tracked (e.g., trends in social media) is much higher.",
      "It\u2019s worth stressing that D-Streams / Spark Streaming targets sub-second latency, but not latency in the few hundred milliseconds or below range.",
      "The key design is to provide both sub-second latency and sub-second recovery from faults and stragglers.",
      "Examples of use cases well matched to these requirements include site activity statistics, cluster monitoring, and tweet spam detection.",
      "In evaluation the authors showed per-node throughput comparable to commercial streaming databases, combined with linear scalability out to 100 nodes processing over 60M records/second.",
      "When a node fails, it recomputes the RDD partitions that were on it by re-running the tasks that built them from the original input data stored reliably in the cluster.",
      "The system also periodically checkpoints state RDDs (e.g., by asynchronously replicating every tenth RDD) to prevent infinite recomputation, but this does not need to happen for all data, because recovery is often fast: the lost partitions can be recomputed in parallel on separate nodes.",
      "In a similar way, if a node straggles, we can speculatively execute copies of its tasks on other nodes, because they will produce the same result.",
      "We note that the parallelism usable for recovery in D-Streams is higher than in upstream backup, even if one ran multiple operators per node.",
      "D-Streams expose parallelism across both partitions of an operator and time.",
      "The micro-batching in D-Streams partitions input records based on their arrival time.",
      "If instead you want to group records based on an external time, you can either add some delay (slack period) to wait for them al to come in, or handle late arrivals explicity in your application code.",
      "On top of the core model, the authors show it is possible to build computations that span several intervals including windowing, aggregation, and state tracking.",
      "The D-Streams model is implemented in Spark Streaming, which led to a number of improvements in the core of Spark itself as well.",
      "These optimizations also improved Spark\u2019s performance in the batch case by 2x.",
      "Finally, because D-Streams use the same execution model as batch platforms, they compose seamlessly with batch and interactive queries.",
      "We used this capability in Spark Streaming to let users combine these models in powerful ways, and showed how it can add rich features to two real applications.",
      "(Those applications were video distribution monitoring, and crowdsourced traffic estimation)."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://people.csail.mit.edu/matei/papers/2013/sosp_spark_streaming.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 7912651
  },
  {
    "blog_id": "411f15b71ed6a664f9d5ac46409b42",
    "summary": [
      "Proposes a novel, end-to-end architecture for generating short email responses.",
      "Single most important benchmark of its success is that it is deployed in Inbox by Gmail and assists with around 10% of all mobile responses.",
      ".",
      "Challenges in deploying Smart Reply in a user-facing product  Responses must always be of high quality.",
      "Ensured by constructing a target response set to select responses from.",
      "The likelihood of choosing the responses must be maximised.",
      "Ensured by normalising the responses and enforcing diversity.",
      "The system should not add latency to emails.",
      "Ensured by using a triggering model to decide if the email is suitable to undergo the response generation pipeline.",
      "Computation time is further reduced by finding approximate best result instead of the best result.",
      "Ensure privacy by encrypting all the data which adds challenge in verifying the model's quality and debugging the system.",
      "Architecture  Preprocess Email  Perform actions like language detection, tokenization, sentence segmentation etc on the input email.",
      "Triggering Model  A feed-forward neural network (with embedding layer and 3 fully connected hidden layers) to decide if the input email is suitable for suggesting responses.",
      "Data  Training set of pairs (o, y) where o is the incoming message and y is a boolean variable to indicate if the message had a response.",
      "Features  Unigrams, bigrams from the messages.",
      "Signals like - is the recipient in the contact list of the sender.",
      "Response Selection  LSTM network to predict the approximate best response for an incoming message o  Network  Sequence to Sequence Learning.",
      "Reads the input message (token by token) and encode a vector representation.",
      "Compute softmax to get the probability of first output token given the input token sequence.",
      "Keep feeding in the previous response tokens and the input token sequence to compute the probability of next output token.",
      "During inference, approximate the most likely response greedily by taking the most likely response at each timestamp and feeding it back or by using the beam search approach.",
      "Response Set Generation  Generate a set of high-quality responses that also capture the variability in the intent of the response.",
      "Canonicalize the email response by extracting the semantic structure using a dependency parser.",
      "Partition all response messages into \"semantic\" clusters.",
      "These semantic clusters define the response space for scoring and selecting possible responses and for promoting diversity among the responses.",
      "Semantic Intent Clustering  Since a large, labelled dataset is not available, a graph based, semi-supervised approach is used.",
      "Graph Construction  Manually define a few clusters with a small number of example responses for each cluster.",
      "Construct a graph with frequent response messages (including the labelled nodes) as response nodes (VR).",
      "For each response node, extract a set of feature nodes (VF) corresponding to features like skip-gram and n-grams and add an edge between the response node and the feature node.",
      "Learn a semantic labelling for all response nodes by propagating semantic intent information (available because of labelled nodes) throughout the graph.",
      "After some iterations, sample some of the unlabeled nodes from the graph, manually label these sample nodes and repeat this algorithm until convergence.",
      "For validation, extract the top k members of each cluster and validate the quality with help of human evaluators.",
      "Suggestion Diversity  Provide users with a varied set of response by omitting redundant response (by not selecting more than one response from any semantic cluster) and by enforcing negative (or positive) responses.",
      "If the top two responses contain at least one positive (negative) response and none of the top three responses is negative (positive), the third response is replaced with a negative (positive) one.",
      "This is done by performing a second LSTM pass where the search is restricted to only positive (or negative) responses in the target set.",
      "Strengths  The system is already in production and assists with around 10% of all mobile responses.",
      "This comment has been minimized.",
      "Sign in to view  Copy link  Quote reply  vegetakarthhik commented  Nov 26, 2018  hey do you have python implementation?"
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1511.08130",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 68642594
  },
  {
    "blog_id": "model-primitive-hierarchical-lifelong-reinforcement-learning",
    "summary": [
      "The paper presents a framework that uses diverse suboptimal world models that can be used to break complex policies into simpler and modular sub-policies.",
      "Given a task, both the sub-policies and the controller are simultaneously learned in a bottom-up manner.",
      "The framework is called as Model Primitive Hierarchical Reinforcement Learning (MPHRL).",
      "Idea  Instead of learning a single transition model of the environment (aka world model) that can model the transitions very well, it is sufficient to learn several (say k) suboptimal models (aka model primitives).",
      "Each model primitive will be good in only a small part of the state space (aka region of specialization).",
      "These model primitives can then be used to train a gating mechanism for selecting sub-policies to solve a given task.",
      "Since these model primitives are sub-optimal, they are not directly used with model-based RL but are used to obtain useful functional decompositions and sub-policies are trained with model-free approaches.",
      "Single Task Learning  A gating controller is trained to choose the sub-policy whose model primitive makes the best prediction.",
      "This requires modeling p(Mk | st, at, st+1) where p(Mk) denotes the probability of selecting the kth model primitive.",
      "This is hard to compute as the system does not have access to st+1 and at at time t before it has choosen the sub-policy.",
      "Properly marginalizing st+1 and at would require expensive MC sampling.",
      "Hence an approximation is used and the gating controller is modeled as a categorical distribution - to produce p(Mk | st).",
      "This is trained via a conditional cross entropy loss where the ground truth distribution is obtained from transitions sampled in a rollout.",
      "The paper notes that technique is biased but reports that it still works for the downstream tasks.",
      "The gating controller composes the sub-policies as a mixture of Gaussians.",
      "For learning, PPO algorithm is used with each model primitives gradient weighted by the probability from the gating controller.",
      "Lifelong Learning  Different tasks could share common subtasks but may require a different composition of subtasks.",
      "Hence, the learned sub-policies are transferred across tasks but not the gating controller or the baseline estimator (from PPO).",
      "Experiments  Domains:  Mujoco ant navigating different mazes.",
      "Stacker arm picking up and placing different boxes.",
      "Implementation Details:  Gaussian subpolicies  PPO as the baseline  Model primitives are hand-crafted using the true next state provided by the environment simulator.",
      "Single Task  Only maze task is considered with the start position (of the ant) and the goal position is fixed.",
      "Observation includes distance from the goal.",
      "Forcing the agent to decompose the problem, when a more direct solution may be available, causes the sample complexity to increase on one task.",
      "Lifelong Learning  Maze  10 random Mujoco ant mazes used as the task distribution.",
      "MPHRL takes almost twice the number of steps (as compared to PPO baseline) to solve the first task but this cost gets amortized over the distribution and the model takes half the number of steps as compared to the baseline (summed over the 10 tasks).",
      "Pick and Place  8 Pick and Place tasks are created with max 3 goal locations.",
      "Observation includes the position of the goal.",
      "Ablations  Overlapping model primitives can degrade the performance (to some extent).",
      "Similarly, the performance suffers when redundant primitives are introduced indicating that the gating mechanism is not very robust.",
      "Sub-policies could quickly adapt to the previous tasks (on which they were trained initially) despite being finetuned on subsequent tasks.",
      "The order of tasks (in the 10-Mazz task) does not degrage the performance.",
      "Transfering the gating controller leads to negative transfer.",
      "Notes  I think the biggest strength of the work is that accurate dynamics model are not needed (which are hard to train anyways!)",
      "through the experimental results are not conclusive given the limited number of domains on which the approach is tested."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1903.01567",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 16057781
  },
  {
    "blog_id": "learning-to-think",
    "summary": [
      "This article presents a roadmap for future progress in developing machines that can learn and \u201cthink\u201d like humans.",
      "The authors outline a number of ideas that they believe are crucial to making progress in artificial intelligence.",
      "A short-term goal of the authors is to improve today\u2019s machine learning techniques so that complex concepts can be learned quickly with only small amounts of data.",
      "Currently, our algorithms require ungainly amounts of data and computation to perform at acceptable levels.",
      "The authors argue that a marriage of two distinct research areas in artificial intelligence, that of statistical pattern recognition and of model-building, will produce a promising avenue for new advances.",
      "Specifically, the authors suggest that learning as a form of explaining observed data by the construction of causal models of the world can be accomplished by deep neural networks.",
      "The first causal model that is discussed is dubbed intuitive physics.",
      "That is, a machine with an internal model of the physics of its environment is able to learn more quickly than without.",
      "This is because the machine will not need to re-learn basic physics principles when attempting to learn new tasks that require it.",
      "The second causal model is intuitive psychology.",
      "A learning agent that assumes that other agents in its environment are rational and have their own goals and beliefs can try to infer what these may be.",
      "Having knowledge of the intentions of other agents in the environment allow the learning agent to achieve its own goals in a much more efficient way.",
      "Compositionality  Compositionality is the classic idea that new representations can be constructed through the combination of primitive elements.",
      "See Lake et.",
      "all 2015 for a discussion on the use of compositionality to generate novel handwritten characters.",
      "This is central to training agents to learn models of complex symbolic concepts.",
      "Learning-to-Learn  Learning-to-learn is closely related to the concept of \u201ctransfer-learning\u201d and \u201crepresentation-learning\u201d in machine learning.",
      "Machines that learn like humans will need to be able to learn rich, informative priors and models of the world that can be applied to a variety of tasks.",
      "Systems should be able to learn to do new tasks as flexibly and rapidly as humans do.",
      "The authors suggest compositionality and the use of causal models to augment the current deep-learning research in transfer-learning.",
      "Thinking fast  There are still a lot of challenges concerning intractability of certain methods and the amount of time necessary for doing optimization over high-dimensional potentially non-convex parameter spaces.",
      "Approximate inference is an area of research that aims to address these challenges.",
      "Monte Carlo methods are promising for many problems, but runs into problems when the hypothesis space is vast.",
      "Deep neural networks may potentially be used for performing probabilistic inference in a generative model or a probabilistic program.",
      "Dangers of model-free RL  There is evidence that the brain uses a form of model-free reinforcement learning to accomplish certain tasks.",
      "However, there is also evidence that the brain has a model-based learning system.",
      "Shifting between model-free and model-based learning methods can benefit from taking advantage of the causal-model/compositionality discussed above.",
      "Current model-free methods, as seen in DQN, Google DeepMind\u2019s Atari agent, are severely limited when it comes to generalizing beyond a certain amount.",
      "For example, the DQN agent would need significant re-training to be able to play a game whose screen was locked to a different resolution, and it would fail to play a game that required a working memory of game states/frames from the past (i.e. the agent would be missing causal information that coudl explain what is happening at the current state of the game).",
      "Strengths  The authors address potential counter-arguments in Section 5.",
      "They are: \u201cComparing the learning speeds of humans and neural networks on specific tasks is not meaningful, because humans have extensive prior experience\u201d, \u201cBiological plausibility suggests theories of intelligence should start with networks\u201d, \u201cLanguage is essential for human intelligence.",
      "Why is it not more prominent here?\u201d  Weaknesses  There isn\u2019t a clear definition of \u201cintelligence\u201d stated in the article.",
      "The closest is the authors\u2019 statement that the goal of those who wish to create machines that learn like humans (e.g. display human-level intelligence) should be to devise ways to carry out learning from far less data and to generalize in richer and more flexible ways."
    ],
    "author_id": "pemami",
    "pdf_url": "http://arxiv.org/pdf/1604.00289v2.pdf",
    "author_full_name": "Patrick Emami",
    "source_website": "https://pemami4911.github.io/index.html",
    "id": 44148071
  },
  {
    "blog_id": "representation-tradeoffs-for-hyperbolic-embeddings",
    "summary": [
      "The paper describes a combinatorial approach to embed trees into hyperbolic spaces without performing optimization.",
      "The resulting mechanism is analyzed to obtain dimensionality-precision tradeoffs.",
      "To embed any metric spaces in the hyperbolic spaces, a hyperbolic generalization of the multidimensional scaling (h-MDS) is proposed.",
      "Preliminaries  Hyperbolic Spaces  Have the \u201ctree\u201d like property ie the shortest path between a pair of points is almost the same as the path through the origin.",
      "Generally, Poincare ball model is used given its advantages like conformity to the Euclidean spaces.",
      "Fidelity Measures  Mean Average Precision - MAP  A local metric that ranks between distances of the immediate neighbors.",
      "Distortion  A global metric that depends on the underlying distances and not just the local relationship between distances.",
      "Combinatorial Construction for embedding hierarchies into Hyperbolic spaces  Embed the given graph G = (V, E) into a tree T.  Embed the tree T into the poincare ball Hd of dimensionality d.  Sarkar\u2019s construction to embed points in a 2-d Poincare ball  Consider two points a and b (from the tree) where b is the parent of a.",
      "Assume that a is embedded as f(a) and b is embedded as f(b) and the children of a needs to be embedded.",
      "Reflect f(a) and f(b) across a geodesic such that f(a) is mapped to 0 (origin) while f(b) is mapped to some new point z.",
      "Children of a are placed at points yi which are equally placed around a circle of radius (er - 1) / (er + 1) and maximally seperated from z, where r is the scaling factor.",
      "Then all the points are reflected back across the geodesic so that all children are at a distance r from f(a).",
      "To embed the tree itself, place the root node at the origin, place its children around it in a circle, then place their children and so on.",
      "In this construct, precision scales logarithmically with the degree of the tree but linearly with the maximum path length.",
      "d-dimensional hyperbolic spaces  In the d-dimensional space, the points are embedded into hyperspheres (instead of circles).",
      "The number of children node that can be placed for a particular angle grows with the dimension.",
      "Increasing dimension helps with bushy trees (with high node degree).",
      "Hyperbolic multidimensional scaling (h-MDS)  Given the pairwise distance from a set of points in the hyperbolic space, how to recover the points?",
      "The corresponding problem in the Euclidean space is solved using MDS.",
      "A variant of MDS called as h-MDS is proposed.",
      "MDS makes a centering assumption that points have 0 mean.",
      "In h-MDS, a new mean (called as the pseudo-Euclidean mean) is introduced to enable recovery via matrix factorization.",
      "Instead of the Poincare model, the hyperboloid model is used (though the points can be mapped back and forth).",
      "pseudo-Euclidean Mean  A set of points can always be centered without affecting their pairwise distance by simply finding their mean and sending it to 0 via isometry  Recovery via matrix factorization  Given the pairwise distances, a new matrix Y is constructed by applying cosh on the pairwise distances.",
      "Running PCA on -Y recovers X up to rotation.",
      "Dimensionality Reduction with PGA (Principal Geodesic Analysis)  PGA is the counterpart of PCA in the hyperbolic spaces.",
      "First the Karcher mean of the given points is computed.",
      "All points xi are reflected so that their mean is 0 in the Poincare disk model.",
      "Combining that with Euclidean reflection formula and hyperbolic metrics leads to a non-convex loss function which can be optimized using gradient descent algorithm.",
      "Experiments  Datasets  Trees: fully balanced and phylogenic trees expressing genetic heritage.",
      "Tree-like hierarchy: WordNet hypernym and graph of Ph.D. advisor-advisee relationships.",
      "No-tree like disease relationships, proteins interactions etc  Results  Combinatorial construction outperforms approaches based on optimization in terms of both MAP and distortion.",
      "eg on WordNet, the combinatorial approach achieves a MAP of 0.989 with just 2 dimensions while the previous best was 0.87 with 200 dimensions."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1804.03329",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 24928430
  },
  {
    "blog_id": "conditional_image_generation_with_pixelcnn_decoders",
    "summary": [
      "What  PixelRNN  PixelRNNs generate new images pixel by pixel (and row by row) via LSTMs (or other RNNs).",
      "Each pixel is therefore conditioned on the previously generated pixels.",
      "Training of PixelRNNs is slow due to the RNN-architecture (hard to parallelize).",
      "Previously PixelCNNs have been suggested, which use masked convolutions during training (instead of RNNs), but their image quality was worse.",
      "They suggest changes to PixelCNNs that improve the quality of the generated images (while still keeping them faster than RNNs).",
      "How  PixelRNNs split up the distribution p(image) into many conditional probabilities, one per pixel, each conditioned on all previous pixels: p(image) = <product> p(pixel i | pixel 1, pixel 2, ..., pixel i-1).",
      "PixelCNNs implement that using convolutions, which are faster to train than RNNs.",
      "These convolutions uses masked filters, i.e. the center weight and also all weights right and/or below the center pixel are 0 (because they are current/future values and we only want to condition on the past).",
      "In most generative models, several layers are stacked, ultimately ending in three float values per pixel (RGB images, one value for grayscale images).",
      "PixelRNNs (including this implementation) traditionally end in a softmax over 255 values per pixel and channel (so 3*255 per RGB pixel).",
      "The following image shows the application of such a convolution with the softmax output (left) and the mask for a filter (right):  Blind spot  Using the mask on each convolutional filter effectively converts them into non-squared shapes (the green values in the image).",
      "Advantage: Using such non-squared convolutions prevents future values from leaking into present values.",
      "Disadvantage: Using such non-squared convolutions creates blind spots, i.e. for each pixel, some past values (diagonally top-right from it) cannot influence the value of that pixel.",
      "They combine horizontal (1xN) and vertical (Nx1) convolutions to prevent that.",
      "Gated convolutions  PixelRNNs via LSTMs so far created visually better images than PixelCNNs.",
      "They assume that one advantage of LSTMs is, that they (also) have multiplicative gates, while stacked convolutional layers only operate with summations.",
      "They alleviate that problem by adding gates to their convolutions:  Equation: output image = tanh(weights_1 * image) <element-wise product> sigmoid(weights_2 * image)  * is the convolutional operator.",
      "tanh(weights_1 * image) is a classical convolution with tanh activation function.",
      "sigmoid(weights_2 * image) are the gate values (0 = gate closed, 1 = gate open).",
      "weights_1 and weights_2 are learned.",
      "Conditional PixelCNNs  When generating images, they do not only want to condition the previous values, but also on a laten vector h that describes the image to generate.",
      "The new image distribution becomes: p(image) = <product> p(pixel i | pixel 1, pixel 2, ..., pixel i-1, h).",
      "To implement that, they simply modify the previously mentioned gated convolution, adding h to it:  Equation: output image = tanh(weights_1 * image + weights_2 .",
      "h) <element-wise product> sigmoid(weights_3 * image + weights_4 .",
      "h)  .",
      "denotes here the matrix-vector multiplication.",
      "PixelCNN Autoencoder  The decoder in a standard autoencoder can be replaced by a PixelCNN, creating a PixelCNN-Autoencoder.",
      "Results  They achieve similar NLL-results as PixelRNN on CIFAR-10 and ImageNet, while training about twice as fast.",
      "Here, \"fast\" means that they used 32 GPUs for 60 hours.",
      "Using Conditional PixelCNNs on ImageNet (i.e. adding class information to each convolution) did not improve the NLL-score, but it did improve the image quality.",
      "They use a different neural network to create embeddings of human faces.",
      "Then they generate new faces based on these embeddings via PixelCNN.",
      "Their PixelCNN-Autoencoder generates significantly sharper (i.e. less blurry) images than a \"normal\" autoencoder."
    ],
    "author_id": "ALEJU",
    "pdf_url": "http://arxiv.org/pdf/1606.05328",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 35973587
  },
  {
    "blog_id": "acquisition_of_localization_confidence_for_accurate_od",
    "summary": [
      "What  Current object detectors predict usually a set of bounding boxes, each having a classification score (\"how likely it is to be of an object class\") and regressed bounding box dimensions (\"where is the box and how high/wide is it\").",
      "These results are then filtered through NMS, suppressing all bounding boxes that overlap strongly with another box that has higher classification score.",
      "Sometimes the regressed dimensions are also iteratively refined (e.g. in two-stage detectors one can argue that there is one refinement after the RPN).",
      "The authors observe that this structure is problematic, as there is no uncertainty-related information regarding the regressed dimensions.",
      "For instance in NMS, the suppression works based on classification scores, but e.g. a too large bounding box might get a higher classification score than one with optimal fit, due to including more context and thereby making the classification more certain.",
      "The classification scores are also very non-monotonic with respect to the bounding box dimensions: Increasing the bounding box size might first lead to higher classification scores, then lower ones, then higher again.",
      "This makes iterative refinement hard or impossible.",
      "They propose IoU-Net, which predicts for each bounding box the expected IoU with the ground truth.",
      "That predicted IoU is then used in NMS as a replacement for the classification score, thereby selecting for the bounding boxes that the model thinks have highest IoU.",
      "The predicted IoU can also be used to perform iterative refinement, as it is more monotonic than the classification score.",
      "They propose \"Precise RoI Pooling\" (PrRoI Pooling), which is a version of RoI-Pooling/RoI-Align that does not suffer from quantization inaccuracies.",
      "How  Predicting IoUs  They feed each RoI's features through two fully connected layers to regress an IoU value for that RoI.",
      "They use one such branch per class.",
      "During training they augment the ground truth bounding boxes to generate examples with lower IoUs.",
      "Visualization:  Relationship between predicted IoUs and real IoUs (right) vs. classification scores and real IoUs (left):  IoU-guided NMS  This is essentially the same as classical NMS.",
      "They use the predicted IoU values to estimate which box to keep if two are sufficiently overlapping.",
      "When one box is suppressed, the remaining box's class score is updated to max(s_1, s_2), where s_1 and s_2 are the class scores of the two boxes.",
      "Optimization-based refinement of bounding boxes  The branch to predict IoUs is differentiable.",
      "They use this to iteratively improve predicted bounding boxes so that the predicted IoU is maximized.",
      "I.e. they backpropagate to the bounding box coordinates and change them according to the gradient.",
      "They scale up the gradients according to the bounding box size on the given axis, which is similar to optimizing in log-space.",
      "They add some early stopping criteria to not execute this indefinitely.",
      "Precise RoI Pooling  During bounding box refinement they use a more accurate (quantization free) RoI-Pooling.",
      "They use bilinear sampling to approximate the feature value of any continuous location within the feature map.",
      "Then they use integration to calculate the pooled value (makes it sound like they always use global pooling?).",
      "So while this method computes the integrals over all possible values within the given range, RoI Align only samples at N=4 locations per side.",
      "Visualization of the differences:  Results  They train and test on COCO.",
      "Their model runs at about 300ms per image on a Titan X.  IoU-guided NMS  IoU-guided NMS performs slightly better than Soft-NMS and significantly better than classical NMS.",
      "IoU-guides NMS shines especially for high IoU APs, improving over Soft-NMS by 1 to 3 percentage points.",
      "Optimization-based refinement  Using optimization-based refinement also improves APs by around 1 to 3 percentage points (over no refinement).",
      "Again, the improvement is most significant for high IoUs, reaching 5 to 6 percentage points better values.",
      "Joint Training  Training the IoU prediction branch jointly with a base network leads to slightly better features, improving APs by 0.4 to 0.6 points.",
      "Combining this with guided-NMS and optimization-based refinement, they improve the AP of a ResNet101+FPN baseline by 2.1 points."
    ],
    "author_id": "ALEJU",
    "pdf_url": "http://openaccess.thecvf.com/content_ECCV_2018/papers/Borui_Jiang_Acquisition_of_Localization_ECCV_2018_paper.pdf",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 8601517
  },
  {
    "blog_id": "neural-message-passing-for-quantum-chemistry",
    "summary": [
      "The paper presents a general message passing architecture called as Message Passing Neural Networks (MPNNs) that unify various existing models for performing supervised learning on molecules.",
      "Variants of the MPNN model achieve very good performance on the task of predicting the property of the molecules.",
      "MPNN  Setting  The input to the model is an undirected graph G where node features are represented as xv (corresponding to node v) and edge features are ev, w (corresponding to edge between nodes v, w).",
      "The idea is to learn a representation (or feature vector) for all the nodes (and possibly edges) in the graph and use that for the downstream supervised learning task.",
      "The model can be easily extended to the setting of directed graphs.",
      "The model works in 2 phases:  Message Passing Phase  All nodes send a message to their neighbouring nodes.",
      "The message is a function of the feature vectors corresponding to the sender node (or vertex), the receiver node and the edge connecting the two nodes.",
      "The feature vectors can be combined to form the message using the message function which can be implemented as a neural network.",
      "Once a node has received messages from all its neighbours, it updated its feature vector by aggregating all the message.",
      "The function used to aggregate and update the feature vector is called as the update function and can be implemented as a neural network.",
      "After updating the feature vectors, the graph could initiate another round of message passing.",
      "After a sufficient number of message passing rounds, the Readout phase is invoked.",
      "Readout Phase  The feature vectors corresponding to different nodes in the graph are aggregated into a single feature vector (corresponding to the feature vector of the graph) using the readout function.",
      "The readout function can also be implemented using a neural network with the condition that it is invariant to the permutation of the nodes within the graph (to ensure that the MPNN is independent of the graph isomorphism).",
      "Existing Variants in literature  The paper provides various examples where the existing architectures could be explained in terms of the message passing framework.",
      "This includes examples like Convolutional Networks on Graphs for Learning Molecular Fingerprints , Gated Graph Sequence Neural Networks , Graph Convolutional Networks etc.",
      "Experiments  Setup  Broadly speaking, the task is to predict the properties of given molecules (regression problem).",
      "The QM9 dataset consists of 130K molecules whose properties have been measured using Quantum Mechanical Simulations (DFT).",
      "Properties to be predicted include atomization energy, enthalpy, highest fundamental vibrational frequency etc.",
      "There are two benchmarks for error:  DFT Error - Estimated average error of DFT approximation  Chemical Accuracy - As established by the chemistry community  Model  Following variants of message function are explored:  Matrix multiplication between Aevw and hv where A is the adjacency matrix hv is the feature corresponding to node v.  Edge Network which is same as matrix multiplication case with the difference that A is a learned matrix for each edge type.",
      "Pair Network where the feature vector corresponding to the source node, target node and edge is fed to a neural network.",
      "Virtual Elements  Since all messages are shared via edges, it could take a long time for the message to move between two ends of the graph.",
      "To fasten this process, virtual elements are provided.",
      "In the first setting, \u201cvirtual edges\u201d are inserted between nodes.",
      "In the second setting, a \u201cmaster\u201d node connects to all the other nodes.",
      "Message Passing Complexity  In a graph with n nodes and d dimensional feature vectors, a single step of message passing would have the worst case time complexity of O(n2d2.",
      "This complexity can be reduced by breaking the d dimensional embedding into k different groups of d/k embeddings which can be updated in parallel.",
      "The complexity of the modified approach is O(n2d2/k.",
      "Results  Best performing MPNN model uses edge network as the message function and set2set as the readout function.",
      "Using group of embeddings helps to improve generalization.",
      "This effect could also be because of ensemble-like nature of the modified architecture.",
      "The model performs worse without the virtual elements.",
      "Takeaways  Long range interaction between vertices is necessary.",
      "Scaling to larger molecule sizes is challenging because the model creates a fully connected graph by incorporating virtual elements."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1704.01212",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 27321339
  },
  {
    "blog_id": "adversarial-patch",
    "summary": [
      "Adversarial patch Brown, Man\u00e9 et al., arXiv 2017  Today\u2019s paper choice is short and sweet, but thought provoking nonetheless.",
      "To a man with a hammer (sticker), everything looks like a hammer.",
      "We\u2019ve seen a number of examples of adversarial attacks on image recognition systems, where the perturbations are designed to be subtle and hard to detect.",
      "But what if you don\u2019t mind the alteration being obvious to the human eye?",
      "Brown et al. show how to create stickers (image patches) that can be added to a scene, and force a classifier into reporting a class of the attacker\u2019s choosing.",
      "We present a method to create universal, robust, targeted adversarial image patches in the real world.",
      "The patches are universal because they can be used to attack any scene, robust because they work under a wide variety of transformations, and targeted because they can cause a classifier to output any target class.",
      "Here\u2019s an example of a printable sticker designed to cause classification as a toaster.",
      "And here\u2019s the sticker placed on a table next to a banana.",
      "The sticker causes the previously reported class banana (97% confidence) to become toaster with 99% confidence.",
      "Because this patch is scene-independent, it allows attackers to create a physical-world attack without prior knowledge of the lighting conditions, camera angle, type of classifier being attacked, or even the other items within the scene\u2026 Additionally, because the attack uses a large perturbation, the existing defense techniques which focus on defending against small perturbations may not be robust to larger perturbations such as these.",
      "Why does it work?",
      "An image may contain several items, but a classifier outputting only one target output has to determine which is the most \u2018salient\u2019 item in the frame.",
      "The adversarial patch exploits this by producing inputs that scream out \u201cI\u2019m an X\u201d for some value of X.",
      "Generating adversarial patches  The patch application operator , A, takes an input a patch p, image x, location l, and transformations t. It first applies the transformations to the patch, and then applies the transformed patch to the image x at location l.  If we\u2019re targeting an output class  , then we can train to optimise the objective function  Where X is a training set of images, T is a distribution over transformations of the patch, and L is a distribution over locations in the image.",
      "This departs from most prior work on adversarial perturbations in the fact that this perturbation is universal in that it works for any background.",
      "Experimental results  The experiment compares four different variations of patches, targeting recognition of an image as a toaster.",
      "Five different ImageNet models are used: inceptionv3, resnet50, xception, VGG16, and VGG19.",
      "The control is a simple picture of a toaster!",
      "The white box single trains a patch and evaluates it on a single model.",
      "The white box ensemble jointly trains a single patch across all five models, and is evaluated by averaging the win rate across them.",
      "The blackbox attack jointly trains a single patch across four of the models, and then evaluates against the fifth (held out) model.",
      "The white box models are very successful even when only 10% of the overall image size.",
      "They clearly capture the essence of toaster much better than the picture of an actual toaster!!",
      "The adversarial patches above reveal their target class to a human observer.",
      "The authors also experimented with disguising the patches using a tie-dye pattern and also applying a peace sign mask.",
      "These look nothing like a toaster to me anymore, but still do pretty well at larger relative sizes:  Many ML models operate without human validation of every input and thus malicious attackers may not be concerned with the imperceptibility of their attacks.",
      "Even if humans are able to notice these patches, they may not understand the intent of the patch and instead view it as a form of art.",
      "This work shows that focusing only on defending against small perturbations is insufficient, as large, local perturbations can also break classifiers."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://arxiv.org/pdf/1712.09665",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 81057283
  },
  {
    "blog_id": "greffskss15",
    "summary": [
      "This paper presents an extensive evaluation of variants of LSTM networks.",
      "Specifically, they start from what they consider to be the vanilla architecture and, from it, also consider 8 variants which correspond to small modifications on the vanilla case.",
      "The vanilla architecture is the one described in Graves & Schmidhuber (2005)  [ref] , and the variants consider removing single parts of it (input,forget,output gates or activation functions), coupling the input and forget gate (which is inspired from GRU) or having full recurrence between all gates (which comes from the original LSTM formulation).",
      "In their experimental setup, they consider 3 datasets: TIMIT (speech recognition), IAM Online Handwriting Database (character recognition) and JSB Chorales (polyphonic music modeling).",
      "For each, they tune the hyper-parameters of each of the 9 architectures, using random search based on 200 samples.",
      "Then, they keep the 20 best hyper-parameters and use the statistics of those as a basis for comparing the architectures.",
      "#### My two cents  This was a very useful ready.",
      "I'd make it a required read for anyone that wants to start using LSTMs.",
      "First, I found the initial historical description of the developments surrounding LSTMs very interesting and clarifying.",
      "But more importantly, it presents a really useful picture of LSTMs that can both serve as a good basis for starting to use LSTMs and also an insightful (backed with data) exposition of the importance of each part in the LSTM.",
      "The analysis based on an fANOVA (which I didn't know about until now) is quite neat.",
      "Perhaps the most surprising observation is that momentum actually doesn't seem to help that much.",
      "Investigating second order interaction between hyper-parameters was a smart thing to do (showing that tuning the learning rate and hidden layer jointly might not be that important, which is a useful insight).The illustrations in Figure 4, layout out the estimated relationship (with uncertainty) between learning rate / hidden layer size / input noise variance and performance / training time is also full of useful information.",
      "I wont repeat here the main observations of the paper, which are laid out clearly in the conclusion (section 6).",
      "Additionally, my personal take-away point is that, in an LSTM implementation, it might still be useful to support the removal peepholes or having coupled input and forget gates, since they both yielded the ultimate best test set performance on at least one of the datasets (I'm assuming it was also best on the validation set, though this might not be the case...)  The fANOVE analysis makes it clear that the learning rate is the most critical hyper-parameter to tune (can be \"make or break\").",
      "That said, this is already well known.",
      "And the fact that it explains so much of the variance might reflect a bias of the analysis towards a situation where the learning rate isn't tuned as well as it could be in practice (this is afterall THE hyper-parameter that neural net researcher spend the most time tuning in practice).",
      "So, as future work, this suggests perhaps doing another round of the same analysis (which is otherwise really neatly setup), where more effort is always put on tuning the learning rate, individually for each of the other hyper-parameters.",
      "In other words, we'd try to ignore the regions of hyper-parameter space that correspond to bad learning rates, in order to \"marginalize out\" its effect.",
      "This would thus explore the perhaps more realistic setup that assumes one always tunes the learning rate as best as possible.",
      "Also, considering a less aggressive gradient clipping into the hyper-parameter search would be interesting since, as the authors admit, clipping within [-1,1] might have been too much and could explain why it didn't help   Otherwise, a really great and useful read!"
    ],
    "author_id": "hlarochelle",
    "pdf_url": "http://arxiv.org/pdf/1503.04069",
    "author_full_name": "Hugo Larochelle",
    "source_website": "https://www.shortscience.org/user?name=hlarochelle",
    "id": 93598871
  },
  {
    "blog_id": "28673525b1713c2d41fd0fac38f81f",
    "summary": [
      "The paper presents some general characteristics that intelligent machines should possess and a roadmap to develop such intelligent machines in small, realistic steps.",
      "Ability to Communicate  The intelligent agents should be able to communicate with humans, preferably using language as the medium.",
      "Such systems can be programmed through natural language and can access much of the human knowledge which is encoded using natural language.",
      "The learning environment should facilitate interactive communication and the machine should have a minimalistic bit interface for IO to keep the interface simple.",
      "Further, the machine should be free to use any internal representation for learning tasks.",
      "Ability to Learn  Learning allows the machine to adapt to the external environment and correct their mistakes.",
      "Users should be able to control the motivation of the machine via a communication channel.",
      "This is similar to the notion of rewards in reinforcement learning.",
      "A simulated ecosystem to educate communication-based intelligent machines  Simulated environment to teach basic linguistic interactions and know-how to operate in the world.",
      "Though the environment should be challenging enough to force the machine to \"learn how to learn\", its complexity should be manageable.",
      "Unlike class AI block worlds, the simulated environment is not intended to teach an exhaustive set of functionality to the agent.",
      "The aim is to teach the machine how to learn efficiently by combining already acquired skills.",
      "Description  Agent  Learner or actor  Teacher  Assigns tasks and rewards to the learner and provides helpful information.",
      "Aim is to kick start the learner's efficient learning capabilities without providing enough direct information.",
      "Environment  Learner explores the environment by giving orders, asking questions and receiving feedback.",
      "Environment uses a controlled language which is more explicit and restricted.",
      "Think of learner as a high-level programming language, the teacher as the programmer and the environment as the compiler.",
      "Interface Channels  Generic input and output channels.",
      "Teacher and environment write to the input channel.",
      "Reward is written to input channel.",
      "Learner writes to the output channel and learns to use ambigous prefixes to address the agents and services it needs to interact with.",
      "Reward  Way to provide feedback to the learner.",
      "Rewards should become sparse as the learner's intelligence grows and \"curiosity\" should be a learnt strategy.",
      "Learner should maximise average reward over time so that faster strategies are preferred in case of equal rewards.",
      "Incremental Structure  Think of learner progressing through different levels where skills from earlier levels can be used in later levels.",
      "Tasks need not be ordered within a level.",
      "Learner starts by performing basic tasks like repeating characters then learns to associate linguistic strings to action sequences.",
      "Further, the learner learns to ask questions and \"read\" natural text.",
      "Time Off  Learner is given time to either explore the environment or to interact with the Teacher or to update its internal structure by replaying the previous experience.",
      "Evaluation  Evaluating the learning agent on only the final behaviour only is not sufficient as it overlooks the number of attempts to reach the optimal behaviour.",
      "Better approach would be to conduct public competition where developers have access to preprogrammed environment for fixed amount of time and learners are evaluated on tasks that are considerably different from the tasks encountered during training.",
      "Tasks  A brief overview of the type of tasks is provided here  Types of Learning  Concept of positive and negative rewards.",
      "Discovery of algorithms.",
      "Remember facts, skills, and learning strategies.",
      "Long term memory  To store facts, algorithms and even ability to learn.",
      "Compositional Learning Skills  Producing new structures by combining together known facts and skills.",
      "Understanding new concepts should not always require training examples.",
      "Computational properties of intelligent machines  Computational model should be able to represent any pattern in data (alternatively, represent any algorithm in fixed length).",
      "Among the various Turning-complete computational systems available, the most natural choice would be a compositional system that can perform computations in parallel.",
      "Alternatively, a non-growing model with immensely large capacity could be used.",
      "In a growing model, new cells are connected to ones that spawned them leading to topological structures that can contribute to learning.",
      "But it is not clear if such topological structures can arise in a large-capacity unstructured model."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1511.08130",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 96002700
  },
  {
    "blog_id": "vqa-visual-question-answering",
    "summary": [
      "Problem Statement  Given an image and a free-form, open-ended, natural language question (about the image), produce the answer for the image.",
      "VQA Challenge and Workshop  The authors organise an annual challenge and workshop to discuss the state-of-the-art methods and best practices in this domain.",
      "Interestingly, the second version is starting on 27th April 2017 (today).",
      "Benefits over tasks like image captioning:  Simple, n-gram statistics based methods are not sufficient.",
      "Requires the system to blend in different aspects of knowledge - object detection, activity recognition, commonsense reasoning etc.",
      "Since only short answers are expected, evaluation is easier.",
      "Dataset  Created a new dataset of 50000 realistic, abstract images.",
      "Used AMT to crowdsource the task of collecting questions and answers for MS COCO dataset (>200K images) and abstract images.",
      "Three questions per image and ten answers per question (along with their confidence) were collected.",
      "The entire dataset contains over 760K questions and 10M answers.",
      "The authors also performed an exhaustive analysis of the dataset to establish its diversity and to explore how the content of these question-answers differ from that of standard image captioning datasets.",
      "Highlights of data collection methodology  Emphasis on questions that require an image, and not just common sense, to be answered correctly.",
      "Workers were shown previous questions when writing new questions to increase diversity.",
      "Answers collected from multiple users to account for discrepancies in answers by humans.",
      "Two modalities supported:  Open-ended - produce the answer  multiple-choice - select from a set of options provided (18 options comprising of popular, plausible, random and ofc correct answer)  Highlights from data analysis  Most questions range from four to ten words while answers range from one to three words.",
      "Around 40% questions are \u201cyes/no\u201d questions.",
      "Significant (>80%) inter-human agreement for answers.",
      "The authors performed a study where human evaluators were asked to answer the questions without looking at the images.",
      "Further, they performed a study where evaluators were asked to label if a question could be answered using common sense and what was the youngest age group, they felt, could answer the question.",
      "The idea was to establish that a sufficient number of questions in the dataset required more than just common sense to answer.",
      "Baseline Models  random selection  prior (\u201cyes\u201d) - always answer as yes.",
      "per Q-type prior - pick the most popular answer per question type.",
      "nearest neighbor - find the k nearest neighbors for the given (image, question) pair.",
      "Methods  2-channel model (using vision and language models) followed by softmax over (K = 1000) most frequent answers.",
      "Image Channel  I - Used last hidden layer of VGGNet to obtain 4096-dim image embedding.",
      "norm I - : l2 normalized version of I.",
      "Question Channel  BoW Q - Bag-of-Words representation for the questions using the top 1000 words plus the top 1- first, second and third words of the questions.",
      "LSTM Q - Each word is encoded into 300-dim vectors using fully connected + tanh non-linearity.",
      "These embeddings are fed to an LSTM to obtain 1024d-dim embedding.",
      "Deeper LSTM Q - Same as LSTM Q but uses two hidden layers to obtain 2048-dim embedding.",
      "Multi-Layer Perceptron (MLP) - Combine image and question embeddings to obtain a single embedding.",
      "BoW Q + I method - concatenate BoW Q and I embeddings.",
      "LSTM Q + I, deeper LSTM Q + norm I methods - image embedding transformed to 1024-dim using a FC layer and tanh non-linearity followed by element-wise multiplication of image and question vectors.",
      "Pass combined embedding to an MLP - FC neural network with 2 hidden layers (1000 neurons and 0.5 dropout) with tanh, followed by softmax.",
      "Cross-entropy loss with VGGNet parameters frozen.",
      "Results  Deeper LSTM Q + norm I is the best model with 58.16% accuracy on open-ended dataset and 63.09% on multiple-choice but far behind the human evaluators (>80% and >90% respectively).",
      "The best model performs well for answers involving common visual objects but performs poorly for answers involving counts.",
      "Vision only model performs even worse than the model which always produces \u201cyes\u201d as the answer."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1505.00468v6",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 66883240
  },
  {
    "blog_id": "using-deep-learning-to-detect-linguistic-cues-of-alzheimers-patients-a606693e54f9",
    "summary": [
      "This paper aims to detect linguistic characteristics and grammatical patterns from speech transcriptions generated by Alzheimer\u2019s disease (AD) patients.",
      "The authors propose several neural models such as CNNs and LSTM-RNNs \u2014 and combinations of them \u2014 to enhance an AD classification task.",
      "The trained neural models are used to interpret linguistic characteristics of AD patients (including gender variation) via activation clustering and first-derivative saliency techniques.",
      "Motivation  Language variation can serve as a proxy that monitors how patients\u2019 cognitive functions have been affected (e.g., issues with word finding and impaired reasoning).",
      "This can equip machines with diagnostic capabilities, which are particularly effective for dealing with AD since it is neither curable or reversible.",
      "Challenges and Limitations  The challenge with detecting AD-positive patients is it requires diverse linguistic and world knowledge.",
      "Consider the following example:  \u201cWell\u2026there\u2019s a mother standing there uh uh washing the dishes and the sink is overspilling\u2026overflowing.\u201d There are several linguistic cues, such as \u201coverspilling\u2026overflowing\u201d, indicating signs of confusion and memory loss, which is very common in AD-positive patients.",
      "Therefore, instead of relying on hand-crafted features, the authors propose a neural model for automatically learning these linguistic cues from the data.",
      "Other important issues observed from the previous literature are as follows:  Hand-crafted features may not be suitable to analyze AD patients data, which convey progressive changes in linguistic patterns.",
      "Hand-crafted features quickly become outdated as language and culture evolve.",
      "Relying on neural networks alone doesn\u2019t offer much interpretability.",
      "Data  This work uses the Dementia Bank dataset , which consists of transcripts and audio recordings of AD (and control) patients.",
      "These records were collected via interviews on several tasks such as \u201cRecall Test\u201d and \u201cBoston Cookie Theft\u201d.",
      "Transcripts were segmented into individual utterances with accompanying part-of-speech (POS) tags.",
      "Models  Three types of neural approaches are proposed: CNN (embedding + convolutional layer + max-pooling layer), LSTM-RNN (embedding + LSTM layer), and CNN-LSTM (basically laying an LSTM on top of CNN \u2014 architecture shown in the figure below).",
      "(See paper for more details.)",
      "Results  The best performing model (POS tags + CNN-LSTM) achieves 91.1% accuracy, which sets a new benchmark for the AD classification task.",
      "See other results below.",
      "The authors observed that almost all AD-positive results were classified correctly and that there were more errors in classifying non-AD samples.",
      "This could be because the dataset contained patients with various degree of symptoms related to AD.",
      "(See paper for more results.)",
      "Analysis  No significant differences in linguistic patterns were observed between male and female AD patients.",
      "Furthermore, interpretation of the linguistic cues captured by the neural models are conducted using two visualization techniques:  Activation Clustering \u2014 offers insights into sentence-level patterns  First Derivative Saliency \u2014 offers insights into word importance  Through the activation clustering, three common linguistic patterns found in AD patients emerged from the clusters: short answers and bursts of speech (e.g., \u201cand\u201d and \u201coh!\u201d), repeated requests for clarification (e.g., \u201cdid I say fact?\u201d), and starting with interjections (\u201cso\u201d and \u201cwell\u201d).",
      "Moreover, for several tasks such as Cookie and Recall, the most commonly used POS tags for AD clusters were show to be distinct.",
      "Through the salience heat maps, the difference in word importance can be seen between control and AD patients.",
      "As shown in the figure below (left), the word \u201cuh\u201d and \u201cum\u201d are important and distinguishable speech traits for classifying AD patients.",
      "The figure (right) shows that the control group does not heavily use these type of filler words.",
      "Future Work and Conclusion  Neural models combined with visualization techniques can offer more insights into the linguistic cues and variations of AD patients.",
      "With that said, such models can be generalized to study other neurological diseases.",
      "Context-aware models and conversational context can help to improve the predictive performance of the models and also offer more interpretability.",
      "References  Detecting Linguistic Characteristics of Alzheimer\u2019s Dementia by Interpreting Neural Models \u2014 (Sweta Karlekar, Tong Niu, and Mohit Bansal)"
    ],
    "author_id": "DAIR-AI",
    "pdf_url": "https://www.aclweb.org/anthology/N18-2110.pdf",
    "author_full_name": "Elvis Saravia",
    "source_website": "https://github.com/dair-ai/nlp_paper_summaries",
    "id": 26262723
  },
  {
    "blog_id": "adam",
    "summary": [
      "What  They suggest a new stochastic optimization method, similar to the existing SGD, Adagrad or RMSProp.",
      "Stochastic optimization methods have to find parameters that minimize/maximize a stochastic function.",
      "A function is stochastic (non-deterministic), if the same set of parameters can generate different results.",
      "E.g. the loss of different mini-batches can differ, even when the parameters remain unchanged.",
      "Even for the same mini-batch the results can change due to e.g. dropout.",
      "Their method tends to converge faster to optimal parameters than the existing competitors.",
      "Their method can deal with non-stationary distributions (similar to e.g. SGD, Adadelta, RMSProp).",
      "Their method can deal with very sparse or noisy gradients (similar to e.g. Adagrad).",
      "How  Basic principle  Standard SGD just updates the parameters based on parameters = parameters - learningRate * gradient.",
      "Adam operates similar to that, but adds more \"cleverness\" to the rule.",
      "It assumes that the gradient values have means and variances and tries to estimate these values.",
      "Recall here that the function to optimize is stochastic, so there is some randomness in the gradients.",
      "The mean is also called \"the first moment\".",
      "The variance is also called \"the second (raw) moment\".",
      "Then an update rule very similar to SGD would be parameters = parameters - learningRate * means.",
      "They instead use the update rule parameters = parameters - learningRate * means/sqrt(variances).",
      "They call means/sqrt(variances) a 'Signal to Noise Ratio'.",
      "Basically, if the variance of a specific parameter's gradient is high, it is pretty unclear how it should be changend.",
      "So we choose a small step size in the update rule via learningRate * mean/sqrt(highValue).",
      "If the variance is low, it is easier to predict how far to \"move\", so we choose a larger step size via learningRate * mean/sqrt(lowValue).",
      "Exponential moving averages  In order to approximate the mean and variance values you could simply save the last T gradients and then average the values.",
      "That however is a pretty bad idea, because it can lead to high memory demands (e.g. for millions of parameters in CNNs).",
      "A simple average also has the disadvantage, that it would completely ignore all gradients before T and weight all of the last T gradients identically.",
      "In reality, you might want to give more weight to the last couple of gradients.",
      "Instead, they use an exponential moving average, which fixes both problems and simply updates the average at every timestep via the formula avg = alpha * avg + (1 - alpha) * avg.",
      "Let the gradient at timestep (batch) t be g, then we can approximate the mean and variance values using:  mean = beta1 * mean + (1 - beta1) * g  variance = beta2 * variance + (1 - beta2) * g^2.",
      "beta1 and beta2 are hyperparameters of the algorithm.",
      "Good values for them seem to be beta1=0.9 and beta2=0.999.",
      "At the start of the algorithm, mean and variance are initialized to zero-vectors.",
      "Bias correction  Initializing the mean and variance vectors to zero is an easy and logical step, but has the disadvantage that bias is introduced.",
      "E.g. at the first timestep, the mean of the gradient would be mean = beta1 * 0 + (1 - beta1) * g, with beta1=0.9 then: mean = 0.9 * g. So 0.9g, not g. Both the mean and the variance are biased (towards 0).",
      "This seems pretty harmless, but it can be shown that it lowers the convergence speed of the algorithm by quite a bit.",
      "So to fix this pretty they perform bias-corrections of the mean and the variance:  correctedMean = mean / (1-beta1^t) (where t is the timestep).",
      "correctedVariance = variance / (1-beta2^t).",
      "Both formulas are applied at every timestep after the exponential moving averages (they do not influence the next timestep)."
    ],
    "author_id": "ALEJU",
    "pdf_url": "https://arxiv.org/pdf/1412.6980",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 46342402
  },
  {
    "blog_id": "ea5a42d17e0fcf18374df8e3e4b3e8",
    "summary": [
      "Introduces a new global log-bilinear regression model which combines the benefits of both global matrix factorization and local context window methods.",
      "Global Matrix Factorization Methods  Decompose large matrices into low-rank approximations.",
      "eg - Latent Semantic Analysis (LSA)  Limitations  Poor performance on word analogy task  Frequent words contribute disproportionately high to the similarity measure.",
      "Shallow, Local Context-Based Window Methods  Learn word representations using adjacent words.",
      "eg - Continous bag-of-words (CBOW) model and skip-gram model.",
      "Limitations  Since they do not operate directly on the global co-occurrence counts, they can not utilise the statistics of the corpus effectively.",
      "GloVe Model  To capture the relationship between words i and j, word vector models should use ratios of co-occurene probabilites (with other words k) instead of using raw probabilites themselves.",
      "In most general form:  F(wi, wj, wk~ ) = Pik/Pjk  We want F to encode information in the vector space (which have a linear structure), so we can restrict to the difference of wi and wj  F(wi - wj, wk~ ) = Pik/Pjk  Since right hand side is a scalar and left hand side is a vector, we take dot product of the arguments.",
      "F( (wi - wj)T, wk~ ) = Pik/Pjk  F should be invariant to order of the word pair i and j.  F(wiTwk~) = Pik  Doing further simplifications and optimisations (refer paper), we get cost function,  J = Sum (over all i, j pairs in the vocabulary)[wiTwk  + bi + bk - log(Xik)]2  f is a weighing function.",
      "f(x) = min((x/xmax)\u03b1, 1)  Typical values, xmax = 100 and \u03b1 = 3/4  b are the bias terms.",
      "Complexity  Depends on a number of non-zero elements in the input matrix.",
      "Upper bound by the square of vocabulary size  Since for shallow window-based approaches, complexity depends on |C| (size of the corpus), tighter bounds are needed.",
      "By modelling number of co-occurrences of words as power law function of frequency rank, the complexity can be shown to be proportional to |C|0.8  Evaluation  Tasks  Word Analogies  a is to b as c is to ___?",
      "Both semantic and syntactic pairs  Find closest d to wb - wc + wa (using cosine similarity)  Word Similarity  Named Entity Recognition  Datasets  Wikipedia Dumps - 2010 and 2014  Gigaword5  Combination of Gigaword5 and Wikipedia2014  CommonCrawl  400,000 most frequent words considered from the corpus.",
      "Hyperparameters  Size of context window.",
      "Whether to distinguish left context from right context.",
      "f - Word pairs that are d words apart contribute 1/d to the total count.",
      "xmax = 100  \u03b1 = 3/4  AdaGrad update  Models Compared With  Singular Value Decomposition  Continous Bag-Of-Words  Skip-Gram  Results  Glove outperforms all other models significantly.",
      "Diminishing returns for vectors larger than 200 dimensions.",
      "Small and asymmetric context windows (context window only to the left) works better for syntactic tasks.",
      "Long and symmetric context windows (context window to both the sides) works better for semantic tasks.",
      "Syntactic task benefited from larger corpus though semantic task performed better with Wikipedia instead of Gigaword5 probably due to the comprehensiveness of Wikipedia and slightly outdated nature of Gigaword5.",
      "Word2vec\u2019s performance decreases if the number of negative samples increases beyond about 10.",
      "For the same corpus, vocabulary, and window size GloVe consistently achieves better results, faster.",
      "This comment has been minimized.",
      "Sign in to view  Copy link  Quote reply  Liskutin commented  Mar 12, 2019  I know its old post, but I have a question.",
      "If the model is basically trying to reconstruct the big co-occurrence matrix [well in the paper they said they transform that into probability ratios matrix, i guess], is the model reconstructing the matrix line by line or word by word?"
    ],
    "author_id": "shugan",
    "pdf_url": "http://arxiv.org/pdf/1607.01759v3",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 48074885
  },
  {
    "blog_id": "5e299ff5f79a4f9da4a2e9281a0676",
    "summary": [
      "Open-domain Question Answering (Open QA) - efficiently querying large-scale knowledge base(KB) using natural language.",
      "Two main approaches:  Information Retrieval  Transform question (in natural language) into a valid query(in terms of KB) to get a broad set of candidate answers.",
      "Perform fine-grained detection on candidate answers.",
      "Semantic Parsing  Interpret the correct meaning of the question and convert it into an exact query.",
      "Limitations:  Human intervention to create lexicon, grammar, and schema.",
      "This work builds upon the previous work where an embedding model learns low dimensional vector representation of words and symbols.",
      ".",
      "Task Definition  Input - Training set of questions (paired with answers).",
      "KB providing a structure among the answers.",
      "Answers are entities in KB and questions are strings with one identified KB entity.",
      "The paper has used FREEBASE as the KB.",
      "Datasets  WebQuestions - Built using FREEBASE, Google Suggest API, and Mechanical Turk.",
      "FREEBASE triplets transformed into questions.",
      "Clue Web Extractions dataset with entities linked with FREEBASE triplets.",
      "Dataset of paraphrased questions using WIKIANSWERS.",
      "Embedding Questions and Answers  Model learns low-dimensional vector embeddings of words in question entities and relation types of FREEBASE such that questions and their answers are represented close to each other in the joint embedding space.",
      "Scoring function S(q, a), where q is a question and a is an answer, generates high score if a answers q.",
      "S(q, a) = f(q)T.g(a)  f(q) maps question to embedding space.",
      "f(q) = W\u03c6(q)  W is a matrix of dimension K * N  K - dimension of embedding space (hyper parameter).",
      "N - total number of words/entities/relation types.",
      "\u03c8(q) - Sparse Vector encoding the number of times a word appears in q.",
      "Similarly, g(a) = W\u03c8(a) maps answer to embedding space.",
      "&psi(a) gives answer representation, as discussed below.",
      "Possible Representations of Candidate Answers  Answer represented as a single entity from FREEBASE and TBD is a one-of-N encoded vector.",
      "Answer represented as a path from question to answer.",
      "The paper considers only one or two hop paths resulting in 3-of-N or 4-of-N encoded vectors(middle entities are not recorded).",
      "Encode the above two representations using subgraph representation which represents both the path and the entire subgraph of entities connected to answer entity as a subgraph.",
      "Two embedding representations are used to differentiate between entities in path and entities in the subgraph.",
      "SubGraph approach is based on the hypothesis that including more information about the answers would improve results.",
      "Training and Loss Function  Minimize margin based ranking loss to learn matrix W.  Stochastic Gradient Descent, multi-threaded with Hogwild.",
      "Multitask Training of Embeddings  To account for a large number of synthetically generated questions, the paper also multi-tasks the training of model with paraphrased prediction.",
      "Scoring function Sprp(q1, q2) = f(q1)Tf(q2), where f uses the same weight matrix W as before.",
      "High score is assigned if q1 and q2 belong to same paraphrase cluster.",
      "Additionally, the model multitasks the task of mapping embeddings of FREEBASE entities (mids) to actual words.",
      "Inference  For each question, a candidate set is generated.",
      "The answer (from candidate set) with the highest set is reported as the correct answer.",
      "Candidate set generation strategy  C1 - All KB triplets containing the KB entity from the question forms a candidate set.",
      "Answers would be limited to 1-hop paths.",
      "C2 - Rank all relation types and keep top 10 types and add only those 2-hop candidates where the selected relations appear in the path.",
      "Results  C2 strategy outperforms C1 approach supporting the hypothesis that a richer representation for answers can store more information.",
      "Proposed approach outperforms the baseline methods but is outperformed by an ensemble of proposed approach with semantic parsing via paraphrasing model."
    ],
    "author_id": "shugan",
    "pdf_url": "https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 23426061
  },
  {
    "blog_id": "a-semantic-loss-function-for-deep-learning-with-symbolic-knowledge",
    "summary": [
      "The paper proposes an approach for using symbolic knowledge in deep learning systems.",
      "These constraints are often expressed as boolean constraints on the output of the deep learning system and directly incorporating these constraints break the differentiability of the system.",
      "Problem Setting  The model is given some input data to perform predictions and symbolic knowledge is provided in form of boolean constraints like exactly-one constraint for one-hot output encoding.",
      "Most approaches tend to encode the symbolic knowledge in the vector space embedding to keep the model pipeline differentiable.",
      "In this process, the precise meaning of symbolic knowledge is often lost.",
      "A differentiable \u201csemantic loss\u201d is derived which captures the meaning of the constraint while being independent of its syntax.",
      "Terminology  A state x (state refers to the instantiation of boolean variables) satisfies a sentence a if a evaluates to true when using the variables as specified by x.",
      "A sentence a entails another sentence b if all states that satisfy a also satisfy b.",
      "The row output vector of the neural network is denoted as p where each value in p denotes the probability of an output.",
      "Three different output constraints are studied:  Exactly-one constraint  Exactly one value in p should be true.",
      "Can be expressed in boolean logic as follows: Let (x1, x2, \u2026, xn) be variables in p. Then (not xi or not xj) for all pair of variables and (x1 or x2 or \u2026 xn).",
      "Valid Simple Path Constraint  Set of edges must form a valid path.",
      "Ordering Constraint  Defining an ordering over the variables.",
      "Semantic Loss  The semantic loss Ls(a, p) is a function of a propositional logic sentence a (the symbolic knowldge constraint) and p (output of the neural network).",
      "a is defined over variables (x1, \u2026, xn) and p is interpreted as a vector of probabilities corresponding to these variables xi\u2019s.",
      "The semantic loss is directly proportional to the negative log likelihood of generating a state that satisfies the constraints when sampling values according to the distribution p.  Main Axioms and Insights  Monotonicity  If a sentence a entails another sentence b then for any given p, Ls(a, p) > Ls(b, p) ie adding more constraints cannot decrease the semantic loss.",
      "Semantic Equivalence  If two sentences are logically equivalent, their semantic loss is the same.",
      "Identity  For any given sentence a, its representation as a sentence is equivalent to its representation as a deterministic vector ie writing the \u201cone-hot\u201d constraint as a boolean expression is equivalent to a one-hot vector.",
      "Satisfaction  If p entails the sentence a then Ls(a, p) = 0.",
      "Label-literal correspondence  When the constraint is defined in terms of a single variable, it can be interpreted as the supervised label.",
      "Hence the semantic loss in case of a single variable should be equivalent to the cross-entropy loss.",
      "Truth  The semantic loss of a true sentence is 0  Non-negativity  Semantic loss should always be non-negative.",
      "Probabilities of variables that are not part of the constraint, do not affect the semantic loss.",
      "It can be shown that the semantic loss function satisfies all these axioms (and the other axioms specified in the paper) and is the only function to do so, up to a multiplicative constant.",
      "Experimental Evaluation  Semantic Loss is used in the semi-supervised setting for Permuted MNIST, Fashion MNIST and CIFAR-10.",
      "The key takeaway is that using semantic loss improves the performance of the state-of-the-art models for Fashion MNIST and CIFAR-10.",
      "One downside is that the effectiveness of the semantic loss in this type of constraint strongly depends on the performance of the underlying model.",
      "Further, the semantic loss does not improve the performance in case of fully supervised scenario.",
      "Further experiments are performed to evaluate the performance of the semantic loss on complex constraints.",
      "Since these tasks aim to highlight the effect of using semantic loss, only simple models (MLPs) are evaluated.",
      "Tractability of Semantic Loss  The semantic loss is similar to the automated reasoning task called as weight model counting (wmc).",
      "Circuit compiler techniques can be used to compute wmc while allowing backpropagation.",
      "Notes  The proposed idea is simple and intuitive and the results on semi-supervised classification task are quite good.",
      "It would be interesting to extend and scale this method for more complex constraints."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1711.11157",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 26668230
  },
  {
    "blog_id": "identifying-impactful-service-system-problems-via-log-analysis",
    "summary": [
      "Identifying impactful service system problems via log analysis He et al., ESEC/FSE\u201918  If something is going wrong in your system, chances are you\u2019ve got two main sources to help you detect and resolve the issue: logs and metrics.",
      "You\u2019re unlikely to be able to get to the bottom of a problem using metrics alone (though you might well detect one that way), so that leaves logs as the primary diagnosis tool.",
      "The online service at Microsoft used as the main case study in the paper produces dozens of Terabytes of logs every day.",
      "Logs play a crucial role in the diagnosis of modern cloud-based online service systems.",
      "Clearly, manual problem diagnosis is very time-consuming and error-prone due to the increasing scale and complexity of large-scale systems.",
      "Log3C analyses logs to look for indications of impactful problems, using correlated KPIs as a guide.",
      "It finds these needles in the haystack with an average precision of 0.877 and an average recall of 0.883.",
      "A distributed version of Log3C has been deployed and used in production at Microsoft for several years, both to support a massive online service (we are not told which one), and integrated into \u201cProduct B\u201d where it is used as a log analysis engine to analyse tens of billions of log messages every day.",
      "Log3C greatly reduces engineer\u2019s efforts on manually inspecting the logs and pinpointing root causes of failures.",
      "Furthermore, fault patterns are also extracted and maintained for analyzing similar problems in the future.",
      "From the experiences thus gained in log analysis the authors draw the following lessons:  Simple anomaly / outlier detection doesn\u2019t cut it.",
      "It\u2019s tempting to believe that systems are regular most of the time, and therefore any outliers are problems, but in practice there\u2019s can be a long tail of infrequent (but genuine) user behaviours.",
      "\u201cOur experiences with the production system reveal that there are indeed many rare user behaviors, which are not real problems.",
      "A lot of effort could be wasted by examining these false positives.\u201c  It\u2019s not just a single incidence of a problem that\u2019s important, but also the underlying trend in the number of incidences of that problem.",
      "Are the number of incidences steadily increasing over a period of time?",
      "That\u2019s of interest.",
      "Have totally new problems appeared?",
      "That could be an indication of a buggy deployment.",
      "Do the number of incidences of problem decrease after a fix, but then settle down to a steady but non-zero level?",
      "This can indicate an incomplete fix or partial solution.",
      "To work its magic, Log3C combines both metrics and logs.",
      "KPIs are used to guide log analysis to focus on periods in time when we have external evidence of a system problem (e.g., the error rate is up).",
      "This helps to separate the long tail of genuine user behaviours from true problems.",
      "Then Log3C uses a novel clustering algorithm called Cascading Clusters in order to be able to effectively cluster the massive amounts of log data generated by the system.",
      "Problems are identified by looking for rare clusters correlated with KPI degradation.",
      "Clustering is made more difficult because logs are highly imbalanced \u2013 the vast majority of log entries are for regular non-problem behaviours.",
      "High level approach  Log3C consists of four steps: log parsing, sequence vectorization, cascading clustering, and correlation analysis.",
      "( Enlarge )  \u2026at each time interval, logs are parsed into log events and vectorized into sequence vectors, which are then grouped into multiple clusters through cascading clustering.",
      "However, we still cannot extrapolate whether a cluster is an impactful problem, which necessitates the use of KPIs.",
      "Consequently, in step four, we correlate clusters and KPIs over different time intervals to find impactful problems.",
      "From log messages to sequence vectors  The first step is to turn log messages into log events (on the assumption you\u2019re not logging as events already).",
      "For each log message such as \u201cHTTP Request URL:  [url]"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3236024.3236083?download=true",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 69502658
  },
  {
    "blog_id": "deep-learning-and-time-to-predict-emojis-4a6256c16475",
    "summary": [
      "Overview  Emoji usage has become a new form of social communication, which is important because it can help to improve communication systems, such as chat applications.",
      "This paper investigates the usage and semantics of emojis over time to analyze seasonal variation of emoji usage.",
      "In addition, they develop an emoji prediction model based on the time information.",
      "Contribution  Multiple emoji predictions studies have been carried out in the past (see notable work by Felbo et al., 2017 ), but non have considered temporal information.",
      "Temporal correlation between emojis and seasonal events are explored and used to disambiguate emoji meanings.",
      "Example  Consider the leaf clover emoji (\uf340), it is usually associated with good luck wishes all year round except in March, where it is mostly used to express event situations related to parties and drinking (due to St. Patrick day).",
      "Challenges  This study demonstrates that temporal information is useful for emoji prediction even for emojis that are not associated with time (\uf4aa and \u2764\ufe0f).",
      "Emojis are innately subjective, which is why it is difficult to analyze their semantic meaning.",
      "Dataset  Twitter is used to collect a 100 million US tweets corpus and organized as follows:  Seasonal Emoji Dataset \u2014 data is divided into four subsets by seasons: Spring, Summer, Autumn, and Winter (see figure below)  Emoji Prediction Dataset \u2014 data is reduced to tweets that only contain one frequent emoji (emoji must belong to the 300 frequent emojis)  Seasonal Emoji Semantic and Usage  Skip-gram word embedding models are trained using the four subsets of the seasonal datasets.",
      "These models provide information that basically helps to describe emojis in terms of their semantic similarity to each other.",
      "(See paper for more details)  By comparing the top 10 emojis associated to each emoji in the embedding space, it was discovered that emojis related to music, animal, sweets, and emotions were not influenced by seasonality (e.g., \uf3b6, \uf3bc, \uf366, \uf420, \uf602, \uf3b8).",
      "This means that these emojis preserved meaning across seasons.",
      "In contrast, sport-related emojis (e.g., \uf3c0, \uf3c6) varied in meaning across seasons, probably due to the high-peak seasons when sports are played.",
      "Another interesting emoji related to school (\uf393), changed meaning across seasons; during Spring it was associated with party emojis, and during Autumn it was associated with school-related emojis.",
      "Check out the top 10 associated emojis per season for the pine emoji (\uf332) in the figure below \u2014 very season-dependent don\u2019t you think?",
      "Can you guess why?",
      "(hint: outdoors vs Christmas).",
      "(See paper for tons of interesting findings)  Emoji Prediction  The second dataset, which includes 300 emoji classes and 900,000 tweets total (3,000 tweets per class), is used for emoji prediction.",
      "The architecture of the emoji prediction model is as follows: character embeddings, word embeddings, and data embeddings are combined through both an early fusion approach and a late fusion approach.",
      "This produces two models (Early and Late).",
      "A third model is trained (W/O) which completely ignores the date embeddings.",
      "(See paper to find out how these embeddings are constructed)  Results  Precision, Recall, and F1 scores are reported for all models in the table below.",
      "We can observe that by combining time information using early fusion, the Early model outperforms the other models.",
      "The emojis that had higher gains in F1 score (without date vs. early date) are presented in the table below.",
      "You can definitely observe that many emojis are season-specific (e.g., \uf340, \uf312) and thus benefit from the date embeddings.",
      "Even emojis that are not associated to time (e.g., \uf5a4, \u2764\ufe0f, \uf4aa) benefit from the temporal information.",
      "Conclusion & Future Work  A multimodal architecture was proposed to conduct emoji prediction based on deep neural networks.",
      "More analysis on the emoji semantics and usage over specific time of the day or week may be able to help improve the date embeddings and the overall predictive models.",
      "This work has a lot of room for improvement and it could be a very interesting topic to combine with emotion recognition, event detection, and computational health studies.",
      "References  Ref:  [url]"
    ],
    "author_id": "DAIR-AI",
    "pdf_url": "https://arxiv.org/pdf/1805.00731",
    "author_full_name": "Elvis Saravia",
    "source_website": "https://github.com/dair-ai/nlp_paper_summaries",
    "id": 44210578
  },
  {
    "blog_id": "meta-reinforcement-learning-of-structured-exploration-strategies",
    "summary": [
      "The paper looks at the problem of learning structured exploration policies for training RL agents.",
      "Structured Exploration  Consider a stochastic, parameterized policy \u03c0\u03b8(a|s) where \u03b8 represents the policy-parameters.",
      "To encourage exploration, noise can be added to the policy at each time step t. But the noise added in such a manner does not have any notion of temporal coherence.",
      "Another issue is that if the policy is represented by a simple distribution (say parameterized unimodal Gaussian), it can not model complex time-correlated stochastic processes.",
      "The paper proposes to condition the policy on per-episode random variables (z) which are sampled from a learned latent distribution.",
      "Consider a distibution over the tasks p(T).",
      "At the start of any episode of the ith task, a latent variable zi is sampled from the distribution N(\u03bci, \u03c3i) where \u03bci and \u03c3i are the learned parameters of the distribution and are referred to as the variation parameters.",
      "Once sampled, the same zi is used to condition the policy for as long as the current episode lasts and the action is sampled from then distribution \u03c0\u03b8(a|s, zi).",
      "The intuition is that the latent variable zi would encode the notion of a task or goal that does not change arbitrarily during the episode.",
      "Model Agnostic Exploration with Structured Noise  The paper focuses on the setting where the structured exploration policies are to be learned while leveraging the learning from prior tasks.",
      "A meta-learning approach, called as model agnostic exploration with structured noise (MAESN) is proposed to learn a good initialization of the policy-parameters and to learn a latent space (for sampling the z from) that can inject structured stochasticity in the policy.",
      "General meta-RL approaches have two limitations when it comes to \u201clearning to explore\u201d:  Casting meta-RL problems as RL problems lead to policies that do not exhibit sufficient variability to explore effectively.",
      "Many current approaches try to meta-learn the entire learning algorithm which limits the asymptotic performance of the model.",
      "Idea behind MAESN is to meta-train policy-parameters so that they learn to use the task-specific latent variables for exploration and can quickly adapt to a new task.",
      "An important detail is that the parameters are optimized to maximize the expected rewards after one step of gradient update to ensure that the policy uses the latent variables for exploration.",
      "For every iteration of meta-training, an \u201cinner\u201d gradient update is performed on the variational parameters and the post-inner-update parameters are used to perform the meta-update.",
      "The authors report that performing the \u201cinner\u201d gradient update on the policy-parameters does not help the overall learning objective and that the step size for each parameter had to be meta-learned.",
      "The variation parameters have the usual KL divergence loss which encourages them to be close to the prior distribution (unit Gaussian in this case).",
      "After training, the variational parameters for each task are quite close to the prior probably because the training objective optimizes for the expected reward after one step of gradient descent on the variational parameters.",
      "Another implementation detail is that reward shaping is used to ensure that the policy gets useful signal during meta-training.",
      "To be fair to the baselines, reward shaping is used while training baselines as well.",
      "Moreover, the policies trained with reward shaping generalizes to sparse reward setup as well (during meta-test time).",
      "Experiments  Three tasks distributions: Robotic Manipulation, Wheeled Locomotion, and Legged Locomotion.",
      "Each task distribution has 100 meta-training tasks.",
      "In the Manipulation task distribution, the learner has to push different blocks from different positions to different goal positions.",
      "In the Locomotion task distributions, the different tasks correspond to the different goal positions.",
      "The experiments show that the proposed approach can adapt to new tasks quickly and the learn coherent exploration strategy.",
      "\u2022 In some cases, learning from scratch also provides a strong asymptotic performance although learning from scratch takes much longer."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1904.06387",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 6726365
  },
  {
    "blog_id": "how-good-are-query-optimizers-really",
    "summary": [
      "How good are query optimizers, really?",
      "Leis et al., VLBD 2015  Last week we looked at cardinality estimation using index-based sampling , evaluated using the Join Order Benchmark.",
      "Today\u2019s choice is the paper that introduces the Join Order Benchmark (JOB) itself.",
      "It\u2019s a great evaluation paper , and along the way we\u2019ll learn a lot about mainstream query optimisers.",
      "The evaluated databases are PostgresQL, HyPer, and three commercial databases named \u2018DBMS A\u2019, B, and C. We don\u2019t know what those databases are, but if I had to make an educated guess I\u2019d say they\u2019re likely to be SQLServer, DB/2, and Oracle (in some order) based on a few hints scattered through the paper.",
      "The goal of this paper is to investigate the contribution of all relevant query optimizer components to end-to-end query performance in a realistic setting.",
      "The heart of the paper is an investigation into the performance of industrial-strength cardinality estimators: the authors show that cardinality estimation is the most important factor in producing good query plans.",
      "Cost models (that consume those estimates) are also important, but not as significant.",
      "Finally, and perhaps unsurprisingly, the more query plans a DBMS considers, the better the overall result.",
      "PostgreSQL optimiser  For background, let\u2019s start out by looking at how the PostgreSQL query optimizer works.",
      "Cardinalities of base tables are estimated using histograms (quantile statistics), most common values with their frequencies, and domain cardinalities (distinct value counts).",
      "These per-attribute statistics are computed by the analyze command using a sample of the relation.",
      "Join sizes are estimated using:  |T1\u00a0\u22c8x=y T2| = (|T1||T2|) / max(dom(x),dom(y))  Join orders, including bushy trees but excluding trees with cross products, are enumerated using dynamic programming.",
      "The cost model used to determine which plan is cheapest is comprised of over 4000 lines of C code and takes into account many subtle factors.",
      "At the core, it combines CPU and I/O costs with certain weights.",
      "\u201cSpecifically, the cost of an operator is defined as a weighted sum of the number of accessed disk pages (both sequential and random) and the amount of data processed in memory.\u201d Setting the weights of those cost variables is a dark art.",
      "The Join Order Benchmark  Many research papers on query processing and optimization use standard benchmarks like TPC-H, TPC-DS, or the Star Schema Benchmark (SSB)\u2026 we argue they are not good benchmarks for the cardinality estimation component of query optimizers.",
      "The reason is that in order to easily be able to scale the benchmark data, the data generators are using the very same simplifying assumptions (uniformity, independence, principle of inclusion) that query optimizers make.",
      "To reinforce the point, take a look at the cardinality estimation errors in PostgreSQL for four representative queries from JOB vs three from TPC-H (note the log scale, and significant underestimation on \u2018real data\u2019 cardinalities):  The Join Order Benchmark is based on the Internet Movie Data Base (IMDB).",
      "\u201cLike most real-world data sets IMDB is full of correlations and non-uniform data distributions, and is therefore much more challenging than most synthetic data sets.\u201d  JOB consists of a total of 113 queries over IMDB, with between 3 and 16 joins per query and an average of 8.",
      "The queries all answer questions that may reasonably have been asked by a movie enthusiast.",
      "For cardinality estimators the queries are challenging due to the significant number of joins and the correlations contained in the data set.",
      "However, we did not try to \u201ctrick\u201d the query optimizer, e.g., by picking attributes with extreme correlations.",
      "The JOB query set is available online at  [url]"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://www.vldb.org/pvldb/vol9/p204-leis.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 53269747
  },
  {
    "blog_id": "bengiovjs15",
    "summary": [
      "This paper considers the problem of structured output prediction, in the specific case where the output is a sequence and we represent the sequence as a (conditional) directed graphical model that generates from the first token to the last.",
      "The paper starts from the observation that training such models by maximum likelihood (ML) does not reflect well how the model is actually used at test time.",
      "Indeed, ML training implies that the model is effectively trained to predict each token conditioned on the previous tokens *from the ground truth* sequence (this is known as \"teacher forcing\").",
      "Yet, when making a prediction for a new input, the model will actually generate a sequence by generating tokens one after another and conditioning on *its own predicted tokens* instead.",
      "So the authors propose a different training procedure, where at training time each *conditioning* ground truth token is sometimes replaced by the model's previous prediction.",
      "The choice of replacing the ground truth by the model's prediction is made by \"flipping a coin\" with some probability, independently for each token.",
      "Importantly, the authors propose to start with a high probability of using the ground truth (i.e. start close to ML) and anneal that probability closer to 0, according to some schedule (thus the name Schedule Sampling).",
      "Experiments on 3 tasks (image caption generation, constituency parsing and speech recognition) based on neural networks with LSTM units, demonstrate that this approach indeed improves over ML training in terms of the various performance metrics appropriate for each problem, and yields better sequence prediction models.",
      "#### My two cents  Big fan of this paper.",
      "It both identifies an important flaw in how sequential prediction models are currently trained and, most importantly, suggests a solution that is simple yet effective.",
      "I also believe that this approach played a non-negligible role in Google's winner system for image caption generation, in the Microsoft COCO competition.",
      "My alternative interpretation of why Scheduled Sampling helps is that ML training does not inform the model about the relative quality of the errors it can make.",
      "In terms of ML, it is as bad to put high probability on an output sequence that has just 1 token that's wrong, than it is to put the same amount of probability on a sequence that has all tokens wrong.",
      "Yet, say for image caption generation, outputting a sentence that is one word away from the ground truth is clearly preferable from making a mistake on a words (something that is also reflected in the performance metrics, such as BLEU).",
      "By training the model to be robust to its own mistakes, Scheduled Sampling ensures that errors won't accumulate and makes predictions that are entirely off much less likely.",
      "An alternative to Scheduled Sampling is DAgger (Dataset Aggregation:  [ref] ), which briefly put alternates between training the model and adding to the training set examples that mix model predictions and the ground truth.",
      "However, Scheduled Sampling has the advantage that there is no need to explicitly create and store that increasingly large dataset of sampled examples, something that isn't appealing for online learning or learning on large datasets.",
      "I'm also very curious and interested by one of the direction of future work mentioned in the conclusion: figuring out a way to backprop through the stochastic predictions made by the model.",
      "Indeed, as the authors point out, the current algorithm ignores the fact that, by sometimes taking as input its previous prediction, this induces an additional relationship between the model's parameters and its ultimate prediction, a relationship that isn't taken into account during training.",
      "To take it into account, you'd need to somehow backpropagate through the stochastic process that generated the previous token prediction.",
      "While the work on variational autoencoders has shown that we can backprop through gaussian samples, backpropagating through the sampling of a discrete multinomial distribution is essentially an open problem.",
      "I do believe that there is work that tried to tackle propagating through stochastic binary units however, so perhaps that's a start.",
      "Anyways, if the authors could make progress on that specific issue, it could be quite useful not just in the context of Schedule Sampling, but possibly in the context of training networks with discrete stochastic units in general!"
    ],
    "author_id": "hlarochelle",
    "pdf_url": "http://papers.nips.cc/paper/5956-scheduled-sampling-for-sequence-prediction-with-recurrent-neural-networks.pdf",
    "author_full_name": "Hugo Larochelle",
    "source_website": "https://www.shortscience.org/user?name=hlarochelle",
    "id": 75442340
  },
  {
    "blog_id": "leases-an-efficient-fault-tolerant-mechanism-for-distributed-file-cache-consistency",
    "summary": [
      "Leases: An efficient fault-tolerant mechanism for distributed file cache consistency \u2013 Gray & Cheriton 1989  This paper introduced the leasing model for distributed systems.",
      "Leases are conceptually very straightforward and bring a surprising number of benefits for such a simple mechanism.",
      "Also in this paper you\u2019ll find the simple formulas that can help you figure out the optimum lease term for your workloads.",
      "Caching introduces the problem of ensuring consistency between the cached data and its primary location of storage.",
      "By consistent, we mean that the behaviour is equivalent to their being only a single (uncached) copy of the data except for the performance benefit of the cache.",
      "Now that\u2019s one of the most straightforward definitions of consistent you\u2019re likely to find!",
      "A distributed system\u2026 can experience partial failures: a host may crash or messages may be lost.",
      "Existing approaches to consistency for file caches fall into two categories: those that assume reliable broadcast, and so do not tolerate communication failures, and those that require a consistency check for every read, and so fail to deliver good performance.",
      "Those were simpler times ;) Leases are a very elegant solution to these problems.",
      "A lease is a contract that gives its holder specified rights over property for a limited period of time.",
      "In the context of caching, a lease grants to its holder control over writes to the covered datum during the term of the lease, such that the server must obtain the approval of the leaseholder before the datum may be written\u2026.",
      "A cache using leases requires a valid lease on the datum (in addition to holding the datum) before it returns the datum in response to a read, or modifies the datum in response to a write.",
      "From a performance perspective, one big advantage of leases is that once a lease is held, subsequent reads can return straightaway.",
      "Leases are also very easy to implement on the server-side: using short leases, you can always just wait for the lease time to expire to get back to a known state.",
      "Short lease terms have several advantages.",
      "One is that they minimize the delay resulting from client and server failures (and partitioning communication failures).",
      "When the server cannot communicate with a client, the server must delay writes to a file for which the failed client holds a lease until that lease has expired.",
      "Short lease terms also minimize the overhead due to false sharing \u2013 where a client wants to write to a file that is covered by a lease held by another client, when in fact that client is no longer using it.",
      "Key to the overall performance of the system is choosing an appropriate lease term:  The choice of lease term is based on the trade-off between minimizing the lease extension overhead versus minimizing false sharing.",
      "Given a read rate of R, write rate of W, and a file shared between S caches then the lease benefit factor B is given by B = 2R/SW.",
      "And you\u2019ll be getting benefit from a leasing scheme so long as the amount of time a client holds a lease (without renewing) is > 1/(R(B-1)).",
      "Leases have very nice fault-tolerant properties:  Leases ensure consistency provided that the hosts and network do not suffer certain Byzantine failures including clock failure.",
      "More specifically, consistency is maintained in spite of message loss (including partition), and client or server failures (assuming writes are persistent at the server across a crash).",
      "Moreover, availability is not reduced by the caches because an unreachable client at most briefly delays write access by other clients.",
      "In a forward-looking statement in the conclusion, the authors state:  Lease appear well-suited to large-scale distributed systems.",
      "The improvement in response time that they offer is more significant for the faster processors and higher delay networks.",
      "In their 2013 \u201c Scaling memcache at Facebook \u201d paper, Nishtala et al. discuss the introduction of a lease mechanism to cope with \u2018thundering herds.\u2019 Without leases, all of the cache misses resulted in a peak database query rate of 17K/s.",
      "With leases,the peak database query rate was 1.3K/s."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://web.stanford.edu/class/cs240/readings/89-leases.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 10374612
  },
  {
    "blog_id": "enabling-blockchain-innovations-with-pegged-sidechains",
    "summary": [
      "Enabling blockchain innovations with pegged sidechains \u2013 Back et al. 2014  A very topical choice today.",
      "Last week a number of key players in the Bitcoin ecosystem published a paper (see link above) discussing a mechanism (\u2018pegged sidechains\u2019) to allowed continued innovation and evolution of Bitcoin and related blockchain-based solutions.",
      "From the abstract:  Since the introduction of Bitcoin in 2009, and the multiple computer science and electronic cash innovations it brought, there has been great interest in the potential of decentralised cryptocurrencies.",
      "At the same time,  implementation changes to the consensus-critical parts of Bitcoin must necessarily be handled very conservatively.",
      "As a result, Bitcoin has greater difficulty than other internet protocols adapting to new demands and accommodating new innovation.",
      "An early solution to the problem of innovation was the development of alternative blockchains, altchains, which modify the bitcoin codebase in some way.",
      "The deep security expertise needed to do this right has lead to a situation where  we have seen a volatile, non-navigable environment develop, where the most visible projects may be the least technically sound.",
      "What kinds of innovation are being held back by this current situation?",
      "Back et al. list six different categories, including:  * exploration of trade-offs between block size and transaction rate, and between security and cost \u2013 it would be nice to be able to make these trade-offs per transaction as transactions vary in value and risk-profile  * trading of assets other than currencies on blockchains  * enhanced privacy and censorship-resistance  privacy and censorship-resistance could be improved by use of cryptographic accumulators,  ring signatures, or Chaumian blinding  (that\u2019s a few \u2018unknown unknowns\u2019 that just became \u2018known unknowns\u2019 for me ;).",
      "Perhaps something to investigate in future editions of #themorningpaper ).",
      "We desire a world in which interoperable altchains can be easily created and used, but without unnecessarily fragmenting markets and development\u2026.",
      "we argue it is possible to achieve these seemingly contradictory goals.",
      "The key insight is that Bitcoin the blockchain is conceptually independent of \u2018bitcoin\u2019 the asset.",
      "If we had technology to support the movement of assets between blockchains, new systems could be developed which users could adopt by simply reusing the existing Bitcoin currency.",
      "A principled manner of transferring assets between blockchains is introduced, the pegged sidechain.",
      "Sidechains are firewalled so that a problem in one sidechain does not affect other chains.",
      "\u2026because sidechains are still blockchains independent of Bitcoin, they are free to experiment with new transaction designs, trust models, economic models, asset issuance semantics, or cryptographic features.",
      "and furthermore,  \u2026these technologies can also be used in complementary currencies.",
      "Examples include community currencies, which are designed to preferentially boost local businesses; business barter associations, which support social programs like education or elderly care; and limited-purpose tokens which are used wihin organisations such as massive multiplayer games, loyalty programs, and online communities.",
      "Using pegged sidechains to manage in-game assets definitely sounds like an interesting area to me.",
      "The actual pegged sidechain mechanism is described in detail in the paper, which I encourage you to read (as always!).",
      "At the core is the idea of a DMMS:  We observe that Bitcoin\u2019s blockheaders can be regarded as an example of a dynamic-membership multi-party signature (or DMMS), which we consider to be of independent interest as a new type of group signature.",
      "Unless you believe that Bitcoin as we know it today is the be-all and end-all of crypto-currencies and trustless distributed systems, then a mechanism that allows innovation to thrive on top without being held back by the necessary slow rate of change of Bitcoin itself is an important contribution.",
      "Related reading from previous editions of #themorningpaper:  Something a little different for today: \"Bitcoin: A peer to peer electronic cash system,\" Nakamoto.",
      "[url]"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://www.blockstream.com/sidechains.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 87460557
  },
  {
    "blog_id": "ask-me-anything-dynamic-memory-networks-for-natural-language-processing",
    "summary": [
      "Dynamic Memory Networks (DMN) is a neural network based general framework that can be used for tasks like sequence tagging, classification, sequence to sequence and question answering requiring transitive reasoning.",
      "The basic idea is that all these tasks can be modelled as question answering task in general and a common architecture could be used for solving them.",
      "Architecture  DMN takes as input a document(sentence, story, article etc) and a question which is to be answered given the document.",
      "Input Module  Concatenate all the sentences (or facts) in the document and encode them by feeding the word embeddings of the text to a GRU.",
      "Each time a sentence ends, extract the hidden representation of the GRU till that point and use as the encoded representation of the sentence.",
      "Question Module  Similarly, feed the question to a GRU to obtain its representation.",
      "Episodic Memory Module  Episodic memory consists of an attention mechanism and a recurrent network with which it updates its memory.",
      "During each iteration, the network generates an episode e by attending over the representation of the sentences, question and the previous memory.",
      "The episodic memory is updated using the current episode and the previous memory.",
      "Depending on the amount of supervision available, the network may perform multiple passes.",
      "eg, in the bAbI dataset, some tasks specify how many passes would be needed and which sentence should be attended to in each pass.",
      "For others, a fixed number of passes are made.",
      "Multiple passes allow the network to perform transitive inference.",
      "Attention Mechanism  Given the input representation c, memory m and question q, produce a scalar score using a 2-layer feedforward network, to use as attention mechanism.",
      "A separate GRU encodes the input representation and weights it by the attention.",
      "Final state of the GRU is fed to the answer module.",
      "Answer Module  Use a GRU (initialized with the final state of the episodic module) and at each timestep, feed it the question vector, last hidden state of the same GRU and the previously predicted output.",
      "Training  There are two possible losses:  Cross-entropy loss of the predicted answer (all datasets)  Cross-entropy loss of the attention supervision (for datasets like bAbI)  Experiments  Question Answering  bAbI Dataset  For most tasks, DMN either outperforms or performs as good as Memory Networks.",
      "For tasks like answering with 2 or 3 supporting facts, DMN lags because of limitation of RNN in modelling long sentences.",
      "Text Classification  Stanford Sentiment Treebank Dataset  DMN outperforms all the baselines for both binary and fine-grained sentiment analysis.",
      "Sequence Tagging  Wall Street Journal Dataset  DMN archives state of the art accuracy of 97.56%  Observations  Multiple passes help in reasoning tasks but not so much for sentiment/POS tags.",
      "Attention in the case of 2-iteration DMN is more focused than attention in 1-iteration DMN.",
      "For 2-iteration DMN, attention in the second iteration focuses only on relevant words and less attention is paid to words that lose their relevance in the context of the entire document.",
      "Notes  It would be interesting to put some mechanism in place to determine the number of episodes that should be generated before an answer is predicted.",
      "A naive way would be to predict the answer after each episode and check if the softmax score of the predicted answer is more than a threshold.",
      "Alternatively, the softmax score and other information could be fed to a Reinforcement Learning (RL) agent which decided if the document should be read again.",
      "So every time an episode is generated, the state is passed to the RL agent which decides if another iteration should be performed.",
      "If it decides to predict the answer and correct answer is generated, the agent gets a large +ve reward else a large -ve reward.",
      "To discourage unnecessary iterations, a small -ve reward could be given everytime the agent decides to perform another iteration."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1506.07285",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 91637218
  },
  {
    "blog_id": "rich_feature_hierarchies_for_accurate_object_detection_and_semantic_segmentation",
    "summary": [
      "What  Previously, methods to detect bounding boxes in images were often based on the combination of manual feature extraction with SVMs.",
      "They replace the manual feature extraction with a CNN, leading to significantly higher accuracy.",
      "They use supervised pre-training on auxiliary datasets to deal with the small amount of labeled data (instead of the sometimes used unsupervised pre-training).",
      "They call their method R-CNN (\"Regions with CNN features\").",
      "How  Their system has three modules: 1) Region proposal generation, 2) CNN-based feature extraction per region proposal, 3) classification.",
      "Region proposals generation  A region proposal is a bounding box candidate that might contain an object.",
      "By default they generate 2000 region proposals per image.",
      "They suggest \"simple\" (i.e. not learned) algorithms for this step (e.g.",
      "objectneess, selective search, CPMC).",
      "They use selective search (makes it comparable to previous systems).",
      "CNN features  Uses a CNN to extract features, applied to each region proposal (replaces the previously used manual feature extraction).",
      "So each region proposal ist turned into a fixed length vector.",
      "They use AlexNet by Krizhevsky et al. as their base CNN (takes 227x227 RGB images, converts them into 4096-dimensional vectors).",
      "They add p=16 pixels to each side of every region proposal, extract the pixels and then simply resize them to 227x227 (ignoring aspect ratio, so images might end up distorted).",
      "They generate one 4096d vector per image, which is less than what some previous manual feature extraction methods used.",
      "That enables faster classification, less memory usage and thus more possible classes.",
      "Classification  A classifier that receives the extracted feature vectors (one per region proposal) and classifies them into a predefined set of available classes (e.g. \"person\", \"car\", \"bike\", \"background / no object\").",
      "They use one SVM per available class.",
      "The regions that were not classified as background might overlap (multiple bounding boxes on the same object).",
      "They use greedy non-maximum suppresion to fix that problem (for each class individually).",
      "That method simply rejects regions if they overlap strongly with another region that has higher score.",
      "Overlap is determined via Intersection of Union (IoU).",
      "Training method  Pre-Training of CNN  They use AlexNet pretrained on Imagenet (1000 classes).",
      "They replace the last fully connected layer with a randomly initialized one that leads to C+1 classes (C object classes, +1 for background).",
      "Fine-Tuning of CNN  The use SGD with learning rate 0.001.",
      "Batch size is 128 (32 positive windows, 96 background windows).",
      "A region proposal is considered positive, if its IoU with any ground-truth bounding box is >=0.5.",
      "SVM  They train one SVM per class via hard negative mining.",
      "For positive examples they use here an IoU threshold of >=0.3, which performed better than 0.5.",
      "Results  Pascal VOC 2010  They: 53.7% mAP  Closest competitor (SegDPM): 40.4% mAP  Closest competitor that uses the same region proposal method (UVA): 35.1% mAP  ILSVRC2013 detection  They: 31.4% mAP  Closest competitor (OverFeat): 24.3% mAP  The feed a large number of region proposals through the network and log for each filter in the last conv-layer which images activated it the most:  Usefulness of layers:  They remove later layers of the network and retrain in order to find out which layers are the most useful ones.",
      "Their result is that both fully connected layers of AlexNet seemed to be very domain-specific and profit most from fine-tuning.",
      "Using VGG16:  Using VGG16 instead of AlexNet increased mAP from 58.5% to 66.0% on Pascal VOC 2007.",
      "Computation time was 7 times higher.",
      "They train a linear regression model that improves the bounding box dimensions based on the extracted features of the last pooling layer.",
      "That improved their mAP by 3-4 percentage points.",
      "The region proposals generated by selective search have a recall of 98% on Pascal VOC and 91.6% on ILSVRC2013 (measured by IoU of >=0.5)."
    ],
    "author_id": "ALEJU",
    "pdf_url": "https://arxiv.org/pdf/1311.2524",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 75865335
  },
  {
    "blog_id": "stackgan",
    "summary": [
      "What  They propose a two-stage GAN architecture that generates 256x256 images of (relatively) high quality.",
      "The model gets text as an additional input and the images match the text.",
      "How  Most of the architecture is the same as in any GAN:  Generator G generates images.",
      "Discriminator D discriminates betweens fake and real images.",
      "G gets a noise variable z, so that it doesn't always do the same thing.",
      "Two-staged image generation:  Instead of one step, as in most GANs, they use two steps, each consisting of a G and D.  The first generator creates 64x64 images via upsampling.",
      "The first discriminator judges these images via downsampling convolutions.",
      "The second generator takes the image from the first generator, downsamples it via convolutions, then applies some residual convolutions and then re-upsamples it to 256x256.",
      "The second discriminator is comparable to the first one (downsampling convolutions).",
      "Note that the second generator does not get an additional noise term z, only the first one gets it.",
      "For upsampling, they use 3x3 convolutions with ReLUs, BN and nearest neighbour upsampling.",
      "For downsampling, they use 4x4 convolutions with stride 2, Leaky ReLUs and BN (the first convolution doesn't seem to use BN).",
      "Text embedding:  The generated images are supposed to match input texts.",
      "These input texts are embedded to vectors.",
      "These vectors are added as:  An additional input to the first generator.",
      "An additional input to the second generator (concatenated after the downsampling and before the residual convolutions).",
      "An additional input to the first discriminator (concatenated after the downsampling).",
      "An additional input to the second discriminator (concatenated after the downsampling).",
      "In case the text embeddings need to be matrices, the values are simply reshaped to (N, 1, 1) and then repeated to (N, H, W).",
      "The texts are converted to embeddings via a network at the start of the model.",
      "Input to that vector: Unclear.",
      "(Concatenated word vectors?",
      "Seems to not be described in the text.)",
      "The input is transformed to a vector via a fully connected layer (the text model is apparently not recurrent).",
      "The vector is transformed via fully connected layers to a mean vector and a sigma vector.",
      "These are then interpreted as normal distributions, from which the final output vector is sampled.",
      "This uses the reparameterization trick, similar to the method in VAEs.",
      "Just like in VAEs, a KL-divergence term is added to the loss, which prevents each single normal distribution from deviating too far from the unit normal distribution N(0,1).",
      "The authors argue, that using the VAE-like formulation -- instead of directly predicting an output vector (via FC layers) -- compensated for the lack of labels (smoother manifold).",
      "Note: This way of generating text embeddings seems very simple.",
      "(No recurrence, only about two layers.)",
      "It probably won't do much more than just roughly checking for the existence of specific words and word combinations (e.g. \"red head\").",
      "Visualization of the architecture:  Results  Note: No example images of the two-stage architecture for LSUN bedrooms.",
      "Using only the first stage of the architecture (first G and D) reduces the Inception score significantly.",
      "Adding the text to both the first and second generator improves the Inception score slightly.",
      "Adding the VAE-like text embedding generation (as opposed to only FC layers) improves the Inception score slightly.",
      "Generating images at higher resolution (256x256 instead of 128x128) improves the Inception score significantly  Note: The 256x256 architecture has more residual convolutions than the 128x128 one.",
      "Note: The 128x128 and the 256x256 are both upscaled to 299x299 images before computing the Inception score.",
      "That should make the 128x128 images quite blurry and hence of low quality.",
      "Example images, with text and stage 1/2 results:  More examples of birds:  Examples of failures:  The authors argue, that most failure cases happen when stage 1 messes up."
    ],
    "author_id": "ALEJU",
    "pdf_url": "https://arxiv.org/pdf/1612.03242",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 7255717
  },
  {
    "blog_id": "johanssonss16",
    "summary": [
      "This paper presents a method to train a neural network to make predictions for *counterfactual* questions.",
      "In short, such questions are questions about what the result of an intervention would have been, had a different choice for the intervention been made (e.g. *Would this patient have lower blood sugar had she received a different medication?*).",
      "One approach to tackle this problem is to collect data of the form $(x_i, t_i, y_i^F)$ where $x_i$ describes a situation (e.g. a patient), $t_i$ describes the intervention made (in this paper $t_i$ is binary, e.g.",
      "$t_i = 1$ if a new treatment is used while $t_i = 0$ would correspond to using the current treatment) and $y_i^F$ is the factual outcome of the intervention $t_i$ for $x_i$.",
      "From this training data, a predictor $h(x,t)$ taking the pair $(x_i, t_i)$ as input and outputting a prediction for $y_i^F$ could be trained.",
      "From this predictor, one could imagine answering counterfactual questions by feeding $(x_i, 1-t_i)$ (i.e. a description of the same situation $x_i$ but with the opposite intervention $1-t_i$) to our predictor and comparing the prediction $h(x_i, 1-t_i)$ with $y_i^F$.",
      "This would give us an estimate of the change in the outcome, had a different intervention been made, thus providing an answer to our counterfactual question.",
      "The authors point out that this scenario is related to that of domain adaptation (more specifically to the special case of covariate shift) in which the input training distribution (here represented by inputs $(x_i,t_i)$) is different from the distribution of inputs that will be fed at test time to our predictor (corresponding to the inputs $(x_i, 1-t_i)$).",
      "If the choice of intervention $t_i$ is evenly spread and chosen independently from $x_i$, the distributions become the same.",
      "However, in observational studies, the choice of $t_i$ for some given $x_i$ is often not independent of $x_i$ and made according to some unknown policy.",
      "This is the situation of interest in this paper.",
      "Thus, the authors propose an approach inspired by the domain adaptation literature.",
      "Specifically, they propose to have the predictor $h(x,t)$ learn a representation of $x$ that is indiscriminate of the intervention $t$ (see Figure 2 for the proposed neural network architecture).",
      "Indeed, this is a notion that is [well established][1] in the domain adaptation literature and has been exploited previously using regularization terms based on [adversarial learning][2] and [maximum mean discrepancy][3].",
      "In this paper, the authors used instead a regularization (noted in the paper as $disc(\\Phi_{t=0},\\Phi_ {t=1})$) based on the so-called discrepancy distance of [Mansour et al. ][4], adapting its use to the case of a neural network.",
      "As an example, imagine that in our dataset, a new treatment ($t=1$) was much more frequently used than not ($t=0$) for men.",
      "Thus, for men, relatively insufficient evidence for counterfactual inference is expected to be found in our training dataset.",
      "Intuitively, we would thus want our predictor to not rely as much on that \"feature\" of patients when inferring the impact of the treatment.",
      "In addition to this term, the authors also propose incorporating an additional regularizer where the prediction $h(x_i,1-t_i)$ on counterfactual inputs is pushed to be as close as possible to the target $y_{j}^F$ of the observation $x_j$ that is closest to $x_i$ **and** actually had the counterfactual intervention $t_j = 1-t_i$.",
      "The paper first shows a bound relating the counterfactual generalization error to the discrepancy distance.",
      "Moreover, experiments simulating counterfactual inference tasks are presented, in which performance is measured by comparing the predicted treatment effects (as estimated by the difference between the observed effect $y_i^F$ for the observed treatment and the predicted effect $h(x_i, 1-t_i)$ for the opposite treatment) with the real effect (known here because the data is simulated).",
      "The paper shows that the proposed approach using neural networks outperforms several baselines on this task.",
      "**My two cents**  The connection with domain adaptation presented here is really clever and enlightening.",
      "This sounds like a very compelling approach to counterfactual inference, which can exploit a lot of previous work on domain adaptation.",
      "The paper mentions that selecting the hyper-parameters (such as the regularization terms weights) in this scenario is not a trivial task.",
      "Indeed, measuring performance here requires knowing the true difference in intervention outcomes, which in practice usually cannot be known (e.g. two treatments usually cannot be given to the same patient once).",
      "In the paper, they somewhat \"cheat\" by using the ground truth difference in outcomes to measure out-of-sample performance, which the authors admit is unrealistic.",
      "Thus, an interesting avenue for future work would be to design practical hyper-parameter selection procedures for this scenario.",
      "I wonder whether the *reverse cross-validation* approach we used in our work on our adversarial approach to domain adaptation (see [Section 5.1.2][5]) could successfully be used here.",
      "Finally, I command the authors for presenting such a nicely written description of counterfactual inference problem setup in general, I really enjoyed it!",
      "[1]:  [url]"
    ],
    "author_id": "hlarochelle",
    "pdf_url": "http://arxiv.org/pdf/1605.03661",
    "author_full_name": "Hugo Larochelle",
    "source_website": "https://www.shortscience.org/user?name=hlarochelle",
    "id": 24753674
  },
  {
    "blog_id": "audio-visual_scene_analysis_with_self-supervised_multisensory_features",
    "summary": [
      "What  They propose a method to pretrain jointly on video and corresponding audio data without labels and then use the results to:  perform sound source logalicization (estimate what in an image causes the current sound),  perform audio-visual action recognition,  perform audio-separation (e.g. remove speech from person not visible in an image, remove speech from a specific visible person).",
      "How  Basic method  They train their network to differentiate between real frames (4.2 seconds) and their corresponding audio data on the one hand and real frames but misaligned audio data on the other hand.",
      "Misaligned here means that the audio is shifted by a few seconds (by 2.0 to 5.8 seconds to be precise).",
      "To solve this task, the network has to learn to understand the scene in order to spot small misalignments.",
      "The network can learn this task with self-supervision, i.e. without annotations.",
      "Architecture  Their network starts with two branches.",
      "One for frames from videos.",
      "One for audio data.",
      "Both branches are then merged and finally end in a fully convolutional layer.",
      "Visualization:  Sound Source Localization  They train their model as described.",
      "The features maps of the final convolutional layer then contain information about the spatial location of the sound source.",
      "They convert these feature maps to a heatmap visualization by first weighting each map (based on its weight to produce the final binary output), then summing the maps and lastly normalizing them to e.g. 0 to 1.0.",
      "Action Recognition  They first pretrain using their described method, then fine-tune on the UCF-101 dataset (using 2.56 second videos with augmentation and small audio-shifts down to one frame).",
      "On/off-screen audio-visual source separation  Their goal is to separate sound from (a) sources visible in the video and (b) sources not visible in the video.",
      "Sources in (b) can be outside of the camera view, but can also be sources intentionally hidden, e.g. by setting the image area of a known source in all frames to zero.",
      "They focus here in speech by people (i.e. the model has to learn to separate between speeches by people visible in the video and those not visible).",
      "They first pretrain a model as described above.",
      "Then they create a new model:  It has one subnetwork for video+audio processing (can reuse the weights of the pretrained network).",
      "It has a second subnetwork to process the audio spectrogram in a U-Net form.",
      "The features of the first subnetwork get merged into the second subnetwork at various resolutions.",
      "The output are two spectrograms: one for the on-screen audio, one for the off-screen audio.",
      "Visualization:  They train this by pairing video sequences from VoxCeleb with their audio (on-screen) and other audio from a random clip (off-screen).",
      "They use a loss with two L1-based regression losses:  where x_F is the on-screen (foreground) audio, f_F(.)",
      "its prediction and analogously x_B and f_B(.)",
      "the background (off-screen) audio and its prediction.",
      "One can make this loss permutation invariant by using min(L(x_M, I), L(I, x_M)).",
      "Results  Sound Source Localization  They train on 750k videos from the AudioSet dataset.",
      "They reached 59.9% accuracy during pretraining (i.e. when discriminating between non-shifted and shifted audio).",
      "Humans reached around 66.6% (under easier test conditions, e.g. stronger audio shifts).",
      "Qualitative results, showing via CAM method the test examples with strongest activation maps:  Note that AudioSet did not only include human speech.",
      "The model just seemed to react strongly to that.",
      "Same as before, but test examples with the weakest activation maps:  Strongest activation maps, separated for three example classes (trained on Kinetics):  Action Recognition  They used the UCF-101 dataset, as mentioned above.",
      "They achieve significantly higher scores than other self-supervised methods.",
      "They are still quite a bit behind the best method (82.1% vs 94.5%), though that one is a supervised method and was pretrained on two large datasets (ImageNet + Kinetics).",
      "They test here pairing videos with audio from random other videos (instead of shifted audio from the same video).",
      "This reduced performance from 84.2% to 78.7%, likely because it is a too easy task that often does not require precise understanding of e.g. the motions in the video.",
      "Setting the audio branch's activation after pretraining to zero and then training on UCF-101 resulted in an accuracy drop of 5 percentage points.",
      "Using no self-supervised pretraining resulted in an accuracy drop of 14 percentage points.",
      "Using spectrograms as audio input performed comparable to raw wave inputs.",
      "On/off-screen audio-visual source separation  They trained on the VoxCeleb dataset, as described above.",
      "They made sure that the speaker identities were disjoint between train and test sets.",
      "They sampled 2.1s clips from each 5s video.",
      "Results are shown in the video (see at top of this file).",
      "Training the model only on a single frame instead of the full video worsened the loss from 11.4 to 14.8.",
      "This drop was especially pronounced in videos with two persons of the same gender, where lip motion is an important clue.",
      "Using the state-of-the-art I3D net pretrained on Kinect as audio-visual feature inputs (instead of their pretrained net) worsened the loss from 11.4 to 12.3.",
      "They suspect that this is because action recognition can work decently with just single-frame analysis, while their pretraining (and this source seperation task) require in-depth motion analysis."
    ],
    "author_id": "ALEJU",
    "pdf_url": "http://openaccess.thecvf.com/content_ECCV_2018/papers/Andrew_Owens_Audio-Visual_Scene_Analysis_ECCV_2018_paper.pdf",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 597051
  },
  {
    "blog_id": "adapters-a-compact-and-extensible-transfer-learning-method-for-nlp-6d18c2399f62",
    "summary": [
      "Parameter inefficiency, in the context of transfer learning for NLP, arises when an entirely new model needs to be trained for every downstream task and the number of parameters grows too large.",
      "A recent paper proposes adapter modules which provide parameter efficiency by only adding a few trainable parameters per task, and as new tasks are added previous ones don\u2019t require revisiting.",
      "The main idea of this paper is to enable transfer learning for NLP on an incoming stream of tasks without training a new model for every new task.",
      "A standard fine-tuning model copies weights from a pre-trained network and tunes them on a downstream task which requires a new set of weights for each task.",
      "In other words, the parameters are adjusted together with new layers for each task.",
      "Fine tuning is advantageous in that it could be more parameter efficient if lower layers of the network are shared between tasks.",
      "Adapters  The proposed adapter model adds new modules between layers of a pre-trained network called adapters.",
      "This means that parameters are copied over from pre-training (meaning they remain fixed) and only a few additional task-specific parameters are added for each new task, all without affecting previous ones.",
      "The innovation here is in the strategy that is used to design the adapter module to achieve parameter efficiency with one single model while not compromising performance.",
      "In fact, a simple model was compared with a fully fine-tuned BERT model on several text classification tasks.",
      "The findings show that only 3% of task-specific parameters are needed to almost match the results of the 100% task-specific parameters used by the fully fine-tuned model.",
      "The traditional way of fine-tuning involves: 1) adding a new layer to fit the targets specified in the downstream task, and 2) co-training the new layer with the original weights.",
      "In contrast, the adapter tuning strategy injects new layers (randomly initialized) into the original network.",
      "Parameter sharing between tasks is supported since the original network\u2019s parameters are frozen.",
      "Keep in mind that only a small number of parameters are introduced in the proposed adapter-based tuning architecture, with the intention of keeping the original network unaffected and the training stable.",
      "The approach is simply to initialize the adapters to a near-identity function so that it can influence the distribution of activations (which can also be opted out) while training.",
      "The adapter-based tuning is used with the popular Transformers, which are known to achieve state-of-the-art (SoTA) performance in many NLP tasks such as machine translation and text classification problems.",
      "The architecture is shown in the figure below:  As you can see in the left of the figure, the standard Transformer is used with an additional adapter layer, added after each sub-layer and before adding the skip connection back.",
      "The output of the adapter layer is then forwarded to the layer normalization.",
      "The adapters project the original feature size to a smaller dimension and then projects them to the original size thereafter, ensuring that the number of parameters stays substantially small as compared to the original model (procedure shown on the right of the figure).",
      "With the reduction of parameters, there is an obvious trade-off between performance and parameter efficiency which is discussed in the experiments below.",
      "Experiments  The resulting effect of the added adapter layers is that they allow the model to focus on the higher layers of the network, which has generally been found effective in transfer learning.",
      "The adapter-based model uses a training procedure similar to BERT for fair comparison (see more details in the paper).",
      "On the GLUE benchmark, adapters achieve a mean GLUE score of 80.0 (with 1.3 times the number of parameters of the pretrained model), compared to 80.4 achieved by a BERT-LARGE (with 9 times the number of parameters of the pretrained model).",
      "See the detailed results in the table below.",
      "To further validate the performance of the adapters, other text classification tasks are used to test the effect of parameter efficiency (see the experimental setup in the paper).",
      "Overall, for all the tasks, adapters perform similar to full fine-tuning, variable fine-tuning (some layers are frozen), and baseline (which uses hyperparameter search), while substantially reduces the number of parameters used.",
      "See the table below for detailed results:  The figure below shows the parameter/performance trade-off aggregated over all the tasks used in the experiments above.",
      "For the GLUE benchmark in particular (see left of the figure), the fewer parameters are used in standard fine-tuning the lower the performance.",
      "Adapters were observed to yield stable performances even when substantially fewer parameters are used.",
      "See other results in the paper related to the performance/parameter trade-off.",
      "The adapters were also tested on SQuAD, which involves a question answering task.",
      "As shown in the figure below, adapters consistently produce comparable performance (on F1 score) to full fine-tuning, while training with substantially fewer parameters.",
      "An extended ablation study is discussed in the paper in which the authors further investigated the robustness and extensibility of the adapters.",
      "In summary, adapter-based tuning demonstrates comparable performance to full fine-tuning while at the same time maintaining high parameter-efficiency.",
      "Parameter-Efficient Transfer Learning for NLP \u2014(Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, Sylvain Gelly)  Paper  Other suggested readings:  Deep Learning for NLP: A Complete Overview  A Light Introduction to Transfer Learning for NLP  XLNet outperforms BERT on several NLP Tasks  A Light Introduction to Transformer-XL"
    ],
    "author_id": "DAIR-AI",
    "pdf_url": "https://arxiv.org/pdf/1902.00751",
    "author_full_name": "Elvis Saravia",
    "source_website": "https://github.com/dair-ai/nlp_paper_summaries",
    "id": 61580548
  },
  {
    "blog_id": "the-madlib-analytics-library",
    "summary": [
      "The MADlib Analytics Library \u2013 MAD Skills, the SQL \u2013 Hellerstein et al. 2012  The way that we use large databases has evolved from being primarily in support of accounting and financial record-keeping, to primarily in support of predictive analytics over a wide range of potentially noisy data.",
      "Analytics at scale requires the marriage of scalable data platforms with analytic libraries.",
      "The section on related work contains a nice overview of the main approaches taken, together with representative systems in each category.",
      "I\u2019ve sketched out this \u2018family tree\u2019 below.",
      "This was a 2012 paper \u2013 what new systems should we also include for a 2015 update?",
      "The analytics use case looks different to the system-of-record use case:  Data scientists make use of database engines in a very different way than traditional data warehousing professionals.",
      "Rather than carefully designing global schemas and \u201crepelling\u201d data until it is integrated, they load data into private schemas in whatever form is convenient.",
      "Rather than focusing on simple OLAP-style drill-down reports, they implement rich statistical models and algorithms in the database, using extensible SQL as a language for orchestrating data movement between disk, memory, and multiple parallel machines.",
      "In short, for data scientists a DBMS is a scalable analytics runtime \u2014 one that is conveniently compatible with the database systems widely used for transactions and accounting.",
      "From these observations came the MAD acronynm to describe the characteristics of this new platform (Magnetic, Agile, and Deep):  Magnetic indicates that the platform attracts all kinds of data to it, rather than repelling data that doesn\u2019t strictly conform to pre-conceived ideals  Agile to represent the fact that data scientists use iterative, interactive processess for modeling, loading, and iterating on data, and  Deep to reflect the statisticals models and algorithms being used.",
      "MADlib is a library of analytic methods that can be installed and executed within a relational database engine that supports extensible SQL.",
      "It is also an open source project with a website at MADlib.net .",
      "MADlib has continued to see healthy growth and development since the publication of this paper, with the 1.7 release coming out less than a month ago (Dec. 31st 2014).",
      "In a first for The Morning Paper, you can even watch a video of Joe Hellerstein and Chris R\u00e9 explaining the background of the project and research:  One standard methodology in the domain of statistical analytics is called SEMMA , which stands for Sample, Explore, Modify, Model, Assess.",
      "The \u2018EMMA\u2019 portion of this cycle identifies a set of fundamental tasks that an analyst needs to perform, but the first, \u201cS\u201d step makes less and less sense in many settings today\u2026.",
      "Winning requires extracting advantages in the long tail of \u201cspecial interests\u201d.",
      "While the Hadoop system has been evolving, we\u2019ve also seen a resurgence of interest (and acknowledgement) of the important role that SQL has to play.",
      "\u201cFor these cases, it would be helpful to push statistical methods into the DBMS.",
      "And as we will see, massively parallel databases form a surprisingly useful platform for sophisticated analytics.\u201d  Ideally, we would like MADlib methods to be written entirely in a straightforward and portable SQL.",
      "Unfortunately, the portable core of \u201cvanilla\u201d SQL is often not quite enough to express the kinds of algorithms needed for advanced analytics.",
      "You need to be able to intelligently partition large matrices, and to quickly invoke well-tuned linear algebra methods.",
      "MADlib exploits user-defined aggregates (UDAs), user-defined functions (UDFs), and a sparse matrix C library to provide efficient representations on disk and in memory.",
      "The most basic building block in the macro-programming of MADlib is the use of user-defined aggregates (UDAs).",
      "In general, aggregates\u2014and the related window functions\u2014are the natural way  in SQL to implement mathematical functions that take as input the values of an arbitrary number of rows (tuples).",
      "DBMSs typically implement aggregates as data-parallel streaming algorithms.",
      "And there is a large body of recent work on online learning algorithms and model-averaging techniques that fit the computational model of aggregates well.",
      "Many statistical methods are iterative \u2013 i.e. they make many passes over a data set.",
      "Several SQL workarounds are described (virtual tables, window aggregates, recursive queries) but none quite met the needs of MADlib.",
      "So a driver UDF was written in Python to control iteration in such a way that all large data movement is done within the database engine and its buffer pool.",
      "What does all this look like to the end user?",
      "Here\u2019s a linear regression example:  psql# SELECT (linregr(y, x)).",
      "* FROM data; -[ RECORD 1     ]+--------------------------------------------  coef         | {1.7307,2.2428} r2           | 0.9475 std_err      | {0.3258,0.0533} t_stats      | {5.3127,42.0640} p_values     | {6.7681e-07,4.4409e-16} condition_no | 169.5093  The paper contains examples showing how linear regression, logistic regression, and k-Means are implemented in MADlib, as well as a discussion of contributions for convex optimisation and statistical text analytics.",
      "The authors conclude:  Scalable analytics are a clear priority for the research and industrial communities.",
      "MADlib was designed to fill a vacuum for scalable analytics in SQL DBMSs, and connect database research to market needs.",
      "In our experience, a parallel DBMS provides a very efficient and flexible dataflow substrate for implementing statistical and analytic methods at scale.",
      "If you want to see what MADlib can do now three years on, take a look at the list of modules in the MADlib documentation.",
      "You can even install MADlib in PostgreSQL and have a play!"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://www.eecs.berkeley.edu/Pubs/TechRpts/2012/EECS-2012-38.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 91451160
  },
  {
    "blog_id": "a-neural-conversation-model",
    "summary": [
      "A Neural Conversation Model Vinyals & Le, ICML 2015  What happens if you build a bot that is trained on conversational data, and only conversational data: no programmed understanding of the domain at all, just lots and lots of sample conversations\u2026?",
      "Building on the sequence to sequence technique that we looked at previously, this is exactly what Vinyals & Le set out to find out.",
      "The main advantage of this (sequence-to-sequence) framework is that it requires little feature engineering and domain specificity whilst matching or surpassing state-of-the-art-results [ in tasks such as machine translation ].",
      "This advance, in our opinion, allows researchers to work on tasks for which domain knowledge may not be readily available, or for tasks which are simply too hard to design rules manually.",
      "Conversational modeling can benefit from this formulation because it requires mapping between queries and responses.",
      "Due to the complexity of this mapping, conversational modeling has previously been designed to be very narrow in domain, with a major undertaking on feature engineering.",
      "The traditional approach to building bots requires a \u2018rather complicated pipeline\u2019 of many stages.",
      "In contrast, the neural conversational model is a end-to-end approach to the problem which lacks any domain knowledge.",
      "The surprising thing, is just how well it works.",
      "We\u2019ll see some example conversations soon.",
      "Of course the bot has no understanding of what the conversation means.",
      "Or maybe it does have some kind of understanding (just like a word vector seems to capture some understanding of the meaning of a word), but it\u2019s not an understanding we can tap into.",
      "Therefore you can\u2019t use this approach to incorporate external knowledge in replies, or to extract intent and take actions etc..",
      "Even so, a bot trained on an IT helpdesk troubleshooting dataset is able to genuinely help people.",
      "Assuming you\u2019re familiar with how sequence-to-sequence learning works, the whole neural conversational model can be explained in one short paragraph.",
      "During training, the conversation so far is used as the input sequence, and the next response is the target output sequence.",
      "Concretely, suppose we observe a conversation with two turns: the first person utters \u201cABC\u201d, and the second person replies \u201cWXYZ\u201d.",
      "We can use a recurrent neural network and train to map \u201cABC\u201d to \u201cWXYZ\u201d as show in Figure 1 [below].",
      "The hidden state of the model when it receives the end of sequence symbol  can be viewed as the thought vector because it stores the information of the sentence, or thought, \u201cABC\u201d.",
      "When conversing, the conversation so far is fed into the input sequence, and the bot responds with the generated output sequence.",
      "The approach was tested with an IT helpdesk dataset (30M tokens, with typical interactions being 400 words long and turn-taking clearly signaled), and the Open Subtitles dataset of sentences uttered by characters in movies (62M training sentences, 26M validation sentences).",
      "Training with the IT helpdesk dataset was done with a single layer LSTM with 1024 memory cells and the 20K most common words.",
      "Here\u2019s an example conversation with the trained bot:  The OpenSubtitles experiment was performed with a two-layered LSTM with 4096 cells per layer and a 100K word vocabulary.",
      "\u201cOur simple recurrent model does often produce plausible answers.\u201d  We find it encouraging that the model can remember facts, understand contexts, and perform common sense reasoning without the complexity in traditional pipelines.",
      "What surprises us is that the model does so without any explicit knowledge representation component except for the parameters in the word vectors.",
      "Perhaps most practically significant is the fact that the model can generalize to new questions.",
      "In other words, it does not simply look up for an answer by matching the question with the existing database.",
      "In fact, most of the questions presented above, except for the first conversation, do not appear in the training set.",
      "Here\u2019s an example general knowledge Q&A \u2013 I don\u2019t think Watson has anything to worry about just yet!",
      "It\u2019s interesting, but also far from something you\u2019d want to rely on in a real chatbot service at this point in time.",
      "The model only gives simple and short answers to questions (which may be \u2018probable\u2019 answers according to the model, but aren\u2019t always correct from our perspective).",
      "Perhaps a more problematic drawback is that the model does not capture a consistent personality.",
      "Indeed, if we ask not identical but semantically similar questions, the answers can sometimes be inconsistent:  So there you have it:  In this paper, we show that a simple language model based on the seq2seq framework can be used to train a conversational engine.",
      "Our modest results show that it can generate simple and basic conversations, and extract knowledge from a noisy but open-domain dataset.",
      "Even though the model has obvious limitations, it is surprising to us that a purely data driven approach without any rules can produce rather proper answers to many types of questions.",
      "However, the model may require substantial modifications to be able to deliver realistic conversations.",
      "My personal takeaway is that while it\u2019s amazing how much can be done just with conversational data to learn from, any real service is going to need to some more complex logic wrapped around it.",
      "One possibility the authors hint at is that the neural conversational model could in principle be combined with other systems to re-score a short-list of candidate responses generated by other means."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://arxiv.org/pdf/1506.05869.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 59277766
  },
  {
    "blog_id": "ai2-training-a-big-data-machine-to-defend",
    "summary": [
      "AI2: Training a big data machine to defend Veeramachaneni et al. IEEE International conference on Big Data Security, 2016  Will machines take over?",
      "The lesson of today\u2019s paper is that we\u2019re better off together.",
      "Combining AI with HI (human intelligence, I felt like we deserved an acronym of our own ;) ) yields much better results than a system that uses only unsupervised learning.",
      "The context is information security, scanning millions of log entries per day to detect suspicious activity and prevent attacks.",
      "Examples of attacks include account takeovers, new account fraud (opening a new account using stolen credit card information), and terms of service abuse (e.g. abusing promotional codes, or manipulating cookies for advantage).",
      "A typical attack has a behavioral signature, which comprises the series of steps involved in commiting it.",
      "The information necessary to quantify these signatures is buried deep in the raw data, and is often delivered as logs.",
      "The usual problem with such outlier/anomaly detection systems is that they trigger lots of false positive alarms, that take substantial time and effort to investigate.",
      "After the system has \u2018cried wolf\u2019 enough times they can become distrusted and of limited use.",
      "AI2 combines the experience and intuition of analysts with machine learning techniques.",
      "An ensemble of unsupervised learning models generates a set of k events to be analysed per day (where the daily budget k of events that can be analysed is a configurable parameter).",
      "The human judgements on these k events are used to train a supervised model, the results of which are combined with the unsupervised ensemble results to refine the k events to be presented to the analyst on the next day.",
      "And so it goes on.",
      "The end result looks a bit like this:  With a daily investigation budget (k) of 200 events, AI2 detects 86.8% of attacks with a false positive rate of 4.4%.",
      "Using only unsupervised learning, on 7.9% of attacks are detected.",
      "If the investigation budget is upped to 1000 events/day, unsupervised learning can detect 73.7% of attacks with a false positive rate of 22%.",
      "At this level, the unsupervised system is generating 5x the false positives of AI2, and still not detecting as many attacks.",
      "Detecting attacks is a true \u2018needle-in-a-haystack\u2019 problem as the following table shows:  Entities in the above refers to the number of unique IP addresses, users, sessions etc.",
      "anaysed on a daily basis.",
      "The very small relative number of true attacks results in extreme class imbalance when trying to learn a supervised model.",
      "AI2 tracks activity based on ingested log records and aggregates activities over intervals of time (for example,counters, indicators \u2013 did this happen in the window at all?",
      "\u2013 elapsed time between events, number of unique values and so on).",
      "These features are passed into an ensemble of three unsupervised outlier detection models:  A Principle Component Analysis (PCA) based model.",
      "The basic idea is to use PCA to determine the most significant features (those that explain most of the variance in the data).",
      "Given an input take its PCA projection, and then from the projection, reconstruct the original variables.",
      "The reconstruction error will be small for the majority of examples, but will remain high for outliers.",
      "A Replicator Neural Network (not to be confused with a Recurrent Neural Network \u2013 both get abbreviated to RNN).",
      "This works on a very similar principal.",
      "The input and output layers have the same number of nodes, and intermediate layers have fewer nodes.",
      "The goal is to train the network to recreate the input at the output layer \u2013 which means it must learn an efficient compressed representation in the lower-dimensional hidden layers.",
      "Once the RNN has been trained, the reconstruction error can be used as the outlier score.",
      "The third unsupervised model uses copula functions to build a joint probability function that can be used to detect rare events.",
      "A copula framework provides a means of inference after modeling a multivariate joint probability distribution from training data.",
      "Because copula frameworks are less well known than other forms of estimation, we will now briefly review copula theory\u2026  (If you\u2019re interested in that review, and how copula functions are used to form a multivariate density function, see section 6.3 in the paper).",
      "The scores from each of the models are translated into probabilities using a Weibull distribution, \u201cwhich is flexible and can model a wide variety of shapes.\u201d This translation means that we can compare like-with-like when combining the results from the three models.",
      "Here\u2019s an example of the combination process using one-day\u2019s worth of data:  The whole AI2 system cycles through training, deployment, and feedback collection/model updating phases on a daily basis.",
      "The system trains unsupervised and supervised models based on all the available data, applies those models to the incoming data, identifies k entities as extreme events or attacks, and brings these to the analyst\u2019s attention.",
      "The analysts deductions are used to build a new predictive model for the next day.",
      "This combined approach makes effective use of the limited available analyst bandwidth, can overcome some of the weaknesses of pure unsupervised learning, and actively adapts and synthesizes new models.",
      "This setup captures the cascading effect of the human-machine interaction: the more attacks the predictive system detects, the more feeback it will receive from the analysts; this feedback, in turn, will improve the accuracy of future predictions."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://people.csail.mit.edu/kalyan/AI2_Paper.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 64175585
  },
  {
    "blog_id": "tail-attacks-on-web-applications",
    "summary": [
      "Tail attacks on web applications Shan et al., CCS\u201917  This paper introduces a stealthy DDoS attack on classic n-tier web applications.",
      "It is designed to push the tail latency high while simultaneously being very hard to detect using traditional monitoring tools.",
      "The attack exploits \u2018millibottlenecks\u2019 \u2014 caused by buffers in the system that fill up when subjected to a short burst of traffic \u2014 by sending timed pulses of attack traffic.",
      "Tail attacks aim to create very short (hundreds of milliseconds) resource contention (e.g., CPU or disk I/O) with dependencies among distributed nodes, while giving an \u201cunsaturated illusion\u201d for state-of-the-art IDS/IPS tools leading to a higher level of stealthiness.",
      "The attacker sends short \u201cON\u201d bursts of attack traffic, where the duration of a burst is typically on the order of milliseconds.",
      "These are followed longer \u201cOFF\u201d periods in which the target system can cool down, clearing up the queued requests and returning to a lower utilisation state.",
      "When an attack pulse is sent, resource millibottlenecks occur in one of the tiers (assuming the pulse parameters are set accordingly).",
      "A millibottleneck stops the saturated tier processing for a short time (order of milliseconds), leading to the filling up of the message queues and thread pools in the bottleneck tier, and quickly propagating the queue overflow to all the upstream tiers of the n-tier system.",
      "Once things are backed up all the way to the front-end, incoming packets for new requests will be dropped, and end-users may see very long response times (on the order of seconds).",
      "Testing across EC2, Azure, and an academic cloud platform \u201cNSF Cloudlab\u201d, tail attacks show a 4x-8x increase in 95th percentile latencies when testing a variety of configurations of the RUBBoS n-tier web application benchmark.",
      "Tail attack analysis  Tail attacks can be modelled using queueing networks.",
      "The following table shows the parameters in the model:  With reference to the parameters above, the attack volume during a burst is given by:  And the period of damage during which requests will be dropped is given by:  End-users with dropped requests perceive a long response time, which can be approximated by:  An attacker needs to choose attack parameters  , and in theory the optimal parameters can be found by solving a nonlinear optimisation problem, or explored via simulation using e.g., the Java Modelling Tools (JMT) open source modelling toolkit for queueing networks.",
      "The authors demonstrate both of these approaches in sections 3.2 and 3.3 respectively.",
      "However, they both fail to address the dynamics of real systems.",
      "For that, we need to introduce a feedback control loop.",
      "Maintaining optimal attack parameters using feedback control  Since baseline workloads vary (and so might system capacity), we need to dynamically adjust attack parameters to keep them in the sweet spot.",
      "Too little and the system will shrug the attacks off.",
      "Too much and detection mechanisms may be triggered.",
      "Enter the feedback controller:  The controller is a Kalman filter which executes recursively for each new observation.",
      "First it creates an a priori estimate of the system state and error matrix, and then these are refined using the current measurement.",
      "Using the Kalman filter, the Controller can predict the required attack parameters at the k-th burst given the historical results of all k-1 bursts, dynamically command the new parameters to the bots, and automatically and effectively launch Tail Attacks.",
      "The observations needed by the filter are produced by the estimator.",
      "A \u201cProber\u201d monitors attacks and is used to infer  (the damage period).",
      "It sends lightweight requests with short expect services times and uses the request-response intervals to monitor the impact of attacks without causing too much load during the \u201cOFF\u201d periods.",
      "The millibottleneck length  is estimated by the attack bots themselves.",
      "These send a burst of heavyweight requests designed to make it through all the tiers of the application, and estimate the bottleneck length by measuring request-response intervals.",
      "Tail attacks in action  The RUBBoS n-tier web application benchmark modelled on Slashdot is deployed in a variety of configurations across three cloud services.",
      "In the table below, the four digit (or three digit) notation #W#A#L#D represents the number of web servers, app servers, load balancers (may not be present) and database servers.",
      "Columns 2 to 5 (in the table above) show the corresponding model parameters in our real cloud production setting experiments controlled by our attack framework.",
      "It clearly shows that our attacks controlled by our algorithm can achieve the predefined targets (5% drop ratio, damage length less than 100ms, millibottleneck length less than 500ms).",
      "Here\u2019s the same story plotted as a sequence of charts:  Detection and defence  Detecting and defending against Tail Attacks requires a three-pronged strategy:  Fine-grained monitoring, with granularity less than the millibottlenecks period.",
      "Burst detection looking for potential attack bursts where requests are dropped, cross-tier queues are overflown, millibottlenecks (on e.g. CPU or I/O) occur, and there is a burst of requests.",
      "IP-based statistical analysis to try and separate the bots from legitimate users.",
      "For case 3, we can define a suspicion index for each IP address as:  Where  and  are the number of attacks for each IP during \u201cON\u201d bursts and the attack interval T (including both \u201cON\u201d and \u201cOFF\u201d periods) respectively.",
      "Plotting this for the RUBBoS experiment for example shows a clear separation between normal users and attackers.",
      "Welcome to the emerging world of low-volume application DDoS attacks!"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://iisp.gatech.edu/sites/default/files/images/tail_attack_-_calton_pu.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 6827132
  },
  {
    "blog_id": "the-network-is-reliable",
    "summary": [
      "The Network is Reliable \u2013 Bailis and Kingsbury 2014  This must be the easiest paper summary to write of the series so far.",
      "The network is reliable?",
      "Oh no it isn\u2019t\u2026  OK, here\u2019s a little more detail :)  Network reliability matters because it prevents us from having reliable communication, and that in turn makes building distributed systems really hard.",
      "(Fallacy #1 in Peter Deutsch\u2019s \u2018 Eight fallacies of distributed computing \u2018 is \u2018The network is reliable\u2019).",
      "In fact, if we look at this list for a moment we can see that the top three fallacies all correspond to interesting failure modes in networks as well as being something you have to take into account in the happy paths.",
      "Sudden large latency spikes for example can be very disruptive to fault detectors, and noisy network neighbours might consume lots of your bandwidth.",
      "The network is reliable.",
      "Latency is zero.",
      "Bandwidth is infinite.",
      "The network is secure.",
      "Topology doesn\u2019t change.",
      "There is one administrator.",
      "Transport cost is zero.",
      "The network is homogeneous.",
      "No.",
      "5 is also something we\u2019ve had to learn to accommodate to a whole new degree with cloud deployments.",
      "Topology changing all the time is the norm.",
      "If we can assume that \u2018the network is reliable enough,\u2019 then it might make sense as an engineering trade-off to become unavailable in the event of a partition.",
      "I\u2019ve heard the argument made that \u2018partitions only happen in the cloud,\u2019 we never really see them in practice in our own data centers.",
      "Conscious trade-off or not, the work of one of the authors of this paper, Kyle Kingsbury, on the wonderful Jepsen Reports shows that many real-world systems struggle with partitions.",
      "The degree of reliability in deployment environments is critical in robust systems design and directly determines the kinds of operations that systems can reliably perform without waiting.",
      "Unfortunately, the degree to which networks are actually reliable in the real world is the subject of considerable and evolving debate.",
      "Some people have claimed that networks are reliable (or that partitions are rare enough in practice) and that we are too concerned with designing for theoretical failure modes.",
      "Conversely, others attest that partitions do occur in their deployments\u2026  We have some pretty good statistics on disk, host, and rack failure rates, but not so much on network failures.",
      "Yet network failures can be much more disruptive than a disk failure for example.",
      "As a result, much of what we believe about the failure modes of real-world distributed systems is founded on guesswork and rumor.",
      "Sysadmins and developers will swap stories over beer, but detailed, public postmortems and comprehensive surveys of network availability are few and far between.",
      "In this article, we\u2019d like to informally bring a few of these stories (which, in most cases, are unabashedly anecdotal) together\u2026  There follows a long collection of stories of every kind of network failure you can imagine.",
      "It\u2019s the cumulative effect that gets you as you read through \u2013 start off on page 2 thinking \u2018yeah ok, isolated scenario(s)\u2019, but the stories keep coming and coming and coming.",
      "By the time you get to page 12, you\u2019ve probably come to the conclusion that Murphy must have been working as a network engineer at the time he formulated his famous law!",
      "Split-brains, partitions, device failures, link failures, high rates of packet loss, maintenance and admin issues, and more are all in here \u2013 resulting in comedies of errors in the systems built on top of them that would be fit for a christmas pantomine if the consequences weren\u2019t so severe!",
      "This reads like a skit from a comedy show:  This 90-second network partition caused file servers using Pacemaker and DRBD (Distributed Replicated Block Device) for HA (high availability) failover to declare each other dead, and to issue STONITH (shoot the other node in the head) messages to one another.",
      "The network partition delayed delivery of those messages, causing some file-server pairs to believe they were both active.",
      "When the network recovered, both nodes shot each other at the same time.",
      "With both nodes dead, files belonging to the pair were unavailable.",
      "You really do need to read through the paper to get the overall impression, a summary cannot do it justice.",
      "As a consequence of all this:  Split-brain is not an academic concern: it happens to all kinds of systems\u2014sometimes for days on end.",
      "Partitions deserve serious consideration\u2026 It\u2019s important to consider this risk before a partition occurs, because it\u2019s much easier to make decisions about partition behavior on a whiteboard than to redesign, reengineer, and upgrade a complex system in a production environment\u2014especially when it\u2019s throwing errors at your users.",
      "For some applications, failure is an option\u2014but you should characterize and explicitly account for it as a part of your design.",
      "Finally, given the additional latency and coordination benefits of partition-aware designs, you might just find that accounting for these partitions delivers benefits in the average case as well.",
      "The authors also acknowledge that there might be reliable networks out there:  On the other hand, some networks really are reliable.",
      "Engineers at major financial firms have anecdotally reported that despite putting serious effort into designing systems that gracefully tolerate partitions, their networks rarely, if ever, exhibit partition behavior.",
      "Cautious engineering and aggressive network advances (along with lots of money) can prevent outages.",
      "Moreover, in this article, we have presented failure scenarios; we acknowledge it\u2019s much harder to demonstrate that network failures have not occurred.",
      "But if I was thinking about future architectures and cloud deployments, I wouldn\u2019t want to rely on it ;)."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/2639988.2655736?download=true",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 12642073
  },
  {
    "blog_id": "combining-static-model-checking-with-dynamic-enforcement-using-the-statecall-policy-language",
    "summary": [
      "Combining static model checking with dynamic enforcement using the Statecall Policy Language \u2013 Madhavapeddy 2009  We know that getting distributed systems right is hard, and subtle, \u2018deep\u2019 bugs can lurk in both algorithms and implementations.",
      "Can we do better than informal reasoning coupled with some unit and integration tests?",
      "Evidence suggests we have to do better!",
      "We\u2019ve previously looked at Amazon\u2019s use of TLA+ , and today\u2019s selection is the first of three papers I\u2019ve selected to probe deeper into this issue.",
      "Today\u2019s choice looks at the Statecall Policy Language (SPL) that was used by Howard et al. in Raft refloated to validate their Raft implementation.",
      "In the next two days we\u2019ll also be looking at distributed model checking made practical with SAMC, and pitting our wits against Molly with Peter Alvaro\u2019s \u2018Lineage Driven Fault Injection.\u2019  The Statecall Policy Language (SPL) was designed to make model checking more accessible to regular programmers.",
      "Models are specified in terms of allowable sequences of program events, and the SPL model can then be translated by a compiler into a variety of forms.",
      "These include:  A translation into PROMELA ,  which can then be used with SPIN to check static properties of the model.",
      "A graphical visualization using GraphViz.",
      "Generated code in your target implementation language (OCaml backend is currently implemented, but the design allows for others) that enables run-time validation (i.e. ensures that the real-world behaviour is not deviating from the model).",
      "Also known as a safety monitor.",
      "Debugging stubs to support an HTML and Javascript debugging view giving a real-time window into all the automata embedded in the program.",
      "Writing as of 2009, Madhavapeddy states:  None of the major implementations of protocols such as HTTP (Apache), SMTP (Sendmail/Postfix), or DNS (BIND) are regularly model-checked by their development teams.",
      "All of them regularly suffer from serious security flaws ranging from low-level buffer overflows to subtle high-level protocol errors, some of which could have been caught by using model checking.",
      "(We\u2019ll see more examples of verification techniques finding meaningful real-world bugs over the next two days).",
      "Here\u2019s an SPL model for ping that supports the -c n (only send n packets in total) and -w (wait instead of timing out) flags/behaviours.",
      "01 automaton ping (int max_count, int count, bool can_timeout) { 02   Initialize; 03   during { 04     count = 0; 05     do { 06       Transmit_Ping; 07       either { 08         Receive_Ping; 09       } or (can_timeout) { 10         Timeout_Ping; 11       }; 12       count = count + 1; 13     } until (count &gt;= max_count); 14   } handle { 15     SIGINFO; 16     Print_Summary; 17   }; 18 }  The Statecalls begin with an initial capital letter: Initialize, Transmit_Ping, Receive_Ping, Timeout_Ping, and Print_Summary.",
      "The automaton defines the allowable sequences of statecalls.",
      "Signal handlers are often a source of bugs due to their extremely asynchronous nature \u2014 SPL provides a during/handle construct (used in the example above, see the lines 03 and 14) which models them by permitting a state transition into alternative statement blocks during normal execution of an SPL specification.",
      "Once you are satisfied with the SPL model you can run the SPL compiler.",
      "The generated code for the executable model can be linked with the real ping implementation.",
      "You can \u2018even do this manually\u2019 if you really want to!",
      "This code is linked in with the main ping application, and appropriate calls to initialize the automaton and invoke statecalls are inserted in the code.",
      "Crucially, we do not mandate a single style of invoking statecalls; instead the programmer can choose between automatic mechanisms (e.g. MPL packet parsing code can automatically invoke statecalls when transmitting or receiving packets), language-assisted means (e.g.",
      "functional combinators, object inheritance, or pre-processors such as cpp), or even careful manual insertion in places where other methods are inconvenient.",
      "Underneath the covers, SPL translates models into an intermediate form based on a Control Flow Automata (CFA) graph.",
      "For more complex protocols, it is possible to \u2018divide-and-conquer\u2019 :  It is often more convenient and readable to break down a complex protocol into smaller blocks which express the same protocol but with certain aspects factored out into simpler state machines.",
      "Accordingly, SPL specifications can define multiple automata, but the external interface hides this abstraction and only exposes a single, flat set of statecalls.",
      "Each automaton then executes in parallel, received statecalls are only dispatched to automata which include the statecall in their alphabet.",
      "The debugging support is a nice touch (see screenshot below):  This page contains a real-time graphical view of all the automata embedded in the program, along with the set of valid states they can transition to next.",
      "Since the granularity of the SPL automata are chosen by the programmer, this is much more useful than the \u201craw\u201d models obtained through static code analysis which often include a lot of superfluous information.",
      "Figure 5 shows a screen capture of the SPL AJAX debugger single-stepping through the global SPL automaton for theMelange SSH server.",
      "The mlssh server is blocked waiting for password authentication, having previously attempted to authenticate via null and public-key authentication.",
      "In our experience, the debugger was a valuable tool to debug complex protocol bugs in our implementation, as the single-stepping view via this debugger is significantly higher level than the alternative provided by either the native OCaml debugger or gdb."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://anil.recoil.org/papers/2009-icfem-spl.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 29796623
  },
  {
    "blog_id": "infogan",
    "summary": [
      "What  Usually GANs transform a noise vector z into images.",
      "z might be sampled from a normal or uniform distribution.",
      "The effect of this is, that the components in z are deeply entangled.",
      "Changing single components has hardly any influence on the generated images.",
      "One has to change multiple components to affect the image.",
      "The components end up not being interpretable.",
      "Ideally one would like to have meaningful components, e.g. for human faces one that controls the hair length and a categorical one that controls the eye color.",
      "They suggest a change to GANs based on Mutual Information, which leads to interpretable components.",
      "E.g. for MNIST a component that controls the stroke thickness and a categorical component that controls the digit identity (1, 2, 3, ...).",
      "These components are learned in a (mostly) unsupervised fashion.",
      "How  The latent code c  \"Normal\" GANs parameterize the generator as G(z), i.e. G receives a noise vector and transforms it into an image.",
      "This is changed to G(z, c), i.e. G now receives a noise vector z and a latent code c and transforms both into an image.",
      "c can contain multiple variables following different distributions, e.g. in MNIST a categorical variable for the digit identity and a gaussian one for the stroke thickness.",
      "Mutual Information  If using a latent code via G(z, c), nothing forces the generator to actually use c. It can easily ignore it and just deteriorate to G(z).",
      "To prevent that, they force G to generate images x in a way that c must be recoverable.",
      "So, if you have an image x you must be able to reliable tell which latent code c it has, which means that G must use c in a meaningful way.",
      "This relationship can be expressed with mutual information, i.e. the mutual information between x and c must be high.",
      "The mutual information between two variables X and Y is defined as I(X; Y) = entropy(X) - entropy(X|Y) = entropy(Y) - entropy(Y|X).",
      "If the mutual information between X and Y is high, then knowing Y helps you to decently predict the value of X (and the other way round).",
      "If the mutual information between X and Y is low, then knowing Y doesn't tell you much about the value of X (and the other way round).",
      "The new GAN loss becomes old loss - lambda * I(G(z, c); c), i.e. the higher the mutual information, the lower the result of the loss function.",
      "Variational Mutual Information Maximization  In order to minimize I(G(z, c); c), one has to know the distribution P(c|x) (from image to latent code), which however is unknown.",
      "So instead they create Q(c|x), which is an approximation of P(c|x).",
      "I(G(z, c); c) is then computed using a lower bound maximization, similar to the one in variational autoencoders (called \"Variational Information Maximization\", hence the name \"InfoGAN\").",
      "Basic equation: LowerBoundOfMutualInformation(G, Q) = E[log Q(c|x)] + H(c) <= I(G(z, c); c)  c is the latent code.",
      "x is the generated image.",
      "H(c) is the entropy of the latent codes (constant throughout the optimization).",
      "Optimization w.r.t.",
      "Q is done directly.",
      "Optimization w.r.t.",
      "G is done via the reparameterization trick.",
      "If Q(c|x) approximates P(c|x) perfectly, the lower bound becomes the mutual information (\"the lower bound becomes tight\").",
      "In practice, Q(c|x) is implemented as a neural network.",
      "Both Q and D have to process the generated images, which means that they can share many convolutional layers, significantly reducing the extra cost of training Q.",
      "Results  MNIST  They use for c one categorical variable (10 values) and two continuous ones (uniform between -1 and +1).",
      "InfoGAN learns to associate the categorical one with the digit identity and the continuous ones with rotation and width.",
      "Applying Q(c|x) to an image and then classifying only on the categorical variable (i.e. fully unsupervised) yields 95% accuracy.",
      "Sampling new images with exaggerated continuous variables in the range [-2,+2] yields sound images (i.e. the network generalizes well).",
      "3D face images  InfoGAN learns to represent the faces via pose, elevation, lighting.",
      "They used five uniform variables for c. (So two of them apparently weren't associated with anything sensible?",
      "They are not mentioned.)",
      "3D chair images  InfoGAN learns to represent the chairs via identity (categorical) and rotation or width (apparently they did two experiments).",
      "They used one categorical variable (four values) and one continuous variable (uniform [-1, +1]).",
      "SVHN  InfoGAN learns to represent lighting and to spot the center digit.",
      "They used four categorical variables (10 values each) and two continuous variables (uniform [-1, +1]).",
      "(Again, a few variables were apparently not associated with anything sensible?)",
      "CelebA  InfoGAN learns to represent pose, presence of sunglasses (not perfectly), hair style and emotion (in the sense of \"smiling or not smiling\").",
      "They used 10 categorical variables (10 values each).",
      "(Again, a few variables were apparently not associated with anything sensible?)"
    ],
    "author_id": "ALEJU",
    "pdf_url": "https://arxiv.org/pdf/1606.03657",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 67744436
  },
  {
    "blog_id": "state-of-the-art-multimodal-sentiment-classification-in-videos-1daa8a481c5a",
    "summary": [
      "This paper proposes a novel method for conducting multimodal sentiment classification from user-generated videos.",
      "Multimodal methods comprise of combining various modes of information such as audio, video, and text.",
      "The framework is mainly based on a long short-term memory (LSTM) model that enables utterances (units of speech bound by breathes or pauses) to capture contextual information.",
      "What is Sentiment Analysis?",
      "A sentiment analysis task involves many NLP sub-tasks and most commonly aims to detect polarity (positive/negative sentiment) in text.",
      "Emotion recognition is a derivative task in which the aim is to predict fine-grained emotions (e.g., fear and joy).",
      "Why Multimodal information?",
      "By combining vocal modulations and facial expressions with textual information, it is possible enrich the feature learning process to better understand affective states of opinion holders.",
      "In other words, there could be other behavioral cues in vocal and visual modalities that could be leveraged.",
      "Contributions  The proposed framework considers the order, inter-dependencies, and relations that exist among utterances in a video, where others treat them independently.",
      "In other words, surrounding context should help to better classify the sentiment conveyed by utterances.",
      "In addition, audio, visual, and textual information are combined to tackle both sentiment and emotion recognition tasks.",
      "Example  Consider the following utterance found in a review: \u201cThe Green Hornet did something similar\u201d.",
      "Without any context, we may perceive this utterance as conveying negative sentiment.",
      "What if we included the nearby utterances: \u201cIt engages the audience more\u201d and \u201cI just love it\u201d.",
      "Would the sentiment change to positive?",
      "You be the judge of that!",
      "Note that it is highly subjective but we can train a machine to detect these correlations automatically.",
      "Models  Two main types of feature extraction methods are proposed:  F1: Context-Independent Features (a.k.a unimodal features for each modality)  Textual feature extraction is performed using a convolutional neural network (CNN) where the input is the transcription of each utterance, which is represented by the concatenation of corresponding word2vec word vectors.",
      "(See paper for more details of CNN)  Audio feature extraction is performed using the openSMILE open-source software, where low-level features, such as voice intensity and pitch, are obtained.",
      "(See paper for more details on audio features)  Visual feature extraction is performed using a 3D-CNN, where frame-level features are learned.",
      "(See paper for more details of 3D-CNN)  F2: Contextualized Features  An LSTM-based network is adopted to perform context-dependent feature extraction by modeling relations among utterances.",
      "Basically, unimodal features are fed as input to a LSTM layer that produces contextualized features as shown in diagram below.",
      "Different variants of the LSTM model are experimented with, such as sc-LSTM (unidirectional LSTM cells), h-LSTM (dense layer ignored), bc-LSTM (bidirectional LSTMs), and uni-SVM (unimodal features are used directly with SVM for classification).",
      "Fusing Modalities  There are basically two frameworks for fusing modalities:  Non-hierarchical Framework \u2014 unimodal features are concatenated and fed into the various contextual LSTM networks proposed above (e.g., h-LSTM).",
      "Hierarchical Framework \u2014 The difference here is that we don\u2019t concatenate unimodal features, we feed each unimodal feature into the LSTM network proposed above.",
      "Think of this framework as having some hierarchy.",
      "In the first level, unimodal features are fed individually to LSTM networks.",
      "The output of the first level are then concatenated and fed into another LSTM network (i.e., second level).",
      "(Check diagram below for overview of hierarchy or see paper for all the details)  Datasets  An important consideration in multimodal sentiment analysis is that person-independent datasets must be designed.",
      "In other words, train/test splits are disjoint with respect to speakers.",
      "The following datasets were used for the experiments:  MOSI \u2014 contains video-based topic reviews annotated by sentiment polarity  MOUD \u2014 contains product review videos annotated by sentiment polarity  IEMOCAP \u2014 contains scripted affect-related utterances annotated by emotion categories  Main Findings  Hierarchy vs Non-Hierarchy: From the results in the table above we can observe that hierarchical model significantly outperform the non-hierarchical frameworks (highlighted in green).",
      "LSTM variants: sc-LSTM and bc-LSTM models perform the best out of the LSTM variants, including the uni-SVM model (results highlighted in red).",
      "These results help to show the importance of considering contextual information when classifying utterances.",
      "Modalities: In general, unimodal classifiers trained on textual information perform best as compared to other individual modalities (results highlighted in blue).",
      "The exception was the MOUD dataset, which involved some translation.",
      "However, combining the modalities tend to boost the performance, indicating that multimodal methods are feasible and effective.",
      "Generalizability: To test for generalizability, the models were trained on one dataset (MOSI) and tested on another (MOUD).",
      "Individually, the visual modality caries the more generalized information.",
      "Overall, fusing the modalities improved the model.",
      "(See paper for more qualitative analysis on the importance of contextualized information for multimodal sentiment classification.)",
      "Call for Research  Here are a few ideas you can try to improve the current work:  Currently, this work aims to evaluate methods on benchmark datasets, which are somewhat clean.",
      "You can try to collect your own datasets and label them automatically, rendering large-scale datasets.",
      "Also, keep in mind the domain; i.e., you can try to work on a different type of dataset that doesn\u2019t include reviews.",
      "It would be interesting to see more cases where contextualized information helps with sentiment classification.",
      "Also, a more advanced idea includes the fusion part of the framework.",
      "You can try to experiment with more sophisticated fusion techniques, such as those used here .",
      "Software: openSMILE \u2014 Software for extracting acoustic features from audio  Dataset: MOSI  Paper: Context-Dependent Sentiment Analysis in User-Generated Videos  Presentation: Video Clip  Have any other questions regarding this paper?",
      "Send me a DM @omarsar0 ."
    ],
    "author_id": "DAIR-AI",
    "pdf_url": "https://www.aclweb.org/anthology/P17-1081.pdf",
    "author_full_name": "Elvis Saravia",
    "source_website": "https://github.com/dair-ai/nlp_paper_summaries",
    "id": 49139111
  },
  {
    "blog_id": "improved_techniques_for_training_gans",
    "summary": [
      "What  They suggest some small changes to the GAN training scheme that lead to visually improved results.",
      "They suggest a new scoring method to compare the results of different GAN models with each other.",
      "How  Feature Matching  Usually G would be trained to mislead D as often as possible, i.e. to maximize D's output.",
      "Now they train G to minimize the feature distance between real and fake images.",
      "I.e. they do:  Pick a layer l from D.  Forward real images through D and extract the features from layer l.  Forward fake images through D and extract the features from layer l.  Compute the squared euclidean distance between the layers and backpropagate.",
      "Minibatch discrimination  They allow D to look at multiple images in the same minibatch.",
      "That is, they feed the features (of each image) extracted by an intermediate layer of D through a linear operation, resulting in a matrix per image.",
      "They then compute the L1-distances between these matrices.",
      "They then let D make its judgement (fake/real image) based on the features extracted from the image and these distances.",
      "They add this mechanism so that the diversity of images generated by G increases (which should also prevent collapses).",
      "Historical averaging  They add a penalty term that punishes weights which are rather far away from their historical average values.",
      "I.e. the cost is distance(current parameters, average of parameters over the last t batches).",
      "They argue that this can help the network to find equilibria that normal gradient descent would not find.",
      "One-sided label smoothing  Usually one would use the labels 0 (image is fake) and 1 (image is real).",
      "Using smoother labels (0.1 and 0.9) seems to make networks more resistent to adversarial examples.",
      "* So they smooth the labels of real images (apparently to 0.9?).",
      "Smoothing the labels of fake images would lead to (mathematical) problems in some cases, so they keep these at 0.",
      "Virtual Batch Normalization (VBN)  Usually BN normalizes each example with respect to the other examples in the same batch.",
      "They instead normalize each example with respect to the examples in a reference batch, which was picked once at the start of the training.",
      "VBN is intended to reduce the dependence of each example on the other examples in the batch.",
      "VBN is computationally expensive, because it requires forwarding of two minibatches.",
      "They use VBN for their G.  Inception Scoring  They introduce a new scoring method for GAN results.",
      "Their method is based on feeding the generated images through another network, here they use Inception.",
      "For an image x and predicted classes y (softmax-output of Inception):  They argue that they want p(y|x) to have low entropy, i.e. the model should be rather certain of seeing a class (or few classes) in the image.",
      "They argue that they want p(y) to have high entropy, i.e. the predicted classes (and therefore image contents) should have high diversity.",
      "(This seems like something that is quite a bit dependend on the used dataset?)",
      "They combine both measurements to the final score of exp(KL(p(y|x) || p(y))) = exp( <sum over images> p(y|xi) * (log(p(y|xi)) - log(p(y))) ).",
      "p(y) can be approximated as the mean of the softmax-outputs over many examples.",
      "Relevant python code that they use (where part seems to be of shape (batch size, number of classes), i.e. the softmax outputs): kl = part * (np.log(part) - np.log(np.expand_dims(np.mean(part, 0), 0))); kl = np.mean(np.sum(kl, 1)); scores.append(np.exp(kl));  They average this score over 50,000 generated images.",
      "Semi-supervised Learning  For a dataset with K classes they extend D by K outputs (leading to K+1 outputs total).",
      "They then optimize two loss functions jointly:  Unsupervised loss: The classic GAN loss, i.e. D has to predict the fake/real output correctly.",
      "(The other outputs seem to not influence this loss.)",
      "Supervised loss: D must correctly predict the image's class label, if it happens to be a real image and if it was annotated with a class.",
      "They note that training G with feature matching produces the best results for semi-supervised classification.",
      "They note that training G with minibatch discrimination produces significantly worse results for semi-supervised classification.",
      "(But visually the samples look better.)",
      "They note that using semi-supervised learning overall results in higher image quality than not using it.",
      "They speculate that this has to do with the class labels containing information about image statistics that are important to humans.",
      "Results  MNIST  They use weight normalization and white noise in D.  Samples of high visual quality when using minibatch discrimination with semi-supervised learning.",
      "Very good results in semi-supervised learning when using feature matching.",
      "Using feature matching decreases visual quality of generated images, but improves results of semi-supervised learning.",
      "CIFAR-10  D: 9-layer CNN with dropout, weight normalization.",
      "G: 4-layer CNN with batch normalization (so no VBN?).",
      "Visually very good generated samples when using minibatch discrimination with semi-supervised learning.",
      "(Probably new record quality.)",
      "Note: No comparison with nearest neighbours from the dataset.",
      "When using feature matching the results are visually not as good.",
      "Again, very good results in semi-supervised learning when using feature matching.",
      "SVHN  Same setup as in CIFAR-10 and similar results.",
      "ImageNet  They tried to generate 128x128 images and compared to DCGAN.",
      "They improved from \"total garbage\" to \"garbage\" (they now hit some textures, but structure is still wildly off).",
      "Generated CIFAR-10-like images (with minibatch discrimination and semi-supervised learning)."
    ],
    "author_id": "ALEJU",
    "pdf_url": "https://arxiv.org/pdf/1606.03498",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 40883301
  },
  {
    "blog_id": "a-language-based-approach-to-unifying-events-and-threads",
    "summary": [
      "A Language-based Approach to Unifying Events and Threads \u2013 Li and Zdancewic, 2006  So far in this mini-series we\u2019ve seen that thread and event-based models are duals , that threads are a bad idea \u2013 you should really be using events, and that events are a bad idea \u2013 you should really be using threads.",
      "What if we could combine the two models and bring together the deadlocks, data races and corruption of threads with the hard to understand control flow of events?",
      ";) Or, if we applied the techniques wisely, the easy to follow control flow of the thread-based approach, with the natural way that events model some domains.",
      "I think the latter is more what the authors had in mind!",
      "The goal is to design a software library that provides two different abstractions for application-level threads: the thread view, which allows per-thread code be written in the natural, imperative, multi-threaded style, and the event view, which allows the threads be passively manipulated by the underlying scheduler in an abstract, type-safe way.",
      "\u2026  This dualized model gives the best of two worlds: the expressiveness of threads and the customizability of events.",
      "After briefly comparing event and thread-based models, the authors go on to present a unified model implemented in  Haskell: \u201cOur experience shows that Haskell is a reasonable language for building scalable systems software; it is expressive, succinct, efficient and type safe; it also interacts well with C libraries and APIs.\u201d  The primary advantage of the thread model is that the programmer can reason about the series of actions taken by a thread in the familiar way, just as for a sequential program\u2026.",
      "Event-driven programming, in contrast, is hard.",
      "The control flow graph has to be decomposed to multiple event handlers and represented as some form of state machines with explicit message passing or in continuation-passing style (CPS).",
      "Event-driven programs however are \u2018usually more flexible and customizable,\u2019 and are a good match for interrupt-driven systems.",
      "Note that the description of \u2018event handlers, state machines, and explicit message passing\u2019 is very reminiscent of the actor model.",
      "The CPS style evokes callbacks and promises.",
      "So how do Li and Zdancewic bring the two together?",
      "Monads provide the thread abstraction by defining an imperative sub-language of Haskell with system calls and thread control primitives  Higher-order functions provide the internal representation of threads in continuation-passing style  Lazy data structures provide the event abstraction, which is a lazy tree that represents the trace of system calls generated by threads.",
      "The goal is to design a monad that provides a thread abstraction, so the programmer can write multi-threaded code using the overloaded \u201cdo\u201d-syntax with a set of system calls.",
      "The implementation of this monad is tricky, but the details are hidden from the programmer.",
      "For example:  sock_accept server_fd do {   new_fd <- sys_nbio (accept server_fd);   if new_fd > 0      then return new_fd      else do {         sys_epoll_wait fd EPOLL_READ;         sock_accept server_fd;      } }  (If you\u2019re not familiar with reading Haskell code, the semi-colon separated statements in the do blocks provide elegant syntactic sugar for sequential composition of functions).",
      "By designing thread control primitives as monadic combinators, the monad interface can be used as an abstraction for multi-threaded programming, because it provides a way of hiding the \u2018internal plumbing\u2019 needed to write programs in CPS style.",
      "Here\u2019s an example of a simple recursive event loop:  worker_epoll sched =   do {         results <- epoll_wait;         mapM (writeChan (ready_queue sched)) results;         worker_epoll sched;   }  (case statements can be used to select among multiple possible events).",
      "This event-driven architecture is similar to that in SEDA, but our events are finer-grained: instead of requiring the programmer manually decompose a computation into stages and specify what stages can be performed in parallel, the event-driven scheduler automatically decomposes a threaded computation into fine-grained segments separated by system calls.",
      "Haskell\u2019s type system ensures that each segment is a purely functional computation without I/O, so such segments can be safely executed in parallel.",
      "A web server is implemented to assess the programming model and performance of the solution.",
      "Not only is the multi-threaded programming style natural and elegant, but the event-driven architecture also makes the scheduler clean.",
      "The scheduler, including the CPS monad, system call implementations, event loops and queues for AIO, epoll, mutexes, file-opening and exception handling (but not counting the wrapper interfaces for C library functions), is only 220 lines of well-structured code.",
      "Performance-wise the authors report that \u2018in our experience Haskell programs, while slower that C programs, are not orders of magnitude slower.\u2019 In their tests, the implementation performed favourably when compared to the Linux Native-POSIX Thread Library (NPTL) and against Apache for disk-bound workloads.",
      "Introducing Haskell STM transactions made the solution slower than C, but this disadvantage becomes less significant as more processors are introduced.",
      "In exchange for performance, Haskell delivers many features that simplify program development, including a very expressive static type system, type inference, lightweight closures, garbage collection, and convenient syntax overloading.",
      "We heavily use these features in our thread library implementation; it might be possible to implement the unified concurrency model in a general-purpose language lacking some of these features, but the results would likely be cumbersome to use.",
      "Nevertheless, it is worth investigating how to apply our approach in more mainstream languages like Java.",
      "Tomorrow we\u2019ll look at a paper that reports on experiences trying to do exactly that.",
      "The bottom line:  Events and threads should be combined into an unified programming model in general-purpose programming languages.",
      "With proper language support, application-level threads can be made extremely lightweight and easy to use.",
      "Our experiments demonstrate that this approach is practical and our programming experience suggests that this is a very appealing way of writing scalable, massively concurrent software."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://thelackthereof.org/docs/library/LZ06b.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 92908954
  },
  {
    "blog_id": "what-bugs-cause-cloud-production-incidents",
    "summary": [
      "What bugs cause production cloud incidents?",
      "Liu et al., HotOS\u201919  Last time out we looked at SLOs for cloud platforms , today we\u2019re looking at what causes them to be broken!",
      "This is a study of every high severity production incident at Microsoft Azure services over a span of six months, where the root cause of that incident was a software bug.",
      "In total, there were 112 such incidents over the period March \u2013 September 2018 (not all of them affecting external customers).",
      "Software bugs are the most common cause of incidents during this period, accounting for around 40% of all incidents (so we can infer there were around 280 incidents total in the pool).",
      "The 112 incidents caused by software bugs are further broken down into categories, with data-format bugs, fault-related bugs, timing bugs, and constant_value bugs being the largest categories.",
      "Interestingly, outages caused by configuration errors represented only a small number of incidents in this study.",
      "This could be an artefact of that data set in some way, or it might be due to the tool chain that Microsoft uses:  The types of bugs we observed in production are biased by the fact that Microsoft uses effective tools to mostly eliminate many types oft bugs before they can manifest in production, and hence our study includes zero or few of such bugs.",
      "For example, we observed only a small number of configuration bugs caused by mis-specification of configuration entries in configuration files, even though such bugs were reported to be common in other settings.",
      "Most Azure code is written in .Net managed languages such as C#, reducing memory leak bugs.",
      "Tools like CHESS and PCT are used to expose shared-memory concurrency bugs.",
      "TLA+ is used to model concurrent and distributed system protocols helping to eliminate high level design and semantic bugs.",
      "In addition, Azure\u2019s Fault Analysis Service supports various types of fault injections during testing, such as node restart, data migration,  and random faults.",
      "Microsoft is also using fuzz testing, cloud contract checking, and network configuration verification tools.",
      "Data formats  Of the software bugs that survive all of this and end up causing high severity incidents, one of.",
      "the most common causes are data format change (21%).",
      "Different components of cloud services interact with each other through various types of \u201cdata\u201d, including inter-process/node messages, persistent files, and so on.",
      "At the same time, cloud software goes through frequent updates.",
      "As a result, different software components in the cloud could hold conflicting assumptions about the format of certain data, leading to service incidents.",
      "It looks like data validation isn\u2019t only useful in a machine learning context !",
      "All but one of the data format bugs involved multiple processes or nodes.",
      "In 40% of cases different parties assume different formats for shared files or database tables.",
      "For example, an upgrade has been deployed to a component \u2018owning\u2019 the table which changes the schema.",
      "The other 60% of cases are caused by a service changing the interface of its external message APIs.",
      "For example, a service that used to return 200 together with an empty list when no results were found changes to returning a 404, and breaks existing clients.",
      "The large scale, frequent updates, and long running nature of cloud services likely have facilitated the occurrence of these bugs.",
      "I\u2019d expect this class of bugs to also surface in microservices systems.",
      "Does that match your experience?",
      "Fault related  Next up is an old chestnut: error and exception handling faults (31%).",
      "Components that fail and report an error that can\u2019t be handled (e.g., missing exception handlers) (43% of this category)  Unresponsive components that hang and are not picked up by fault-detection mechanisms, leading to user-visible timeouts (29% of this category)  Silent corruption of data with no error detection code in place, leading to incorrect results returned to users (17% of this category)  Exception / error handlers contribute to the incidents by either ignoring error reports (35%), over-reacting (35%), or containing bugs within the handlers themselves leading to infinite loops, timing issues, and so on (30%).",
      "Timing incidents  13% of the incidents are related to timing, with only 14% of the timing incidents actually recorded as deadlocks.",
      "Compared to classic timing bugs racing between threads in the same process, here many of these bugs are about race conditions between multiple nodes and many of them are racing on persistent data like cached firewall rules, configuration, database tables, and so on.",
      "Constant-value setting incidents  7% of all software bug incidents are caused by mistakes in constants: hard-code configuration, special purpose strings such as URLs, and enum-typed values.",
      "Bug resolution  Facing tight time pressure, more often than not, software bug incidents were resolved through a variety of mitigation techniques (56%) without patching the buggy code (44%), providing quick solutions to users and maximizing service availability.",
      "(Mitigated incidents may well have been followed up later with code changes, but these aren\u2019t recorded in the respective incident reports).",
      "When mitigating there are three main types of mitigation uncovered:  Code mitigation involves rolling back to an earlier version of the software  Data mitigation involves manually restoring, cleaning up, or deleting data in a file, table, etc.",
      "Environment mitigation involves killing and restarting processes, migrating workloads, adding fail-over resources, etc..",
      "When mitigating, environment mitigations are the most common.",
      "Although the kind of mitigate employed does vary based on root cause:  Much recent work looked at how to automatically generate new patches.",
      "In comparison, automatically generating mitigation steps has not been well studied and is worth more attention in the future."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://people.cs.uchicago.edu/~shanlu/paper/hotos19_azure.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 48091191
  },
  {
    "blog_id": "synthesizing_the_preferred_inputs_for_neurons_in_neural_networks_via_deep_generator_networks",
    "summary": [
      "What  They suggest a new method to generate images which maximize the activation of a specific neuron in a (trained) target network (abbreviated with \"DNN\").",
      "E.g. if your DNN contains a neuron that is active whenever there is a car in an image, the method should generate images containing cars.",
      "Such methods can be used to investigate what exactly a network has learned.",
      "There are plenty of methods like this one.",
      "They usually differ from each other by using different natural image priors.",
      "A natural image prior is a restriction on the generated images.",
      "Such a prior pushes the generated images towards realistic looking ones.",
      "Without such a prior it is easy to generate images that lead to high activations of specific neurons, but don't look realistic at all (e.g. they might look psychodelic or like white noise).",
      "That's because the space of possible images is extremely high-dimensional and can therefore hardly be covered reliably by a single network.",
      "Note also that training datasets usually only show a very limited subset of all possible images.",
      "Their work introduces a new natural image prior.",
      "How  Usually, if one wants to generate images that lead to high activations, the basic/naive method is to:  Start with a noise image,  Feed that image through DNN,  Compute an error that is high if the activation of the specified neuron is low (analogous for high activation),  Backpropagate the error through DNN,  Change the noise image according to the gradient,  Repeat.",
      "So, the noise image is basically treated like weights in the network.",
      "Their alternative method is based on a Generator network G.  That G is trained according to the method described in Generating Images with Perceptual Similarity Metrics based on Deep Networks .",
      "Very rough outline of that method:  First, a pretrained network E is given (they picked CaffeNet, which is a variation of AlexNet).",
      "G then has to learn to inverse E, i.e. G receives per image the features extracted by a specific layer in E (e.g.",
      "the last fully connected layer before the output) and has to generate (recreate) the image from these features.",
      "Their modified steps are:  (New step) Start with a noise vector,  (New step) Feed that vector through G resulting in an image,  (Same) Feed that image through DNN,  (Same) Compute an error that is low if the activation of the specified neuron is high (analogous for low activations),  (Same) Backpropagate the error through DNN,  (Modified) Change the noise vector according to the gradient,  (Same) Repeat.",
      "Visualization of their architecture:  Additionally they do:  Apply an L2 norm to the noise vector, which adds pressure to each component to take low values.",
      "They say that this improved the results.",
      "Clip each component of the noise vector to a range [0, a], which improved the results significantly.",
      "The range starts at 0, because the network (E) inverted by their Generator (G) is based on ReLUs.",
      "a is derived from test images fed through E and set to 3 standard diviations of the mean activation of that component (recall that the \"noise\" vector mirrors a specific layer in E).",
      "They argue that this clipping is similar to a prior on the noise vector components.",
      "That prior reflects likely values of the layer in E that is used for the noise vector.",
      "Results  Examples of generated images:  Early vs. late layers  For G they have to pick a specific layer from E that G has to invert.",
      "They found that using \"later\" layers (e.g. the fully connected layers at the end) produced images with more reasonable overall structure than using \"early\" layers (e.g.",
      "first convolutional layers).",
      "Early layers led to repeating structures.",
      "Datasets and architectures  Both G and DNN have to be trained on datasets.",
      "They found that these networks can actually be trained on different datasets, the results will still look good.",
      "However, they found that the architectures of DNN and E should be similar to create the best looking images (though this might also be down to depth of the tested networks).",
      "Verification that the prior can generate any image  They tested whether the generated images really show what the DNN-neurons prefer and not what the Generator/prior prefers.",
      "To do that, they retrained DNNs on images that were both directly from the dataset as well as images that were somehow modified.",
      "Those modifications were:  Treated RGB images as if they were BGR (creating images with weird colors).",
      "Copy-pasted areas in the images around (creating mosaics).",
      "Blurred the images (with gaussian blur).",
      "The DNNs were then trained to classify the \"normal\" images into 1000 classes and the modified images into 1000 other classes (2000 total).",
      "So at the end there were (in the same DNN) neurons reacting strongly to specific classes of unmodified images and other neurons that reacted strongly to specific classes of modified images.",
      "When generating images to maximize activations of specific neurons, the Generator was able to create both modified and unmodified images.",
      "Though it seemed to have some trouble with blurring.",
      "That shows that the generated images probably indeed show what the DNN has learned and not just what G has learned.",
      "Uncanonical images  The method can sometimes generate uncanonical images (e.g. instead of a full dog just blobs of texture).",
      "They found that this seems to be mostly the case when the dataset images have uncanonical pose, i.e. are very diverse/multi-modal."
    ],
    "author_id": "ALEJU",
    "pdf_url": "http://arxiv.org/pdf/1605.09304v3",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 18958324
  },
  {
    "blog_id": "deep_generative_image_models_using_a_laplacian_pyramid_of_adversarial_networks",
    "summary": [
      "What  The original GAN approach used one Generator (G) to generate images and one Discriminator (D) to rate these images.",
      "The laplacian pyramid GAN uses multiple pairs of G and D.  It starts with an ordinary GAN that generates small images (say, 4x4).",
      "Each following pair learns to generate plausible upscalings of the image, usually by a factor of 2.",
      "(So e.g. from 4x4 to 8x8.)",
      "This scaling from coarse to fine resembles a laplacian pyramid, hence the name.",
      "How  The first pair of G and D is just like an ordinary GAN.",
      "For each pair afterwards, G recieves the output of the previous step, upscaled to the desired size.",
      "Due to the upscaling, the image will be blurry.",
      "G has to learn to generate a plausible sharpening of that blurry image.",
      "G outputs a difference image, not the full sharpened image.",
      "D recieves the upscaled/blurry image.",
      "D also recieves either the optimal difference image (for images from the training set) or G's generated difference image.",
      "D adds the difference image to the blurry image as its first step.",
      "Afterwards it applies convolutions to the image and ends in one sigmoid unit.",
      "The training procedure is just like in the ordinary GAN setting.",
      "Each upscaling pair of G and D can be trained on its own.",
      "The first G recieves a \"normal\" noise vector, just like in the ordinary GAN setting.",
      "Later Gs recieve noise as one plane, so each image has four channels: R, G, B, noise.",
      "Results  Images are rated as looking more realistic than the ones from ordinary GANs.",
      "The approximated log likelihood is significantly lower (improved) compared to ordinary GANs.",
      "The generated images do however still look distorted compared to real images.",
      "They also tried to add class conditional information to G and D (just a one hot vector for the desired class of the image).",
      "G and D learned successfully to adapt to that information (e.g. to only generate images that seem to show birds).",
      "Basic training and sampling process.",
      "The first image is generated directly from noise.",
      "Everything afterwards is de-blurring of upscaled images.",
      "Rough chapter-wise notes  Introduction  Instead of just one big generative model, they build multiple ones.",
      "They start with one model at a small image scale (e.g. 4x4) and then add multiple generative models that increase the image size (e.g.",
      "from 4x4 to 8x8).",
      "This scaling from coarse to fine (low frequency to high frequency components) resembles a laplacian pyramid, hence the name of the paper.",
      "Related Works  Types of generative image models:  Non-Parametric: Models copy patches from training set (e.g. texture synthesis, super-resolution)  Parametric: E.g.",
      "Deep Boltzmann machines or denoising auto-encoders  Novel approaches: e.g. DRAW, diffusion-based processes, LSTMs  This work is based on (conditional) GANs  Approach  They start with a Gaussian and a Laplacian pyramid.",
      "They build the Gaussian pyramid by repeatedly decreasing the image height/width by 2: [full size image, half size image, quarter size image, ...]  They build a Laplacian pyramid by taking pairs of images in the gaussian pyramid, upscaling the smaller one and then taking the difference.",
      "In the laplacian GAN approach, an image at scale k is created by first upscaling the image at scale k-1 and then adding a refinement to it (de-blurring).",
      "The refinement is created with a GAN that recieves the upscaled image as input.",
      "Note that the refinement is a difference image (between the upscaled image and the optimal upscaled image).",
      "The very first (small scale) image is generated by an ordinary GAN.",
      "D recieves an upscaled image and a difference image.",
      "It then adds them together to create an upscaled and de-blurred image.",
      "Then D applies ordinary convolutions to the result and ends in a quality rating (sigmoid).",
      "Model Architecture and Training  Datasets: CIFAR-10 (32x32, 100k images), STL (96x96, 100k), LSUN (64x64, 10M)  They use a uniform distribution of [-1, 1] for their noise vectors.",
      "For the upscaling Generators they add the noise as a fourth plane (to the RGB image).",
      "CIFAR-10: 8->14->28 (height/width), STL: 8->16->32->64->96, LSUN: 4->8->16->32->64  CIFAR-10: G=3 layers, D=2 layers, STL: G=3 layers, D=2 layers, LSUN: G=5 layers, D=3 layers.",
      "Experiments  Evaluation methods:  Computation of log-likelihood on a held out image set  They use a Gaussian window based Parzen estimation to approximate the probability of an image (note: not very accurate).",
      "They adapt their estimation method to the special case of the laplacian pyramid.",
      "Their laplacian pyramid model seems to perform significantly better than ordinary GANs.",
      "Subjective evaluation of generated images  Their model seems to learn the rough structure and color correlations of images to generate.",
      "They add class conditional information to G and D. G indeed learns to generate different classes of images.",
      "All images still have noticeable distortions.",
      "Subjective evaluation of generated images by other people  15 volunteers.",
      "They show generated or real images in an interface for 50-2000ms.",
      "Volunteer then has to decide whether the image is fake or real.",
      "10k ratings were collected.",
      "At 2000ms, around 50% of the generated images were considered real, ~90 of the true real ones and <10% of the images generated by an ordinary GAN."
    ],
    "author_id": "ALEJU",
    "pdf_url": "http://arxiv.org/pdf/1506.05751",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 80770343
  },
  {
    "blog_id": "olive-oil-is-made-of-olives-baby-oil-is-made-for-babies-paper-summary-a6f9b5544761",
    "summary": [
      "This article summarizes a novel technique for a very complex task in NLP known as noun compound classification.",
      "Paper Title  Olive Oil is Made of Olives, Baby Oil is Made for Babies: Interpreting Noun Compounds using Paraphrases in a Neural Model \u2014 Vered Shwartz and Chris Waterson  Basic Overview  This paper addresses an important NLP task \u2014 the automatic interpretation of the relation between the constituents of a noun compound.",
      "Motivation  Consider the following noun compound examples: olive oil and baby oil.",
      "You can observe that the word \u201colive\u201d in the phrase \u201colive oil\u201d describes a SOURCE relation, and the word \u201cbaby\u201d in \u201cbaby oil\u201d describes a PURPOSE relation.",
      "In other words, babies should never be put in the same context as olives in terms of what they represent in the real world.",
      "This distinction is important because it can be used for various applications that require complex text understanding capabilities.",
      "Example  Imagine you asked Google search what olive oil is made up of.",
      "If Google search is smart it should respond \u201colive\u201d.",
      "Now imagine you asked Google what baby oil is made up of.",
      "Definitely not babies!",
      "The answer should be other ingredients of oil or the main ingredient of oil.",
      "This is a very important distinction!",
      "It\u2019s a challenging task because the meaning of both types of oils is not easy to interpret given the meaning of its constituent words.",
      "Can you think of more examples?",
      "Try and you will see why this area of NLP research is important.",
      "As an NLP researcher, I can even see how this is useful for disambiguating between sentiment phrases.",
      "(More on this in another article.)",
      "Literature Review  Two very common approaches are used to address this issue: paraphrases and noun-compound representations.",
      "The first approach maps relationships between constituents and the latter approach makes use of distributional representations of the individual constituents.",
      "(Don\u2019t panic, I will explain what they mean in a little bit.)",
      "A more recent work (Dima, 2016) showed that constituent embeddings are effective to represent the noun-compounds (hereinafter also referred to as NCs.",
      "The main reason for why it works is attributed to a phenomenon referred to as lexical memorization .",
      "Contribution  This paper proposes a neural paraphrasing approach that combines path embeddings (which represents the relationships between NCs) and distributional information (obtained directly from word embeddings) to conduct the NC classification task.",
      "The authors also experiment with settings where lexical memorization is avoided to show that their method is more robust and their results are not attributed to this phenomenon.",
      "Model  The authors used HypeNET to learn patterns connecting the joint occurrences of instances of constituents in a corpus.",
      "These are also referred to as path embeddings.",
      "Three models were combined to perform the NCs relation classification: path-based, integrated, and integrated-NC.",
      "Each model incrementally adds new features (in this case different distributional inputs) which essentially adds more contextualized information to the overall input vector.",
      "This process can be seen more clearly in the diagram below:  The path embeddings (colored purple in the diagram) are learned using a vanilla LSTM with input vectors representing a concatenation of the following vectors: lemma, part-of-speech tag, dependency label, and direction vectors.",
      "(See paper for more details).",
      "NC labels (relations) are obtained using a distant supervision approach via the output of the LSTM.",
      "Evaluation  Two datasets, obtained from Tratz (2011) , were used to evaluate the proposed neural paraphrasing model.",
      "Several comparison models were proposed, including several baseline models and re-trained models adopted from previous work and the state-of-the-art method.",
      "(See paper for more details on the experimental setup).",
      "Table 1 shows that for most cases the integrated models (Int and Int-NC) outperform all other models on the using different data splitting strategies (shown in Split column).",
      "There is very little difference between the results obtained from the Int model and Int-NC model, indicating that the NC embeddings did not contribute much to the classification task.",
      "Analysis  Further analysis was conducted on the random splitting strategy to analyze variations in results of the different models.",
      "In Table 3, you can observe some of the relations that yielded reasonable performance (e.g., MEASURE and PERSONAL TITLE)  The authors also discovered that complex relations perform poorly such as LEXICALIZED for the NC, \u201csoap opera\u201d and OBJECTIVE for NC, \u201crecovery plan\u201d.",
      "(See more interesting examples in the paper).",
      "Table 4 below provides examples of NC embeddings obtained from the test set (left) and an example of the most similar NC in the embeddings (right).",
      "The authors observed that only 27.61% of the NCs were mostly similar to NCs with the same label.",
      "They attributed this behavior to inconsistent annotations rather than quality of embeddings.",
      "My Further Ideas and Conclusion  Visualize NC vector embeddings to observe relation clusters and patterns that may have similar properties.",
      "Study more closely the lexical memorization phenomena, which the paper helps to \u201cslightly\u201d address using the path-based component of the whole model.",
      "Overall, the performance was improved but there is still a lot more room for improvement in terms of data quality and modeling.",
      "The NC embeddings are not helping the current model, so it provides a feasible future research direction without changing the overall structure of the model.",
      "Resources  The original paper | Code Repository (Tensorflow implementation)   [url]"
    ],
    "author_id": "DAIR-AI",
    "pdf_url": "https://arxiv.org/pdf/1803.08073",
    "author_full_name": "Elvis Saravia",
    "source_website": "https://github.com/dair-ai/nlp_paper_summaries",
    "id": 58308538
  },
  {
    "blog_id": "deep_continuous_fusion_for_multi-sensor_3d_object_detection",
    "summary": [
      "What  They propose an object detector that fuses 2D information (from images) and 3D information (from LiDAR, i.e. point cloud) to predict 3D objects in birds eye view (BEV).",
      "Their method is based on projecting 2D images into point clouds (usually it's the other way round, i.e. point clouds are projected into 2D images).",
      "They propose a layer to perform the 2D-3D fusion at multiple image scales.",
      "The result is a fast and fairly accurate object detector.",
      "How  Basic Architecture  They feed the images through a ResNet18 branch (initialized via ImageNet pretraining).",
      "They feed the point cloud through a custom residual network.",
      "Input is the voxelized point cloud in BEV.",
      "The network is comparable to ResNet18.",
      "They use a total of 36 convolutions in five blocks, with a growing number of convolutions per block and downscaling at the start of each block.",
      "This network is initialized via Xavier initialization.",
      "They features extracted by the image branch are fused at multiple scales into the point cloud BEV branch using Continuous Fusion layers.",
      "The output of the point cloud BEV branch is comparable to many other object detectors:  Per spatial location classification (object class + background) and regression predictions (x, y, z and width, length, height).",
      "They use two anchor boxes, one for objects with a rotation of 0 degrees and one for objects with a rotation of 90 degrees.",
      "As usually, they apply Non-Maximum-Suppression to the resulting bounding boxes.",
      "Visualization:  Continuous Fusion Layer  The Continuous Fusion Layer fuses 2D (camera) and 3D (LiDAR) features.",
      "Its basic principle is to project 2D information into 3D.",
      "(Counterintuitively, it still performs first a projection from 3D to 2D.)",
      "As their network works in BEV, the fusion merges information from a dense 2D grid (camera) with another dense 2D grid (LiDAR in BEV) instead of a point cloud.",
      "Assume you have a LIDAR BEV feature map with one channel, i.e. a 2D grid.",
      "You want to merge information from the camera into one single spatial location (e.g. column 10, row 20) of the grid.",
      "The following steps are then performed:  For the given spatial location, find the K nearest neighbour points in the BEV point cloud.",
      "(E.g. find the K neighbours of y=10, x=20.)",
      "Re-add the z coordinate to each of these K nearest neighbours (projects them from BEV 3D back to full 3D).",
      "Project these 3D points onto the 2D image plane.",
      "This indicates which image areas might contain information relevant for the 3D BEV location.",
      "Use bilinear interpolation to estimate the image's features at the projected locations.",
      "For each location also estimate the offset vector from the BEV spatial location (e.g. y=10, x=20) to the 3D point (I guess in BEV?).",
      "Concat the feature vector and offset vector from step (4) to one vector.",
      "Then apply fully connected layers to that (in their case three layers).",
      "This generates a vector per projected point.",
      "Sum these vectors.",
      "Concat the resulting vector to the point cloud BEV feature map from step (1) at the chosen starting location (e.g. y=10, x=20).",
      "Visualization of the steps:  Other stuff  As is common, their classification loss is binary cross entropy and their regression loss is smooth L1.",
      "As is common, they encode the x-, y- and z-coordinates (predicted via the regression branch) is relative offsets to the anchor's center.",
      "As is common, they encode the widths, lengths and heights logarithmically, e.g. log(width/width_anchor).",
      "They use Adam without weight decay.",
      "For training on KITTI, they augment the dataset, e.g. via scaling, translation and rotation of the point cloud and camera image (matched to each other, so that projections from one sensor to the other remain sensible).",
      "They crop the point cloud to 70m in front of the ego vehicle and 40m to the left and right.",
      "They voxelize the point cloud to 512x448x32 voxels.",
      "(Sounds like they apply 2D convs to an input with 32 channels encoding height information.)",
      "Results  KITTI  Note: They only evaluate on the class \"car\", arguing that -- in the case of their model -- there is not enough training data for the other classes in KITTI.",
      "They achieve accuracies roughly comparable with other well-performing 3D object detectors.",
      "(Though not state of the art.)",
      "Their detector runs at about 15fps or 66ms per scene, beating most other models.",
      "They are roughly twice as fast as AVOD-FPN, but quite a bit less accurate (~5 percentage points for moderate 3D AP).",
      "Using only LiDAR (and not images) as input significantly worsens results.",
      "Using a fusion strategy similar in which only the BEV locations closest to 3D points are projected to the image plane (no K-NN) and without using offset vectors significantly worsens results.",
      "Removing the offset vector significantly worsens results.",
      "Picking K>1 for the K-nearest-neighbours search does not produce better results than K=1.",
      "Limiting the maximum distance during the K-nearest-neighbour search to 10m has only a tiny positive impact on AP.",
      "Ablation results (\"geometric feature\" is the offset vector):  TOR4D (Uber dataset)  They evaluate here on more classes and increase the forward distance to 100m.",
      "They decrease the parameters in the network to keep the time per scene roughly comparable (despite increase in max forward distance).",
      "They evaluate the achieved AP by class and distance:"
    ],
    "author_id": "ALEJU",
    "pdf_url": "http://openaccess.thecvf.com/content_ECCV_2018/papers/Ming_Liang_Deep_Continuous_Fusion_ECCV_2018_paper.pdf",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 36200725
  },
  {
    "blog_id": "how-to-memorize-a-random-60-bit-string",
    "summary": [
      "How to memorize a random 60-bit string \u2013 Ghazvininejad et al. 2105  A bit of fun for today \u2013 this paper has been the source of many articles around the net over the last couple of weeks (though not many have dug into the actual algorithms\u2026 ).",
      "Inspired by an XKCD cartoon , the challenge is to convert a randomly generated 60-bit string into a memorable sequence of English words that can deterministically be used to recover the bit string and hence be used as a password.",
      "Including the XKCD algorithm as a baseline, the authors develop four additional approaches and evaluate them all for ease of human memorisation.",
      "The XKCD and poetry methods perform the best under this test, so let\u2019s look at those two in more details.",
      "60-bit XKCD-inspired passwords  XKCD issue #936  Start with a randomly chosen 60-bit password.",
      "Divide this up into four 15-bit segments, and use each as an index into a 32,768 word dictionary.",
      "(The XKCD original used a 44-bit password, 11-bit segments, and a 2048 word dictionary).",
      "This produces passwords such as \u2018fees wesley inmate decentralization,\u2019 \u2018photos bros nan plain,\u2019 and \u2019embarass debating gaskell jennie.\u2019  These passwords are nonsensical, but have the advantage of only being four words long.",
      "Three variations are tried (First-letter mnemonic, All-letter method, and Frequency method) that encode bits or bit-sequences as letters and then generate sentences from them.",
      "I particularly like the All-letter-method that pairs English words with bit-phrases and then uses the Moses machine translation toolkit to search for the 1-best translation of the 60-bit input string, using this phrase table and a 5-gram English language model.",
      "Ingenious, and it produces great phrases \u2013 for example, \u201cFox news networks are seeking views from downtown streets.\u201d Users ultimately found these sentences harder to remember though due to their length.",
      "Poetry-based passwords  My first original poetry contribution so far in The Morning Paper:  If I can make my password rhyme, Then learning it takes much less time.",
      "Don\u2019t worry, I suspect it will also be my last ;).",
      "In ancient times, people recorded long, historical epics using poetry, to enhance memorability.",
      "We follow this idea by turning each system-assigned 60-bit string into a short, distinct English poem.",
      "Our format is the rhyming iambic tetrameter couplet.",
      "(Two lines of eight syllables, stress pattern 01010101, and lines ending in a pair of rhyming words.)",
      "So now all you have to do is find an algorithm that generates memorable rhyming iambic tetrameter couplets that correspond to a unique 60-bit code!",
      "The authors take this daunting sounding challenge in their stride.",
      "First they create a Finite State Transducer that translates English words into sequences capturing their essential properties.",
      "Finite State Transducers (FSTs) are quite common in language processing.",
      "An FST is a finite-state automaton that produces an output tape as well as reading from its input tape.",
      "From the wikipedia page :  The two tapes of a transducer are typically viewed as an input tape and an output tape.",
      "On this view, a transducer is said to transduce (i.e. translate) the contents of its input tape to its output tape, by accepting a string on its input tape and generating another string on its output tape.",
      "It may do so non-deterministically, and it may produce more than one output for each input string.",
      "A transducer may also produce no output for a given input string, in which case it is said to reject the input.",
      "In general, a transducer computes a relation between two formal languages.",
      "For our purposes though, we can simply think of this stage as mapping each English word to a set of encodings.",
      "For the sample word \u2018create,\u2019 these encodings would be:  0 1         // cre-ATE  0 1 EY-T    // cre-ATE at end of a line, EY-T rhyming pattern 1r 0r       // cre-ATE in the second line (r for reverse) EY-T 1r 0r  // cre-ATE at end of the second line (r for reverse)  A Finite State Acceptor is constructed with a \u2018path\u2019 for each legal poem.",
      "This only accepts sequences of the form:  0 1 0 1 0 1 0 1 X X 1r 0r 1r 0r 1r 0r 1r 0r  (The second line is generated in reverse order so that rhyming can be enforced locally \u2013 X stands for any rhyme pattern, e.g. EY-T).",
      "It remains to map an arbitrary 60-bit string onto a path in the FSA.",
      "Let k be the integer representation of the 60-bit string.",
      "If the FSA contains exactly 260 paths, we can easily select the kth path using the following method.",
      "At each node N of the FSA, we store the total number of paths from N to the final state\u2014this takes linear time if we visit states in reverse topological order.",
      "We then traverse the FSA deterministically from the start state, using k to guide the path selection.",
      "The generated FSA actually contains 279 paths, giving more than a million poem choices for each 60-bit string.",
      "This gives opportunity to use the 5-gram language model again to output the best one:  More precisely, given a 60-bit input string k, we extract not only the kth FSA path, but also the k + i \u00b7 260 paths, with i ranging from 1 to 999,999.",
      "We explicitly list out these paths, reversing the second half of each, and score them with our 5-gram LM.",
      "We output the poem with the 1-best LM score.",
      "For example:  Diversity inside replied, retreats or colours justified  To reconstruct the original 60-bit string k, we first find the FSA path corresponding to the user-recalled English string (with second half reversed).",
      "We use depth-first search to find this path.",
      "Once we have the path, it is easy to determine which numbered path it is, lexicographically speaking, using the node-labeling scheme above to recover k."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://www.isi.edu/natural-language/mt/memorize-random-60.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 11354749
  },
  {
    "blog_id": "learning-the-structure-of-generative-models-without-labeled-data",
    "summary": [
      "Learning the structure of generative models without labeled data Bach et al., ICML\u201917  For the last couple of posts we\u2019ve been looking at Snorkel and BabbleLabble which both depend on data programming \u2013 the ability to intelligently combine the outputs of a set of labelling functions.",
      "The core of data programming is developed in two papers, \u2018 Data programming: creating large training sets, quickly \u2019 (Ratner 2016) and today\u2019s paper choice, \u2018 Learning the structure of generative models without labeled data \u2019 (Bach 2017).",
      "The original data programming paper works explicitly with input pairs (x,y) (e.g. the chemical and disease word pairs we saw from the disease task in Snorkel) which (for me at least) confuses the presentation a little compared to the latter ICML paper which just assumes inputs  (which could of course have pair structure, but we don\u2019t care about that at this level of detail).",
      "Also in the original paper dependencies between labelling functions are explicitly specified by end users (as one of four types: similar, fixing, reinforcing, and exclusive) and built into a factor graph.",
      "In the ICML paper dependencies are learned.",
      "So I\u2019m going to work mostly from \u2018Learning the structure of generative models without labeled data\u2019 in this post, and concentrate just on the core development of the theory.",
      "A generative probabilistic model for accuracy estimation under independence  We have input data points  , and for each  a latent variable  that is its true label.",
      "We don\u2019t know  , but we do have  labelling functions  .",
      "Each labelling functions takes as input an data point  and produces as output a value  which is either true, abstain, or false (  ).",
      "(The approach generalises to the multi-class case).",
      "We start out by assuming that the outputs of the labelling functions are conditionally independent given the true label.",
      "Consider a labelling function  , which gives output  on input  (with true label  ).",
      "We can model the accuracy of this labelling function with  .",
      "We\u2019ll say the accuracy function has value 1 if  correctly labels an example, and 0 otherwise.",
      "In our setting this can be succinctly expressed as:  The accuracy function itself is controlled by some parameter  modelling how accurate each labelling function is.",
      "Now for  input examples and  labelling functions, we have a conditionally independent model  where  is the set of true labels  .",
      "The unknown parameters  modelling the accuracy of the labelling functions can now be estimated by minimising the negative log marginal likelihood of failing to give the correct label (guessing a wrong label, or abstaining).",
      "We can do this using standard stochastic gradient descent, where the gradient for parameter  is given by the number of examples  correctly labels minus the number of examples it fails to label correctly.",
      "Structure learning  The conditionally independent model is a common assumption, and using a more sophisticated generative model currently requires users to specify its structure.",
      "As we know from the Snorkel paper, unfortunately labelling functions are often not independent.",
      "To address this issue, we generalize the conditionally independent model as a factor graph with additional dependencies, including higher-order factors that connect multiple labeling function outputs for each data point  and label  .",
      "Suppose there is a set  of dependency types of interest, where a dependency type represents a form of dependency between labelling functions, such as correlation.",
      "Unknown to us, there is also a set  of index tuples which indicate the set of labelling functions that participate in each dependency of type  .",
      "This gives us a general model of the form  and we want to learn the parameters  .",
      "Estimating the structure of the distribution  is challenging because Y is latent; we never observe its value, even during training.",
      "We must therefore work with the marginal likelihood  .",
      "Learning the parameters of the generative model requires Gibbs sampling to estimate gradients.",
      "As the number of possible dependencies increases at least quadratically in the number of labeling functions, this heavyweight approach to learning does not scale.",
      "The way out is to consider each labelling function in turn, optimising the log marginal pseudolikelihood of its outputs conditioned on the outputs of all the other labelling functions.",
      "where  is a hyperparameter.",
      "By conditioning on all other labeling functions in each term\u2026, we ensue that the gradient can be computed in polynomial time with respect to the number of labeling functions, data points, and possible dependencies; without requiring any sampling or variational approximations.",
      "It all comes together in the following algorithm:  The hyperparameter  controls the threshold and regularization strength and requires tuning.",
      "For the other parameters the authors fix these at step size =  , epoch count = 10, and truncation frequency = 10.",
      "Theoretical guarantees  Assuming that there exist some set of parameters which can capture the true dependencies within the model we are using, and that all non-zero parameters are bounded away from zero (have at least a minimum magnitude  ), then we can uncover the exact dependency structure with probability at least  given  inputs, where  Where  is the maximum number of possible dependencies a single labelling function (or true label) can be involved in, and  is a measure of how much better our dependency estimates are with each labeling function than without (my loose interpretation of  \u2013 see eqn 7 in section 4 of the paper for the official definition).",
      "If we further assume that the only dependency types are accuracy and correlation dependencies then we need an input dataset of size  Practical results  Our analysis\u2026 guarantees that the sample complexity grows at worst on the order  for  labeling functions.",
      "In practice, we find that structure learning performs better than this guaranteed rate, depending linearly on the number of true correlations and logarithmically on the number of possible correlations.",
      "Compared to estimating structures via parameter learning over all possible dependencies, algorithm one is 100x faster and selects only 1/4 of the extraneous correlations."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://arxiv.org/pdf/1703.00854",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 33126132
  },
  {
    "blog_id": "hyperface",
    "summary": [
      "What  They suggest a single architecture that tries to solve the following tasks:  Face localization (\"Where are faces in the image?\")",
      "Face landmark localization (\"For a given face, where are its landmarks, e.g. eyes, nose and mouth?\")",
      "Face landmark visibility estimation (\"For a given face, which of its landmarks are actually visible and which of them are occluded by other objects/people?\")",
      "Face roll, pitch and yaw estimation (\"For a given face, what is its rotation on the x/y/z-axis?\")",
      "Face gender estimation (\"For a given face, which gender does the person have?\")",
      "How  Pretraining the base model  They start with a basic model following the architecture of AlexNet.",
      "They train that model to classify whether the input images are faces or not faces.",
      "They then remove the fully connected layers, leaving only the convolutional layers.",
      "Locating bounding boxes of face candidates  They then use a selective search and segmentation algorithm on images to extract bounding boxes of objects.",
      "Each bounding box is considered a possible face.",
      "Each bounding box is rescaled to 227x227.",
      "Feature extraction per face candidate  They feed each bounding box through the above mentioned pretrained network.",
      "They extract the activations of the network from the layers max1 (27x27x96), conv3 (13x13x384) and pool5 (6x6x256).",
      "They apply to the first two extracted tensors (from max1, conv3) convolutions so that their tensor shapes are reduced to 6x6xC.",
      "They concatenate the three tensors to a 6x6x768 tensor.",
      "They apply a 1x1 convolution to that tensor to reduce it to 6x6x192.",
      "They feed the result through a fully connected layer resulting in 3072-dimensional vectors (per face candidate).",
      "Classification and regression  They feed each 3072-dimensional vector through 5 separate networks:  Detection: Does the bounding box contain a face or no face.",
      "(2 outputs, i.e. yes/no)  Landmark Localization: What are the coordinates of landmark features (e.g.",
      "mouth, nose, ...).",
      "(21 landmarks, each 2 values for x/y = 42 outputs total)  Landmark Visibility: Which landmarks are visible.",
      "(21 yes/no outputs)  Pose estimation: Roll, pitch, yaw of the face.",
      "(3 outputs)  Gender estimation: Male/female face.",
      "(2 outputs)  Each of these network contains a single fully connected layer with 512 nodes, followed by the output layer with the above mentioned number of nodes.",
      "Architecture Visualization:  Training  The base model is trained once (see above).",
      "The feature extraction layers and the five classification/regression networks are trained afterwards (jointly).",
      "The loss functions for the five networks are:  Detection: BCE (binary cross-entropy).",
      "Detected bounding boxes that have an overlap >=0.5 with an annotated face are considered positive samples, bounding boxes with overlap <0.35 are considered negative samples, everything in between is ignored.",
      "Landmark localization: Roughly MSE (mean squared error), with some weighting for visibility.",
      "Only bounding boxes with overlap >0.35 are considered.",
      "Coordinates are normalized with respect to the bounding boxes center, width and height.",
      "Landmark visibility: MSE (predicted visibility factor vs. expected visibility factor).",
      "Only for bounding boxes with overlap >0.35.",
      "Pose estimation: MSE.",
      "Gender estimation: BCE.",
      "Testing  They use two postprocessing methods for detected faces:  Iterative Region Proposals:  They localize landmarks per face region.",
      "Then they compute a more appropriate face bounding box based on the localized landmarks.",
      "They feed that new bounding box through the network.",
      "They compute the face score (face / not face, i.e. number between 0 and 1) for both bounding boxes and choose the one with the higher score.",
      "This shrinks down bounding boxes that turned out to be too big.",
      "The method visualized:  Landmarks-based Non-Maximum Suppression:  When multiple detected face bounding boxes overlap, one has to choose which of them to keep.",
      "A method to do that is to only keep the bounding box with the highest face-score.",
      "They instead use a median-of-k method.",
      "Their steps are:  Reduce every box in size so that it is a bounding box around the localized landmarks.",
      "For every box, find all bounding boxes with a certain amount of overlap.",
      "Among these bounding boxes, select the k ones with highest face score.",
      "Based on these boxes, create a new box which's size is derived from the median coordinates of the landmarks.",
      "Compute the median values for landmark coordinates, landmark visibility, gender, pose and use it as the respective values for the new box.",
      "Results  Example results:  They test on AFW, AFWL, PASCAL, FDDB, CelebA.",
      "They achieve the best mean average precision values on PASCAL and AFW (compared to selected competitors).",
      "AFW results visualized:  Their approach achieve good performance on FDDB.",
      "It has some problems with small and/or blurry faces.",
      "If the feature fusion is removed from their approach (i.e. extracting features only from one fully connected layer at the end of the base network instead of merging feature maps from different convolutional layers), the accuracy of the predictions goes down.",
      "Their architecture ends in 5 shallow networks and shares many layers before them.",
      "If instead these networks share no or few layers, the accuracy of the predictions goes down.",
      "The postprocessing of bounding boxes (via Iterative Region Proposals and Landmarks-based Non-Maximum Suppression) has a quite significant influence on the performance.",
      "Processing time per image is 3s, of which 2s is the selective search algorithm (for the bounding boxes)."
    ],
    "author_id": "ALEJU",
    "pdf_url": "http://arxiv.org/pdf/1603.01249v2",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 57956236
  },
  {
    "blog_id": "generative_moment_matching_networks",
    "summary": [
      "What  Generative Moment Matching Networks (GMMN) are generative models that use maximum mean discrepancy (MMD) for their objective function.",
      "MMD is a measure of how similar two datasets are (here: generated dataset and training set).",
      "GMMNs are similar to GANs, but they replace the Discriminator with the MMD measure, making their optimization more stable.",
      "How  MMD calculates a similarity measure by comparing statistics of two datasets with each other.",
      "MMD is calculated based on samples from the training set and the generated dataset.",
      "A kernel function is applied to pairs of these samples (thus the statistics are acutally calculated in high-dimensional spaces).",
      "The authors use Gaussian kernels.",
      "MMD can be approximated using a small number of samples.",
      "MMD is differentiable and therefor can be used as a standard loss function.",
      "They train two models:  GMMN: Noise vector input (as in GANs), several ReLU layers into one sigmoid layer.",
      "MMD as the loss function.",
      "GMMN+AE: Same as GMMN, but the sigmoid output is not an image, but instead the code that gets fed into an autoencoder's (AE) decoder.",
      "The AE is trained separately on the dataset.",
      "MMD is backpropagated through the decoder and then the GMMN.",
      "I.e. the GMMN learns to produce codes that let the decoder generate good looking images.",
      "MMD formula, where x_i is a training set example and y_i a generated example.",
      "Architectures of GMMN (left) and GMMN+AE (right).",
      "Results  They tested only on MNIST and TFD (i.e. datasets that are well suited for AEs...).",
      "Their GMMN achieves similar log likelihoods compared to other models.",
      "Their GMMN+AE achieves better log likelihoods than other models.",
      "GMMN+AE produces good looking images.",
      "GMMN+AE produces smooth interpolations between images.",
      "Generated TFD images and interpolations between them.",
      "Rough chapter-wise notes  (1) Introduction  Sampling in GMMNs is fast.",
      "GMMNs are similar to GANs.",
      "While the training objective in GANs is a minimax problem, in GMMNs it is a simple loss function.",
      "GMMNs are based on maximum mean discrepancy.",
      "They use that (implemented via the kernel trick) as the loss function.",
      "GMMNs try to generate data so that the moments in the generated data are as similar as possible to the moments in the training data.",
      "They combine GMMNs with autoencoders.",
      "That is, they first train an autoencoder to generate images.",
      "Then they train a GMMN to produce sound code inputs to the decoder of the autoencoder.",
      "(2) Maximum Mean Discrepancy  Maximum mean discrepancy (MMD) is a frequentist estimator to tell whether two datasets X and Y come from the same probability distribution.",
      "MMD estimates basic statistics values (i.e. mean and higher order statistics) of both datasets and compares them with each other.",
      "MMD can be formulated so that examples from the datasets are only used for scalar products.",
      "Then the kernel trick can be applied.",
      "It can be shown that minimizing MMD with gaussian kernels is equivalent to matching all moments between the probability distributions of the datasets.",
      "(4) Generative Moment Matching Networks  Data Space Networks  Just like GANs, GMMNs start with a noise vector that has N values sampled uniformly from [-1, 1].",
      "The noise vector is then fed forward through several fully connected ReLU layers.",
      "The MMD is differentiable and therefor can be used for backpropagation.",
      "Auto-Encoder Code Sparse Networks  AEs can be used to reconstruct high-dimensional data, which is a simpler task than to learn to generate new data from scratch.",
      "Advantages of using the AE code space:  Dimensionality can be explicitly chosen.",
      "Disentangling factors of variation.",
      "They suggest a combination of GMMN and AE.",
      "They first train an AE, then they train a GMMN to generate good codes for the AE's decoder (based on MMD loss).",
      "For some reason they use greedy layer-wise pretraining with later fine-tuning for the AE, but don't explain why.",
      "(That training method is outdated?)",
      "They add dropout to their AE's encoder to get a smoother code manifold.",
      "Practical Considerations  MMD has a bandwidth parameter (as its based on RBFs).",
      "Instead of chosing a single fixed bandwidth, they instead use multiple kernels with different bandwidths (1, 5, 10, ...), apply them all and then sum the results.",
      "Instead of MMD^2 loss they use sqrt(MMD^2), which does not go as fast to zero as raw MMD, thereby creating stronger gradients.",
      "Per minibatch they generate a small number of samples und they pick a small number of samples from the training set.",
      "They then compute MMD for these samples.",
      "I.e. they don't run MMD over the whole training set as that would be computationally prohibitive.",
      "(5) Experiments  They trained on MNIST and TFD.",
      "They used an GMMN with 4 ReLU layers and autoencoders with either 2/2 (encoder, decoder) hidden sigmoid layers (MNIST) or 3/3 (TFD).",
      "They used dropout on the encoder layers.",
      "They used layer-wise pretraining and finetuning for the AEs.",
      "They tuned most of the hyperparameters using bayesian optimization.",
      "They use minibatch sizes of 1000 and compute MMD based on those (i.e. based on 2000 points total).",
      "Their GMMN+AE model achieves better log likelihood values than all competitors.",
      "The raw GMMN model performs roughly on par with the competitors.",
      "Nearest neighbor evaluation indicates that it did not just memorize the training set.",
      "The model learns smooth interpolations between digits (MNIST) and faces (TFD)."
    ],
    "author_id": "ALEJU",
    "pdf_url": "http://arxiv.org/pdf/1502.02761",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 80719489
  },
  {
    "blog_id": "large-scale-image-captioning-a2c0191ffd3c",
    "summary": [
      "Image captioning involves the task of generating natural language descriptions of visual content with the use of datasets comprising of image-caption pairs.",
      "The figure below visually demonstrates the task and the datasets used in this type of work along with some examples.",
      "For instance, the second image on the left shows a child sitting on a couch, which can also be inferred from the accompanying image-caption shown in the example.",
      "On the left-hand side, we have image-caption examples obtained from COCO , which is a very popular object-captioning dataset.",
      "nocaps (shown on the right) is the benchmark dataset proposed in this paper and includes three different settings: in-domain (only COCO classes), near-domain (COCO and novel classes), and out-of-domain (only novel classes).",
      "These settings are explained later, for now, you only need to be concerned with the fact that the proposed dataset, nocaps, aims to complement current image captioning datasets, such as COCO, as opposed to completely replacing them.",
      "The challenge with current image captioning models is that they generalize poorly to images in the wild.",
      "This happens because most models are trained to capture a tiny amount of visual concepts as compared to what a human may encounter in everyday life.",
      "Take the COCO dataset, for example, models trained on it can only describe images containing dogs and umbrellas, but not dolphins.",
      "In order to build more robust real-world applications, such as an assistant for people with impaired vision, the above limitations need to be addressed.",
      "Specifically, large-scale classes of objects need to be supported to generalize better on an image captioning task.",
      "The proposed work supports 500+ novel classes, a huge improvement compared to the 80 classes found in COCO.",
      "This paper aims to develop image captioning models that learn visual concepts from alternative data sources such as object detection datasets.",
      "One of those large-scale object detection datasets is Open Images V4 .",
      "The training dataset for the benchmark consists of a combination of COCO and Open Images V4 training sets.",
      "Keep in mind that no extra image-caption pairs are provided besides those found in COCO since the Open Images V4 training portion only consists of images annotated with bounding boxes.",
      "The validation and test set are comprised of images from the Open Images object detection dataset.",
      "Overall, the authors propose a benchmark with 10 reference captions per image and many more visual concepts as contained in COCO.",
      "In addition, 600 classes are incorporated via the object detection dataset, which is significantly larger than COCO which contains only 80 object classes.",
      "Each selected image was captioned by 11 AMT workers via caption collection interfaces as shown in the Figure below.",
      "Note that priming refers to the technique where workers are given a small guide (in this case labels) to help with annotating rare images.",
      "In summary, as compared to COCO captions, the proposed benchmark, nocaps, have greater visual diversity, more object classes per image, and longer and more diverse captions (with large vocabulary).",
      "See paper for more information on how both the dataset and benchmark are prepared.",
      "The benchmark system utilizes COCO paired image-caption data to learn to generate syntactically correct captions while leveraging Open Images object detection dataset to learn more visual concepts.",
      "In essence, the COCO dataset is the only image-caption information considered for training, while captions from nocaps validation set are used for validation and testing datasets.",
      "One of the aims of the nocaps benchmark is to increase the difficulty of the image captioning task by increasing diversity of captions and images.",
      "However, the authors note that the performance, obtained with automatic evaluation metrics, is weaker than the human baseline.",
      "But the hope is to improve interpretation of results and obtain more insights.",
      "The authors propose to investigate two popular methods for object captioning on their benchmark: Neural Baby Talk (NBT) and Up-Down, with and without constrained beam search (CBS).",
      "A Faster R-CNN model is trained on image feature representations extracted from both the Visual Genome and Open Image datasets.",
      "As a reminder, with COCO, it is very common to use object detection features trained on Visual Genome since the images are sourced from COCO.",
      "Specifically, VG features refer to the use of Visual Genome alone and VGOI refers to the combination of Visual Genome and Open Images datasets.",
      "(Learn more about the experimental setup from the paper).",
      "The experimental results are reported in the table above.",
      "We can observe that the Up-Down model, with VG features alone (row 1), perform better than when using VGOI, perhaps indicating that classes in Open Images may be a lot more sparse, increasing the complexity of the task.",
      "Results from the Neural Baby Talk (NBT) model can also be observed to be lower than the Up-Down model.",
      "However, both methods are outperformed by the \u201cHuman\u201d model, particularly for the nocaps validation dataset.",
      "You can find a more detailed discussion of the results in the paper.",
      "Finally, below we can observe a few image examples from nocaps with the generated captions produced by each model type.",
      "The in-domain model (trained only on COCO) fails to identify novel objects such as gun/rifle and insect/centipede due to the shortage of visual concepts as explained earlier.",
      "Near-domain means that both object classes from COCO and Open Images were used.",
      "Out-of-domain means that no COCO classes were used.",
      "For both near-domain and out-of-domain images, the captions are somewhat better but still need improvement.",
      "Overall, the performance of the benchmark models, which use the nocap dataset, improve marginally over a strong baseline but fall short when compared to the human baseline, which means there is still room for improvement in image-captioning tasks.",
      "Reference  nocaps: novel object captioning at scale \u2014 Harsh Agrawal, Karan Desai, Xinlei Chen, Rishabh Jain, Dhruv Batra, Devi Parikh, Stefan Lee, Peter Anderson"
    ],
    "author_id": "DAIR-AI",
    "pdf_url": "https://arxiv.org/pdf/1812.08658",
    "author_full_name": "Elvis Saravia",
    "source_website": "https://github.com/dair-ai/nlp_paper_summaries",
    "id": 8363020
  },
  {
    "blog_id": "the-semantic-elegance-of-applicative-languages",
    "summary": [
      "The Semantic Elegance of Applicative Languages \u2013 Turner \u201981.",
      "Here\u2019s a paper you can enjoy simply for its prose!",
      "In what does the alleged superiority of applicative languages consist?",
      "In what indeed!",
      "And while we\u2019re at it, what\u2019s an applicative language?",
      "I looked up a few definitions; if we call it a functional language I don\u2019t think we\u2019ll go too far wrong.",
      "Let us resume\u2026  In what does the alleged superiority of applicative languages consist?",
      "In the last analysis the answer must be in terms of the reduction in the time required to produce a correct program to solve a given problem.",
      "On reflection I decided that the best way to demonstrate this would be to take some reasonably non-trivial problem and show how, by proceeding within a certain kind of applicative language framework it was possible to develop a  working solution with a fraction of the effort that would have been necessary in a conventional imperative language.",
      "The cynical among you might be thinking: he\u2019s picked a problem that\u2019s well suited to be solved with a functional programming approach, and then shown that the functional approach is a good way of solving it!",
      "Even if that is true, it\u2019s still a joy to get an insight into the mind of an advanced functional programmer.",
      "It reminds me of one of those chess books where a great player explains what they were thinking move by move in an annotated game.",
      "The problem is to enumerate all of the possible paraffin molecules.",
      "These are made of carbon C and hydrogen H atoms.",
      "Each C makes 4 bonds, and paraffins don\u2019t allow double bonds or cycles.",
      "H                H   H     |                |   | H - C - H   ,    H - C - C - H    , ...     |                |   |     H                H   H  See the problem statement and example in the paper \u2013 it is succintly explained, and for maximum value you should pause and spend some time thinking how you would solve it before reading on.",
      "The tricky parts come when the \u2018C\u2019s themselves start to make complex shapes (T\u2019s etc.",
      "), and you need to weed out duplicates through symmetries.",
      "from the point of view  of a programmer who had not tried it before, the problem seemed difficult enough to be interesting.",
      "At least several competent programmers I know reported to me that they had found it so.",
      "Central to the solution is figuring out when two molecules are equivalent.",
      "Turner defines operations \u2018invert\u2019, \u2018rotate\u2019 and \u2018swap\u2019 which make structure-preserving changes to the shape of a molecule.",
      "There then follows a very elegant solution based on determining a \u2018closure under laws.\u2019  -- a and b are equivalent if b is a member of the set of  -- all equivalent representations of a...   equiv a b = member (equivclass a) b  equivclass a = closure_under_laws [rotate, invert, swap] [a]  The key idea is embodied in the function \u201cclosure under laws\u201d which takes a set of functions and a set of objects and finds the closure of the latter under repeated applications of the members of the former.",
      "With another neat (and commonly used) functional technique of generating an infinite list of molecules from which we can display as many as we like, the initial version of the program is complete.",
      "This solution, however, runs with appalling slowness (I tried it) mainly because of easily removable inefficiencies in our definition of \u201cpara\u201d.",
      "There is a minor problem and a major problem\u2026  The major problem is due to repeated calculation of the same sub-molecules many times over.",
      "Failure to memoise leads to an exponential deterioration in performance!",
      "For a recursive function, like \u201cpara\u201d, memoisation leads to an exponential improvement in performance.",
      "(or to put it another way, failure to memoise leads an exponential deterioration in performance!)",
      "There follows some discussion of the author\u2019s lessons learned during this exercise:  the effort needed to derive a solution can be reduced to a small fraction of that required in a traditional programming language  an applicative language (even if it\u2019s not the language used for the ultimate implementation) can be an extremely valuable tool for the development and testing of algorithms  the language supports very nicely a separation of concerns in which you first make things correct without worrying about efficiency, and then repair efficiency by applying transformations known to preserve correctness:  In a surprising large number of cases it turns out that a small number of standard optimisations are sufficient to bring about the necessary improvement in performance.",
      "Two in particular seem to be of such general applicability as to deserve special mention in a next (and final) section of this paper\u2026  And what are these two methods?",
      "Memoisation (discussed earlier), and filter promotion.",
      "Filter promotion is the idea that instead of generating a list containing terms we don\u2019t want, and then filtering them out (e.g. filter pred xs), push the filter predicate down into the list generator so that we don\u2019t generate the unwanted terms in the first place.",
      "We can get a considerable improvement in performance, however, by pushing the filter inside the generator \u201cpartitions\u201d so that the unwanted lists are not created in the first place  It\u2019s a short read, and hard to communicate the value in a summary.",
      "I hope I have whetted your appetite to go and read the whole paper.",
      "If you want to play around with the code, I ported the solution to Haskell here .",
      "Though I bet you can improve on my code given I\u2019m not especially fluent in Haskell\u2026"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://nsl.com/misc/sasl/paraffins-turner.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 88700162
  },
  {
    "blog_id": "face_attribute_prediction_using_off-the-shelf_cnn_features",
    "summary": [
      "What  When using pretrained networks (like VGG) to solve tasks, one has to use features generated by these networks.",
      "These features come from specific layers, e.g. from the fully connected layers at the end of the network.",
      "They test whether the features from fully connected layers or from the last convolutional layer are better suited for face attribute prediction.",
      "How  Base networks  They use standard architectures for their test networks, specifically the architectures of FaceNet and VGG (very deep version).",
      "They modify these architectures to both use PReLUs.",
      "They do not use the pretrained weights, instead they train the networks on their own.",
      "They train them on the WebFace dataset (350k images, 10k different identities) to classify the identity of the shown person.",
      "Attribute prediction  After training of the base networks, they train a separate SVM to predict attributes of faces.",
      "The datasets used for this step are CelebA (100k images, 10k identities) and LFWA (13k images, 6k identities).",
      "Each image in these datasets is annotated with 40 binary face attributes.",
      "Examples for attributes: Eyeglasses, bushy eyebrows, big lips, ...",
      "The features for the SVM are extracted from the base networks (i.e. feed forward a face through the network, then take the activations of a specific layer).",
      "The following features are tested:  FC2: Activations of the second fully connected layer of the base network.",
      "FC1: As FC2, but the first fully connected layer.",
      "Spat 3x3: Activations of the last convolutional layer, max-pooled so that their widths and heights are both 3 (i.e. shape Cx3x3).",
      "Spat 1x1: Same as \"Spat 3x3\", but max-pooled to Cx1x1.",
      "Results  The SVMs trained on \"Spat 1x1\" performed overall worst, the ones trained on \"Spat 3x3\" performed best.",
      "The accuracy order was roughly: Spat 3x3 > FC1 > FC2 > Spat 1x1.",
      "This effect was consistent for both networks (VGG, FaceNet) and for other training datasets as well.",
      "FC2 performed particularly bad for the \"blurry\" attribute (most likely because that was unimportant to the classification task).",
      "Accuracy comparison per attribute:  The conclusion is, that when using pretrained networks one should not only try the last fully connected layer.",
      "Many characteristics of the input image might not appear any more in that layer (and later ones in general) as they were unimportant to the classification task."
    ],
    "author_id": "ALEJU",
    "pdf_url": "http://arxiv.org/pdf/1602.03935v2",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 62769214
  },
  {
    "blog_id": "horde",
    "summary": [
      "A key idea pushed in this paper is that a value function represents semantic knowledge.",
      "Indeed, they state, \u201cA value function asks a question\u2013what will the cumulative reward be?\u2013and an approximate value function provides an answer to that question\u201d.",
      "Accordingly, they introduce Generalized Value Functions (GVFs), a construct to expand the knowledge that value functions can encapsulate to make them capable of representing knowledge about the world.",
      "A GVF is parameterized with four functions, a policy, pseudo-reward function, pseudo-terminal reward function, and pseudo-termination function, called question functions.",
      "They introduce Horde, an architecture for learning 1 or more approximate GVFs in parallel, where each \u201cdemon\u201d of the Horde is responsible for learning a piece of knowledge that contributes to the whole.",
      "Approximate GVFs can be learned off-policy.",
      "The paper uses GQ($\\lambda$) to train each demon, and hence a feature vector $\\phi$, behavior policy $b$, and eligibilty trace function $\\lambda$ must be specified; these are collectively called answer functions, since they are used to numerically find the value of approximate GVFs (answering the \u201cquestion\u201d).",
      "They show that a physical robot with many sensors is able to learn to predict how many steps it can go before needing to stop before hitting a wall (via 1 \u201cpredictive\u201d demon, i.e., a demon that seeks to accurately \u201cpredict\u201d the cumulative return by learning the approximate GVF for a given policy).",
      "The robot also uses 8 control demons to learn to separately maximize returns for \u201cmaxing out\u201d 8 different sensors.",
      "Finally, they trained 1 control demon to learn a light-seeking behavior.",
      "Recently, Barreto, et.",
      "al 2017 developed the ideas of successor features (SF), a value function representation that decouples environment dynamics from the reward function.",
      "They use it to show transfer between tasks.",
      "They discuss how their method is a special case of a GVF, but that it provides a method for \u201cselecting\u201d pseudo-rewards.",
      "This paper differs from the options framework in that options essentially define a hierarchy of policy abstractions.",
      "The authors note that GVFs could be combined with this approach, however."
    ],
    "author_id": "pemami",
    "pdf_url": "https://www.cs.swarthmore.edu/~meeden/DevelopmentalRobotics/horde1.pdf",
    "author_full_name": "Patrick Emami",
    "source_website": "https://pemami4911.github.io/index.html",
    "id": 8799962
  },
  {
    "blog_id": "tracking-occluded-objects-by-reasoning-about-containment",
    "summary": [
      "This paper looks at tracking severely occluded objects in long video sequences.",
      "I like this passage:  Although some recent work adopted deep neural networks to extract contexts for object detection and tracking, these data-driven feedforward methods have well-known problems: i) They are black-box models that cannot be explained and only applicable with supervised training by fitting the typical context of the object, thus difficult to generalize to new tasks.",
      "ii) Lacking explicit representation to handle occlusions, low resolution, and lighting variations\u2014there are millions of ways to occlude an object in a given image  (Wang et al. 2017), making it impossible to have enough data for training and testing such black box models.",
      "In this paper, we go beyond passive recognition by reasoning about time-varying containment relations.",
      "They look at containment relations induced by human activity.",
      "A containment relation occurs when an object contains or holds another object, obscuring it from view.",
      "Contained objects have the same trajectories as the container.",
      "They use an idea that I have thought about as well and think is powerful; rather than only relying on detections for next state hypotheses, they suggest alternative hypotheses based on occlusion reasoning.",
      "The other hypotheses come from containment relations and blocked relations; if an object is contained, it inherits the track state of its container.",
      "If an object is blocked, its track state is considered stationary (not always true!).",
      "This tracking scenario is unique because they only consider human action as the source of state change (i.e., this approach doesn\u2019t apply to general ped or vehicle tracking).",
      "They use state-of-the-art detection and activity recognition to carry out object tracking.",
      "They use the network flow approach for occlusion-aware data association in a sliding window.",
      "I think their algorithm has to decide between containment and blocked occlusion events via the activity recognition.",
      "They note that their approach is limited by how good object detection is.",
      "I think an important direction to consider is when external forces other than people can act on the object.",
      "This requires a lot more complex modeling of what an occluded object might be doing.",
      "Also, we need better object detection!",
      "Interesting related works  In Zhang, Li, and Nevatia 2008 , they use an Explicit Occlusion Model (EOM) in the network flow data association model.",
      "Good potential baseline."
    ],
    "author_id": "pemami",
    "pdf_url": "https://pdfs.semanticscholar.org/4170/0dc1e60f5c8eaef409ef014f37c8b9e1b8cd.pdf",
    "author_full_name": "Patrick Emami",
    "source_website": "https://pemami4911.github.io/index.html",
    "id": 63755811
  },
  {
    "blog_id": "neural-module-networks",
    "summary": [
      "For the task of Visual Question Answering , decompose a question into its linguistic substructures and train a neural network module for each substructure.",
      "Jointly train the modules and dynamically compose them into deep networks which can learn to answer the question.",
      "Start by analyzing the question and decide what logical units are needed to answer the question and what should be the relationship between them.",
      "The paper also introduces a new dataset for Visual Question Answering which has challenging, highly compositional questions about abstract shapes.",
      "Inspiration  Questions tend to be compositional.",
      "Different architectures are needed for different tasks - CNNs for object detection, RNNs for counting.",
      "Recurrent and Recursive Neural Networks also use the idea of a different network graph for each input.",
      "Neural Module Network for VQA  Training samples of form (w, x, y)  w - Natural Language Question  x - Images  y - Answer  Model specified by collection of modules {m} and a network layout predictor P.  Model instantiates a network based on P(w) and uses that to encode a distribution P(y|w, x, model_params)  Modules  Find: Finds objects of interest.",
      "Transform: Shift regions of attention.",
      "Combine: Merge two attention maps into a single one.",
      "Describe: Map a pair of attention and input image to a distribution over the labels.",
      "Measure: Map attention to a distribution over the labels.",
      "Natural Language Question to Networks  Map question to the layout which specifies the set of modules and connections between them.",
      "Assemble the final network using the layout.",
      "Parse the input question to obtain set of dependencies and obtain a representation similar to combinatory logic.",
      "eg \u201cwhat is the colour of the truck?\u201d becomes \u201ccolour(truck)\u201d  The symbolic representation is mapped to a layout:  All leaves become find module.",
      "All internal nodes become transform/combine module.",
      "All root nodes become describe/measure module.",
      "Answering Natural Language Question  Final model combines output from a simple LSTM question encoder with the output of the neural module network.",
      "This helps in modelling the syntactic and semantic regularities of the question.",
      "Experiments  Since some modules are updated more frequently than others, adaptive per weight learning rates are better.",
      "The paper introduces a small SHAPES datasets (64 images and 244 unique questions per image).",
      "Neural Module Network achieves a score of 90% on SHAPES dataset while VIS + LSTM baseline achieves an accuracy of 65.3%.",
      "Even on natural images (VQA dataset), the neural module network outperforms the VIS + LSTM baseline."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1511.02799",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 98238949
  },
  {
    "blog_id": "the-lottery-ticket-hypothesis-training-pruned-neural-networks",
    "summary": [
      "Empirical evidence indicates that at training time, the neural networks need to be of significantly larger size than necessary.",
      "The paper purposes a hypothesis called the lottery ticket hypothesis to explain this behaviour.",
      "The idea is the following - Successful training of a neural network depends on a lucky random initialization of a subcomponent of the network.",
      "Such components are referred to as lottery tickets.",
      "Larger networks are more likely to have these lottery tickets and hence are easier to train.",
      "Methodology  Various aspects of the hypothesis are explored empirically.",
      "Two tasks are considered - MNIST and XOR.",
      "For each task, the paper considers networks of different sizes and empirically shows that larger networks are more likely to converge (or have better performance) for a fixed number of epochs as compared to the smaller networks.",
      "Given a large, trained network, some weights (or units) of the network are pruned and the resulting network is reset to its initial random weights.",
      "The resulting network is the lottery-ticket in the sense that when the pruned network is trained, it is more likely to converge than an otherwise randomly initialised network of the same size.",
      "Further, it is more likely to match the original, larger network in terms of performance.",
      "The paper explores different aspects of this experiment:  Pruning Strategies:  One-shot strategy prunes the network in one-go while the iterative strategy prunes the network iteratively.",
      "Though the latter is computationally more intensive, it is more likely to find a lottery ticket.",
      "Size of the pruned network affects the speed of convergence when training the lottery ticket.",
      "If only the architecture or only the initial weights of the lottery ticket are used, the resulting network tends to converge more slowly and achieves a lower level of performance.",
      "This indicates that the lottery ticket depends on both the network architecture and the weight initialization.",
      "Discussion  The paper includes some more interesting experiments.",
      "For instance, the distribution of the initialization in the weights that survived the pruning suggests that small weights from before training tend to remain small after training.",
      "One interesting experiment would be to show the performance of the pruned network before resetting its weights and retraining again.",
      "This performance should be compared with the performance of the initial large network and the performance of the lottery ticket after training.",
      "Overall, the experiments are not sufficient to conclude anything about the correctness of the hypothesis.",
      "The proposition itself is very interesting and could enhance our understanding of how the neural networks work."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1803.03635",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 66006367
  },
  {
    "blog_id": "a3c",
    "summary": [
      "The authors presented a number of asynchronous DRL algorithms with the intention of developing RL agents that can be trained on CPUs with multithreading.",
      "The algorithms used multiple threads to run copies of the environment and generate uncorrelated sequences of training samples.",
      "Parameters were then sent to a shared parameter server at regular intervals.",
      "Because this promotes non-stationarity for the sequences of SARSA tuples, experience replay is not necessarily needed.",
      "The implementations of RMSProp and Momentum SGD used by the authors employed a Hogwild!-inpsired lock free scheme for maximum efficiency.",
      "A3C is the \u201cbest\u201d agent that was presented in this paper.",
      "It is an asynchronous advantage actor-critic algorithm.",
      "It maintains an approximation of the policy, an estimate of the value function, and computes an \u201cadvantage\u201d function and a variance-reducing baseline Degris, et al. 2012 .",
      "An entropy regularization term was also used to discourage premature convergence.",
      "Notes  Implemented 1-step Q-learning, 1-step SARSA, n-step Q-learning, and Advantage Actor-Critic  A benefit is that learning is stabilized without having to use experience replay  Reduction in training time that is roughly linear in the number of parallel actor-learners.",
      "Is this good?",
      "Seems like we can do better.",
      "The RL algorithms used are fairly data-inefficient.",
      "Not really a noticeable difference in performance between n-step Q-learning and 1-step, except on TORCS.",
      "A3C uses n-step lookahead as well  Evidence  State-of-the-art results were obtained on some of the Atari games (ALE).",
      "An LSTM-based A3C agent was tested with Deepmind\u2019s Labyrinth environment.",
      "They also tested on the TORCS car racing environment and MuJoCo, the continuous-space physics simulation engine.",
      "Strengths  Presenting the algorithms in pseudocode is very helpful to the reader.",
      "The authors went into implementation details, which is also helpful for those who wish to check the results for themselves.",
      "Future Directions  How can we reduce/control the asymptotic variance of these temporal difference methods?",
      "Also, of the actor-critic method?",
      "(Need to research this topic more).",
      "Dueling Networks for the state value and advantage functions  Reducing over-estimation bias of Q-values (Double DQN, etc)  Try comparing with async Recurrent-DDPG?",
      "A3C seems similar to DDPG- difference is with the deterministic vs. stochastic gradient update."
    ],
    "author_id": "pemami",
    "pdf_url": "http://arxiv.org/pdf/1602.01783v1.pdf",
    "author_full_name": "Patrick Emami",
    "source_website": "https://pemami4911.github.io/index.html",
    "id": 4353776
  },
  {
    "blog_id": "ccec626e68e495fd4577ecdca36b7b",
    "summary": [
      "The paper proposes a new RNN Encoder-Decoder architecture that can improve the performance of statistical machine translation (SMT) systems.",
      "RNN Encoder-Decoder  Model consists of two RNNs  Encoder  Learns to encode a variable-length input sequence into a fixed-length vector representation.",
      "Decoder  Learns to decode a given fixed-length vector representation into a variable-length target sequence.",
      "Two networks are trained jointly to maximise the conditional probability of the target sequence given the input source sequence.",
      "Trained model can be used to:  generate a target sequence, given an input sequence.",
      "score a given pair of input and output sequences.",
      "Hidden Unit that adaptively remembers and forgets.",
      "Hidden unit updated to have a  reset gate that adaptively drop any hidden state information that it finds irrelevant.",
      "update gate that controls how much information from the previous state to carry over.",
      "Each hidden unit has separate reset and update gates which improve the memory capacity and makes it easier to train.",
      "Statistical Machine Translation (SMT)  In the phrase-based SMT framework, the translation model is factorised into the translation probabilities of matching phrases in the source and target sentences.",
      "RNN Encoder-Decoder can be used to rescore the phrase pairs in the phrase table  Experiments  Details  1000 hidden units.",
      "Activation function in proposed hidden unit - hyperbolic tangent function  Non-recurrent weights initialized by sampling from an isotropic Gaussian distribution (mean = 0, sd = 0.01)  Recurrent weights initialized by sampling from white Gaussian distribution and using its left singular vectors.",
      "Adadelta and SGD  Observations  Train the model to translate an English phrase to French phrase.",
      "Using the model to score phrase pairs in the standard phrase-based SMT system improves the translation performance.",
      "Train a CSLM (Continuous Space Language Model) and compare phrase scores from trained model with those given by CSLM.",
      "RNN Encoder\u2013Decoder is better at capturing the linguistic regularities in the phrase table.",
      "RNN Encoder-Decoder learns a continuous space representation for phrases that preserves both the semantic and syntactic structure.",
      "This comment has been minimized.",
      "Sign in to view  Copy link  Quote reply  shahbazsyed commented  Mar 23, 2017  Hi,  Thanks for the gist!",
      "Can you kindly explain what is the \"fixed-length vector representation\" of the input sequence which is generated by the encoder?",
      "Is it a concatenation of all the word vectors in a given sentence ?",
      "This comment has been minimized.",
      "Sign in to view  Copy link  Quote reply  varunjanga commented  Apr 27, 2017  Hi shahbazsyed, if I understand it correctly input sequence can be variable length sequence of words which will be provided as an input to encoder to output fixed-length vector representation which is basically some fixed length(say 128) vector of numbers."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1406.1078",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 81916449
  },
  {
    "blog_id": "z-forcing",
    "summary": [
      "Intro  A new training procedure for recurrent VAEs is proposed.",
      "Recall that for VAEs, we model a joint distribution over observations $x$ and latent variables $z$, and assume that $z$ is involved in the generation of $x$.",
      "This distribution is parameterized by $\\theta$.",
      "Maximizing the marginal log-likelihood $p_{\\theta}(x)$ wrt $\\theta$ is intractable bc it requires integrating over $z$.",
      "Instead, introduce a variational distribution $q_{\\phi}(z|x)$ and maximize a lower bound on the marginal log-likelihood\u2013the ELBO.",
      "Stochastic recurrent networks  When applying VAEs to sequences, it has been proposed to use recurrent networks for the recognition network (aka inference network aka variation posterior) and the generation network (aka decoder aka conditional probability of the next observation given previous observations and latents).",
      "These probabilistic models can be autoregressive (in this paper, they use LSTMs with MLPs for predicting the parameters of Gaussian distributions).",
      "It is common to model these conditional distributions with Gaussians for continuous variables or categoricals for discrete variables.",
      "Usually, the prior over latent variables is also learned with a parametric model.",
      "If I\u2019m not mistaken, learning the parameters of these parametric models with a training data set, and the using them at test time for fast inference is referred to as amortized variational inference, which appears to have correlaries in our cognition .",
      "Z-forcing  Strong autoregressive decoders overpower the latent variables $z$, preventing the CPD from learning complex multi-modal distributions.",
      "To mitigate this, they introduce an auxiliary cost to the training objective.",
      "An extra parametric model is introduced, $p_{\\eta}(b | z)$, that \u201cforces\u201d the latents to be predictive of the hidden states $b$ of the \u201cbackward network\u201d (the inference network).",
      "Experiments  They validate the approach on speech modeling (TIMIT, Blizzard) and language modeling.",
      "The metric is average LL.",
      "On Seqeuential MNIST, z-forcing is competitive with \u201cdeeper\u201d recurrent generative models like PixelRNN.",
      "Some fun language modeling results interpolating the latent space  Takeaways  It\u2019s always a consideration as to whether increasing the complexity of an approach (adding an extra network and auxiliary cost) is worth the effort vs. simpler approaches that can get almost the same performance.",
      "The results on TIMIT and Blizzard are pretty convincing.",
      "The authors also suggest incorporating the auxiliary loss with PixelRNN/CNN in future work."
    ],
    "author_id": "pemami",
    "pdf_url": "http://papers.nips.cc/paper/7248-z-forcing-training-stochastic-recurrent-networks.pdf",
    "author_full_name": "Patrick Emami",
    "source_website": "https://pemami4911.github.io/index.html",
    "id": 66226432
  },
  {
    "blog_id": "4dbe1aa678188399254bb3d0078e1d",
    "summary": [
      "The authors train a character-RNN (using mLSTM units) over Amazon Product Reviews (82 million reviews) and use the char-RNN as the feature extractor for sentiment analysis.",
      "These unsupervised features beat state of the art results for the dataset while are outperformed by supervised approaches on other datasets.",
      "Most important observation is that the authors find a single neuron (called as the sentiment neuron) which alone achieves a test accuracy of 92.3% thus giving the impression that the sentiment concept has been captured in that single neuron.",
      "Switching this neuron on (or off) during the generative process produces positive (or negative) reviews.",
      "Notes  The paper aims to evaluate if the low level features captured by char-RNN can support learning of high-level representations.",
      "Link to the blog by OpenAI  The paper mentions two possible reasons for weak performance of purely unsupervised networks:  Distributional issues - Sentence vectors trained on books may not generalise to product reviews.",
      "Limited Capacity of models - Resulting in representational underfitting.",
      "Single layer with 4096 units.",
      "Multiplicative LSTM units are used instead of standard LSTM units as they are observed to converge faster.",
      "Compact model with a high ratio of compute to total params (1.12 buts per byte)  L1 penalty is used instead of L2 as it reduces sample complexity when there are many irrelevant features.",
      "Found a single neuron (sentiment neuron) which alone captures most of the sentiment concept.",
      "Capacity Ceiling  Even increasing the dataset by 4 orders of magnitude leads to a very small improvement in accuracy (~1%).",
      "One possbile reason could be the change in data distribution - trained on Amazon Reviews and tested on Yelp Reviews.",
      "Similary, the linear model (trained on top of feature vectors) has its own limitations in terms of capacity.",
      "The model does not work well on out of domain tasks like semantic relatedness over image descriptions.",
      "The paper shows that positive (or negative) reviews can be generated by switching the sentiment neuron on (or off) during the generative process.",
      "A tweet by @AlecRad says that zeroing the sentiment neuron drops the performance only by 2% on SST and 10% on IMDB indicating that the network has still learnt a distributed representation.",
      "Open Questions  Is this phenomenon of disentangling of high level concepts specific to sentiment analysis?",
      "How do we explain the compression of almost all the sentiment in a single unit?",
      "Use of hierarchial models for increasing the capacity of char-RNN."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1704.01444",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 39570633
  },
  {
    "blog_id": "rainbow",
    "summary": [
      "What  They combine several previous improvements for reinforcement learning to one algorithm.",
      "The combination beats previous methods by a good margin.",
      "They analyze which of the used improvements has most influence on the result.",
      "How  They use the following improvements:  Double Q-learning  Uses two networks during training.",
      "One predicts Q-values, the other is updated.",
      "Usually done by using a copy with freezed parameters.",
      "Prioritized Replay  Samples experiences from the replay memory based on the difference between predicted and real Q-values.",
      "Recent experiences also get higher priority.",
      "Dueling Networks  Splits the Q-value prediction into a value stream (mean value over all values) and an advantage stream (advantage of specific actions over others).",
      "Multi-Step Learning  Splits direct reward + Q(next state, next action) into direct reward + direct reward of next N actions + Q(next Nth state, next Nth action) (weighted with discrount factor).",
      "I.e. per training example, some experiences from the future are directly used to compute the rewards and only after some point the Q-function is used.",
      "Distributional RL  Seems to be nothing else but switching the regression (of Q-values) to classification in order to avoid standard mode problems with regression.",
      "The range of possible reward values is partitioned into N bins.",
      "Noisy Nets  Gets rid of the exploration factor in epsilon-greedy strategies.",
      "Instead it uses a noisy fully connected layer where the noise weights are learned by the network.",
      "As the network becomes more accurate at predicting good Q-values, it automatically decreases the noise.",
      "They combine all of the mentioned methods.",
      "They use a KL term for weighting in Prioritized Replay (to account for the Distributional RL).",
      "Training  They start training after 80k frames.",
      "They use Adam.",
      "When using epsilon-greedy instead of noise nets, they anneal epsilon to 0.01 at 250k frames.",
      "For multi-step learning they use n=3 future experiences.",
      "Results  They evaluate Rainbow 57 Atari games.",
      "Rainbow beats all other methods used on their own, both in learning speed and maximum skill level.",
      "It performs far better than the classic DQN approach.",
      "Average performance:  Ablation  Removing the priority replay, multi-step learning or distributional RL significantly worsens the performance.",
      "Removing noise nets also harms the performance, although a bit less.",
      "Removing double Q-learning or dueling networks seems to have no significiant effect.",
      "Visualization:  Learning curves by game:"
    ],
    "author_id": "ALEJU",
    "pdf_url": "https://arxiv.org/pdf/1710.02298",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 7590139
  },
  {
    "blog_id": "a-fast-and-accurate-dependency-parser-using-neural-networks",
    "summary": [
      "The paper proposes a neural network classifier to perform transition-based dependency parsing using dense vector representation for the features.",
      "Earlier approaches used a large, manually designed sparse feature vector which took a lot of time and effort to compute and was often incomplete.",
      "Description of the system  The system described in the paper uses arc-standard system (a greedy, transition-based dependency parsing system).",
      "Words, POS tags and arc labels are represented as d dimensional vectors.",
      "Sw, St, Sl denote the set of words, POS and labels respectively.",
      "Neural network takes as input selected words from the 3 sets and uses a single hidden layer followed by Softmax which models the different actions that can be chosen by the arc-standard system.",
      "Uses a cube activation function to allow interaction between features coming from the set of words, POS and labels in the first layer itself.",
      "These features come from different embeddings and are not related as such.",
      "Using separate embedding for POS tags and labels allow for capturing aspects like NN (singular noun) should be closer to NNS (plural noun) than DT (determiner).",
      "Input to the network contains words on the stack and buffer and their left and right children (read upon transition-based parsing), their labels and corresponding arc labels.",
      "Output generated by the system is the action to be taken (transition to be performed) when reading each word in the input.",
      "This sequential and deterministic nature of the input-output mapping allows the problem to be modelled as a supervised learning problem and a cross entropy loss can be used.",
      "L2-regularization term is also added to the loss.",
      "During inference, a greedy decoding strategy is used and transition with the highest score is chosen.",
      "The paper mentions a pre-computation trick where matrix computation of most frequent top 10000 words is performed beforehand and cached.",
      "Experiments  Dataset  English Penn Treebank (PTB)  Chinese Penn Treebank (CTB)  Two dependency representations used:  CoNLL Syntactic Dependencies (CD)  Stanford Basic Dependencies (SD)  Metrics:  Unlabeled Attached Scores (UAS)  Labeled Attached Scores (LAS)  Benchmarked against:  Greedy arc-eager parser  Greedy arc-standard parser  Malt-Parser  MSTParser  Results  The system proposed in the paper outperforms all other parsers in both speed and accuracy.",
      "Analysis  Cube function gives a 0.8-1.2% improvement over tanh.",
      "Pretained embeddings give 0.7-1.7% improvement over training embeddings from scratch.",
      "Using POS and labels gives an improvement of 1.7% and 0.4% respectively."
    ],
    "author_id": "shugan",
    "pdf_url": "https://nlp.stanford.edu/pubs/emnlp2014-depparser.pdf",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 96831204
  },
  {
    "blog_id": "a-decomposable-attention-model-for-natural-language-inference",
    "summary": [
      "The paper proposes an attention based mechanism to decompose the problem of Natural Language Inference (NLI) into parallelizable subproblems.",
      "Further, it uses much fewer parameters as compared to any other model while obtaining state of the art results.",
      "The motivation behind the paper is that the tasks like NLI do not require deep modelling of the sentence structure and comparison of local text substructures followed by aggregation can also work very well  Approach  Given two sentences a and b, the model has to predict whether they have an \u201centailment\u201d relationship, \u201cneutral\u201d relationship or \u201ccontradiction\u201d relationship.",
      "Embed  All the words are mapped to their corresponding word vector representation.",
      "In subsequent steps, \u201cword\u201d refers to the word vector representation of the actual word.",
      "Attend  For each word i in a and j in b, obtain unnormalized attention weights *e(i, j)=F(i)TF(j) where F is a feed-forward neural network.",
      "For i, compute a \u03b2i by performing softmax-like normalization of j using e(i, j) as the weight and normalizing for all words j in b.  \u03b2i captures the subphrase in b that is softly aligned to a.",
      "Similarly compute \u03b1j for j.",
      "Compare  Create two set of comparison vectors, one for a and another for b  For a, v1, i = G(concatenate(i, \u03b2i)).",
      "Similarly for b, v2, j = G(concatenate(j, \u03b1j))  G is another feed-forward neural network.",
      "Aggregate  Aggregate over the two set of comparison vectors to obtain v1 and v2.",
      "Feed the aggregated results through the final classifier layer.",
      "Multi-class cross-entropy loss function.",
      "The paper also explains how this representation can be augmented using intra-sentence attention to the model compositional relationship between words.",
      "Computational Complexity  Computationally, the proposed model is asymptotically as good as LSTM with attention.",
      "Assuming that dimensionality of word vectors > length of the sentence (reasonable for the given SNLI dataset), the model is asymptotically as good as regular LSTM.",
      "Further, the model has the advantage of being parallelizable.",
      "Experiment  On Stanford Natural Language Inference (SNLI) dataset, the proposed model achieves the state of the art results even when it uses an order of magnitude lesser parameters than the next best model.",
      "Adding intra-sentence attention further improve the test accuracy by 0.5 percent.",
      "Notes  A similar approach could be tried on paraphrase detection problem as even that problem should not require very deep sentence representation.",
      "Quora Duplicate Question Detection Challenege would have been an ideal dataset but it has a lot of out-of-vocabulary information related to named entities which need to be accounted for."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1606.01933",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 36779622
  },
  {
    "blog_id": "9dc0444142be8bd8a7404a226880eb",
    "summary": [
      "The paper proposes an adversarial approach for estimating generative models where one model (generative model) tries to learn a data distribution and another model (discriminative model) tries to distinguish between samples from the generative model and original data distribution.",
      "Adversarial Net  Two models - Generative Model(G) and Discriminative Model(D)  Both are multi-layer perceptrons.",
      "G takes as input a noise variable z and outputs data sample x(=G(z)).",
      "D takes as input a data sample x and predicts whether it came from true data or from G.  G tries to minimise log(1-D(G(z))) while D tries to maximise the probability of correct classification.",
      "Think of it as a minimax game between 2 players and the global optimum would be when G generates perfect samples and D can not distinguish between the samples (thereby always returning 0.5 as the probability of sample coming from true data).",
      "Alternate between k steps of training D and 1 step of training G so that D is maintained near its optimal solution.",
      "When starting training, the loss log(1-D(G(z))) would saturate as G would be weak.",
      "Instead maximise log(D(G(z)))  The paper contains the theoretical proof for global optimum of the minimax game.",
      "Experiments  Datasets  MNIST, Toronto Face Database, CIFAR-10  Generator model uses RELU and sigmoid activations.",
      "Discriminator model uses maxout and dropout.",
      "Evaluation Metric  Fit Gaussian Parzen window to samples obtained from G and compare log-likelihood.",
      "Strengths  Computational advantages  Backprop is sufficient for training with no need for Markov chains or performing inference.",
      "A variety of functions can be used in the model.",
      "Since G is trained only using the gradients from D, fewer chances of directly copying features from the true data.",
      "Can represent sharp (even degenerate) distributions.",
      "Weakness  D must be well synchronised with G.  While G may learn to sample data points that are indistinguishable from true data, no explicit representation can be obtained.",
      "Possible Extensions  Conditional generative models.",
      "Inference network to predict z given x.",
      "Implement a stochastic extension of the deterministic Multi-Prediction Deep Boltzmann Machines  Using discriminator net or inference net for feature selection.",
      "Accelerating training by ensuring better coordination between G and D or by determining better distributions to sample z from during training."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1406.2661",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 51450104
  },
  {
    "blog_id": "multipolicy-decision-making",
    "summary": [
      "This paper presents an integrated behavioral anticipation and decision-making system that models behavior for both our vehicle and nearby vehicles as the result of closed-loop policies.",
      "Only a finite set of a priori known policies are considered.",
      "Bayesian changepoint detection is used to estimate which policy a given vehicle was executing at each point in its history of actions, then infer the likelihood of each potential intention of the vehicle.",
      "A statistical test is proposed based on changepoint detection to identify anomalous behavior of other vehicles, such as driving in the wrong direction or swerving out of lanes.",
      "Evidence  Anomaly detection was explored by recording three trajectories corresponding to two bikes and a bus.",
      "The bikes crossed an intersection from a sidewalk, while the bus made a significantly wide turn.",
      "System was able to detect these trajectories as anomalous (Not within the set of known policies)  Evaluated in simulated driving environment  Notes  Bayesian Changepoint detection infers the points in the history of observations where the underlying policy that generated the observations changed.",
      "Then, the likelihood of all available policies for the target car given the distribution over the car\u2019s potential policies at the current timestep can be computed (sounds like HMM).",
      "The CHAMP algorithm infers the maximum a posteriori set of times at which changepoints between policies have occurred, yielding a set of segments.",
      "Given a segment from time s to t and a policy pi, CHAMP approx the log of the policy-evidence for that segment via the (Bae)yesian information criterion (BIC)  Viterbi path is found for the most likely sequence of latent policies  For decision-making, a set of samples are drawn from the distribution over policies of other cars where each sample assigns a policy to each nearby vehicle, excluding the ego car.",
      "For each policy available to the ego car (not all policies are available in every scenario e.g. intersection handling policy is not applicable when driving on a highway), and for each sample s, the process is rolled out forward in time until the decision horizon.",
      "This yields a set of simulated trajectories.",
      "The reward is evaluated for each element of the set of simulated trajectories and the maximal policy for the ego vehicle is chosen.",
      "This repeats continuously in a receding horizon manner.",
      "Reward function  distance to the goal at the end of the evaluation horizon  minimum distance to obstacles to evaluate safety  lane choice bias to add a preference for the right lane  maximum yaw rate and longitudinal jerk to measure passenger comfort"
    ],
    "author_id": "pemami",
    "pdf_url": "http://www.roboticsproceedings.org/rss11/p43.pdf",
    "author_full_name": "Patrick Emami",
    "source_website": "https://pemami4911.github.io/index.html",
    "id": 93980002
  },
  {
    "blog_id": "isomap",
    "summary": [
      "Tenenbaum, et al. 2000  Isomap, seemingly named for \u201cIsometric mapping\u201d, seeks to provide a solution to the problem of non-linear dimensionality reduction.",
      "The method is especially suitable for high-dimensional manifolds that exhibit non-Euclidean geometry, such that the Euclidean distance between data points returns distances that are not actually realistic for the underlying low-dimensional manifold.",
      "The intuition for this approach lies in the use of the all-pairs shortest path algorithm to improve upon Multi-dimensional scaling.",
      "Under general conditions on the density and curvature of the points, a geodesic distance can be estimated between far away points on the high-dimensional manifold via the all-pairs shortest path that converges to the true distance in the limit.",
      "Then, similar to MDS, Isomap attempts to find coordinate vectors for a low-dimensional space within which the distances between points are preserved as much as possible.",
      "This essentially results in the selection of the largest p eigenvectors of the matrix of estimated distances on the high-dimensional manifold (transformed to inner products).",
      "To make the algorithm work, the first step consists of clustering the data points either using k-NN or $\\epsilon$-balls.",
      "Edges are placed between all points clustered together, to form the graph upon which all-pairs shortest path is run.",
      "In this paper, the authors present examples of applying Isomap to a dataset of faces, MNIST, and the \u201cswiss roll\u201d dataset.",
      "Interestingly, they are able to map the faces dataset to a 3-D space, capturing left-right poses, up-down poses, and variations in ambient lighting.",
      "They show that PCA and MDS converge (the residual loss goes to 0) but they are unable to recover the true dimensionality of the low-dimensional manifold.",
      "This seems to be troublesome, because if one naively applies PCA to a dataset and the residual loss goes to 0, it appears then that the user of this algorithm will mistakenly believe they have recovered the true low-dimensional manifold.",
      "It would be interesting to then run a classifier on this low-dimensional representation produced by PCA, and then check the performance against the same classifier using the low-dimensional representation learned by Isomap.",
      "I imagine that the Isomap classifier will have slightly better performance."
    ],
    "author_id": "pemami",
    "pdf_url": "https://web.mit.edu/cocosci/Papers/sci_reprint.pdf",
    "author_full_name": "Patrick Emami",
    "source_website": "https://pemami4911.github.io/index.html",
    "id": 18027931
  },
  {
    "blog_id": "the-rise-of-the-citizen-developer-assessing-the-security-impact-of-online-app-generators",
    "summary": [
      "The rise of the citizen developer: assessing the security impact of online app generators Oltrogge et al., IEEE Security & Privacy 2018  \u201cLow code\u201d, \u201cno code\u201d, \u201ccitizen developers\u201d, call it what you will, there\u2019s been a big rise in platforms that seek to make it easy to develop applications for non-export developers.",
      "Today\u2019s paper choice studies the online application generator (OAG) market for Android applications.",
      "When what used to be a web site (with many successful web site  templating and building options around) is often in many cases now also or instead a mobile app, so it makes sense that the same kind of templating and building approach should exist there too.",
      "For a brief period at the end of last year, Apple flirted with banning such apps from their app store , before back-tracking just a couple of weeks after the initial announcement.",
      "After reading today\u2019s paper I can\u2019t help but feel that perhaps they were on to something.",
      "Not that templated apps are bad per se, but when the generated apps contain widespread vulnerabilities and privacy issues, then that is bad.",
      "With the increasing use of OAGs the duty of generating secure code shifts away from the app developer to the generator service.",
      "This leaves the question of whether OAGs can provide save and privacy-preserving default implementations of common tasks to generate more secure apps at an unprecedented scale.",
      "Being an optimist by nature, my hope was that such app generation services would improve the state of security, because spending time and effort getting it right once would pay back across all of the generated apps.",
      "In theory that could still happen, but in practice it seems the opposite is occurring.",
      "It doesn\u2019t seem to make a lot of difference whether you use a free online app generator (what are their incentives, and where does their revenue come from?",
      "Always good questions to ask), or a paid service, the situation is not good.",
      "The re-configuration attacks that the authors discover are particularly devastating.",
      "Online app generation services and penetration  Online application generators enable app development using wizard-like, point-and-click web interfaces in which developers only need to add and suitably interconnect UI elements that represent application components\u2026 There is no need and typically no option to write custom code.",
      "The authors started by searching the web for advertised online app generation platform, resulting in the set of services shown in the table below (the Como the authors refer to is I believe now called \u2018swiftic\u2019 \u2013  [url]"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://saschafahl.de/papers/appgens2018.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 58874274
  },
  {
    "blog_id": "vendrovkfu15",
    "summary": [
      "This paper proposes to learn embeddings of text and/or images according to a dissimilarity metric that is asymmetric and implements the notion of partial order.",
      "For example, we'd like the metric to capture that the sentence \"a dog in the yard\" is more specific than just \"a dog\".",
      "Similarly, given the image of a scene and a caption describing it, we'd also like to capture that the image is more specific than the caption, since captions only describe the main elements of the scene.",
      "We'd also like to capture the hypernym relation between single words, e.g. where \"woman\" is more specific than \"person\".",
      "To achieve this, they propose to use the following dissimilarity metric:  $$E(x,y) = ||max(0,y-x)||^2$$  where x and y are embedding vectors and the max operation is applied element-wise.",
      "The way to use this metric is to learn embeddings such that, for a pair x,y where the object (e.g. \"a dog in the yard\") represented by $x$ is more specific than the object (e.g.",
      "\"a dog\") represented by $y$, then $E(x,y)$ is as small as possible.",
      "For example, let's assume that $x$ and y are the output of a neural network, where each output dimension detects a certain concept, i.e. is non-zero only if the concept associated with that dimension is present in the input.",
      "For x representing \"a dog in the yard\", we could expect having only two dimensions that are non-zero: one detecting the concept \"dog\" (let's note it $x_j$) and another detecting the concept \"yard\" ($x_k$).",
      "For y representing \"a dog\", only the dimension associated with \"dog\" ($y_j$) would be non-zero and have the same value as $x_j$.",
      "In this situation, it is easy to see that $E(x,y)$ would be 0, but $E(y,x)$ would be greater than zero, thus capturing appropriately the asymmetric relationship between the two.",
      "The authors show in the paper how to leverage this new asymmetric metric in training losses that are appropriate for 3 problems: hypernym detection, caption-image retrieval and textual entailment.",
      "They show that the proposed metric yields superior performance on these problems compared to symmetric metrics that have been used by prior work."
    ],
    "author_id": "hlarochelle",
    "pdf_url": "http://arxiv.org/pdf/1511.06361",
    "author_full_name": "Hugo Larochelle",
    "source_website": "https://www.shortscience.org/user?name=hlarochelle",
    "id": 66403710
  },
  {
    "blog_id": "principled-detection-of-out-of-distribution-examples-in-neural-networks",
    "summary": [
      "Problem Statement  Given a pre-trained neural network, which is trained using data from some distribution P (referred to as in-distribution data), the task is to detect the examples coming from a distribution Q which is different from P (referred to as out-of-distribution data).",
      "For example, if a digit recognizer neural network is trained using MNIST images, an out-of-distribution example would be images of animals.",
      "Neural Networks can make high confidence predictions even in such cases where the input is unrecognisable or irrelevant.",
      "The paper proposes ODIN which can detect such out-of-distribution examples without changing the pre-trained model itself.",
      "ODIN  Uses 2 major techniques  Temperature Scaling  Softmax classifier for the classification network can be written as:  pi(x, T) = exp(fi(x)/T) / sum(exp(fj(x)/T))  where x is the input, p is the softmax probability and T is the temperature scaling parameter.",
      "Increasing T (up to some extent) boosts the performance in distinguishing in-distribution and out-of-distribution examples.",
      "Input Preprocessing  Add small perturbations to the input (image) before feeding it into the network.",
      "x_perturbed = x - \u03b5 * sign(-\u03b4xlog(py(x, T)))  where \u03b5 is the perturbation magnitude  The perturbations are such that softmax scores between in-distribution and out-of-distribution samples become separable.",
      "Given an input (image), first perturb the input.",
      "Feed the perturbed input to the network to get its softmax score.",
      "If the softmax score is greater than some threshold, mark the input as in-distribution and feed in the unperturbed version of the input to the network for classification.",
      "Otherwise, mark the input as out-of-distribution.",
      "For detailed mathematical treatment, refer section 6 and appendix in the paper  Experiments  Code available on github  Models  DenseNet with depth L = 100 and growth rate k = 12  Wide ResNet with depth = 28 and widen factor = 10  In-Distribution Datasets  CIFAR-10  CIFAR-100  Out-of-Distribution Datasets  TinyImageNet  LSUN  iSUN  Gaussian Noise  Metrics  False Positive Rate at 95% True Positive Rate  Detection Error - minimum misclassification probability over all thresholds  Area Under the Receiver Operating Characteristic Curve  Area Under the Precision-Recall Curve  ODIN outperforms the baseline across all datasets and all models by a good margin.",
      "Notes  Very simple and straightforward approach with theoretical justification under some conditions.",
      "Limited to examples from Vision so can not judge its applicability for NLP tasks."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1706.02690",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 38830210
  },
  {
    "blog_id": "masseo15",
    "summary": [
      "This paper presents a method for \"learning the learning rate\" of a stochastic gradient descent method, in the context of online learning.",
      "Indeed, variations on the chosen learning rate or learning rate schedule can have a large impact in observed performance of stochastic gradient descent.",
      "Moreover, in the context of online learning, where we are interested in achieving high performance not only at convergence but every step of the way, the \"choosing the learning rate\" problem is even more crucial.",
      "The authors present a method which attempts to train the learning rate itself by gradient descent.",
      "This is achieved by \"unrolling\" the parameter updates of our model across the time steps of online learning, which exposes the interaction between the learning rate and the sum of losses of the model across these time steps.",
      "The authors then propose a way to approximate the gradient of the sum of losses with respect to the learning rate, so that it can be used to perform gradient updates on the learning rate itself.",
      "The gradient on the learning rate has to be approximated, for essentially the same reason that gradients to train a recurrent neural network online must be approximated (see also my notes on another good paper by Yann Ollivier here:  [ref] ).",
      "Another approximation is introduced to avoid having to compute an Hessian matrix.",
      "Nevertheless, results suggest that the proposed approximation works well and can improve over a fixed learning with a reasonable rate decay schedule  #### My two cents  I think the authors are right on the money as to the challenges posed by online learning.",
      "I think these challenges are likely to be greater in the context of training neural networks online, for which little satisfactory solutions exist right now.",
      "So this is a direction of research I'm particularly excited about.",
      "At this points, the experiments consider fairly simple learning scenarios, but I don't see any obstacle in applying the same method to neural networks.",
      "One interesting observation from the results is that results are fairly robust to variations of \"the learning rate of the learning rate\", compared to varying and fixing the learning rate itself.",
      "Finally, I haven't had time to entirely digest one of their theoretical result, suggesting that their approximation actually corresponds to an exact gradient taken \"alongside the effective trajectory\" of gradient descent.",
      "However, that result seems quite interesting and would deserve more attention."
    ],
    "author_id": "hlarochelle",
    "pdf_url": "http://arxiv.org/pdf/1511.02540",
    "author_full_name": "Hugo Larochelle",
    "source_website": "https://www.shortscience.org/user?name=hlarochelle",
    "id": 34187605
  },
  {
    "blog_id": "multi-scale_context_aggregation_by_dilated_convolutions",
    "summary": [
      "What  They describe a variation of convolutions that have a differently structured receptive field.",
      "They argue that their variation works better for dense prediction, i.e. for predicting values for every pixel in an image (e.g.",
      "coloring, segmentation, upscaling).",
      "How  One can image the input into a convolutional layer as a 3d-grid.",
      "Each cell is a \"pixel\" generated by a filter.",
      "Normal convolutions compute their output per cell as a weighted sum of the input cells in a dense area.",
      "I.e. all input cells are right next to each other.",
      "In dilated convolutions, the cells are not right next to each other.",
      "E.g. 2-dilated convolutions skip 1 cell between each input cell, 3-dilated convolutions skip 2 cells etc.",
      "(Similar to striding.)",
      "Normal convolutions are simply 1-dilated convolutions (skipping 0 cells).",
      "One can use a 1-dilated convolution and then a 2-dilated convolution.",
      "The receptive field of the second convolution will then be 7x7 instead of the usual 5x5 due to the spacing.",
      "Increasing the dilation factor by 2 per layer (1, 2, 4, 8, ...) leads to an exponential increase in the receptive field size, while every cell in the receptive field will still be part in the computation of at least one convolution.",
      "They had problems with badly performing networks, which they fixed using an identity initialization for the weights.",
      "(Sounds like just using resdiual connections would have been easier.)",
      "Receptive fields of a 1-dilated convolution (1st image), followed by a 2-dilated conv.",
      "(2nd image), followed by a 4-dilated conv.",
      "(3rd image).",
      "The blue color indicates the receptive field size (notice the exponential increase in size).",
      "Stronger blue colors mean that the value has been used in more different convolutions.",
      "Results  They took a VGG net, removed the pooling layers and replaced the convolutions with dilated ones (weights can be kept).",
      "They then used the network to segment images.",
      "Their results were significantly better than previous methods.",
      "They also added another network with more dilated convolutions in front of the VGG one, again improving the results.",
      "Their performance on a segmentation task compared to two competing methods.",
      "They only used VGG16 without pooling layers and with convolutions replaced by dilated convolutions."
    ],
    "author_id": "ALEJU",
    "pdf_url": "https://arxiv.org/pdf/1511.07122",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 48411412
  },
  {
    "blog_id": "abductive-commonsense-reasoning",
    "summary": [
      "The paper presents the task of abductive NLP (pronounced as alpha NLP) where the model needs to perform abductive reasoning.",
      "Abductive reasoning is the inference to the most plausible explanation.",
      "Even though it is considered to be an important component for understanding narratives, the work in this domain is sparse.",
      "A new dataset called as Abstractive Reasoning in narrative Text (ART) consisting of 20K narrative contexts and 200k explanations is also provided.",
      "The dataset models the task as multiple-choice questions to make the evaluation process easy.",
      "Task Setup  Given a pair of observations O1 and O2 and two hypothesis h1 and h2, the task is to select the most plausible hypothesis.",
      "In general, P(h | O1, O2) is propotional to P(h |O1)P(O2|h, O1).",
      "Different independence assumptions can be imposed on the structure of the problem eg one assumption could be that the hypothesis is independent of the observations or the \u201cfully connected\u201d assumption would jointly model both the observations and the hypothesis.",
      "Dataset  Along with crowdsourcing several plausible hypotheses for each observation instance pair, an adversarial filtering algorithm (AF) is used to remove weak pairs of hypothesis.",
      "Observation pairs are created using the ROCStories dataset which is a collection of short, manually crafted stories of 5 sentences.",
      "The average word length for both the content and the hypothesis is between 8 to 9.",
      "To collect plausible hypothesis, the crowd workers were asked to fill in a plausible \u201cin-between\u201d sentence in natural language.",
      "Given the plausible hypothesis, the crowd workers were asked to create an implausible hypothesis by editing fewer than 6 words.",
      "Adversarial filtering approach from Zellers et al. is used with BERT as the adversary.",
      "A temperature parameter is introduced to control the maximum number of instances that can be changed in each adversarial filtering iteration.",
      "Key Observations  Human performance: 91.4%  Baselines like SVM classifier, the bag-of-words classifier (using Glove) and max-pooling overt BiLSTM representation: approx 50%  Entailment NLI baseline: 59%.",
      "This highlights the additional complexity of abductive NLI as compared to entailment NLI.",
      "BERT: 68.9%  GPT: 63.1%  Numerical and spatial knowledge-based data points are particularly hard.",
      "The model is more likely to fail when the narrative created by the incorrect hypothesis is plausible"
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1908.05739",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 67930407
  },
  {
    "blog_id": "ranzatocaz15",
    "summary": [
      "This paper is concerned with the problem of predicting a sequence at the output, e.g. using an RNN.",
      "It aims at addressing the issue it refers to as exposure bias, which here refers to the fact that while at training time the RNN producing the output sequence is being fed the ground truth previous tokens (words) when producing the next token (something sometimes referred to as teacher forcing, which really is just maximum likelihood), at test time this RNN makes predictions using recursive generation, i.e. it is instead recursively fed by its own predictions (which might be erroneous).",
      "Moreover, it also proposes a training procedure that can take into account a rich performance measure that can't easily be optimized directly, such as the BLEU score for text outputs.",
      "The key observation is that the REINFORCE algorithm could be used to optimize the expectation of such arbitrarily complicated performance measures, for outputs produced by (stochastic) recursive generation.",
      "However, REINFORCE is a notoriously unstable training algorithm, which can often work terribly (in fact, the authors mention that they have tried using REINFORCE only, without success).",
      "Thus, they instead propose to gradually go from training according to maximum likelihood / teacher forcing to training using the REINFORCE algorithm on the expected performance measure.",
      "The proposed procedure, dubbed MIXER (Mixed Incremental Cross-Entropy Reinforce), goes as follows: 1.",
      "Train model to optimize the likelihood of the target sequence, i.e. minimize the per time-step cross-entropy loss.",
      "2.",
      "Then, for a target sequence of size T, optimize the cross-entropy for the T-\u0394 first time steps of the sequence and use Reinforce to get a gradient on the expected loss (e.g. negative BLEU) for the recursive generation of the rest of the \u0394 time steps.",
      "3.",
      "Increase \u0394 and go back to 2., until \u0394 is equal to T.  Experiments on 3 text benchmarks (summarization, machine translation and image captioning) show that this approach yields models that produces much better outputs when not using beam search (i.e. using greedy recursive generation) to generate an output sequence, compared to other alternatives such as regular maximum likelihood and Data as Demonstrator (DaD).",
      "DaD is similar to the scheduled sampling method of Bengio et al. (see my note:  [ref] ), in that at training time, some of the previous tokens fed to the model are predicted tokens instead of ground truths.",
      "When using beam search, MIXER is only outperformed by DaD on the machine translation task."
    ],
    "author_id": "hlarochelle",
    "pdf_url": "http://arxiv.org/pdf/1511.06732",
    "author_full_name": "Hugo Larochelle",
    "source_website": "https://www.shortscience.org/user?name=hlarochelle",
    "id": 53809041
  },
  {
    "blog_id": "when-recurrent-models-don-t-need-to-be-recurrent",
    "summary": [
      "The paper explores \u201cif a well behaved RNN can be replaced by a feed-forward network of comparable size without loss in performance.\u201d  \u201cWell behaved\u201d is defined in terms of control-theoretic notion of stability.",
      "This roughly requires that the gradients do not explode over time.",
      "The paper shows that under the stability assumption, feedforward networks can approximate RNNs for both training and inference.",
      "The results are empirically validated as well.",
      "Problem Setting  Consider a general, non linear dynamical system given by a differential state transition map \u03a6w.",
      "The hidden ht = \u03a6w(ht-1, xt).",
      "Assumptions:  \u03a6 is smooth in w and h.  h0 = 0  \u03a6w(0, 0) = 0 (can be ensured by translation)  Stable models are the ones where \u03a6 is contractive ie \u03a6w(h, x) - \u03a6w(h\u2019, x) is less than \u039b * (h - h\u2019)  For example, in RNN, stability would require that norm(w) is less than (Lp)-1 where Lp is the Lipschitz constant of the point-wise non linearity used.",
      "The feedforward approximation uses a finite context (of length k) and is a truncated model.",
      "A non-parametric function f maps the output of the recurrent model to prediction.",
      "If f is desired to be a parametric model, its parameters can be pushed to the recurrent model.",
      "Theoretical Results  For a \u039b-contractive system, it can be proved that for a large k (and additional Lipschitz assumptions) the difference in prediction between the recurrent and truncated mode is negligible.",
      "If the recurrent model and truncated feed-forward network are initialized at the same point and trained over the same input for N-step, then for an optimal k, the weights of the two models would be very close in the Euclidean space.",
      "It can be shown that this small difference does not lead to large gradient differences during subsequent update steps.",
      "This can be roughly interpreted as - if the gradient descent can train a stable recurrent network, it can also train a feedforward model and vice-versa.",
      "The stability condition is important as, without that, truncated models would be bad (even for large values of k).",
      "Further, it is difficult to show that gradient descent converges to a stationary point."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1805.10369",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 68635574
  },
  {
    "blog_id": "recurrent-environment-simulators",
    "summary": [
      "This paper extends the results on action-conditional video prediction from Oh, et al., 2015 .",
      "The motivation behind this line of research is to investigate whether training RL agents to learn how their actions affect the environment reduces the amount of time they spend in exploration.",
      "The authors outline the following main challenges:  The properties of generalization and sensitivity to model structure of these methods are poorly understood  Accurate prediction for long time periods into the future is hard  Models that predict the high-dim image directly each time an action is taken are inefficient  The general approach is to use a Conv-RNN to take an observation from the environment and produce a high-dim state representation.",
      "This high-dim state representation can be combined with the action taken by the agent to predict how the environment will change.",
      "In other words, the goal is to learn a parametric model that approximates the state-action transition.",
      "One of the main contributions of this work is fusing the action with the hidden state representation when predicting the next hidden state representation in time.",
      "In previous work, the action was used instead to directly predict the next image.",
      "Why?",
      "Authors suggest it could \u201cenable the model to incorporate action information more effectively\u201d.",
      "Using observations directly from the environment to predict the next hidden state representation forces the agent to make predictions only when the next frame is available, i.e., 1-step prediction.",
      "Rolling forward the agent\u2019s own predictions in the future allows it to predict many time steps ahead in a more efficient manner.",
      "Prediction-dependent transitions are ones of the form  where $s_t$ is the hidden state representation.",
      "There\u2019s a trade-off between short- and long-term accuracy and prediction- vs observation-dependent transitions.",
      "When prediction-dependent transitions are mostly used, the agent learns better global dynamics of the game and can do better with long-term accuracy.",
      "When observation-dependent transitions are mixed with prediction-dependent, the agent pays more attention to details that provide it with better short-term accuracy.",
      "They evaluated their model on Breakout, Freeway, and Pong by replacing the real game with the learned environment simulator.",
      "A human would \u201cplay\u201d the game, and the learned simulator would return the next frame given the action input by the human.",
      "Related works  Action-conditional Video Prediction  UNREAL  Learning to Act By Predicting the Future"
    ],
    "author_id": "pemami",
    "pdf_url": "https://arxiv.org/pdf/1704.02254",
    "author_full_name": "Patrick Emami",
    "source_website": "https://pemami4911.github.io/index.html",
    "id": 1055799
  },
  {
    "blog_id": "desire",
    "summary": [
      "Lee, et al., 2016  This paper presents a framework for predicting how a scene containing multiple interacting agents will play out by leveraging a number of powerful techniques.",
      "DESIRE stands for Deep Stochastic IOC RNN Encoder-decoder framework.",
      "The following are the biggest obstacles for successfully predicting future actions taken by interacting agents given a visual snapshot of the present:  The space of future states for each agent in a scene is hard to optimize over; even when taking the context of the scene into account, there could be many plausible outcomes that seem equally likely  In turn, this makes rolling out and scoring potential future trajectories computationally expensive, since it requires sampling a large number of trajectories.",
      "This isn\u2019t relevant for offline processing, but for real-time robotics applications, this is important  The multi-agent problem; how to infer interactions between multiple actors in a scene?",
      "Taking into account long-term prediction rewards, rather than just one-step prediction  How to define a multi-objective loss function that doesn\u2019t commit errors such as averaging over all future possibilities  DESIRE attempts to address these challenges as follows:  Diverse sample generation using a conditional Variational Autoencoder.",
      "This allows for differentiable efficient sampling of plausible futures  RNN encoder-decoder neural architecture that allows for mapping trajectories represented by world coordinates in $\\mathbb{R}^2$ or $\\mathbb{R}^3$ to a high-dimensional distributed representation that can be efficiently combined with the Conditoinal VAE  An Inverse Optimal-Control (IOC) based Ranking and Refinement module that determines the most likely hypotheses, while incorporating scene context and interactions.",
      "The IOC module estimates a regression vector to refine each prediction sample.",
      "A convolutional neural network is used to carry out scene context fusion.",
      "This enables encoding features such as object velocities into the hypothesis scoring component.",
      "The model is trained end-to-end with a total loss consisting of multiple auxiliary losses (e.g., reconstruction error from the output trajectory of the decoder during training).",
      "The authors evaluate its performance on the KITTI and Stanford Drone tracking datasets.",
      "They show the performance with the metric \u201cerror in meters\u201d for future time steps at intervals of 1, 2, 3, and 4 seconds.",
      "Overall, the proposed model seems quite complex, with many sub-components.",
      "However, the motivation behind each component is reasonable and the model is shown to perform well on the datasets.",
      "The separation of the system into a Sample Generation Module and the Ranking and Refinement Module is reminiscent of the actor-critic architecture in RL.",
      "The Ranking and Refinement Module uses unsupervised learning losses to learn to score/refine the \u201cactor\u201d, or Sample Generation Module."
    ],
    "author_id": "pemami",
    "pdf_url": "https://arxiv.org/pdf/1704.04394v1",
    "author_full_name": "Patrick Emami",
    "source_website": "https://pemami4911.github.io/index.html",
    "id": 81535007
  },
  {
    "blog_id": "mnihr16",
    "summary": [
      "This paper explores the use of so-called Monte Carlo objectives for training directed generative models with latent variables.",
      "Monte Carlo objectives take the form of the logarithm of a Monte Carlo estimate (i.e. an average over samples) of the marginal probability $P(x)$.",
      "One important motivation for using Monte Carlo objectives is that they can be shown (see the Importance Weighted Variational Autoencoder paper  [ref]  and my notes on it) to correspond to bounds on the true likelihood of the model, and one can tighten the bound simply by drawing more samples in the Monte Carlo objective.",
      "Currently, the most successful application of Monte Carlo objectives is based on an importance sampling estimate, which involves training a proposal distribution $Q(h|x)$ in addition to the model $P(x,h)$.",
      "This paper considers the problem of training with gradient descent on such objectives, in the context of a model to which the reparametrization trick cannot be used (e.g. for discrete latent variables).",
      "They analyze the sources of variance in the estimation of the gradients (see Equation 5) and propose a very simple approach to reducing the variance of a sampling-based estimator of these gradients.",
      "First, they argue that gradients with respect to the $P(x,h)$ parameters are less susceptible to problems due to high variance gradients.",
      "Second, and most importantly, they derive a multi-sample estimate of the gradient that is meant to reduce the variance of gradients on the proposal distribution parameters $Q(h|x)$.",
      "The end result is the gradient estimate of Equations 10-11.",
      "It is based on the observation that the first term of the gradient of Equation 5 doesn't distinguish between the contribution of each sampled latent hi.",
      "The key contribution is this: they notice that one can incorporate a variance reducing baseline for each sample hi, corresponding to the Monte Carlo estimate of the log-likelihood when removing hi from the estimate (see Equation 10).",
      "The authors show that this is a proper baseline, in that using it doesn't introduce a bias in the estimation for the gradients.",
      "Experiments show that this approach yields better performance than training based on Reweighted Wake Sleep  [ref]  or the use of NVIL baselines  [ref] , when training sigmoid belief networks as generative models or as structured output prediction (image completion) models on binarized MNIST."
    ],
    "author_id": "hlarochelle",
    "pdf_url": "http://arxiv.org/pdf/1602.06725",
    "author_full_name": "Hugo Larochelle",
    "source_website": "https://www.shortscience.org/user?name=hlarochelle",
    "id": 10218080
  },
  {
    "blog_id": "word-representations-via-gaussian-embedding",
    "summary": [
      "Existing word embedding models like Skip-Gram , GloVe etc map words to fixed sized vectors in a low dimensional vector space.",
      "This fixed point setting cannot capture uncertainty about representation.",
      "Further, these fixed point vectors are compared with measures like dot product and cosine similarity which are not suitable for capturing asymmetric properties like textual entailment and inclusion.",
      "The paper proposes to learn Gaussian function embeddings (with diagonal covariance) for the word vectors.",
      "This way, the words are mapped to soft regions in the embedding space which enables modeling uncertainty and asymmetric properties like inclusion and uncertainty.",
      "Implementation  Approach  KL divergence is used as the asymmetric distance function for comparing the distributions.",
      "Unlike the Word2Vec model, the proposed model uses ranking-based loss.",
      "Similarity Measures used  Symmetric Similarity  For two gaussian distributions, Pi and Pj, compute the inner product E(Pi, Pj) as N(0; meani - meanj, sigmai + sigmaj).",
      "Compute the gradient of mean and sigma with respect to log(E).",
      "The resulting loss function can be interpreted as pushing the means closer which encouraging the two gaussians to be more concentrated.",
      "Asymmetric Similarity  Use KL divergence to encode the context distribution.",
      "The benefit over the symmetric setting is that now entailment type relations can also be modeled.",
      "For example, a low KL divergence from x to y indicates that y can be encoded as x or that y \u201centails\u201d x.",
      "Learning  One of the two notions of similarity is chosen and max-margin is used as the loss function.",
      "Mean is regularized by adding a simple constraint on the L2-norm.",
      "For covariance matrix, the eigenvalues are constrained to lie within a hypercube.",
      "This ensures that the positive-definite property of the covariance matrix is maintained while having a constraint on the size.",
      "Observations  Polysemous words have higher variance in their word embeddings as compared to specific words.",
      "KL divergence (with diagonal covariance) outperforms other models.",
      "Simple tree hierarchies can also be modeled by embedding into the Gaussian space.",
      "A Gaussian is created for each node with randomly initialized mean and the same set of embeddings is used for nodes and context.",
      "For word similarity benchmarks, embeddings with spherical covariance have a slight edge over embeddings with diagonal covariance and outperform the Skip-Gram model in all the cases.",
      "Future Work  Use combinations of low rank and diagonal matrices for covariances.",
      "Improved optimisation strategies.",
      "Trying other distributions like Student\u2019s-t distribution."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1412.6623",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 26515037
  },
  {
    "blog_id": "8fe14b74c7292129c6c5ecb37f33b5",
    "summary": [
      "NMT(Neural Machine Translation) systems perform poorly with respect to OOV(out-of-vocabulary) words or rare words.",
      "The paper presents a word-alignment based technique for translating such rare words.",
      "Technique  Annotate the training corpus with information about what do different OOV words (in the target sentence) correspond to in the source sentence.",
      "NMT learns to track the alignment of rare words across source and target sentences and emits such alignments for the test sentences.",
      "As a post-processing step, use a dictionary to map rare words from the source language to target language.",
      "Annotating the Corpus  Copy Model  Annotate the OOV words in the source sentence with tokens unk1, unk2,..., etc such that repeated words get the same token.",
      "In target language, each OOV word, that is aligned to some OOV word in the source language, is assigned the same token as the word in the source language.",
      "The OOV word in the target language, which has no alignment or is aligned with a known word in the source language.",
      "is assigned the null token.",
      "Pros  Very straightforward  Cons  Misses out on words which are not labelled as OOV in the source language.",
      "PosAll - Positional All Model  All OOV words in the source language are assigned a single unk token.",
      "All words in the target sentences are assigned positional tokens which denote that the jth word in the target sentence is aligned to the ith word in the source sentence.",
      "Aligned words that are too far apart, or are unaligned, are assigned a null token.",
      "Pros  Captures complete alignment between source and target sentences.",
      "Cons  It doubles the length of target sentences.",
      "PosUnk - Positional Unknown Model  All OOV words in the source language are assigned a single unk token.",
      "All OOV words in the target sentences are assigned unk token with the position which gives the relative position of the word in the target language with respect to its aligned source word.",
      "Pros:  Faster than PosAll model.",
      "Cons  Does not capture alignment for all words.",
      "Experiments  Dataset  Subset of WMT'14 dataset  Alignment computed using the Berkeley Aligner  Used architecture from Sequence to Sequence Learning with Neural Networks paper .",
      "Results  All the 3 approaches (more specifically the PosUnk approach) improve the performance of existing NMTs in the order PosUnk > PosAll > Copy.",
      "Ensemble models benefit more than individual models as the ensemble of NMT models works better at aligning the OOV words.",
      "Performance gains are more when using smaller vocabulary.",
      "Rare word analysis shows that performance gains are more when proposition of OOV words is higher."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1410.8206",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 74164301
  },
  {
    "blog_id": "mobilenets",
    "summary": [
      "What  They suggest a factorization of standard 3x3 convolutions that is more efficient.",
      "They build a model based on that factorization.",
      "The model has hyperparameters to choose higher performance or higher accuracy.",
      "How  Factorization  They factorize the standard 3x3 convolution into one depthwise 3x3 convolution, followed by a pointwise convoluton.",
      "Normal 3x3 convolution:  Computes per filter and location a weighted average over all filters.",
      "For kernel height kH, width kW and number of input filters/planes Fin, it requires kH*kW*Fin computations per location.",
      "Depthwise 3x3 convolution:  Computes per filter and location a weighted average over one input filter.",
      "E.g. the 13th filter would only computed weighted averages over the 13th input filter/plane and ignore all the other input filters/planes.",
      "This requires kH*kW*1 computations per location, i.e. drastically less than a normal convolution.",
      "Pointwise convolution:  This is just another name for a normal 1x1 convolution.",
      "This is placed after a depthwise convolution in order to compensate the fact that every (depthwise) filter only sees a single input plane.",
      "As the kernel size is 1, this is rather fast to compute.",
      "Visualization of normal vs factorized convolution:  Models  They use two hyperparameters for their models.",
      "alpha: Multiplier for the width in the range (0, 1].",
      "A value of 0.5 means that every layer has half as many filters.",
      "roh: Multiplier for the resolution.",
      "In practice this is simply the input image size, having a value of {224, 192, 160, 128}.",
      "Results  ImageNet  Compared to VGG16, they achieve 1 percentage point less accuracy, while using only about 4% of VGG's multiply and additions (mult-adds) and while using only about 3% of the parameters.",
      "Compared to GoogleNet, they achieve about 1 percentage point more accuracy, while using only about 36% of the mult-adds and 61% of the parameters.",
      "Note that they don't compare to ResNet.",
      "Results for architecture choices vs. accuracy on ImageNet:  Relation between mult-adds and accuracy on ImageNet:  Object Detection  Their mAP is a bit on COCO when combining MobileNet with SSD (as opposed to using VGG or Inception v2).",
      "Their mAP is quite a bit worse on COCO when combining MobileNet with Faster R-CNN.",
      "Reducing the number of filters (alpha) influences the results more than reducing the input image resolution (roh).",
      "Making the models shallower influences the results more than making them thinner."
    ],
    "author_id": "ALEJU",
    "pdf_url": "https://arxiv.org/pdf/1704.04861",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 80262838
  },
  {
    "blog_id": "revisiting-semi-supervised-learning-with-graph-embeddings",
    "summary": [
      "The paper presents a semi-supervised learning framework for graphs where the node embeddings are used to jointly predict both the class labels and neighbourhood context.",
      "Usually, graph embeddings are learnt in an unsupervised manner and can not leverage the supervising signal coming from the labelled data.",
      "The framework is called Planetoid (Predicting Labels And Neighbors with Embeddings Transductively Or Inductively from Data) .",
      "Problem Setting  Given a graph G = (V, E) and xL and xU as feature vectors for labelled and unlabelled nodes and yL as labels for the labelled nodes, the problem is to learn a mapping (classifier) f: x -> y  There are two settings possible:  Transductive - Predictions are made only for those nodes which are already observed in the graph at training time.",
      "Inductive - Predictions are made for nodes whether they have been observed in the graph at training time or not.",
      "Approach  The general semi-supervised learning loss would be LS + \u03bbLU where LS is the supervised learning loss while LU is the unsupervised learning loss.",
      "The unsupervised loss is a variant of the Skip-gram loss with negative edge sampling.",
      "More specifically, first a random walk sequence S is sampled.",
      "Then either a positive edge is sampled from S (within a given context distance) or a negative edge is sampled.",
      "The label information is injected by using the label as a context and minimising the distance between the positive edges (edges where the nodes have the same label) and maximising the distance between the negative edges (edges where the nodes have different labels).",
      "Transductive Formulation  Two separate fully connected networks are applied over the node features and node embeddings.",
      "These 2 representations are then concatenated and fed to a softmax classifier to predict the class label.",
      "Inductive Formulation  In the inductive setting, it is difficult to obtain the node embeddings at test time.",
      "One naive approach is to retrain the network to obtain the embeddings on the previously unobserved nodes but that is inefficient.",
      "The embeddings of node x are parameterized as a function of its input feature vector and is learnt by applying a fully connected neural network on the node feature vector.",
      "This provides a simple way to extend the original approach to the inductive setting.",
      "Results  The proposed approach is evaluated in 3 settings (text classification, distantly supervised entity extraction and entity classification) and it consistently outperforms approaches that use just node features or node embeddings.",
      "The key takeaway is that the joint training in the semi-supervised setting has several benefits over the unsupervised setting and that using the graph context (in terms of node embeddings) is much more effective than using graph Laplacian-based regularization term."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1603.08861",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 16973657
  },
  {
    "blog_id": "arguing_machines",
    "summary": [
      "What  They present a method to detect hard edge cases in datasets of self-driving cars.",
      "These are cases that might lead to accidents.",
      "The method is based on running two models simultaneously and measuring their disagreement with each other.",
      "How  They use two different models:  Tesla-AD: Proprietary autopilot from Tesla.",
      "CNN: Their own CNN, trained on a Tesla dataset (images from car's perspective + steering & acceleration annotation; 420 hours of driving).",
      "The network's architecture is similar to the one in the NVIDIA paper.",
      "Their considered five different inputs for their own CNN:  M5: RGB images (same as in the NVIDIA paper)  M4: Images of edges in each color channel (i.e. three edge maps).",
      "(No detailed explanation on how the edges were detected.)",
      "M3: Grayscale images, from t-20, t-10 and t (where t is the current frame).",
      "M2: Grayscale difference images t - t-10 (i.e. difference between the current frame and the one 10 frames ago), t - t-5, t - t-1.",
      "M1: Grayscale difference images t-20 - t-30, t-10 - t-20, t - t-10.",
      "Visualization:  Training  They split the data into subgroups, each being defined up by the steering wheel angle.",
      "E.g. all example with an angle in the range [-10, 10] end in one group.",
      "They select equally from all groups and get 100k training and 50k validation images.",
      "This prevents the model from only training on examples showing straight driving.",
      "Disagreement  Their intention is to find hard example / edge cases.",
      "They do this by measuring the disagreement between the models (Tesla-AD and CNN).",
      "To do that, they first clip the predicted angles to the range [-10, 10].",
      "Then they normalize the result to the range [-1, 1].",
      "They sum the differences of these predictions over a one second window (30 examples).",
      "If the sum exceeds a threshold delta, they consider the models to be in disagreement and the example to be an edge case.",
      "They use delta=10.",
      "Results  Input M1 performed best, followed by M2, M3, M4 and M5 (in that order).",
      "(Measured via MAE of CNN predictions vs. ground truth annotations.)",
      "They can predict with 90% accuracy whether there will be a disengagement in the next 5 seconds (i.e. human driver took control in the dataset)."
    ],
    "author_id": "ALEJU",
    "pdf_url": "https://arxiv.org/pdf/1710.04459",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 47397989
  },
  {
    "blog_id": "hamiltonian-neural-networks",
    "summary": [
      "The paper proposes a very cool idea at the intersection of deep learning and physics.",
      "The idea is to train a neural network architecture that builds on the concept of Hamiltonian Mechanics (from Physics) to learn physical conservation laws in an unsupervised manner.",
      "Link to the code  Link to author\u2019s blog  Hamiltonian Mechanics  It is a branch of physics that can describe systems which follow some conservation laws and invariants.",
      "Consider a set of N pair of coordinates [(q1, p1), \u2026, (qN, pN)] where q = [q1, \u2026, qN] dnotes the position of the set of objects while p = [p1, \u2026, pN] denotes the momentum of the set of variables.",
      "Together these N pairs completely describe the system.",
      "A scalar function H(q, p), called as the Hamiltonian is defined such that the partial derivative of H with respect to p is equal to derivative of q with respect to time t and the negative of partial derivative of H with respect to q is equal to derivative of p with respect to time t.  This can be expressed in the form of the equation as follows:  The Hamiltonian can be tied to the total energy of the system and can be used in any system where the total energy is conserved.",
      "Hamiltonian Neural Network (HNN)  The Hamiltonian H can be parameterized using a neural network and can learn conserved quantities from the data in an unsupervised manner.",
      "The loss function looks as follows:  The partial derivatives can be obtained by computing the in-graph gradient of the output variables with respect to the input variables.",
      "Observations  For setups where the energy must be conserved exactly, (eg ideal mass-spring and ideal pendulum), the HNN learn to preserve an energy-like scalar.",
      "For setups where the energy need not be conserved exactly, the HNNs still learn to preserve the energy thus highlighting a limitation of HNNs.",
      "In case of two body problems, the HNN model is shown to be much more robust when making predictions over longer time horizons as compared to the baselines.",
      "In the final experiment, the model is trained on pixel observations and not state observations.",
      "In this case, two auxiliary losses are added: auto-encoder reconstruction loss and a loss on the latent space representations.",
      "Similar to the previous experiments, the HNN model makes robust predictions over much longer time horizons."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1906.01563",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 63758896
  },
  {
    "blog_id": "diversity-is-all-you-need-learning-skills-without-a-reward-function",
    "summary": [
      "The paper proposes an approach to learn useful skills without a reward function by maximizing an information theoretic objective by using a maximum entropy policy.",
      "Skills are defined as latent-conditioned policies that alter the state of the environment in a consistent way.",
      "Link to the code  Setup  Unsupervised \u201cexploration\u201d stage followed by supervised stage.",
      "Desirable Qualities of Skills  Skills should dictate the states that the agent visits.",
      "Different skills should visit different states to be distinguishable.",
      "States (not actions) should be used to distinguish between skills as not all actions change the state (for the outside observer).",
      "Skills are encouraged to be diverse and \u201cexploratory\u201d by learning skills that act randomly (have high entropy).",
      "Loss Formulation  (S, A) - state and action  z ~ p(z) - latent variable to condition the policy.",
      "Skill - policy conditioned on a fixed z.",
      "Objective is to maximize the mutual information between skill and state (MI(A; Z)) ie skill should control which state is visited or the skill should be inferrable from the state visited.",
      "Simultaneously minimize the mutual information between skills and actions given the state to ensure that the state (and not the action) is used to distinguish the skills.",
      "Maximize the entropy of the mixture of policies (p(z) and all the skills).",
      "Implementation  Policy \u03c0(a | s, z)  Task reward replaced by the pseduoreward logq\u03c6(z | s) - log(p(z)).",
      "During unsupervised training, z is sampled at the start of the episode and then not changed during the episode.",
      "Learning agent gets rewards for visiting the states that are easy to discriminate while the discriminator updated to correctly predict z from the states visited.",
      "Observations  Analysis of Learned Skills  The agent learns a diverse set of primitive behaviors for all tasks ranging from 2 DoF to 111 DoF.",
      "for inverted pendulum and mountain car, the skills become increasingly diverse throughout training.",
      "Use of uniform prior, in place of a learned prior, for p(z) allows for discovery of more diverse skills.",
      "The proposed approach can be used as a pretraining technique where the best-performing primitives (from unsupervised training) can be finetuned with the task-specific rewards.",
      "The discovered skills can be used for hierarchical RL by learning a meta-policy(which chooses the skill to execute for k steps).",
      "Modifying the discriminator in the proposed formulation can be used to bias DIAYN towards discovering a particular type of policies.",
      "This provides a mechanism for incorporating \u201csupervision\u201d in the learning setup.",
      "The \u201cdiscovered\u201d primitives can also be used for imitation learning."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1802.06070",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 24312671
  },
  {
    "blog_id": "auvolatv15",
    "summary": [
      "`Update 2015/11/23: Since I first wrote this note, I became involved in the next iterations of this work, which became v2 of the arXiv manuscript.",
      "The notes below were made based on v1.`  This paper considers the problem of Maximum Inner Product Search (MIPS).",
      "In MIPS, given a query $q$ and a set of inputs $x_i$, we want to find the input (or the top n inputs) with highest inner product, i.e. $argmax_i q' x_i$.",
      "Recently, it was shown that a simple transformation to the query and input vectors made it possible to approximately solve MIPS using hashing methods for Maximum Cosine Similarity Search (MCSS), a problem for which solutions are readily available (see section 2.4 for a brief but very clear description of the transformation).",
      "In this paper, the authors combine this approach with clustering, in order to improve the quality of retrieved inputs.",
      "Specifically, they consider the spherical k-means algorithm, which is a variant of k-means in which data points are clustered based on cosine similarity instead of the euclidean similarity (in short, data points are first scaled to be of unit norm, then in the training inner loop points are assigned to the cluster centroid with highest dot product and cluster centroids are updated as usual, except that they are always rescaled to unit norm).",
      "Moreover, they consider a bottom-up application of the algorithm to yield a hierarchical clustering tree.",
      "They propose to use such a hierarchical clustering tree to find the top-n candidates for MIPS.",
      "The key insight here is that, since spherical k-means relies on cosine similarity for finding the best cluster, and since we have a transformation that allows the maximisation of inner product to be approximated by the maximisation of cosine similarity, then a tree to find MIPS candidates could be constructed by running spherical k-means on the inputs transformed by the same transformation used for hashing-based MIPS.",
      "In order to make the search more robust to border issues when a query is close to the frontier between clusters, at each level of the tree they consider more than one candidate cluster during top-down search, so as to merge the candidates in several leaves of the tree at the very end of a full top down query.",
      "Their experiments using search with word embeddings show that the quality of the top 1, 10 and 100 MIPS candidates using their spherical k-means approach is better than using two hashing-based search methods."
    ],
    "author_id": "hlarochelle",
    "pdf_url": "http://arxiv.org/pdf/1507.05910",
    "author_full_name": "Hugo Larochelle",
    "source_website": "https://www.shortscience.org/user?name=hlarochelle",
    "id": 63531440
  },
  {
    "blog_id": "image_crowd_counting_using_cnn_and_mrf",
    "summary": [
      "What  They suggest a method to estimate the number of visible people in images of crowds.",
      "Their method is based on a combination of CNNs and MRFs.",
      "How  They split each image of crowds into overlapping square patches.",
      "Each patch is resized to 224x224 and fed through ResNet-152.",
      "(Sounds like they don't fine-tune that model.)",
      "They extract the features from the last layer fc1000.",
      "(A bit weird considering those are the class probabilities.)",
      "They apply a few fully connected layers to that result in order to arrive at one value, which regresses the number of people visible in that patch.",
      "Those last fully connected layers are trained via a simple mean squared (or also absolute) error.",
      "The ground truth labels are computed from the total count of visible people in the image (sum of patch counts equals total number of people).",
      "They argue that count values should be similar between adjacent patches and smoothen them using a MRF:  p is a patch (same for q) and P are all patches  c_p/c_q is the count of patch p/q  D_p(c_p) is the cost of assigning count c_p to patch p, usually given by lambda * (ground_truth - c_p)^2, where lambda is a trained(?)",
      "weight (per patch?)",
      "V(c_p - c_q) is the smoothness cost, usually given by (c_p - c_q)^2  Training goal of the MRF formulation is to minimize the energy E(c), which is achieved by mostly assigning ground truth counts (D(.",
      ")), but also keeping the counts smooth to the neighbors (V(.)).",
      "The energy function is minimized using belief propagation.",
      "(But doesn't that have to be reexecuted for every single image?",
      "Only lambda is apparently trained, which is image-specific.",
      "That would probably be slow and requires the usage of ground truth annotation for all images, including validation/test?!)",
      "Visualization of the steps:  Results  UCF dataset  Contains 50 grayscale images with 63705 annotated people (each head is annotated).",
      "They achieve the best score among all competitors (about 10% lower in MAE compared to best alternative).",
      "Shanghaitech dataset  Contains 1198 images with 330165 annotated people (also head annotations).",
      "They achieve the best score among all competitors (about 30% lower in MAE compared to best alternative).",
      "Visualization of smoothing from MRF:"
    ],
    "author_id": "ALEJU",
    "pdf_url": "https://arxiv.org/pdf/1706.03686",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 87558652
  },
  {
    "blog_id": "exploring-models-and-data-for-image-question-answering",
    "summary": [
      "Problem Statement: Given an image, answer a given question about the image.",
      "Assumptions:  The answer is assumed to be a single word thereby bypassing the evaluation issues of multi-word generation tasks.",
      "VIS-LSTM Model  Treat the input image as the first word in the question.",
      "Obtain the vector representation (skip-gram) for words in the question.",
      "Obtain the VGG Net embeddings of the image and use a linear transformation (dimensionality reduction weight matrix) to match the dimensions of word embeddings.",
      "Keep image embedding frozen during training and use an LSTM to combine the word vectors.",
      "LSTM outputs are fed into a softmax layer which generates the answer.",
      "Dataset  DAtaset for QUestion Ansering on Real-world images (DAQUAR)  1300 images and 7000 questions with 37 object classes.",
      "Downside is that even guess work can yield good results.",
      "The paper proposed an algorithm for generating questions using MS-COCO dataset.",
      "Perform preprocessing steps like breaking large sentences and changing indefinite determines to definite ones.",
      "object questions, number questions, colour questions and location questions can be generated by searching for nouns, numbers, colours and prepositions respectively.",
      "Resulting dataset has ~120K questions across above 4 semantic types.",
      "Models  VIS+LSTM - explained above  2-VIS+BLSTM - Add the image features twice, in beginning and in the end (using different linear transformations) plus use bidirectional LSTM  IMG+BOW - Multinomial logistic regression on image features without dimensionality reduction + bag of words (averaging word vectors).",
      "FULL - Simple average of above 2 models.",
      "Baseline  Includes models where the answer is guessed, or only image or question features are used or image features along with prior knowledge of object are used.",
      "Also includes a KNN model where the system finds the nearest (image, question) pair.",
      "Metrics  Accuracy  Wu-Palmer similarity measure  Observations  The VIS-LSTM model outperforms the baselines while the FULL model benefits from averaging across all the models.",
      "Some useful information seems to be lost when downsizing the VGG vectors.",
      "Fine tuning the word vectors helps with performance.",
      "Normalising CNN hidden image features into zero mean and unit variance leads to faster training.",
      "Model does not perform well on the task of considering spatial relations between multiple objects and counting objects when multiple objects are present"
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1505.02074",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 18322264
  },
  {
    "blog_id": "convolutional_pose_machines",
    "summary": [
      "What  They suggest a new model for human pose estimation (i.e. to lay a \"skeleton\" over the image of a person).",
      "Their model has a (more or less) recurrent architecture.",
      "Initial estimates of keypoint locations are refined in several steps.",
      "The idea of the recurrent architecture is derived from message passing, unrolled into one feed-forward model.",
      "How  Architecture  They generate the end result in multiple steps, similar to a recurrent network.",
      "Step 1:  Receives the image (368x368 resolution).",
      "Applies a few convolutions to the image in order to predict for each pixel the likelihood of belonging to a keypoint (head, neck, right elbow, ...).",
      "Step 2 and later:  (Modified) Receives the image (368x368 resolution) and the previous likelihood scores.",
      "(Same) Applies a few convolutions to the image in order to predict for each pixel the likelihood of belonging to a keypoint (head, neck, right elbow, ...).",
      "(New) Concatenates the likelihoods with the likelihoods of the previous step.",
      "(New) Applies a few more convolutions to the concatenation to compute the final likelihood scores.",
      "Visualization of the architecture:  Loss function  The basic loss function is a simple mean squared error between the expected output maps per keypoint and the predicted ones.",
      "In the expected output maps they mark the correct positions of the keypoints using a small gaussian function.",
      "They apply losses after each step in the architecture, argueing that this helps against vanishing gradients (they don't seem to be using BN).",
      "The expected output maps of the first step actually have the positions of all keypoints of a certain type (e.g. neck) marked, i.e.",
      "if there are multiple people in the extracted image patch there might be multiple correct keypoint positions.",
      "Only at step 2 and later they reduce that to the expected person (i.e. one keypoint position per map).",
      "Results  Example results:  Self-correction of predictions over several timesteps:  They beat existing methods on the datasets MPII, LSP and FLIC.",
      "Applying a loss function after each step (instead of only once after the last step) improved their results and reduced problems related to vanishing gradients.",
      "The effective receptive field size of each step had a significant influence on the results.",
      "They increased it to up to 300px (about 80% of the image size) and saw continuous improvements in accuracy."
    ],
    "author_id": "ALEJU",
    "pdf_url": "https://arxiv.org/pdf/1602.00134",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 71225651
  },
  {
    "blog_id": "reading-wikipedia-to-answer-open-domain-questions",
    "summary": [
      "The paper presents a new machine comprehension dataset for question answering in real life setting (say when interacting with Cortana/Siri).",
      "Unique Aspects of the dataset  Existing machine comprehension (MC) datasets are either too small or synthetic (with a distribution different from that or real-questions posted by humans).",
      "MARCO questions are sampled from real, anonymized user queries.",
      "Most datasets would provide a comparatively small and clean context to answer the question.",
      "In MARCO, the context documents (which may or may not contain the answer) are extracted using Bing from real-world documents.",
      "As such the questions and the context documents are noisy.",
      "In general, the answer to the questions are restricted to an entity or text span within the document.",
      "In case of MARCO, the human judges are encouraged to generate complete sentences as answers.",
      "Dataset Description  First release consists of 100K questions with the aim of releasing 1M questions in the future releases.",
      "All questions are tagged with segment information.",
      "A subset of questions has multiple answers and another subset has no answers at all.",
      "Each record in the dataset contains the following information:  Query - The actual question  Passage - Top 10 contextual passages extracted from web search engine (which may or may not contain the answer to the question).",
      "Document URLs - URLs for the top documents (which are the source of the contextual passages).",
      "Answer - Answer synthesised by human evaluators.",
      "Segment - Query type, description, neumeric, entity, location, person.",
      "Experimental Results  Metrics  Accuracy and precision/recall for numeric questions  ROGUE-L/paraphrasing aware evaluation framework for long, textual answers.",
      "Among generative models, Memory Networks performed better than seq-to-seq.",
      "In the cloze-style test, ReasoNet achieved an accuracy of approx.",
      "59% while Attention Sum Reader achieved an accuracy of approx 55%.",
      "Current QA systems (including the ones using memory and attention) derive their power from supervised data and are very different from how humans do reasoning.",
      "Imagenet dataset pushed the state-of-the-art performance on object classification to beyond human accuracy.",
      "Similar was the case with speech recognition dataset from DARPA which led to the advancement of speech recognition.",
      "Having a large, diverse and human-like questions dataset is a fundamental requirement to advance the field and the paper aims to provide just the right kind of dataset."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1704.00051",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 58884367
  },
  {
    "blog_id": "bouchardtpg15",
    "summary": [
      "SGD is a widely used optimization method for training the parameters of some model f on some given task.",
      "Since the convergence of SGD is related to the variance of the stochastic gradient estimate, there's been a lot of work on trying to come up with such stochastic estimates with smaller variance.",
      "This paper does it using an importance sampling (IS) Monte Carlo estimate of the gradient, and learning the proposal distribution $q$ of the IS estimate.",
      "The proposal distribution $q$ is parametrized in some way, and is trained to minimize the variance of the gradient estimate.",
      "It is trained simultaneously while the model $f$ that SGD (i.e. the SGD that uses IS to get its gradient) is training.",
      "To make this whole story more recursive, the proposal distribution $q$ is also trained with SGD :-) This makes sense, since one expects the best proposal to depend on the value of the parameters of model $f$, so the best proposal $q$ should vary as $f$ is trained.",
      "One application of this idea is in optimizing a classification model over a distribution that is imbalanced class-wise (e.g. there are classes with much fewer examples).",
      "In this case, the proposal distribution determines how frequently we sample examples from each class (conditioned on the class, training examples are chosen uniformly).",
      "#### My two cents  This is a really cool idea.",
      "I particularly like the application to training on an imbalanced classification problem.",
      "People have mostly been using heuristics to tackle this problem, such as initially sampling each class equally as often, and then fine-tuning/calibrating the model using the real class proportions.",
      "This approach instead proposes a really elegant, coherent, solution to this problem.",
      "I would have liked to see a comparison with that aforementioned heuristic (for mainly selfish reasons :-) ).",
      "They instead compare with an importance sampling approach with proposal that assigns the same probability to each class, which is a reasonable alternative (though I don't know if it's used as often as the more heuristic approach).",
      "There are other applications, to matrix factorization and reinforcement learning, that are presented in the paper and seem neat, though I haven't gone through those as much.",
      "Overall, one of my favorite paper this year: it's original, tackles a problem for which I've always hated the heuristic solution I'm using now, proposes an elegant solution to it, and is applicable even more widely than that setting."
    ],
    "author_id": "hlarochelle",
    "pdf_url": "http://arxiv.org/pdf/1506.09016",
    "author_full_name": "Hugo Larochelle",
    "source_website": "https://www.shortscience.org/user?name=hlarochelle",
    "id": 23242120
  },
  {
    "blog_id": "neural_doodle",
    "summary": [
      "What  They describe a method to transfer image styles based on semantic classes.",
      "This allows to:  (1) Transfer styles between images more accurately than with previous models.",
      "E.g. so that the background of an image does not receive the style of skin/hair/clothes/... seen in the style image.",
      "Skin in the synthesized image should receive the style of skin from the style image.",
      "Same for hair, clothes, etc.",
      "(2) Turn simple doodles into artwork by treating the simplified areas in the doodle as semantic classes and annotating an artwork with these same semantic classes.",
      "(E.g. \"this blob should receive the style from these trees.\")",
      "How  Their method is based on Combining Markov Random Fields and Convolutional Neural Networks for Image Synthesis .",
      "They use the same content loss and mostly the same MRF-based style loss.",
      "(Apparently they don't use the regularization loss.)",
      "They change the input of the MRF-based style loss.",
      "Usually that input would only be the activations of a VGG-layer (for the synthesized image or the style source image).",
      "They add a semantic map with weighting gamma to the activation, i.e. <representation of image> = <activation of specific layer for that image> || gamma * <semantic map>.",
      "The semantic map has N channels with 1s in a channel where a specific class is located (e.g. skin).",
      "The semantic map has to be created by the user for both the content image and the style image.",
      "As usually for the MRF loss, patches are then sampled from the representations.",
      "The semantic maps then influence the distance measure.",
      "I.e. patches are more likely to be sampled from the same semantic class.",
      "Higher gamma values make it more likely to sample from the same semantic class (because the distance from patches from different classes gets larger).",
      "One can create a small doodle with few colors, then use the colors as the semantic map.",
      "Then add a semantic map to an artwork and run the algorithm to transform the doodle into an artwork.",
      "Results  More control over the transfered styles than previously.",
      "Less sensitive to the style weighting, because of the additional gamma hyperparameter.",
      "Easy transformation from doodle to artwork.",
      "Turning a doodle into an artwork.",
      "Note that the doodle input image is also used as the semantic map of the input."
    ],
    "author_id": "ALEJU",
    "pdf_url": "http://arxiv.org/pdf/1603.01768",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 55048906
  },
  {
    "blog_id": "coop-inverse-rl",
    "summary": [
      "In the future, AI and people will work together, and hence we must concern ourselves with ensuring that the AI will have interests aligned with our own.",
      "The authors suggest that it is in our best interests to find a solution to the \u201cvalue-alignment problem\u201d.",
      "As recently pointed out by Ian Goodfellow, this may not always be a good idea .",
      "Cooperative Inverse Reinforcement Learning (CIRL) is a formulation of a cooperative, partial information game between a human and a robot.",
      "Both share a reward  function, but the robot does not initially know what it is.",
      "One of the key departures from classical Inverse Reinforcement Learning is that the \u201cteacher\u201d, which in this case is the human, is not assumed to act optimally.",
      "Rather, it is shown that sub-optimal actions on the part of the human can result in the robot learning a better reward function.",
      "The structure of the CIRL formulation is such that it should encourage the  human to not attempt to teach by demonstration in a way that greedily maximizes immediate reward.",
      "Rather, the human learns how to \u201cbest respond\u201d to the robot.",
      "Further Notes  CIRL can be formulated as a dec-POMDP, and reduced to a single-agent POMDP.",
      "The authors solved a 2D navigation task with CIRL to demonstrate the inferiority of having the human follow a \u201cdemonstration-by-expert\u201d policy as opposed to a \u201cbest-response\u201d policy.",
      "In the experiments, the authors used regret as a performance measure for learning the reward function with respect to a fully-observed setting where the robot knows the ground truth of the hidden reward function.",
      "Another performance measure used is the KL-divergence between the max-entropy trajectory distributions induced by the estimate of the reward parameters and the ground truth parameters.",
      "Finally, the L2-norm is used as a measure between the vector of rewards defined by the estimate of the reward parameters and the ground truth parameters.",
      "Comments  I believe that we\u2019ll see some work coming out of OpenAI following this line of research in the near future (where you have AI and humans learning to colloborate safely).",
      "It is also interesting to note that the experiments conducted in this paper are on a grid-world and far from being applied in a real world setting (until we get much better POMDP solvers, that is)."
    ],
    "author_id": "pemami",
    "pdf_url": "http://arxiv.org/pdf/1606.03137v2.pdf",
    "author_full_name": "Patrick Emami",
    "source_website": "https://pemami4911.github.io/index.html",
    "id": 87068449
  },
  {
    "blog_id": "systematic_testing_of_cnns_for_autonomous_driving",
    "summary": [
      "What  They suggest a framework that can be used to test CNNs (for self-driving cars).",
      "The framework makes use of synthetically generated examples.",
      "How  Their framework is split into three parts: Image generator, sampling methods and visualization tools.",
      "Image generator  The image generator uses real images from a dataset as input.",
      "It may change these images via basic transformations (e.g. brightness, saturation) or by drawing objects on them (e.g.",
      "cars).",
      "Though they seem to only apply contrast changes and project cars (at x/y-positions).",
      "The car projection also makes use of the road location in order to draw cars smaller if they are further away.",
      "Visualization:  Sampling methods  Each modification applied by the image generator can be expressed as one or more components of a vector (e.g. 2.0 for \"increase brightness to 2x the initial value\").",
      "Such a generated example/vector is then a point in a space.",
      "The space contains all possible images that can be generated.",
      "Generating images and testing a CNN on them is expensive.",
      "So ideally one wants have a good search method to find images that confuse the CNN.",
      "They generate candidate points in the example space using Halton and lattice-based sequences.",
      "(Uniform sampling of candidate points would be possible, but would not cover the space optimally.)",
      "Using that method, they can generate example inputs and the corresponding CNN scores.",
      "They then train a gaussian process model on these (image, CNN score) pairs.",
      "This allows them to more efficiently generate images that probably cause the CNN to fail.",
      "Visualization tools  They generate plots from example/input vectors and measured scores (confidence, IoU).",
      "Results  Example results for SqueezeDet and Yolo bounding box detectors, applied to a single test image with a car projected onto various locations:  The size of each point indicates the IoU, i.e. small points indicate that the CNN missed the car or predicted a bad bounding box).",
      "Blue points indicate that the network predicted a low confidence value.",
      "SqueezeDet seems to have problems with cars at the center and right of the roud.",
      "Yolo's confidence value seem to follow the distance of the car.",
      "Its achieved IoUs seem to be lower in general with a blind spot on the right of the road.",
      "Corresponding 3d plots of points, organized by x/y position of the car, IoU and confidence value (:"
    ],
    "author_id": "ALEJU",
    "pdf_url": "https://arxiv.org/pdf/1708.03309",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 30877354
  },
  {
    "blog_id": "79796c70565e3761e86d0f932a3de5",
    "summary": [
      "The paper presents Deep Convolutional Generative Adversarial Nets (DCGAN) - a topologically constrained variant of conditional GAN.",
      "Benefits  Stable to train  Very useful to learn unsupervised image representations.",
      "Model  GANs difficult to scale using CNNs.",
      "Paper proposes following changes to GANs:  Replace any pooling layers with strided convolutions (for discriminator) and fractional strided convolutions (for generators).",
      "Remove fully connected hidden layers.",
      "Use batch normalisation in both generator (all layers except output layer) and discriminator (all layers except input layer).",
      "Use LeakyReLU in all layers of the discriminator.",
      "Use ReLU activation in all layers of the generator (except output layer which uses Tanh).",
      "Datasets  Large-Scale Scene Understanding.",
      "Imagenet-1K.",
      "Faces dataset.",
      "Hyperparameters  Minibatch SGD with minibatch size of 128.",
      "Weights initialized with 0 centered Normal distribution with standard deviation = 0.02  Adam    Optimizer  Slope of leak = 0.2 for LeakyReLU.",
      "Learning rate = 0.0002, \u03b21 = 0.5  Observations  Large-Scale Scene Understanding data  Demonstrates that model scales with more data and higher resolution generation.",
      "Even though it is unlikely that model would have memorized images (due to low learning rate of minibatch SGD).",
      "Classifying CIFAR-10 dataset  Features  Train in Imagenet-1K and test on CIFAR-10.",
      "Max pool discriminator's convolutional features (from all layers) to get 4x4 spatial grids.",
      "Flatten and concatenate to get a 28672-dimensional vector.",
      "Linear L2-SVM classifier trained over the feature vector.",
      "82.8% accuracy, outperforms K-means (80.6%)  Street View House Number Classifier  Similar pipeline as CIFAR-10  22.48% test error.",
      "The paper contains many examples of images generated by final and intermediate layers of the network.",
      "Images in the latent space do not show sharp transitions indicating that network did not memorize images.",
      "DCGAN can learn an interesting hierarchy of features.",
      "Networks seems to have some success in disentangling image representation from object representation.",
      "Vector arithmetic can be performed on the Z vectors corresponding to the face samples to get results like smiling woman - normal woman + normal man = smiling man visually.",
      "This comment has been minimized.",
      "Sign in to view  Copy link  Quote reply  2017develper commented  Apr 14, 2018  i want to find a tutorial with gan and unsupervised learning in python please can you help me  This comment has been minimized.",
      "Sign in to view  Copy link  Quote reply  vishal5212 commented  May 16, 2019  no"
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1511.06434",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 78339451
  },
  {
    "blog_id": "seguipv15",
    "summary": [
      "This paper discusses some amazing results.",
      "The goal is to learn how to count by end-to-end training.",
      "The network input is an image and the output is a count of the objects inside it.",
      "They do not perform any direct training using the locations of the objects in the image.",
      "The reason for avoiding direct training is that labeled data is expensive.",
      "Employing a surrogate objective ,such as the count of items in the image, is much cheaper and makes more sense because it is the goal of the system we want to learn.",
      "This paper states that it is possible!",
      "The discuss experiments on two datasets; one of MNIST digits placed in an image and one with the UCSD Pedestrian Database.",
      "The network description seems to be general and they don't report any special constraints on the design  `\"We consider networks of two or more convolutional layers followed by one or more fully connected layers.",
      "Each convolutional layer consist of several elements: a set of convolutional filters, ReLU non-linearities, max pooling layers and normalization layers.",
      "\"` and `\"We use a five layers architecture CNN with two convolutional layers followed by three fully connected layers\"`.",
      "They provide these two tables for their designs:  $$\\begin{array}{c|c|c|c}  Conv1 & Conv2 & FC1 & FC2  \\\\ \\hline 10\\text{x}15\\text{x}15 & 10\\text{x}3\\text{x}3 & 32 & 6 \\\\ \\text{x2 pool} & \\text{x2 pool} & & \\\\ \\hline \\end{array}\\\\ \\text{CNN arch for numbers}$$  $$ \\begin{array}{c|c|c|c|c}  Conv1 & Conv2 & FC1 & FC2 & FC3 \\\\ \\hline 8\\text{x}9\\text{x}9 & 8\\text{x}5\\text{x}5 & 128 & 128 & 25 \\\\ \\text{x2 pool} & \\text{x2 pool} & & \\\\ \\hline \\end{array}\\\\ \\text{CNN arch for people}$$  They state that they use a method based on hypercolumns  [ref]  but the description is not clear at all: `\" Starting with the hypercolumn representation on the last layer we cluster the resulting hypercolumns into a set of prototypes using an online k-means algorithm.",
      "Then, a MIL approach with positive and negative instances with the concept of interest is used.",
      "\"`  !",
      "[]( [url]"
    ],
    "author_id": "joecohen",
    "pdf_url": "https://arxiv.org/pdf/1505.08082",
    "author_full_name": "Joseph Cohen",
    "source_website": "https://www.shortscience.org/user?name=joecohen",
    "id": 62996279
  },
  {
    "blog_id": "hierarchical-rl-using-an-ensemble-of-proprioceptive-periodic-policies",
    "summary": [
      "The paper proposes a simple and robust approach for hierarchically training an agent in the sparse reward setup.",
      "The broad idea is to train low-level primitives that are sufficiently diverse (so that they can be composed for solving higher level tasks) and to train a high level primitive that learns to combine these primitives for any given downstream task.",
      "Approach  The state can be divided into two components: proprioceptive states sp (measurement of agent\u2019s own body that can be directly controlled by the agent) and the external states se/  Low-Level Policy Training  Low-level policies should be:  Diverse: should cover all the skills that the agent might have to perform.",
      "Effective: can make significant changes to the environment.",
      "Controllable: easy for high-level policies to use and control  For the low-level policy, the per-time step reward is directly proportional to change in the external state.",
      "The same reward is used for all the agents and environments(except regulated with environment specific controls and survival rewards).",
      "Phase conditioned policies  Good movement policies are expected to be at least roughly periodic and phase input (or time index) is used to achieve periodicity.",
      "Phase conditioned policy (=f(sp, \u03c6)) where \u03c6 = {0, 1, \u2026, k-1} is the phase index.",
      "At each timestep t, the model receives observation sp and phase index \u03c6 = t%k.",
      "The phase index is represented by a vector b\u03c6.",
      "For phase conditioned policies, the agent state and actions are encouraged to be cyclic with the help of a cyclic loss.",
      "Experiments  Environments: Ant and Humanoid from Mujoco.",
      "Low-level control:  Using phase-conditioning is helpful when training low-level primitives.",
      "High-level control:  Cross Maze Environment with fixed goals  3 goals along 3 paths  Proposed method converges faster and to a smaller final distance to the goal showing that it is both efficient and consistent (with smaller variance across random seeds).",
      "Random Goal Maze  The goal is randomly drawn from a set of goals.",
      "\u201cCross\u201d (shaped) maze and \u201cskull\u201d (shaped) mazes are considered.",
      "Even with velocity rewards and pretraining on low-level objectives (which can be thought of as exploration bonuses), the baseline fails to get close to the goal locations while the proposed model reach the goal most of the times.",
      "The main results are reported using PPO though repeating the experiments with A2C and DQN show that the idea is fairly robust.",
      "The paper reported that in their experiments, finetuning the lower level primitives did not help much though it might not be the case of other environments."
    ],
    "author_id": "shugan",
    "pdf_url": "https://openreview.net/pdf?id=SJz1x20cFQ",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 2577868
  },
  {
    "blog_id": "linguistic-knowledge-as-memory-for-recurrent-neural-networks",
    "summary": [
      "   Training RNNs to model long term dependencies is difficult but in some cases, the information about dependencies between elements (of the sequence) may be present in the form of symbolic knowledge.",
      "For example, when encoding sentences, coreference, and hypernymy relations can be extracted between tokens.",
      "These elements(tokens) can be connected with each other with different kind of edges resulting in the graph data structure.",
      "One approach could be to model this knowledge(encoded in the graph) using a graph neural network (GNN).",
      "The authors prefer to encode the information into 2 DAGs (via topological sorting) as training the GNN could add some extra overhead.",
      "This results into the Memory as Acyclic Graph Encoding RNN (MAGE-RNN) architecture.",
      "Its GRU version is referred to as MAGE-GRU.",
      "Given an input sequence of tokens [x1, x2, \u2026, xT] and information about which tokens relate to other tokens, a graph G is constructed with different (possibly typed) edges.",
      "Given the graph G, two DFS orderings are computed - forward DFS and backward DFS.",
      "MAGE-RNN uses separate networks for accessing the forward and backward DFS orders.",
      "A separate hidden state is maintained for each entity type to separate memory content from addressing.",
      "For any DFS order (forward or backward), the representation at time t is given as the concatenation of representation of different edge types at that time.",
      "The hidden states (for different edge types at time t) are updated in the topological order using the current state of all incoming edges at xt.",
      "The representation of the DFS order is given as the sequence of all the previous representations.",
      "In some cases, elements across multiple sequences could be related to each other.",
      "In that case, the graph is decomposed into a collection of DAGs and use MAGE-GRU on the DAGs by taking one random permutation of the sequences and decomposing it into the forward and the backward graphs.",
      "The model is evaluated on the task of text comprehension with coreference on bAbi dataset (story based QA), LAMBADA dataset (broad context language modeling) and CNN dataset (cloze-style QA).",
      "MAGE-GRU was used as a replacement for GRU units in bi-directional GRUs and GA-Reader architecture.",
      "DAG-RNN and shared version of MAGE-GRU (with shared edge types) are the other baselines.",
      "For all the cases, the model with MAGE-GRU works the best."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1703.02620",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 30748495
  },
  {
    "blog_id": "litbz15",
    "summary": [
      "This paper presents a feed-forward neural network architecture for processing graphs as inputs, inspired from previous work on Graph Neural Networks.",
      "In brief, the architecture of the GG-NN corresponds to $T$ steps of GRU-like (gated recurrent units) updates, where T is a hyper-parameter.",
      "At each step, a vector representation is computed for all nodes in the graph, where a node's representation at step t is computed from the representation of nodes at step $t-1$.",
      "Specifically, the representation of a node will be updated based on the representation of its neighbors in the graph.",
      "Incoming and outgoing edges in the graph are treated differently by the neural network, by using different parameter matrices for each.",
      "Moreover, if edges have labels, separate parameters can be learned for the different types of edges (meaning that edge labels determine the configuration of parameter sharing in the model).",
      "Finally, GG-NNs can incorporate node-level attributes, by using them in the initialization (time step 0) of the nodes' representations.",
      "GG-NNs can be used to perform a variety of tasks on graphs.",
      "The per-node representations can be used to make per-node predictions by feeding them to a neural network (shared across nodes).",
      "A graph-level predictor can also be obtained using a soft attention architecture, where per-node outputs are used as scores into a softmax in order to pool the representations across the graph, and feed this graph-level representation to a neural network.",
      "The attention mechanism can be conditioned on a \"question\" (e.g. on a task to predict the shortest path in a graph, the question would be the identity of the beginning and end nodes of the path to find), which is fed to the node scorer of the soft attention mechanism.",
      "Moreover, the authors describe how to chain GG-NNs to go beyond predicting individual labels and predict sequences.",
      "Experiments on several datasets are presented.",
      "These include tasks where a single output is required (on a few bAbI tasks) as well as tasks where a sequential output is required, such as outputting the shortest path or the Eulerian circuit of a graph.",
      "Moreover, experiments on a much more complex and interesting program verification task are presented."
    ],
    "author_id": "hlarochelle",
    "pdf_url": "http://arxiv.org/pdf/1511.05493",
    "author_full_name": "Hugo Larochelle",
    "source_website": "https://www.shortscience.org/user?name=hlarochelle",
    "id": 50438213
  },
  {
    "blog_id": "vincentbb15",
    "summary": [
      "This paper presents a linear algebraic trick for computing both the value and the gradient update for a loss function that compares a very high-dimensional target with a (dense) output prediction.",
      "Most of the paper exposes the specific case of the squared error loss, though it can also be applied to some other losses such as the so-called spherical softmax.",
      "One use case could be for training autoencoders with the squared error on very high-dimensional but sparse inputs.",
      "While a naive (i.e. what most people currently do) implementation would scale in $O(Dd)$ where $D$ is the input dimensionality and d the hidden layer dimensionality, they show that their trick allows to scale in $O(d^2)$.",
      "Their experiments show that they can achieve speedup factors of over 500 on the CPU, and over 1500 on the GPU.",
      "#### My two cents  This is a really neat, and frankly really surprising, mathematical contribution.",
      "I did not suspect getting rid of the dependence on D in the complexity would actually be achievable, even for the \"simpler\" case of the squared error.",
      "The jury is still out as to whether we can leverage the full power of this trick in practice.",
      "Indeed, the squared error over sparse targets isn't the most natural choice in most situations.",
      "The authors did try to use this trick in the context of a version of the neural network language model that uses the squared error instead of the negative log-softmax (or at least I think that's what was done...",
      "I couldn't confirm this with 100% confidence).",
      "They showed that good measures of word similarity (Simlex-999) could be achieved in this way, though using the hierarchical softmax actually achieves better performance in about the same time.",
      "But as far as I'm concerned, that doesn't make the trick less impressive.",
      "It's still a neat piece of new knowledge to have about reconstruction errors.",
      "Also, the authors mention that it would be possible to adapt the trick to the so-called (negative log) spherical softmax, which is like the softmax but where the numerator is the square of the pre-activation, instead of the exponential.",
      "I hope someone tries this out in the future, as perhaps it could be key to making this trick a real game changer!"
    ],
    "author_id": "hlarochelle",
    "pdf_url": "http://papers.nips.cc/paper/5865-efficient-exact-gradient-update-for-training-deep-networks-with-very-large-sparse-targets.pdf",
    "author_full_name": "Hugo Larochelle",
    "source_website": "https://www.shortscience.org/user?name=hlarochelle",
    "id": 43791696
  },
  {
    "blog_id": "deep-reinforcement-learning-in-a-handful-of-trials-using-probabilistic-dynamics-models",
    "summary": [
      "The paper proposes a new algorithm called as Probabilistic Ensemble with Trajectory sampling (PETS) that combines uncertainty aware deep learning models (ensemble of deep learning models that encode uncertainty) with sampling-based uncertainty propagation.",
      "PETS improves over other probabilistic MBRL approaches by isolating epistemic uncertainty (due to limited training data) and aleatoric uncertainty (inherent in the system).",
      "Uncertainty-Aware Neural Network Dynamics Model  Aleatoric uncertainty can be accounted for by learning a parameterized distribution (probabilistic neural network) trained with negative log-likelihood.",
      "Epistemic uncertainty can be accounted for by either having an infinite amount of data or by using ensembles.",
      "The paper uses a neural network to predict the mean and standard deviation of a gaussian distribution which defines the predictive model.",
      "This setup is referred to as the \u201cprobabilistic\u201d model and denoted by P.  The alternate setup of the deterministic model is where a neural network is used to make a point prediction (and is denoted by D).",
      "Ensemble of probabilistic models is denoted as PE while that of deterministic models is denoted as DE.",
      "Planning and Control with learned Dynamics  Model Predictive Control (MPC) is used for planning.",
      "Given a start state and an action sequence, the probabilistic dynamics model induces a distribution over the trajectories.",
      "The first action, among the sequence of optimized actions, is executed.",
      "Instead of random shooting, Cross Entropy Method (CEM) is used.",
      "Trajectory Sampling  Let us say there are B bootstrap models in the ensemble.",
      "Given the current state, P particles are created and each particle is propagated using one of the bootstrap models.",
      "Two variants are considered:  TS1 - At each timestep, each particle samples a bootstrap.",
      "In this case, particle separation can not be attributed to ti the compounding effects of the bootstraps.",
      "TS$\\infty$ - The bootstrapped model (per particle) is sampled just once and is not changed after that.",
      "This setup separates aleatoric and epistemic uncertainty.",
      "Aleatoric state variance is the average variance of the particles of the same bootstrap while epistemic state variance is the variance of the average of particles of same bootstrap indexes.",
      "Result  The proposed approach reaches the asymptotic performance of state-of-the-art model-free algorithms in much fewer samples.",
      "The general performance trend is probabilistic emsemble > probabilisitc model > deterministc ensemble > determinisitc model./.",
      "Initial experiments for learning policy by propagating gradients through the ensemble of models did not work and has been left as future work."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1805.12114.pdf",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 2905851
  },
  {
    "blog_id": "1f387f084355dfafdf7550b1899af6",
    "summary": [
      "The paper presents a database of ranked English and Spanish paraphrases derived by:  Extracting lexical, phrasal, and syntactic paraphrases from large bilingual parallel corpora.",
      "Computing the similarity scores for the pair of paraphrases using Google ngrams and the Annotated Gigaword corpus.",
      "Extracting Paraphrase from Bilingual Text  The basic idea is that if two English strings e1 and e2 translate to the same foreign string f (also called pivot), they should have the same meaning.",
      "Informally speaking, the input to the system is translation triplets of the form < f, e, \u03c6 >, where  f is a foreign string  e is an english string  \u03c6 is a vector of feature functions  The system can pivot over f to create paraphrase triplets < e1, e2, \u03c6p > where \u03c6p is computed using translation feature vectors \u03c61 and \u03c62  For example, conditional paraphrase probability p(e2|e1) can be computed by marginalizing over all shared foreign language translations f:  p(e2|e1) = Sum over all f, p(e2|f)p(e1|f)  Scoring Paraphrases Using Monolingual Distributional Similarity  Measure similarity of phrases using Distributional similarity.",
      "Can be used to rerank the paraphrases obtained from bilingual text or to obtain the paraphrases which could not be obtained from bilingual text alone.",
      "To describe a given phrase e1, collect contextual features like:  n-gram based features for words (to the left and right of the given phrase)  Lexical, lemma-based, POS and named entity unigrams and bigrams  Dependency link features  Syntactic features  Aggregate all the features, over all the occurences of e, to obtain distributional signature se.",
      "Define similarity between 2 phrases e1 and e2 as :  *sim(e1, e2) = dot(se1, s2)/(|se1||se2|)  Paper mentions two instances:  English paraphrases - 169.6 Million paraphrases  Spanish paraphrases - 161.6 Million paraphrases  Analysis  The paper performed tests to analyse the precision-recall tradeoff for coverage of Propbank predictions and predicate-argument tuples.",
      "Human evaluation was performed over a sample of 1900 paraphrases to establish the correlation of PPDB scores with human judgement.",
      "Areas of Improvement  Segregation of data by domain or topic  Support for more languages  Improving paraphrasing scores by using additional sources of information and better handling of paraphrases ambiguity.",
      "This comment has been minimized.",
      "Sign in to view  Copy link  Quote reply  sahilbadyal commented  Sep 21, 2018  Could you please explain the ALIGNMENT column in PPDB2.0"
    ],
    "author_id": "shugan",
    "pdf_url": "http://www.cis.upenn.edu/~ccb/ppdb/pdf/ppdb-naacl-2013.pdf",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 65062778
  },
  {
    "blog_id": "feature_pyramid_networks_for_object_detection",
    "summary": [
      "What  They suggest a modified network architecture for object detectors (i.e. bounding box detectors).",
      "The architecture aggregates features from many scales (i.e. before each pooling layer) to detect both small and large object.",
      "The network is shaped similar to an hourglass.",
      "How  Architecture  They have two branches.",
      "The first one is similar to any normal network: Convolutions and pooling.",
      "The exact choice of convolutions (e.g. how many) and pooling is determined by the used base network (e.g.",
      "~50 convolutions with ~5x pooling in ResNet-50).",
      "The second branch starts at the first one's output.",
      "It uses nearest neighbour upsampling to re-increase the resolution back to the original one.",
      "It does not contain convolutions.",
      "All layers have 256 channels.",
      "There are connections between the layers of the first and second branch.",
      "These connections are simply 1x1 convolutions followed by an addition (similar to residual connections).",
      "Only layers with similar height and width are connected.",
      "Visualization:  Integration with Faster R-CNN  They base the RPN on their second branch.",
      "While usually an RPN is applied to a single feature map of one scale, in their case it is applied to many feature maps of varying scales.",
      "The RPN uses the same parameters for all scales.",
      "They use anchor boxes, but only of different aspect ratios, not of different scales (as scales are already covered by their feature map heights/widths).",
      "Ground truth bounding boxes are associated with the best matching anchor box (i.e. one box among all scales).",
      "Everything else is the same as in Faster R-CNN.",
      "Integration with Fast R-CNN  Fast R-CNN does not use an RPN, but instead usually uses Selective Search to find region proposals (and applies RoI-Pooling to them).",
      "Here, they simply RoI-Pool from the FPN's output of the second branch.",
      "They do not pool over all scales.",
      "Instead they pick only the scale/layer that matches the region proposal's size (based on its height/width).",
      "They process each pooled RoI using two 1024-dimensional fully connected layers (initalizes randomly).",
      "Everything else is the same as in Fast R-CNN.",
      "Results  Faster R-CNN  FPN improves recall on COCO by about 8 points, compared to using standard RPN.",
      "Improvement is stronger for small objects (about 12 points).",
      "For some reason no AP values here, only recall.",
      "The RPN uses some convolutions to transform each feature map into region proposals.",
      "Sharing the features of these convolutions marginally improves results.",
      "Fast R-CNN  FPN improves AP on COCO by about 2 points.",
      "Improvement is stronger for small objects (about 2.1 points)."
    ],
    "author_id": "ALEJU",
    "pdf_url": "https://arxiv.org/pdf/1612.03144",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 66681632
  },
  {
    "blog_id": "refining-source-representations-with-relation-networks-for-neural-machine-translation",
    "summary": [
      "The paper introduces Relation Network (RN) that refines the encoding representation of the given source document (or sentence).",
      "This refined source representation can then be used in Neural Machine Translation (NMT) systems to counter the problem of RNNs forgetting old information.",
      "Limitations of existing NMT models  The RNN encoder-decoder architecture is the standard choice for NMT systems.",
      "But the RNNs are prone to forgetting old information.",
      "In NMT models, the attention is modeled in the unit of words while the use of phrases (instead of words) would be a better choice.",
      "While NMT systems might be able to capture certain relationships between words, they are not explicitly designed to capture such information.",
      "Contributions of the paper  Learn the relationship between the source words using the context (neighboring words).",
      "Relation Networks (RNs) build pairwise relations between source words using the representations generated by the RNNs.",
      "The RN would sit between the encoder and the attention layer of the encoder-decoder framework thereby keeping the main architecture unaffected.",
      "Relation Network  Neural network which is desgined for relational reasoning.",
      "Given a set of inputs * O = o1, \u2026, on *, RN is formed as a composition of inputs:    RN(O) = f(sum(g(oi, oj))), f and g are functions used to learn the relations (feed forward networks)  g learns how the objects are related hence the name \u201crelation\u201d.",
      "Components:  CNN Layer  Extract information from the words surrounding the given word (context).",
      "The final output of this layer is the sequence of vectors for different kernel width.",
      "Graph Propagation (GP) Layer  Connect all the words with each other in the form of a graph.",
      "Each output vector from the CNN corresponds to a node in the graph and there is an edge between all possible pair of nodes.",
      "The information flows between the nodes of the graph in a message passing sort of fashion (graph propagation) to obtain a new set of vectors for each node.",
      "Multi-Layer Perceptron (MLP) Layer  The representation from the GP Layer is fed to the MLP layer.",
      "The layer uses residual connections from previous layers in form of concatenation.",
      "Datasets  IWSLT Data - 44K sentences from tourism and travel domain.",
      "NIST Data - 1M Chinese-English parallel sentence pairs.",
      "Models  MOSES - Open source translation system -  [url]"
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1805.11154.pdf",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 36278339
  },
  {
    "blog_id": "cheri-abi",
    "summary": [
      "CheriABI: enforcing valid pointer provenance and minimizing pointer privilege in the POSIX C run-time environment Davis et al., ASPLOS\u201919  Last week we saw the benefits of rethinking memory and pointer models at the hardware level when it came to object storage and compression ( Zippads ).",
      "CHERI also rethinks the way that pointers and memory work, but the goal here is memory protection.",
      "The scope of the work stands out as particularly impressive:  We have adapted a complete C, C++, and assembly-language software stack, including the open source FreeBSD OS (nearly 800 UNIX programs and more than 200 libraries including OpenSSH, OpenSSL, and bsnmpd) and PostgreSQL database, to employ ubiquitous capability-based pointer and virtual-address protection.",
      "The protections are hardware implemented and cannot be forged in software.",
      "The process model, user-kernel interactions, dynamic linking, and memory management concerns are all in scope, and the protection spans the OS/DBMS boundary.",
      "The basic question here is whether it is practical to support a large-scale C-language software stack with strong pointer-based protection\u2026 with only modest changes to existing C code-bases and with reasonable performance cost.",
      "We answer this question affirmatively.",
      "That \u2018reasonable\u2019 performance cost is a 6.8% slowdown, significantly better than e.g. the 50% overheads of Address Sanitizer .",
      "Conceptual model  CHERI is guided by two underlying principles:  The well-known principle of least privilege: running software should have the minimum privileges possible to do what it needs to do, and  A new principle identified in this work, the principle of intentional use: where a set of privileges is available to a piece of software, an invoked privilege should be selected explicitly rather than implicitly.The conceptual model that ensues has the following properties:  memory accesses are based not just on arbitrary integers (checked against only the process address space), but also on abstract capabilities that confer an appropriate set of memory permissions.",
      "abstract capabilities are constructed only through legitimate provenance chains of operations, successively reducing permissions from initial maximally permissive capabilities provided at machine reset  code is not given access to excessive capabilities  And this all has to work for whole-system executions, not just the C-language portion of user processes.",
      "The goal of all this of course is to prevent attackers injecting, manipulating or abusing pointers in the runtime environment.",
      "CHERI implementation  CHERI adds a new hardware data type for strongly protected C-language pointers, the CHERI capability (the evaluation uses an FPGA-based implementation).",
      "A capability combines a good old-fashioned address pointer with bounds constraining the range of addresses and permissions limiting its use.",
      "The resulting pointers are 128-bits wide, together with one out-of-band tag bit.",
      "Provenance validation ensures that only capabilities derived via valid transformations of valid capabilities using capability instructions can be used  Capability integrity prevents direct in-memory manipulation of architectural capability encodings.",
      "If any violation is detected the tag bit is cleared, and the data can no longer be interpreted as a capability.",
      "Monotonicity prevents the permissions or bounds associated with a capability from being increased.",
      "The instruction set contains explicit instructions for working with capabilities, and legacy instructions addressing memory via vitual addresses are indirected through a \u2018default data capability\u2019 (DDC) register.",
      "The core of CHERI has been covered in earlier papers, what\u2019s new in this paper is the extension of capability support to the full userspace process environment including interactions with system calls, signals, dynamic linking, and process debugging.",
      "This enables all legacy loads and stores via the DDC to be eliminated.",
      "The abstract capability model is implemented with a subtle combination of architectural capabilities (as provided by the hardware) and the critical systems code involved in managing paging, context switching, linking, memory allocation, and suchlike.",
      "The work includes changes to the CHERI ISA, the C compiler, the C language runtime, the virtual memory APIs, and the CheriBSD kernel.",
      "At hardware reset the boot code is granted maximally permissive architectural capabilities.",
      "The kernel then narrows these to ones separately covering userspace, kernel code, and kernel data.",
      "When a process address space is then replaced by execve, the kernel establishes new memory mappings for the contents of the address space, subdividing the previously created userspace capability.",
      "On a context switch the kernel saves and restores user-thread register capability state, and updates virtual-physical mappings.",
      "Similar housekeeping needs to be done when swapping and on signal delivery.",
      "All standard methods of accessing process memory have been altered to use an explicit capability, so the kernel can only access the memory specified and authorized by the user process, as shown below.",
      "Evaluation  The CheriABI implementation is used to compile FreeBSD and PostgreSQL, then all of the respective test suites are run.",
      "In the table below, the numbers in each column represent the number of test programs, not the number of individual tests.",
      "The MIPS rows show the test suite results on a standard mips64 system.",
      "Digging into the PostgreSQL test failures, just over half are due to test assumptions about output order or pointer size, the remaining half-dozen or so still need further investigation.",
      "Most programs (almost 800 C programs in the FreeBSD source tree) require no modifications.",
      "The following table breaks down the types of changes required for those that do:  The biggest cause of change (42 cases) is calling conventions (CC) in BSD libraries when using variadic arguments.",
      "With capabilities these require correct function prototypes, and when programs declare their own callbacks fixing each one is the only solution.",
      "Microbenchmarks (MiBench) and the FreeBSD system call timing benchmarks show modest performance impact in some cases, and performance improvements in others (3.4% slower to 9.8% faster for system calls).",
      "For a macro-benchmark PostgreSQL\u2019s initdb tool was used.",
      "PostgreSQL runs 6.8% slower as a CheriABI binary.",
      "The memory safety benefits are evaluated using the BOdiagsuite of 291 programs.",
      "Each program has three memory-safety violating variants: min is typically an off-by-one error, med is an off-by-8-bytes error, and large is an off-by 4096 bytes error.",
      "CheriABI is compared against vanilla mips64 and Address Sanitizer.",
      "It shows a very high success rate in detecting these safety violations.",
      "Note that Address Sanitizer has high overheads (3x stack memory, 12.5% total memory, and around 50% performance).",
      "In addition to finding test-suite issues, we have found and fixed dozens of bugs including buffer bounds violations and variadic argument misuse in FreeBSD programs, libraries, and tests.",
      "The last word  We have demonstrated a complete memory-safe UNIX system that is practical for general use\u2026 our implementation of CheriABI shows the existence of a path forward from our current run-time foundations set on the shifting sands of integer pointers, to a future where strong referential integrity enforces the principles of least privilege and intentionality even on lowest-level software.",
      "And an afterword!",
      "It strikes me that you can think of a foreign-key reference in a database a bit like a memory address pointer (the virtual address space it indexes into consists of the rows of the foreign table).",
      "Today those foreign keys are just like unprotected integer pointers in the world of virtual memory \u2013 they have no associated capabilities or protections, can be directly manipulated, etc.. What if queries returned \u2018capabilities\u2019 instead of raw keys??",
      "That might be an interesting model for thinking about the four Ps: provenance, purpose, permissions and privacy."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://www.cl.cam.ac.uk/research/security/ctsrd/pdfs/201904-asplos-cheriabi.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 65595064
  },
  {
    "blog_id": "elus",
    "summary": [
      "What  ELUs are an activation function  The are most similar to LeakyReLUs and PReLUs  How (formula)  f(x):  if x >= 0: x  else: alpha(exp(x)-1)  f'(x) / Derivative:  if x >= 0: 1  else: f(x) + alpha  alpha defines at which negative value the ELU saturates.",
      "E. g. alpha=1.0 means that the minimum value that the ELU can reach is -1.0  LeakyReLUs however can go to -Infinity, ReLUs can't go below 0.",
      "Form of ELUs(alpha=1.0) vs LeakyReLUs vs ReLUs.",
      "Why  They derive from the unit natural gradient that a network learns faster, if the mean activation of each neuron is close to zero.",
      "ReLUs can go above 0, but never below.",
      "So their mean activation will usually be quite a bit above 0, which should slow down learning.",
      "ELUs, LeakyReLUs and PReLUs all have negative slopes, so their mean activations should be closer to 0.",
      "In contrast to LeakyReLUs and PReLUs, ELUs saturate at a negative value (usually -1.0).",
      "The authors think that is good, because it lets ELUs encode the degree of presence of input concepts, while they do not quantify the degree of absence.",
      "So ELUs can measure the presence of concepts quantitatively, but the absence only qualitatively.",
      "They think that this makes ELUs more robust to noise.",
      "Results  In their tests on MNIST, CIFAR-10, CIFAR-100 and ImageNet, ELUs perform (nearly always) better than ReLUs and LeakyReLUs.",
      "However, they don't test PReLUs at all and use an alpha of 0.1 for LeakyReLUs (even though 0.33 is afaik standard) and don't test LeakyReLUs on ImageNet (only ReLUs).",
      "Comparison of ELUs, LeakyReLUs, ReLUs on CIFAR-100.",
      "ELUs ends up with best values, beaten during the early epochs by LeakyReLUs.",
      "(Learning rates were optimized for ReLUs.)",
      "Rough chapter-wise notes  Introduction  Currently popular choice: ReLUs  ReLU: max(0, x)  ReLUs are sparse and avoid the vanishing gradient problem, because their derivate is 1 when they are active.",
      "ReLUs have a mean activation larger than zero.",
      "Non-zero mean activation causes a bias shift in the next layer, especially if multiple of them are correlated.",
      "The natural gradient (?)",
      "corrects for the bias shift by adjusting the weight update.",
      "Having less bias shift would bring the standard gradient closer to the natural gradient, which would lead to faster learning.",
      "Suggested solutions:  Centering activation functions at zero, which would keep the off-diagonal entries of the Fisher information matrix small.",
      "Batch Normalization  Projected Natural Gradient Descent (implicitly whitens the activations)  These solutions have the problem, that they might end up taking away previous learning steps, which would slow down learning unnecessarily.",
      "Chosing a good activation function would be a better solution.",
      "Previously, tanh was prefered over sigmoid for that reason (pushed mean towards zero).",
      "Recent new activation functions:  LeakyReLUs: x if x > 0, else alpha*x  PReLUs: Like LeakyReLUs, but alpha is learned  RReLUs: Slope of part < 0 is sampled randomly  Such activation functions with non-zero slopes for negative values seemed to improve results.",
      "The deactivation state of such units is not very robust to noise, can get very negative.",
      "They suggest an activation function that can return negative values, but quickly saturates (for negative values, not for positive ones).",
      "So the model can make a quantitative assessment for positive statements (there is an amount X of A in the image), but only a qualitative negative one (something indicates that B is not in the image).",
      "They argue that this makes their activation function more robust to noise.",
      "Their activation function still has activations with a mean close to zero.",
      "Zero Mean Activations Speed Up Learning  Natural Gradient = Update direction which corrects the gradient direction with the Fisher Information Matrix  Hessian-Free Optimization techniques use an extended Gauss-Newton approximation of Hessians and therefore can be interpreted as versions of natural gradient descent.",
      "Computing the Fisher matrix is too expensive for neural networks.",
      "Methods to approximate the Fisher matrix or to perform natural gradient descent have been developed.",
      "Natural gradient = inverse(FisherMatrix) * gradientOfWeights  Lots of formulas.",
      "Apparently first explaining how the natural gradient descent works, then proofing that natural gradient descent can deal well with non-zero-mean activations.",
      "Natural gradient descent auto-corrects bias shift (i.e. non-zero-mean activations).",
      "If that auto-correction does not exist, oscillations (?)",
      "can occur, which slow down learning.",
      "Two ways to push means towards zero:  Unit zero mean normalization (e.g. Batch Normalization)  Activation functions with negative parts  Exponential Linear Units (ELUs)  Formula  f(x):  if x >= 0: x  else: alpha(exp(x)-1)  f'(x) / Derivative:  if x >= 0: 1  else: f(x) + alpha  alpha defines at which negative value the ELU saturates.",
      "alpha=0.5 => minimum value is -0.5 (?)",
      "ELUs avoid the vanishing gradient problem, because their positive part is the identity function (like e.g. ReLUs)  The negative values of ELUs push the mean activation towards zero.",
      "Mean activations closer to zero resemble more the natural gradient, therefore they should speed up learning.",
      "ELUs are more noise robust than PReLUs and LeakyReLUs, because their negative values saturate and thus should create a small gradient.",
      "\"ELUs encode the degree of presence of input concepts, while they do not quantify the degree of absence\"  Experiments Using ELUs  They compare ELUs to ReLUs and LeakyReLUs, but not to PReLUs (no explanation why).",
      "They seem to use a negative slope of 0.1 for LeakyReLUs, even though 0.33 is standard afaik.",
      "They use an alpha of 1.0 for their ELUs (i.e. minimum value is -1.0).",
      "MNIST classification:  ELUs achieved lower mean activations than ReLU/LeakyReLU  ELUs achieved lower cross entropy loss than ReLU/LeakyReLU (and also seemed to learn faster)  They used 5 hidden layers of 256 units each (no explanation why so many)  (No convolutions)  MNIST Autoencoder:  ELUs performed consistently best (at different learning rates)  Usually ELU > LeakyReLU > ReLU  LeakyReLUs not far off, so if they had used a 0.33 value maybe these would have won  CIFAR-100 classification:  Convolutional network, 11 conv layers  LeakyReLUs performed better during the first ~50 epochs, ReLUs mostly on par with ELUs  LeakyReLUs about on par for epochs 50-100  ELUs win in the end (the learning rates used might not be optimal for ELUs, were designed for ReLUs)  CIFER-100, CIFAR-10 (big convnet):  6.55% error on CIFAR-10, 24.28% on CIFAR-100  No comparison with ReLUs and LeakyReLUs for same architecture  ImageNet  Big convnet with spatial pyramid pooling (?)",
      "before the fully connected layers  Network with ELUs performed better than ReLU network (better score at end, faster learning)  Networks were still learning at the end, they didn't run till convergence  No comparison to LeakyReLUs"
    ],
    "author_id": "ALEJU",
    "pdf_url": "http://arxiv.org/pdf/1511.07289",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 13655370
  },
  {
    "blog_id": "fran",
    "summary": [
      "Functional Reactive Animation \u2013 Elliott & Hudak 1997  This is the paper widely acknowledged to have given birth to (Functional) Reactive Programming or FRP.",
      "The challenge that Elliott and Hudak faced was to provide an elegant and expressive way to specify animations without resorting to tedious frame-by-frame constructions.",
      "A key insight is that animations are all about how something changes over time, and time is conceptually continuous.",
      "Fran makes the notion of continuous behaviours first class in the programming model.",
      "The construction of richly interactive multimedia animations (involving audio, pictures, video, 2D and 3D graphics) has long been a complex and tedious job.",
      "Much of the difficulty, we believe, stems from the lack of sufficiently high-level abstractions, and in particular from the failure to clearly distinguish between modeling and presentation, or in other words, between what an animation is and how it should be presented\u2026 The benefits of a modeling approach to animation are similarto those in favor of a functional (or other declarative) programming paradigm, and include clarity, ease of construction, composability, and clean semantics.",
      "The modeling approach also makes animations easier to author, and easier to optimize.",
      "Fran is a collection of recursive data types, functions, and primitive graphics routines brought together around four central concepts: behaviors, events, declarative reactivity, and polymorphic media.",
      "The most novel aspect of Fran is its implicit treatment of time.",
      "Behaviours (temporal modeling)  A behaviour is a value that can vary over time.",
      "Behaviors are first-class values, and are built up compositionally; concurrency (parallel composition) is expressed naturally and implicitly.",
      "The simplest primitive behaviour is time itself.",
      "And cos time, for example,  is a behaviour  that varies as the cosine of time, created by lifting the cosine function to apply to behaviours.",
      "We can create a simple animation that changes the shape of a square over time as follows:  bigger (cos time) square  bigger scales its second argument by the amount specified in the first argument.",
      "Since the first argument is a behaviour, the result is also a behaviour.",
      "Using the over function as an infix operator we can place a circle on top whose size changes as the sine of time:  bigger (sin time) circle `over` bigger (cos time) square  Event Modeling  Events are also first class values in Fran.",
      "They can represent actual events in the external world (for example, a button press), and they can also be expressed as predicates (for example, based on proximity).",
      "Events are \u2018what\u2019, \u2018when\u2019 pairs.",
      "The when component is a lower bound on the time of the event \u2013 for examplelbp t0 represents the first left button press (\u2018what\u2019), occurring after time t0.",
      "Declarative reactivity  Here we see the first use of the \u2018reactive\u2019 term\u2026  Many behaviors are naturally expressed in terms of reactions to events.",
      "But even these \u201creactive behaviors\u201d have declarative semantics in terms of temporal composition, rather than an imperative semantics in terms of the state changes often employed in event-based formalisms.",
      "b `untilB` e  Exhibits behaviour b until event e occurs, after which it behaves according to the behaviour associated with e. Using this we can describe a colour cycle as follows:  colorCycle t0 =        red `untilB` lbp t0 *=> \\t1 -> green `untilB` lbp t1 *=> \\t2 -> colorCycle t2  update: fixed &gt; vs > in the above, thanks David for pointing out the issue.",
      "Fran is implemented in Haskell, where \\x -> \u2026 defines a lambda function with bound variable x.",
      "This reactive behaviour can thus be interpreted as:  show red until the first left-button press after time t0, and then  show green until the first left-button press after time t1 (the time of the previous lbp), and then  repeat the color cycle starting at time t2 (the time of the second lbp)  Polymorphic Media  The variety of time-varying media (images, video, sound, 3D geometry) and parameters of those types (spatial transformations, colors, points, vectors, numbers) have their own type-specific operations (e.g. image rotation, sound mixing, and numerical addition), but fit into a common framework of behaviors and reactivity.",
      "For instance, the \u201cuntilB\u201d operation used above is polymorphic, applying to all types of time-varying values.",
      "Examples  The paper includes a formal semantics of behaviours and events, as well as some implementation notes that describe an \u2018interval analysis\u2019 technique for detecting predicate events.",
      "We\u2019ll look in more detail at design and implementation considerations for reactive frameworks later this week.",
      "For now, let\u2019s just look at a few more code examples to get a feel for the expressivity of modeling that behaviours and events support.",
      "A behaviour that varies smoothly and cyclically between -1 and 1:  wiggle  = sin (pi * time)  And a behaviour that builds on this to smoothly vary between a high and low value:  wiggleRange lo hi =    lo + (hi - lo) * (wiggle + 1)/2  A red pulsating ball:  pBall = withColor red           (bigger (wiggleRange 0.5 1) circle)  Behaviours are composable, as we\u2019ve been seeing already.",
      "Let\u2019s use pBall as a building block\u2026  rBall = move (vectorPolar 2.0 time)               (bigger 0.1 pBall)  \u201cwhich yields a ball moving in a circular motion with radius 2.0 at a rate proportional to time.",
      "The ball itself is the same as pBall (red and pulsating), but 1/10 the original size.\u201d  An image that tracks the position of the mouse:  followMouse im t0 = move (mouse t0) im  As a final example, let\u2019s develop a modular program to describe \u201cbouncing balls.\u201d First note that the physical equations describing the position y and velocity v at time t of an object being accelerated by gravity g are:  where y0 and v0 are the initial position and velocity, respectively of the object at time t0.",
      "In Fran these equations are simply:  y = lift0 y0 + integral v t0 x = lift0 v0 + integral g t0  update: corrected typo in the x expression, thanks to zteve for pointing it out.",
      "Next we define a function bounce that, in addition to computing the position of an object based on the above equations, also determines when the ball has hit either the floor or the ceiling, and if so reverses the direction of the ball while reducing its velocity by a certain reciprocity, to account for loss of energy during the collision\u2026  (See figure 1 in the paper for the bounce code).",
      "Using bounce we can also simulate horizontal movement if we simply use 0 for acceleration.",
      "Here\u2019s a bouncing ball in a box:  moveXY x y   (withColor green circle) where   x = bounce xMin xMax x0 vx0 0 t0   y = bounce yMin yMax y0 vy0 g t0  See the full paper for further examples."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://conal.net/papers/icfp97/icfp97.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 40561438
  },
  {
    "blog_id": "choosing-a-cloud-dbms",
    "summary": [
      "Choosing a cloud DBMS: architectures and tradeoffs Tan et al., VLDB\u201919  If you\u2019re moving an OLAP workload to the cloud (AWS in the context of this paper), what DBMS setup should you go with?",
      "There\u2019s a broad set of choices including where you store the data, whether you run your own DBMS nodes or use a service, the kinds of instance types to go for if you do run your own, and so forth.",
      "Tan et al. use the TPC-H benchmark to assess Redshift, Redshift Spectrum, Athena, Presto, Hive, and Vertica to find out what works best and the trade-offs involved.",
      "We focused on OLAP-oriented parallel data warehouse products available for AWS and restricted our attention to commercially available systems.",
      "As it is infeasible to test every OLAP system runnable on AWS, we chose widely-used systems that represented a variety of architectures and cost models.",
      "My key takeaways as a TL;DR:  Store your data in S3  Use portable data format that gives you future flexibility to process it with multiple different systems (e.g. ORC or Parquet)  Use Athena for workloads it can support (Athena could not run 4 of the 22 TPC-H queries, and Spectrum could not run 2 of them), especially if you are doing less frequent ad-hoc queries.",
      "Which I\u2019m quite happy to see as my most recent data pipeline is based around Lambda, S3, and Athena, and it\u2019s been working great for my use case.",
      "The design space  We group the DBMS design choices and tradeoffs into three broad categories, which result from the need for dealing with (A) external storage; (B) query executors that are spun on demand; and (C) DBMS-as-a-service offerings.",
      "With regard to external storage, you could use S3 with remote storage accessible over a REST API, or block-based storage with EBS and Instance Store (InS), with EBS being the closest match for traditional database systems.",
      "InS does now offer an NVMe variant too, and the authors perform limited testing on that as well.",
      "For query executors that can be frequently started and stopped the authors explore performance with cold and warm caches (where applicable), and also the horizontal and vertical scaling performance.",
      "For the as-a-service offerings, what levels of flexibility do they offer compared to running your own systems, and how do they compare on cost?",
      "The test  The tests were done using 1000 scale factor TPC-H data (1TB uncompressed) \u2013 large enough to be I/O constrained yet still enabling queries to complete in seconds to minutes.",
      "Each systems begins from a cold start unless explicitly stated otherwise in the results.",
      "For S3 tests, data  was stored in ORC format, apart from for Vertica which used its own proprietary format.",
      "For those systems where you provide your own compute instances, the default configuration tested used a 4-node r4.8xlarge cluster with 10Gb/s networking.",
      "For cost calculations, the costs are a combination of compute costs, storage costs, data scan costs, and software license costs.",
      "Key findings  The experimental results focus on six main areas of comparison:  query restrictions  system initialisation time  query performance  cost  data compatibility with other systems  scalability  Query restrictions  Neither Spectrum nor Athena could run the full TPC-H query suite.",
      "On Athena, Q2 timed out after an hour, and Q8, Q9 and Q21 failed after exhausting resources.",
      "On Spectrum, Q15 failed because views are not supported, and Q22 failed because complex subqueries are not supported.",
      "(See \u00a72.4 in the TPC-H Benchmark Standard for details of the queries).",
      "System initialisation time  Initialisation time measures how easy it is to launch and use a given DBMS (doesn\u2019t apply to Athena, which is \u2018always-on\u2019).",
      "Most systems have initialisation times in the range of 5-7 minutes, with Redshift an outlier at around 12 minutes.",
      "It is advantageous in the cloud to shut down compute resources when they are not being used, but there is then a query latency cost.",
      "All cloud nodes require time to initialize and pass system checks before a DBMS can be started, with systems using local storage like Redshift taking longest to initialize.",
      "Serverless o\ufb00erings like Athena provide an alternative \u201cinstant on\u201d query service.",
      "Query performance  Query performance is measured from both warm and cold caches.",
      "Of the 16/22 queries that can be run across all of the system under test, Athena and Redshift are the best performers (though interestingly, not Redshift Spectrum).",
      "If you can afford to keep it running (no initialisation costs on subsequent queries), and with a warm cache, Redshift offers the best performance for frequent querying.",
      "The warm cache benefits across the board are not as great as might be expected, but this is a characteristic of the workload where most queries are not I/O bound and those that are tend to be shorter.",
      "Another interesting experiment here compared the effects on performance of different storage types.",
      "In general, InS performance is faster than EBS, and EBS is faster than S3 on the same system, as expected.",
      "However, the magnitude of the difference may be exaggerated depending on how well that system utilizes a specific storage type.",
      "The results found that there is a performance advantage from faster storage, but it\u2019s not as big as you might think.",
      "Using cheap remote object storage instead of more expensive EBS block storage seems fine, and even on heavily I/O bound workloads the cost advantage of S3 far exceeds its performance disadvantage.",
      "Cost  The following charts show the query costs for both cold start and subsequent runs:  Redshift Spectrum stands out as the expensive option here!",
      "\u201cRedshift is not a cost-effective system if one relies heavily on pulling data from S3.\u201c  Athena\u2019s cost-per-query is on a par with other systems, which coupled with its good query performance gives it very competitive cost/performance.",
      "Therefore, a cloud DBMS user should consider Athena as an option for select workloads and utilize storage that can be accessed by serverless options.",
      "When it comes to the storage part of the cost equation, using EBS is much more expensive than S3.",
      "\u201cOur setup found a 50x storage cost increase for EBS while only providing a 2x performance speedup.\u201c  Data compatibility  Because the cloud offers the ability to easily launch new systems, being able to leverage different systems for different workloads is advantageous.",
      "However, ETL costs can make some system types infeasible to use when workloads change, thereby limiting the cloud offerings one can leverage.",
      "Green cells in the compatibility matrix below show where systems can use compatible formats from the same storage system, e.g. ORC on S3.",
      "Scalability  We analyzed performance when scaling horizontally and vertically\u2026 The main theme is that horizontal scaling is generally advantageous, while vertical scaling is generally disadvantageous.",
      "Scaling tests on AWS proprietary system are limited by the options they provide to the end user.",
      "The last word  Each of these findings poses opportunities for future work to explore specific architectural tradeoffs further.",
      "Additionally, future studies could analyze concurrency, test a different suite such as TPC-DS, evaluate different data sizes, and evaluate more systems."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://vldb.org/pvldb/vol12/p2170-tan.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 73515355
  },
  {
    "blog_id": "incorporating-a-copying-mechanism-in-sequence-to-sequence-learning",
    "summary": [
      "Incorporating copying mechanism in sequence to sequence learning Gu et al. 2016, with a side-helping of Neural machine translation by jointly learning to align and translate Bahdanau et al.",
      "ICLR 2015  Today\u2019s paper shows how the sequence-to-sequence conversational model we looked at yesterday can be made to seem more natural by including a \u201ccopying mechanism\u201d that is an important part of human conversation.",
      "Consider the following examples where the blue subsequence from the input (I) is repeated in the response (R):  It\u2019s easy for humans to do, but it turns out we require quite a lot of additional machinery over and above the base sequence-to-sequence approach in order to teach computers to do it.",
      "We argue that it will benefit many Seq2Seq tasks to have an elegant unified model that can accommodate both understanding and rote memorization (copying).",
      "Towards this goal, we propose CopyNet, which is not only capable of the regular generation of words but also the operation of copying appropriate segments of the input sequence.",
      "Despite the seemingly \u201chard\u201d operation of copying, CopyNet can be trained in an end-to-end fashion.",
      "One of the areas where the copying mechanism seems to add a lot of value is in coping with \u201cout of vocabulary\u201d (OOV) words.",
      "Words not included in the vocabulary could be entity names for example, and echoing these in responses is often appropriate.",
      "Background: adding an attention mechanism to seq2seq  CopyNet itself builds on the work of Bahdanau et al. in \u201cNeural Machine Translation by Jointly Learning to Align and Translate.\u201d So let\u2019s take a brief detour to understand the basic attention mechanism idea in that paper.",
      "The setting is machine translation using sequence-to-sequence , which struggles to cope with long sentences where all of the necessary information in the source sentence needs to be compressed into a fixed-length vector.",
      "The performance of a basic encoder-decoder pair deteriorates rapidly as the length of the input sentence increases.",
      "So the trick is not to try and compress the whole sentence when looking for a translation, but instead to focus attention on different parts of the source sentence at different points in the translation.",
      "The most important distinguishing feature of this approach from the basic encoder\u2013decoder is that it does not attempt to encode a whole input sentence into a single fixed-length vector.",
      "Instead, it encodes the input sentence into a sequence of vectors and chooses a subset of these vectors adaptively while decoding the translation.",
      "This frees a neural translation model from having to squash all the information of a source sentence, regardless of its length, into a fixed-length vector.",
      "We show this allows a model to cope better with long sentences.",
      "The resulting architecture uses a bi-directional RNN as an encoder (more on that shortly), and a decoder that \u2018emulates searching through a source sentence during decoding a translation.\u2019  The decoder relies on a series of context annotations, one for each word in the source sentence.",
      "Each annotation hi contains information about the whole input sequence, with a strong emphasis on the parts surrounding the ith word of the input sequence.",
      "A learned alignment model weights these annotations to score how well the inputs around position j and the outputs at position i match.",
      "Intuitively, this implements a mechanism of attention in the decoder.",
      "The decoder decides parts of the source sentence to pay attention to.",
      "By letting the decoder have an attention mechanism, we relieve the encoder from the burden of having to encode all information in the source sentence into a fixed- length vector.",
      "With this new approach the information can be spread throughout the sequence of annotations, which can be selectively retrieved by the decoder accordingly.",
      "In normal encoding, we just read each input symbol starting from the first symbol and proceeding to the last one.",
      "But we want the annotation for each word to summarize not only what has come before it, but what follows it.",
      "The solution is simple \u2013 encode the sequence both forwards and backwards (a bi-directional RNN) and concatenate the forward and backward state for each symbol.",
      "CopyNet overview  CopyNet follows the general encoder-decoder pattern, and uses a bi-directional RNN to encode the source sequence.",
      "The encoded representation can be thought of as a short-term memory, M.  The decoder reads M and predicts the target sequence.",
      "It uses a similar attention mechanism to Bahdanau et al., with a few important differences:  Words are predicted based on a probabilistic model combining two modes: a generate mode and a copy mode.",
      "The generate mode uses the same scoring function as a generic encoder-decoder (see e.g. Bahdanau et al.).",
      "Copy mode picks words from the source sequence using the hidden states in M to represent each word, using a non-linear activation function (tanh).",
      "The two modes are combined with a shared normalization term, and so are basically competing through a softmax function.",
      "See section 3.2 for further details, I\u2019m not sure I can do them justice here\u2026  CopyNet uses both the previous state and a weighted sum of the hidden states in M in order to update each decoding state at every step.",
      "This selective read is designed for the copy mode and focuses attention on the source sequence encoded in the hidden state.",
      "A properly trained encoder will have captured both the semantics of a word and its location in the input in the hidden states in M.  We hypothesize that COPYNET uses a hybrid strategy for fetching the content in M, which combines both content-based and location-based addressing.",
      "Both addressing strategies are coordinated by the decoder RNN in managing the attentive read and selective read, as well as determining when to enter/quit the copy-mode\u2026 Unlike the explicit design for hybrid addressing in the Neural Turing Machine , COPYNET is more subtle; it provides the architecture that can facilitate some particular location-based addressing and lets the model figure out the details from the training data for specific tasks.",
      "CopyNet experiments  The authors use CopyNet in three different tasks: a synthetic dataset to show that it can learn rules requiring copying of symbols outside of its vocabulary (it can); a text summarization task; and simple single-turn dialogues.",
      "On text summarization, CopyNet \u2018beats the competitor models by a big margin.\u2019 But it\u2019s the dialogue performance we\u2019re most interested in this week.",
      "Dialogue data is collected from Baidu Tieba with real-life conversations covering greetings and sports etc.",
      "Patterns are mined from the set, e.g. \u201cHi, my name is Adrian\u201d followed by \u201cHi, Adrian\u201d can lead to the pattern \u201chi, my name is X -> hi, X\u201d simply by looking for copied subsequences.",
      "The dataset is enlarged by filling the slots with suitable subsequences (e.g. name entities, dates etc.).",
      "Using this slot filling, two datasets are created based on 173 collected patterns.",
      "CopyNet is able to accurately replicate critical segments from the input using copy mode, and generates the rest of the answers smoothly using the generate mode.",
      "(Click to enlarge)."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://arxiv.org/pdf/1603.06393v3.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 29703573
  },
  {
    "blog_id": "ddsketch",
    "summary": [
      "DDSketch: a fast and fully-mergeable quantile sketch with relative-error guarantees Masson et al., VLDB\u201919  Datadog handles a ton of metrics \u2013 some customers have endpoints generating over 10M points per second!",
      "For response times (latencies) reporting a simple metric such as \u2018average\u2019 is next to useless.",
      "Instead we want to understand what\u2019s happening at different latency percentiles (e.g p99).",
      "The ability to compute quantiles over aggregated metrics has been recognized to be an essential feature of any monitoring system\u2026 Given how expensive calculating exact quantiles can be for both storage and network bandwidth, most monitoring system will compress the data into sketches and compute approximate quantiles.",
      "Fortunately there are plenty of quantile sketching algorithms available including the GK-sketch, the t-digest, the HDR histogram, and the Moments sketch that we looked at last year.",
      "For reasons we\u2019ll see shortly though, none of those were good enough for Datadog, so they developed their own sketching data structure, DDSketch.",
      "Officially in the paper DDSketch stands for \u2018Distributed Distribution Sketch\u2019 but that seems a bit of a stretch\u2026 surely it\u2019s the \u2018Datadog Sketch\u2019 !",
      "A glance at the code repository for the Python implementation confirms my suspicion: there are several references to \u2018DogSketch\u2019 in the commit history and the codebase still ;).",
      "In putting together this write-up I\u2019m grateful to the first author, Charles Masson, who gave a great talk on this paper at the VLDB conference and was kind enough to share his slides with me.",
      "Some of the images in this post are taken from Charles\u2019 slide deck with permission.",
      "Why do we need a new sketch?",
      "A characteristic of latency distributions is that they have a long tail.",
      "This can play havoc with quantile sketches that are based on rank error.",
      "At the 50th percentile, an accuracy within 0.5% is still pretty tightly bound in absolute terms.",
      "But the further out into the tail we go, the more spread apart things get:  In this example, you can see that the GK-sketch, which uses rank error, is out by 1300ms compared to the ground truth.",
      "And look how noisy the sketch is!",
      "In another example in the paper, a rank accuracy of 0.5% at the 99%-ile guarantees us a value between the 98.5th and 99.5th percentiles\u2026  In this case, this is anywhere from 2 to 20 seconds, which from an end user\u2019s perspective is the difference between an annoying delay and giving up on the request.",
      "t-digest sketches are also rank based, but give lower errors on quantiles further away from the medium than uniform rank-error sketches.",
      "But even so, the error is still relatively high on heavily-tailed datasets.",
      "The HDR Histogram uses relative error, which retains accuracy into the tail (you\u2019d expect nothing less from it\u2019s creator, Gil Tene, who surely knows a thing or two about performance analysis !).",
      "Its two weaknesses are that it can only handle a bounded (but large!)",
      "range, and that it has no published guarantees.",
      "The Moments sketch only guarantees it\u2019s accuracy for the average rank error (not the worst case).",
      "We want a fast-to-insert, fully mergeable (we can combine sketches in a distributed manner), space-efficient quantile sketch with relative error guarantees.",
      "So it looks like we\u2019re going to need a new sketch!",
      "How DDSketch works  First let\u2019s define what we mean by relative error.",
      "An \u03b1-accurate  -sketch outputs quantiles within  of the true value  for all quantiles  ,  .",
      "E.g., if \u03b1 is 1%, then we\u2019ll be within 1% of the true value.",
      "The basic version of the sketch gives (0,1) \u03b1-accurate sketches (i.e. they\u2019re accurate over the full range).",
      "We\u2019re going to divide response times into buckets, and count the number of points that fall into each bucket.",
      "The trick is to assign the bucket sizes (boundaries) such that when we compute quantiles using the bucket counts we retain the desired accuracy.",
      "Let  .",
      "So for  we have  .",
      "Bucket  will be used to store values between  and  .",
      "In practical terms, when we see a value  we take  and round up to the nearest whole number to get the bucket number.",
      "Then we just increment the counter for that bucket.",
      "Insertion is therefore relatively fast, and merging two sketches with the same  is as simple as summing up bucket counts by index.",
      "Now, when we want to know the estimate of the qth quantile we add up the bucket counts until we find the bucket containing the qth quantile value.",
      "With n total elements counted, it looks like this:  However buckets are stored in memory (e.g., as a dictionary that maps indices to bucket counters, or as a list of bucket counters for contiguous indices), the memory size of the sketch is at least linear in the number of non-empty buckets.",
      "So given a pathological input of n points each falling into a different bucket, we have worse case size O(N).",
      "To keep a bound on the size we can make one small tweak, yielding the the full DDSketch in all its glory:  The full version of DDSketch is a simple modification that addresses its unbounded growth by imposing a limit of  on the number of buckets it keeps track of.",
      "It does so by collapsing the buckets for the smallest indices:  When we merge two sketches that have both independently kept their number of buckets within the limit, we might end up with more buckets than we wanted.",
      "So we do the same bucket collapsing trick on merging as well to fix that.",
      "The paper of course has proofs that all this provides the desired guarantees, but I\u2019m just going to take their word for it ;).",
      "For response times we don\u2019t need to worry about negative values.",
      "But if you have a use case where you do, the solution is to keep one DDSketch for positive numbers, and another one for negative numbers.",
      "DDSketch in action  How well does it work?",
      "How about this:  Beautiful tracking of the true values without any discernible noise :).",
      "In the evaluation DDSketch (fast) stores buckets in a contiguous way for fast addition, and DDSketch (vanilla) stores buckets in a sparse way for smaller memory footprint.",
      "DDSketch is pretty good in terms of memory usage:  Has fast insert times:  And super-fast merges:  [DDSketch is] the first fully-mergeable, relative-error quantile sketching algorithm with formal guarantees.",
      "The sketch is extremely fast and accurate, and is currently being used by Datadog at a wide-scale.",
      "Yes, the academic literature will probably continue to refer to it as the \u2018Distributed Distribution\u2019 sketch.",
      "But you and I can just call it the \u201cDatadog\u201d sketch :).",
      "The kind folks at Datadog have even open sourced implementations for you in Python , Go , and Java ."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://www.vldb.org/pvldb/vol12/p2195-masson.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 40771210
  },
  {
    "blog_id": "economic-factors-of-vulnerability-trade-and-exploitation",
    "summary": [
      "Economic factors of vulnerability trade and exploitation Allodi, CCS\u201917  Today we\u2019re going on a tour inside a prominent Russian cybercrime market, identified in the paper as \u2018RuMarket\u2019 (not its real name).",
      "The author infiltrated the market using a fake identity, and was subsequently able to collect data on market activities from 2010 to 2017.",
      "RuMarket is a forum-based market that can be reached from the open Internet.",
      "Access to the market requires explicit admission by the market administrators, who validate the access request by performing background checks on the requester\u2026 As members of the market, we have access to all the (history of) product information, trades and prices available to active participants.",
      "This analysis spans seven years of market activity (July 2010 \u2013 April 2017).",
      "The study particularly looks at the trade in vulnerabilities.",
      "By far the most activity (by an order of magnitude) surrounds vulnerabilities that are marketed in reference to one or more CVEs.",
      "Searching and filtering on vulnerabilities traded by CVE resulted in a set of 89 traded exploits over 57 unique vulnerabilities, that were embedded in 38 different packages.",
      "The packages can be one of three different types:  Standalone packages sell stand-alone exploit that are then personalised by the buyer (e.g., to add their own shellcode).",
      "Malware packages are exploits embedded in malware packaging services.",
      "EKITS are exploit packages offered under an \u2018exploit-as-a-service\u2019 model in which the seller maintains a service which customers rent to deliver their own attacks.",
      "The authors find 26 standalone packages, 6 malware packages, and 6 exploit kits.",
      "The exploit kits of course contain an exploit portfolio within them that the kit owners keep up to date.",
      "Over time, the number of vendors trading in the market has been steadily growing:  The top EKIT packages remain active in the market board for more than 3 years, whereas standalone and malware packages have a shorter shelf-life.",
      "Overall, the average package remains active for a year following initial publication.",
      "The lifecycle of an exploit  Most new exploits appear in the market as standalone products, and later on these get re-packaged into malware and exploit kit distributions.",
      "The overall trend is for exploit kits to specialise using fewer, more reliable exploits that at their original introduction.",
      "Standalone exploits therefore seem to play a role in the \u2018innovation\u2019 process in RuMarket; this may indicate the presence of an \u2018exploit chain\u2019 whereby the most reliable and effective standalone exploits are selected for future inclusion in exploit kit products for deployment at scale.",
      "The top 2.5% of standalone exploits are published in the market within a week of the public disclosure of the corresponding CVE, the top 25% within 40 days, and the top 50% within 2.5 months.",
      "The gap between CVE disclosure and exploits packaged in malware or exploit kits is much longer \u2013 about half a year on average.",
      "Overall, most packaged exploits take around four months from disclosure date to be published.",
      "If we look at the speed of exploit packaging by year though, a trend emerges:  We observe that the mean exploit age decreases steadily for more recent publication dates, indicating that exploit vendors are becoming faster in releasing exploits for newly disclosed vulnerabilities.",
      "The coefficient of linear regression indicates that exploits appear at an approximately 30% faster rate every year.",
      "Exploit targets  The favourite exploit targets are Microsoft, Adobe, and Oracle products.",
      "Microsoft vulnerabilities account for more than half of the standalone products.",
      "After 2013, with the release of plug-in blocking in browsers and improvements to the default security settings in Java, the packaging of Oracle exploits drops off.",
      "How much is that exploit in the window?",
      "Standalone packages mostly bundle a single exploit, and trade for up to 8000 USD, with a mean price of 2000 USD.",
      "Malware packages trade at between 400 and 4000 USD, with most in the 1000-2000 USD range.",
      "Exploit kits are typically rented for a period of two to three weeks.",
      "Basic exploit kits go for 150-400 USD, and those embedding a wider range of exploits rent for 400-2000 USD.",
      "The prices for standalone and malware packages show an increasing trend, whereas exploit kit prices are steadily decreasing.",
      "If we break the figures down by exploit target, we see that Microsoft exploits are the most expensive, and within the Microsoft category, exploits targeting Office and Windows are the most expensive of all.",
      "(Which makes sense, because these give the attacker the most potential targets).",
      "We find that baseline prices for exploits vary widely by software vendor, and are negatively correlated with the age of the exploit; Adobe and Microsoft exploits retain their value significantly longer than Oracle exploits.",
      "This may indicate a prolonged economic interest in the exploitation of Microsoft and Adobe vulnerabilities, a finding consistent with related work on the persistence of vulnerabilities on end-user systems.",
      "Are traded exploits used in the wild?",
      "Symantec\u2019s threat explorer and attack signature databases contain information on exploits in the wild, with associated CVE-IDs.",
      "Thus it becomes possible to look at trading activity of packaged exploits, and compare it to attacks in the wild.",
      "Exploits with more active forum discussions correlate with a higher chance of exploitation in the wild:  Moreover, the lower the cost of the exploit package, the more likely it is to see the exploit used in the wild:  \u2026everything else being equal, exploits bundled in more expensive packages are less likely to be detected in the wild than comparable exploits bundled in less expensive packages.",
      "These findings weigh favorably on the existence of a relation between market activity and exploit deployment in the wild.",
      "Exploit kits play an important role here.",
      "The exploit kit model allows vendors to reduce their exploit development and deployment costs, thus offering exploits at lower prices.",
      "The combination of lower prices and \u2018as-a-service\u2019 delivery makes the exploits available to a wider audience, generating more attacks overall.",
      "This suggests that the criminal business model may play a central role in the diffusion of cyber-attacks, and calls for additional studies characterizing this effect.",
      "The severity of the vulnerability also matters.",
      "Critical vulnerabilities with exploits traded in RuMarket have a much higher chance of exploitation (93%) than non-critical vulnerabilities (62%).",
      "Bug bounties  The prices available for exploits in modern bug-bounty programs are roughly in line or below the prices for the same exploits on RuMarket.",
      "Thus it is still more profitable for a vendor to sell their exploits to different buyers multiple times on the RuMarket than to move to \u2018legitimate\u2019 vulnerability markets as in the latter case you can trade the exploit only once.",
      "(And what\u2019s stopping a vendor from trading an exploit in both markets, on the assumption that patching in the wild takes time???).",
      "The last word  Our findings quantify a strong correlation between market activities and likelihood of exploit.",
      "We find that the analyzed market shows signs of expansion, and that exploit-as-a-service models may allow for drastic cuts in exploit development costs.",
      "Further, we find that exploit prices are aligned with or above those of \u2018legitimate\u2019 vulnerability markets, supporting work on the identification of incentives for responsible vulnerability disclosure and attack economics."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://arxiv.org/pdf/1708.04866",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 45765062
  },
  {
    "blog_id": "gan-dissection-visualizing-and-understanding-generative-adversarial-networks",
    "summary": [
      "GAN dissection: visualizing and understanding generative adversarial networks Bau et al., arXiv\u201918  Earlier this week we looked at visualisations to aid understanding and interpretation of RNNs, today\u2019s paper choice gives us a fascinating look at what happens inside a GAN (generative adversarial network).",
      "In addition to the paper, the code is available on GitHub and video demonstrations can be found on the project home page .",
      "We\u2019re interested in GANs that generate images.",
      "To a human observer, a well-trained GAN appears to have learned facts about the objects in the image: for example, a door can appear on a building but not on a tree.",
      "We wish to understand how a GAN represents such a structure.",
      "Do the objects emerge as pure pixel patterns without any explicit representation of objects such as doors and trees, or does the GAN contain internal variables that correspond to the objects that humans perceive?",
      "If the GAN does contain variables for doors and trees, do those variables cause the generation of those objects, or do they merely correlate?",
      "How are relationships between objects represented?",
      "The basis for the study is three variants of Progressive GANs trained on LSUN scene datasets.",
      "To understand what\u2019s going on inside these GANs the authors develop a technique involving a combination of dissection and intervention.",
      "Given a trained segmentation model (i.e., a model that can map pixels in an image to one of a set of pre-defined object classes),  we can dissect the intermediate layers of the GAN to identify the level of agreement between individual units and each object class.",
      "The segmentation model used in the paper was trained on the ADE20K scene dataset and can segment an input image into 336 object classes, 29 parts of large objects, and 25 materials.",
      "Dissection can reveal units that correlate with the appearance of objects of certain classes, but is the relationship causal?",
      "Two difference types of intervention help us to understand this better.",
      "First, we can ablate those units (switch them off), and see if the correlated objects disappear from an image in which they were previously present.",
      "Second, we can force the units on and see if the correlated objects appear in an image in which they were previously absent.",
      "The very first figure in the paper provides an excellent overview.",
      "Here we can see (a) a set of generated images of churches; and (b) the results of dissection identify GAN units matching trees.",
      "When we ablate those units ( c ) the trees largely disappear, and when we deliberately activate them (d) trees re-appear.",
      "The same insights can be used for human-guided model improvements.",
      "Here we see generated images with artefacts (f).",
      "If we identify the GAN units that cause those artefacts (e), and ablate them we can remove unwanted artefacts from generated images (g).",
      "Characterising units by dissection  For dissection we take a upsampled and thresholded feature map of a unit and compare it to the segmentation map of a given object class.",
      "The extent of agreement is captured using an intersection-over-union (IoU) measure.",
      "We take the intersection of the thresholded image and the pixels defined as belonging to the segment class, and divide it by their union.",
      "The result tells us what fraction of the combined pixels are correlated with the class.",
      "The following examples show units with high IoU scores for the classes \u2018table\u2019 and \u2018sofa\u2019.",
      "Finding causal relationships through intervention  We can say that a given hidden unit causes the generation of object(s) of a given class if ablating that unit causes the object to disappear, and activating it causes the object to appear.",
      "Averaging effects over all locations and images gives us the average causal effect (ACE) of a unit on the generation of a given class.",
      "While these measures can be applied to a single unit, we have found that objects tend to depend on more than one unit.",
      "Thus we need to identify a set of units U that maximize the average causal effect for an object class  .",
      "This set is found by optimising an objective that looks for a maximum class difference between images with partial ablation and images with partial insertion, using a parameter than controls the contribution of each unit.",
      "Here you can see the effects of increasing larger sets of hidden units, in this case identified as associated with the class \u2018tree\u2019.",
      "Findings from GAN analysis  Units emerge that correlate with instances of an object class, with diverse visual appearances.",
      "The units are learning abstractions.",
      "The set of all object classes matched by units of a GAN provide a map of what a GAN has learned about the data.",
      "The units that emerge are object classes appropriate to the scene type: for example, when we examine a GAN trained on kitchen scenes, we find units that match stoves, cabinets, and the legs of tall kitchen stools.",
      "Another striking phenomenon is that many units represent parts of objects: for example, the conference room GAN contains separate units for the body and head of a person.",
      "The type of information represented changes from layer to layer.",
      "Early layers  remain entangled, middle layers have many units matching semantic objects and object parts, and later layers have units matching pixel patterns such as materials, edges, and colors.",
      "Here\u2019s an interesting layer-by-layer breakdown of a Progressive GAN trained to generate LSUN living room images:  Compared to a baseline Progressive GAN, adding minibatch stddev statistics increases the realism of the outputs.",
      "The unit analysis shows that it also increases the diversity of the concepts represented by units.",
      "Turning off (ablating) units identified as associated with common object classes causes the corresponding objects to mostly disappear from the generated scenes.",
      "Not every object can be erased though.",
      "Sometimes the object seems to be integral to the scene.",
      "For example, when generating conference rooms the size and density of tables and chairs can be reduced, but they cannot be eliminated entirely.",
      "By forcing units on we can try to insert objects into scenes.",
      "For example, activating the same \u2018door units\u2019 across a variety of scenes causes doors to appear \u2013 but the actual appearance of the door will vary in accordance with the surrounding scene.",
      "We also observe that doors cannot be added in most locations.",
      "The locations where a door can be added are highlighted by a yellow box\u2026 it is not possible to trigger a door in the sky or on trees.",
      "Interventions provide insight into how a GAN enforces relationships between objects.",
      "Even if we try to add a door in layer 4, that choice can be vetoed later if the object is not appropriate for the context.",
      "By carefully examining representation units, we have found many parts of GAN representations can be interpreted, not only as signals that correlate with object concepts but as variables that have a causal effect on the synthesis of objects in the output.",
      "These interpretable effects can be used to compare, debug, modify, and reason about a GAN model.",
      "There remain open questions for future work.",
      "For example, why can a door not be inserted in the sky?",
      "How does the GAN suppress the signal in the later layers?",
      "Understanding the relationships between the layers of a GAN is the next hurdle\u2026"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://arxiv.org/pdf/1811.10597",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 41840073
  },
  {
    "blog_id": "towards-multiverse-databases",
    "summary": [
      "Towards multiverse databases Marzoev et al., HotOS\u201919  A typical backing store for a web application contains data for many users.",
      "The application makes queries on behalf of an authenticated user, but it is up to the application itself to make sure that the user only sees data they are entitled to see.",
      "Any frontend can access the whole store, regardless of the application user consuming the results.",
      "Therefore, frontend code is responsible for permission checks and privacy-preserving transformations that protect user\u2019s data.",
      "This is dangerous and error-prone, and has caused many real-world bugs\u2026 the trusted computing base (TCB) effectively includes the entire application.",
      "The central idea behind multiverse databases is to push the data access and privacy rules into the database itself.",
      "The database takes on responsibility for authorization and transformation, and the application retains responsibility only for authentication and correct delegation of the authenticated principal on a database call.",
      "Such a design rules out an entire class of application errors, protecting private data from accidentally leaking.",
      "It would be safer and easier to specify and transparently enforce access policies once, at the shared backend store interface.",
      "Although state-of-the-are databases have security features designed for exactly this purpose, such as row-level access policies and grants of views, these features are too limiting for many web applications.",
      "In particular, data-dependent privacy policies may not fit neatly into row- or column-level access controls, and it may be permissible to expose aggregate or transformed information that traditional access control would prevent.",
      "With multiverse databases, each user sees a consistent \u201cparallel universe\u201d database containing only the data that user is allowed to see.",
      "Thus an application can issue any query, and we can rest safe in the knowledge that it will only see permitted data.",
      "The challenging thing of course, is efficiently maintaining all of these parallel universes.",
      "We\u2019ll get to that, but first let\u2019s look at some examples of privacy policies and how they can be expressed.",
      "Expressing privacy policies  In the prototype implementation, policies are expressed in a language similar to Google Cloud Firestore security rules.",
      "A policy just needs to be a deterministic function of a given update\u2019s record data and the database contents.",
      "Today the following are supported:  Row suppression policies (e.g. exclude rows matching this pattern)  Column rewrite policies (e.g.",
      "translate / mask values)  Group policies, supporting role-based (i.e., data-dependent access controls)  Aggregation policies, which restrict a universe to see certain tables or columns only in aggregated or differentially private form.",
      "Consider a class discussion forum application (e.g. Piazza) in which students can post questions that are anonymous to other students, but not anonymous to instructors.",
      "We can express this policy with a combination of row suppression and column rewriting:  Maybe we want to allow teaching assistants (TAs) to see anonymous posts in the classes they teach.",
      "We can define a group via a membership condition and then attach policies to that group:  Write policies (not supported in the current implementation) permit specification of allowed updates.",
      "For example:  An aggregation policy could be used to rewrite any matching aggregation into a differentially-private version.",
      "The basis for this could be e.g. Chan et al.\u2019s \u2018 Private and continual release of statistics \u2019.",
      "Composing such policies with other policies remains an open research question.",
      "Managing universes  A multiverse database consists of a base universe, which represents the database without any read-side privacy policies applied, and many user universes, which are transformed copies of the database.",
      "For good query performance we\u2019d like to pre-compute these per-user universes.",
      "If we do that naively though, we\u2019re going to end up with a lot of universes to store and maintain and the storage requirements alone will be prohibitive.",
      "A space- and compute-efficient multiverse database clearly cannot materialize all user universes in their entirety, and must support high-performance incremental updates to the user universes.",
      "It therefore requires partially-materialized views that support high-performance updates.",
      "Recent research has provided this missing key primitive.",
      "Specifically, scalable, parallel streaming dataflow computing systems now support partially-stateful and dynamically-changing dataflows.",
      "These ideas make an efficient multiverse database possible.",
      "So, we make the database tables in the base universe be the root vertices of a dataflow, and as the base universe is updated records move through the flow into user universes.",
      "Where an edge in the dataflow graph crosses a universe boundary, any necessary dataflow operators to enforce the required privacy policies are inserted.",
      "All applicable policies are applied on every edge that transitions into a given user universe, so whichever path data takes to get there we know the policies will have been enforced.",
      "We can build the dataflow graph up dynamically, extending the flow\u2019s for a user\u2019s universe the first time a query is executed.",
      "The amount of computation required on a base update can be reduced by sharing computation and cached data between universes.",
      "Implementing this as a joint partially-stateful dataflow is the key to doing this safely.",
      "By reasoning about all users\u2019 queries as a joint dataflow, the system can detect such sharing: when identical dataflow paths exist, they can be merged.",
      "Logically distinct, but functionally equivalent dataflow vertices can also share a common backing store.",
      "Any record reaching such a vertex in a given universe implies that universe has access to it, so the system can safely expose the shared copy.",
      "Just as user universes can be created on demand, so inactive universes can be destroyed on demand as well.",
      "Under the covers, these are all manipulations of the dataflow graph, which partially-stateful dataflow can support without downtime.",
      "Prototype evaluation  The authors have built a prototype implementation of these ideas based on the Noria dataflow engine.",
      "It runs to about 2,000 lines of Rust.",
      "A Piazza-style class forum discussion application with 1M posts, 1,000 classes, and a privacy policy allowing TAs to see anonymous posts is used as the basis for benchmarking.",
      "The team compare the prototype with 5,000 active user universes, a MySQL implementation with inlined privacy policies (\u2018with AP\u2019) and a MySQL implementation that does not enforce the privacy policy (\u2018without AP\u2019):  Since the prototype is serving reads from a pre-computed universe stored in memory cached results are fast and make for a very favourable comparison against MySQL.",
      "Writes are significantly slower though (about 2x) \u2013 much of this overhead is in the implementation rather than essential.",
      "Memory footprint is 0.5GB with one universe, and 1.1GB with 5,000 universes, introduces a shared record store for identical queries reduces their space footprint by 94%.",
      "These results are encouraging, but a realistic multiverse database must further reduce memory overhead and efficiently run millions of user universes across machines.",
      "Neither Noria nor any other current dataflow system support execution of the huge dataflows that such a deployment requires.",
      "In particular, changes to the dataflow must avoid full traversals of the dataflow graph for faster universe creation.",
      "Support for write authorization policies (with some tricky consistency considerations for data-dependent policies) is future work, as is the development of a policy-checker (perhaps similar to Amazon\u2019s SMT-based policy checker for AWS ) to help ensure policies themselves are consistent and complete.",
      "Our initial results indicate that a large, dynamic, and partially-stateful dataflow can support practical multiverse databases that are easy to use and achieve good performance and acceptable overheads.",
      "We are excited to further explore the multiverse database paradigm and associated research directions."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://people.csail.mit.edu/malte/pub/papers/2019-hotos-multiversedb.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 79305631
  },
  {
    "blog_id": "using-word-embedding-to-enable-semantic-queries-on-relational-databases",
    "summary": [
      "Using word embedding to enable semantic queries in relational databases Bordawekar and Shmeuli, DEEM\u201917  As I\u2019m sure some of you have figured out, I\u2019ve started to work through a collection of papers from SIGMOD\u201917.",
      "Strictly speaking, this paper comes from the DEEM workshop held in conjunction with SIGMOD, but it sparked my imagination and I hope you\u2019ll enjoy it too.",
      "Plus, as a bonus it\u2019s only four pages long!",
      "What do you get if you cross word embedding vectors with a relational database?",
      "The ability to ask a new class of queries, which the authors term cognitive intelligence (CI) queries, that ask about the semantic relationship between tokens in the database, rather than just syntactic matching as is supported by current queries.",
      "It\u2019s a really interesting example of AI infusing everyday systems.",
      "We begin with a simple observation: there is a large amount of untapped latent information within a database relation.",
      "This is intuitively clear for columns that contain unstructured text.",
      "But even columns that contain different types of data, e.g., strings, numerical values, images, dates, etc., possess significant latent information in the form of inter- and intra-column relationships.",
      "If we understood the meaning of these tokens in the database (at least in some abstract way that was comparable), we could ask queries such as \u201cshow me all the rows similar to this.\u201d That\u2019s something you can\u2019t easily do with relational databases today \u2013 excepting perhaps for range queries on specific types such as dates.",
      "Where can we get comparable abstract representations of meaning though?",
      "The answer is already given away in the paper title of course \u2013 this is exactly what word embedding vectors do for us!",
      "If you\u2019re not familiar with word embedding vectors, we covered word2vec and GloVe in The Morning Paper a while back.",
      "In fact, \u201c The Amazing Power of Word Vectors \u201d continues to be one of the most read pieces on this blog.",
      "In short:  The idea of word embedding is to fix a d-dimensional vector space and for each word in a text corpus associate a dimension d vector of reals numbers that encodes the meaning of that word\u2026 If two words have similar meaning, their word vectors point in very similar directions.",
      "The authors use word2vec in their work, though as they point out they could equally have used GloVe.",
      "How do we get word embedding vectors for database content?",
      "One approach is to use word vectors that have been pre-trained from external sources.",
      "You can also learn directly from the database itself.",
      "Think of each row as corresponding to a sentence, and a relation as a document.",
      "Word embedding then can extract latent semantic information in terms of word (and in general, token) associations and co-occurrences and encode it in word vectors.",
      "Thus, these vectors capture first inter- and intra-attribute relationships within a row (sentence) and then aggregate these relationships across the relation (document) to compute the collective semantic relationships.",
      "In their prototype implementation, the authors first textify (!)",
      "the data in a database table (e.g., using a view), and then use a modified version of word2vec to learn vectors for the words (database tokens) in the extracted text.",
      "This phase can also use an external source (e.g. Wikipedia articles) for model training.",
      "We use word as a synonym to token although some tokens may not be valid words in any natural language.",
      "Following vector training, the resultant vectors are stored in a relational system table.",
      "At runtime, the system (built on Spark using Spark SQL and the DataFrames API) uses UDFs to fetch trained vectors from the system and answer CI queries.",
      "CI Queries  Broadly, there are two classes of cognitive intelligence queries: similarity and prediction queries\u2026 The key characteristic of the CI queries is that these queries are executed, in part, using the vectors in the word embedding model.",
      "If the word embedding model is generated using the database being queried, it captures meaning in the context of the associated relational table, as specified by the relational view.",
      "If a model is rebuilt using a different relational view, a CI query may return different results for the new model.",
      "It\u2019s time to look at some concrete examples to make all this a bit clearer.",
      "Given a similarityUDF that can tell us how similar two sets of word vectors are, we can ask a query such as:  In this case, the vector sets correspond to the items purchased by the corresponding customers.",
      "What this query will return is pairs of customers that have similar purchasing histories!",
      "The pattern observed in this query can be applied to other domains as well, e.g., identifying patients that are taking similar drugs, but with different brand names or identifying food items with similar ingredients, or finding mutual funds with similar investment strategies.",
      "The key difference to a traditional query is that we\u2019re matching by semantic similarity, not by values.",
      "Recall that word embeddings also support inductive reasoning (e.g., the classic King is to Man as Queen is to ?",
      "style queries).",
      "You can exploit this capability in CI queries too.",
      "In the following toy example, we\u2019re looking for food product pairs that relate to each other as \u2018peanut-butter\u2019 relates to \u2018jelly\u2019.",
      "(For example, the query may return the pair \u2018chips\u2019, \u2018salsa\u2019).",
      "The analogyUDF computes the differences (peanut butter \u2013 jelly) and (p1 \u2013 p2) and looks at the cosine similarity of those differences.",
      "The analogy capabilities of CI queries have several applications in the enterprise space, e.g., associating customers with either most-common or least-common purchases in a given domain (e.g., books, electronics, etc.).",
      "I understand the analogy query mechanism, but I\u2019m not sure I quite get the example the authors are trying to give above.",
      "Neither finding product popularity, nor seeing whether a customer has purchased a low-popularity (high popularity) item seems to need an analogy?",
      "Here\u2019s an example of my own \u2013 recommendation by analogy: razor is to blade as [product the customer just put in their basket] is to ?.",
      "(Probably not about to replace frequent itemset mining anytime soon!)",
      "Our final example shows how embeddings trained using external data can be used in queries.",
      "Suppose we trained word embeddings with a data set that reveals information about fruits and their allergenic properties.",
      "We would have a relationship between the vector for \u2018allergenic\u2019 and the vectors for allergenic fruit names.",
      "Now we can ask:  This example demonstrates a very powerful ability of CI queries that enables users to query a database using a token (e.g., allergenic) not present in the database.",
      "The last word  Will all relational databases one day come with CI querying capabilities built-in?",
      "In summary, we believe this work is a step towards empowering database systems with built-in AI capabilities\u2026 We believe CI queries are applicable to a broad class of application domains including healthcare, bio-informatics, document searching, retail analysis, and data integration.",
      "We are currently working on applying the CI capabilities to some of these domains."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3076246.3076251?download=true",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 9563928
  },
  {
    "blog_id": "corals-who-are-my-potential-new-customers-tapping-into-the-wisdom-of-customers-decisions",
    "summary": [
      "CORALS: who are my potential new customers?",
      "Tapping into the wisdom of customers\u2019 decisions Li et al., WSDM\u201919  The authors of this paper won round 9 of the Yelp dataset challenge for their work.",
      "The goal is to find new target customers for local businesses by mining location-based checkins of users, user preferences, and online reviews.",
      "Location-based social networks attract millions of users to share their social friendship and their locations via check-ins.",
      "For example, an average of 142 million users check in at local businesses via Yelp every month.",
      "Foursquare and 55 million monthly active users and 8 million daily check-ins on the Swarm application.",
      "Facebook Local, powered by 70 million businesses, facilitates the discovery of local events and places for over one billion active daily users.",
      "(And of course these are just the explicit check-ins, location tracking has proved tempting to many businesses without requiring explicit user checkin actions.",
      "For example: Facebook is tracking your phone\u2019s location\u2026 , Google\u2019s location history is still recording your every move\u2026 , Google tracks your movements, like it or not , \u2026).",
      "Check-ins give us a location history.",
      "Preference for a given business will be influenced by the proximity of that business to the places a given user frequents, in accordance with Tobler\u2019s first law of geography : \u201ceverything is related to everything else, but near things are more related than distant things.\u201d Which translates into:  \u2026 the propensity of a customer for a local business in inversely proportional to the distance between the customer and the business.",
      "It\u2019s not quite that straightforward though.",
      "Customers will be prepared to travel more or less far depending on the type of business.",
      "For example, customers at the Phoenix art museum travel farther to get there on average than customers at a McDonald\u2019s.",
      "For most people, their lives gravitate around two location centres (\u2018exploration centres\u2019): home and work:  This is what that looks like for two individual users:  In addition to location and their general preferences, customers are also increasingly influenced by reviews.",
      "A 2016 study by BrightLocal found that 92% of customers regularly or occasionally read online reviews, which help them judge the quality of services offered.",
      "\u2026 the impact of online reviews is non-negligible and growing.",
      "One of the interesting findings from the evaluation is that to a reasonable extent, \u201cyou\u2019re only as good as your last review!\u201d The chart below shows a big jump in MAP when including information from the most recent review, and only a marginal performance gain with additional reviews.",
      "This is mainly due to the fact that customers only read a few latest reviews to perceive the reputation of the local business.",
      "Anyway, we\u2019re getting ahead of ourselves\u2026  CORALS  CORALS is a customer recommendation model based on historical check-in information, which integrates customer personal preferences, geographical influence, and business reputation (reviews).",
      "These are modelled by:  , the preference of customer i for business b,  , the geographical convenience of business b for customer i  , the reliance of customer i on the reputation  of business b  The overall tendency of a customer to visit a given business is just a linear combination of these three factors:  .",
      "The probability that a given customer will check-in at a given business location is learned by a series of pairwise comparisons.",
      "Say we know that customer i has checked in at location b.",
      "Sample another customer j at random.",
      "If j has not checked in at location b, then intuitively it ought to be that the probability for i to check in at location b should be higher than the probability of j to check in there.",
      "The probability that customer i is more likely to visit a business b than customer j is denoted by  , with  representing the model parameters.",
      "It\u2019s just a linear combination of the same three factors, but this time looking at the distances between them (and then using the sigmoid function, represented by  in this case, to turn the sum into a probability).",
      "The overall model is constructed by maximising over all observed and sampled checkins:  where  models the parameters  with a Gaussian prior.",
      "Optimisation proceeds as follows.",
      "First the parameters  are initialised using the normal distribution.",
      "Then we iterate for a fixed number of iterations, performing  the following steps in each iteration:  For each observed check in (b, i),  sample random customers j who have not visited b, making up to $latex s_{max} attempts to find a _j_ such that the preference order between _i_ and _j_ is _not_ predicted correctly.",
      "If such a violation is found, update the corresponding parameter  .",
      "After iterating over every observed checkin, evaluate performance on the validation set.",
      "Accept the updates to  from this iteration if performance has improved on the validation set, otherwise reject them.",
      "Modelling preferences  The personal preference of customer i for business b is given by  where  and  are business and customer vector representations in the preference hidden space.",
      "(Learned by e.g. collaborative filtering or matrix factorisation techniques).",
      "Modelling reputations  The reliance of a customer i on the reputation of a business b is given by  where  and  are business and customer vector representations in the reputation hidden space.",
      "For business reputation vectors, word and sentence embeddings are used to turn review text into a vector.",
      "The n most recent reviews are used to form the final reputation vector, with n = 1 being the default.",
      "(Because ratings on a 0-5 scale weren\u2019t available?",
      "I presume so.",
      "Someone must have studied the relative influence of words and rating numbers in online reviews\u2026 this infographic from Vendasta that turned up in a quick Google search certainly suggests stars are more important than the text).",
      "Modelling geographical convenience  The final missing component is the geographical convenience of a business b for customer i,  .",
      "To learn this the authors use a Gaussian mixture model taking a weighted sum over 2-vectors representing the latitude and longitude of locations visited by i.  Expectation-Maximization is used to estimate the parameters of the model to maximise the likelihood of sequences of check-ins made by users.",
      "How well does it work?",
      "The evaluation is conducted on the Yelp challenge dataset and on a Foursquare dataset.",
      "CORALS is compared against 12 other recommendation algorithms, as well as versions of itself using alternative optimisation strategies (see \u00a73.2 for the full list).",
      "The bottom line is that CORALS is consistently one of the top scoring models across a range of cities:  ( Enlarge )  We can also see for example how geography and reputation have differing levels of influence for different kinds of businesses:  The results demonstrate that CORALS outperforms all these baselines by a significant margin in most scenarios.",
      "In addition to identifying potential new customers, we also break down the analysis for different types of businesses to evaluate the impact of various factors that may affect customers\u2019 decisions.",
      "This information, in turn, provides a great resource for local businesses to adjust their advertising strategies and business services to attract more prospective customers."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3289600.3290995?download=true",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 28037079
  },
  {
    "blog_id": "skyway-connecting-managed-heaps-in-distributed-big-data-systems",
    "summary": [
      "Skyway: connecting managed heaps in distributed big data systems Nguyen et al., ASPLOS\u201918  Yesterday we saw how to make Java objects persistent using NVM-backed heaps with Espresso.",
      "One of the drawbacks of using that as a persistence mechanism is that they\u2019re only stored in the memory of a single node.",
      "If only there was some way to create a cluster of JVMs, and efficiently copy objects across remote heaps in the cluster\u2026 Meet Skyway!",
      "Skyway is aimed at JVM-based big data systems (think Spark, Flink) that end up spending a lot of their time serializing and deserializing objects to move them around the cluster (e.g., to and from workers \u2013 see \u2018 Making sense of performance in data analytics frameworks \u2019).",
      "Java comes with a default serialization mechanism, and there are also many third party libraries.",
      "Kryo is the recommended library for use with Spark.",
      "Consider a small Spark cluster (3 worker nodes each with a 20 GB heap) running a triangle counting algorithm over the LiveJournal graph (about 1.2GB).",
      "With both the standard Java serializers and Kryo, serialization and deserialization combined account for a significant portion of the overall execution time (more than 30%).",
      "Where does all the time go?",
      "To transfer an object o from one JVM to another takes three steps:  A serialization procedure turns the whole object graph reachable from o into a binary sequence.",
      "During this process the serializer extracts the object data, strips the header, removes all references stored in an object, and changes the representation of certain metadata.",
      "The byte sequence is sent across the wire  A deserialization procedure reads the byte sequence, creates objects accordingly, and rebuilds the object graph in the managed heap of the receiver machine.",
      "In a big data system, a transfer can involve millions of objects, which means invoking, e.g., reflection APIs millions of times or more.",
      "Moreover, the Java serializer represents every type by a string containing the name of a class and all its superclasses.",
      "These type strings can consume a huge portion of the byte sequence transferred across the network, and reflection has to be used on the receiver end to resolve types from the string.",
      "Reflection is also heavily used when repairing object references in the graph on the receiving end.",
      "The key problem with existing S/D (serialization/deserialization) libraries is that, with an existing JVM, there are no alternative routes to transfer objects other than first disassembling and pushing them down to a (different) binary format, and the reassembling and pulling them back up into a remote heap.",
      "In this paper, we advocate to build a \u201cskyway\u201d between managed heaps so that data objects no longer need to pushed down to a lower level for transfer.",
      "At a high level, Skyway is fairly easy to understand.",
      "It extends the JVM (OpenJDK) to enable object graphs to be moved as is from one heap to another, and immediately used on a remote node right after the move.",
      "Given a root object o, Skyway performs a GC-like traversal copying every reachable object into an output buffer while performing only a very lightweight adjustment to machine-dependent metadata.",
      "Crucially, the object format is not changed and every object is transferred as a whole.",
      "This includes the hashcode, so that hash-based data structures can be used on the receiver node without rehashing.",
      "Types are represented by a global type-numbering procedure which assumes a master-workers pattern and keeps a registry of all types and their ids at the master.",
      "Workers communicate with the master to obtain ids for classes upon class loading.",
      "Absolute addresses in objects are turned into relative addresses when copied into the output buffer.",
      "The output buffer is streamed to an input buffer at the remote node, where the relative addresses are turned back into absolute addresses in the target heap.",
      "\u2026data processing applications frequently shuffle many millions of objects and do so in strongly delimited phases.",
      "Hence, sending objects in batch without changing their formats provides significant execution efficiency.",
      "Second, the use of modern network technology enables extra bytes to be quickly transferred without incurring much overhead.",
      "Skyway under the covers  Skyway uses a GC-like mechanism to discover the object graph reachable from a set of root objects.",
      "Objects encountered during the traversal are copied into an output buffer (located in off-heap native memory so it doesn\u2019t interfere with GC), which is streamed to the corresponding buffer(s) on the receiving node.",
      "Input buffers are allocated in the old generation (tenured) of the managed heap, and  can span multiple memory chunks \u2013 handy since you don\u2019t always know the ultimate size of the buffer when streaming starts.",
      "Root objects in a stream are demarcated by special top marks which helps the receiver to efficiently read entire graphs without needing to parse all of their content.",
      "Once a data transfer is complete, Skyway updates the card table of the Parallel Scavenge GC making the new objects reachable via garbage collection.",
      "Skyway can support heterogeneous clusters, where JVMs may have different object formats, by adjusting the format of objects as they are copied into the output buffer.",
      "The implementation on top of OpenJDK 1.8.0 touches the classloader subsystem, the object/heap layout, and the Parallel Scavenge garbage collector.",
      "Skyway develops a distributed type-registration system that automatically allows different representations of the same class on different JVM instances to share the same integer ID.",
      "This system completely eliminates the need to represent types during data transfer\u2026  The driver / master JVM maintains a complete type registry covering all classes that have been loaded in the cluster, initially populated by scanning its own loaded classes after JVM startup.",
      "When a worker JVM starts up it requests a copy of the registry from the driver, giving it a view of all classes loaded in the cluster to that point.",
      "If a worker JVM loads a class that does not yet exist in its local registry view it checks with the driver to obtain an ID for it.",
      "Whereas the standard Java serializer sends a type string over the network with every object, Skyway sends a type string at most once for every class on each machine during the entire computation.",
      "Performance evaluation  The first evaluation compares Skyway against 90 existing serialization libraries using the Java serializer benchmark set (JSBS).",
      "Results for the fastest 28 are shown below.",
      "Skyway is the fastest of the lot: 2.2x faster than Kryo-manual (intrusive), and 67x faster than the Java serializer.",
      "The next experiment modifies Spark (v1.2.0) to replace the use of Kryo-manual with the Skyway library.",
      "Four programs (Word Count, Page Rank, Connected Components, and Triangle Counting) are each run over four different graph inputs:  The following charts summarise the performance of Java serialisation, Kryo, and Skyway for each of the programs across each of the input graphs:  Skyway makes Spark run 36% faster than the Java serializer, and 16% faster than Kryo.",
      "Skyway is also evaluated against Flink 1.3.2 (released Aug 2017) in a batch processing mode.",
      "The TPC-H data generator is used to generate a 100GB input dataset, and 5 representative TPC-H queries are transformed into Flink applications.",
      "The performance of Skyway vs Flink\u2019s built-in serializer is show below:  Skyway improves Flink\u2019s performance by 19% on average.",
      "Our evaluation shows that Skyway outperforms all existing S/D libraries and improves widely-deployed systems such as Spark and Flink."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://people.cs.uchicago.edu/~shanlu/paper/asplos18_skyway.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 84655302
  },
  {
    "blog_id": "an-overview-of-troubling-trends-in-machine-learning-scholarship-582df3caa518",
    "summary": [
      "A new paper presented in ICML 2018 describes some of the troubling trends in machine learning (ML) scholarship.",
      "The authors, Zachary C. Lipton and Jacob Steinhardt, discuss the implications of these trends on the field of ML and the proper distilling of machine learning research and knowledge.",
      "Machine learning (ML) research aims to disseminate knowledge about data-driven algorithms through theoretical characterizations and or empirical results.",
      "The field of ML is growing rapidly, at an unprecedented scale and speed, with certain troubling patterns emerging which could negatively affect meaningful progress in the field, poorly serving to researchers and enthusiasts.",
      "ML research is most valuable when communicated properly to the interested reader by avoiding the presentation of speculations as facts and other misleading interpretations and explanations.",
      "Lipton and Steinhardt propose several ways to combat some of the troubling trends occurring in the ML community, particularly as it relates to the poor dissemination of foundational knowledge and empirical results.",
      "The overall goal is to communicate research and findings with greater clarity and to pay closer attention to the following troubling patterns:  Explanation vs.",
      "Speculation  In writing, speculation encompasses a whole range of techniques used to explain concepts and intuitions without providing proper evidence or formal definitions.",
      "For example, we claim that X produces Y given some conditions Z.",
      "We know X and Y to be true, but we don\u2019t know if all conditions Z apply because we haven\u2019t formally defined them.",
      "Yet we claim that all conditions Z seem to work even when we haven\u2019t defined them properly.",
      "Just because the reader isn\u2019t aware or doesn\u2019t care about all Zs it doesn\u2019t mean the claim warrants validity \u2014 it\u2019s \u201cexplanation disguised as speculation.\u201d  This is a risky form of science communication since other researchers may blindly use them as facts in their work, further increasing the severity and reach of the initial speculation.",
      "It\u2019s important to avoid asserting speculations as facts and explicitly separating them from facts when using them.",
      "(See actual examples in the paper)  Failure to Identify the Sources of Empirical Gains  Sometimes ML researchers propose complex models that combine several techniques to address a problem.",
      "Even though the proposed approach does well, authors sometimes fail to identify the source/s of empirical gains.",
      "The problem is that this may generate a false impression that the authors did a lot of work when in reality the improvements were incremental and only occurred due to a small change.",
      "This can sometimes be minimized and clarified by performing proper ablation studies.",
      "(See more examples in the paper)  Mathiness  Mathiness is a concept proposed by economist Paul Romer that describes the idea of using both natural language and mathematics to explain concepts but failing to provide tight links between statements and symbols.",
      "Mathiness manifests in many ways such as authors trying to pile math equations to convey technical depth without proper explanations and linkings.",
      "The problem with this approach is that clarity suffers, distracting the readers from acquiring the important knowledge and insights of the research.",
      "The best remedy seems to be avoiding the use of mathiness and providing clear explanations of the formulations so that it benefits all kinds of readers and not just ML researchers.",
      "Misuse of Language  Lipton and Steinhardt currently identified three types of language misuse in machine learning research: suggestive definitions, overloaded terminology, and suitcase words.",
      "Let\u2019s briefly describe the three below:  Suggestive definitions \u2014 This occurs when authors coin suggestive terms that convey human qualities (i.e., anthropomorphic characterizations) such as \u201cthought vectors.\u201d These type of terms are not necessarily bad but if not qualified may only confuse readers.",
      "Overloaded terminology \u2014 This refers to the inappropriate or contradictory use of a technical term that already holds a very precise meaning in the literature.",
      "An example is the misuse of the term \u201cdeconvolution\u201d to refer to transpose convolution as opposed to \u201creversing a convolution\u201d which is what it originally describes.",
      "Suitcase words \u2014 This refers to terms that may pack many different meanings, often creating a confusion to readers with different backgrounds.",
      "These type of words could also manifest in the form of suggestive definitions and overloaded terminology as explained above.",
      "Some notorious examples include \u201cgeneralization\u201d, \u201cinterpretability\u201d, and \u201cbias.\u201d Loosely using these type of words is common practice but often detracts and confuse readers.",
      "Potential Causes Behind Troubling Trends  Lipton and Steinhardt also discuss the potential causal factors to these troubling trends in ML scholarship:  Complacency in the face of progress \u2014 This describes the notion that authors feel entitled to make weak arguments because they are already providing strong results.",
      "This poses a problem since reviewers may feel pressured to accept potentially flawed papers only to compensate for the quantitative findings.",
      "Growing pains \u2014 ML is expanding rapidly and thus increases the entrance of inexperienced researchers which are more susceptible to the misuse of language.",
      "This is not to say that experienced researchers cannot fall into these patterns, especially when their reviewing responsibilities increase with the rapid growth of the field.",
      "This is not to discourage junior researchers, but spreading awareness of these issues is key to conducting proper research.",
      "Misaligned incentives \u2014 As ML becomes more popular, both the media and startup investors seek to provide incentives for things which may not be aligned with the objectives of the field.",
      "For instance, anthropomorphic descriptions, such as \u201csimulated brain\u201d, may be effective for popular coverage but could actually cause confusion in the ML community and literature.",
      "Suggestions  Besides recommending that authors \u2014 experienced and inexperienced \u2014 refrain from participating in these trends, Lipton and Steinhard outline a few preliminary suggestions:  What, Why, How \u2014 Identify \u201cwhat worked\u201d and provide clear details explaining \u201cwhy it worked\u201d, and not just \u201chow well it worked.\u201d  Insights \u2014 Authors are encouraged to provide error analysis, ablation studies, and robustness check.",
      "Besides identifying empirical gains, authors can also discuss insights with strong evidence to support them.",
      "Writing tips \u2014 Authors are recommended to revise important questions and theorems reported in the research, with clarity, flow, and precision in mind.",
      "Open problems can also be separated from closed ones when offering discussions to avoid confusion and to encourage follow-up research.",
      "Other suggestions are also provided for publishers and reviewers.",
      "(See paper for more details)  Overall, it\u2019s important to avoid the use of confusing terminology (e.g. anthropomorphic characterization) and unsupported claims.",
      "Greater rigor is essential for the healthy growth of ML scholarship and scientific progress more broadly.",
      "There are special cases where the recommendations set forth above may actually hurt the fast pace of research.",
      "In some cases, original ideas may even be impeded.",
      "However, historically speaking, undisciplined scholarship affects both researchers and the public, which means there could potentially emerge a crisis and a \u201clack of meaningful progress.\u201d  Final Words From the Editor  Machine learning has grown tremendously over the past few years and it promises to be at the epicenter of \u201ceverything technology.\u201d It\u2019s the responsibility of everyone \u2014 researchers, students, reviewers, industry, startups, lawmakers, and everyone involved \u2014 to participate in the debate and to ensure the proper distilling of machine learning research and knowledge in a transparent and accurate way.",
      "We are responsible for guiding the conversation in the right direction and to avoid harming the ML field and science more broadly.",
      "Ref:  [url]"
    ],
    "author_id": "DAIR-AI",
    "pdf_url": "https://arxiv.org/pdf/1807.03341",
    "author_full_name": "Elvis Saravia",
    "source_website": "https://github.com/dair-ai/nlp_paper_summaries",
    "id": 2899329
  },
  {
    "blog_id": "zippads",
    "summary": [
      "Compress objects, not cache lines: an object-based compressed memory hierarchy Tsai & Sanchez, ASPLOS\u201919  Last time out we saw how Google have been able to save millions of dollars though memory compression enabled via zswap.",
      "One of the important attributes of their design was easy and rapid deployment across an existing fleet.",
      "Today\u2019s paper introduces Zippads, which compared to a state of the art compressed memory hierarchy is able to achieve a 1.63x higher compression ratio and improve performance by 17%.",
      "The big idea behind zippads is simple and elegant, but the ramifications go deep: all the way down to a modified instruction set (ISA)!",
      "So while you probably won\u2019t be using Zippads in practice anytime soon, it\u2019s a wonderful example of what\u2019s possible when you\u2019re prepared to take a fresh look at \u201cthe way we\u2019ve always done things.\u201d  The big idea  Existing cache and main memory compression techniques compress data in small fixed-size blocks, typically cache lines.",
      "Moreover, they use simple compression algorithms that focus on exploiting redundancy within a block.",
      "These techniques work well for scientific programs that are dominated by arrays.",
      "However, they are ineffective on object-based programs because objects do not fall neatly into fixed-size blocks and have a more irregular layout.",
      "Scientific computing and machine learning applications may be heavily array dominated, but many applications are dominated by objects, which have a much more irregular layout and very different access patterns.",
      "Looking across a set of eight Java benchmarks, we find that only two of them are array dominated, the rest having between 40% to 75% of the heap footprint allocated to objects, the vast majority of which are small.",
      "Existing compression algorithms that rely on similarities between nearby words won\u2019t work as well on these applications.",
      "There are two big \u201cAha!\u201d moments that underpin this work.",
      "The first one is that object-based applications perform memory accesses within objects and follow pointers to other objects:  Therefore objects, not cache lines, are the right compression unit.",
      "The second insight is that although nearby words may not look similar, different instances of the same object type do.",
      "Consider a B-Tree node from the B-tree Java benchmark:  Uncompressed, it\u2019s memory layout looks like (a) below.",
      "Compressed using LCP (a hybrid BDI + FPC compression technique) we can achieve a 10% compression ratio as shown in (b).",
      "If we compress objects instead of cache lines though, we can get to a 56% compression ratio (c).",
      "Finally, if we also use cross-object compression in which a base object is stored for each object type, and only the delta from the base object is stored for every instance, then we can get the compression ratio up as high as 95% (d)!",
      "Implications  \u2026 to realize these insights, hardware needs to access data at object granularity and must have control over pointers between objects.",
      "This is where Hotpads comes in.",
      "Hotpads is a hardware-managed hierarchy of scratchpad-like memories called pads.",
      "Pads are designed to store variable sized objects efficiently, and a key feature is that they transfer objects across pad levels implicitly (just like cache levels) based on memory accesses.",
      "Objects are first allocated in the L1 pad and move up the hierarchy as they are evicted.",
      "Short-lived objects may be garbage collected before they ever reach main memory.",
      "The highest hierarchy level an object has reached is called its canonical level, and this level acts as the object\u2019s backing store.",
      "Hotpad modifies the ISA to make all this work (see Table 2 below).",
      "The ISA changes are transparent to application programmers, but require runtime and compiler modifications.",
      "Collection evictions that move objects up the hierarchy occur entirely in hardware and are much faster than software GT because pads are small.",
      "Eviction cost is proportional to pad size, due to the constraint that objects may only point to objects at the same or higher canonical level.",
      "Thus we know when evicting an object from a smaller pad that no object at a larger pad can be holding a reference to it.",
      "Zippads  Zippads extends Hotpads with compression.",
      "It can work with conventional compression algorithms such as BDI and FPC, but it works best with the COCO cross-object compression scheme.",
      "New objects start their lifetime uncompressed, and if and when they make to to the last-level pad they are compressed there.",
      "This is a key difference from conventional hierarchies, where objects are mapped to a main memory address to begin with, forcing the problem of translating from uncompressed to compressed addresses.",
      "Compression information is encoded directly into pointers:  On dirty writebacks we have to consider the scenario whereby the new size of the compressed object no longer fits in its location.",
      "In this case Zippads allocates a new location and leaves behind a forwarding thunk in the old location.",
      "Overflows are rare in practice.",
      "Zippads also enhances Hotpad\u2019s periodic garbage collection to work on compressed objects.",
      "COCO  COCO compresses an object by storing only the bytes that differ from a given base object.",
      "The compressed object format has three elements:  A base object id  A diff bitmap with one bit per byte of the object.",
      "Bit i is set if the ith byte differs from the base object.",
      "A string of byte diffs containing the bytes that are different from the base object.",
      "All of this happens in hardware:  COCO compression/decompression circuits are simple to implement and only require narrow comparators and multiplexors.",
      "Our implementations compress/decompress one word (8 bytes) per cycle\u2026 We have written the RTL for these circuits and synthesized them at 45nm using yosys and the FreePDK45 standard cell library.",
      "The process for selecting the base object for each type is simple: COCO simply uses the first object of each type that it sees.",
      "Base objects themselves are kept in a small and fast base object cache so that COCO has easy and fast access to them.",
      "What about arrays?",
      "We said at the start of all this that objects and arrays work best with different kinds of compression.",
      "We want Zippads to compress both well.",
      "Zippads addresses this by using different compression algorithms: COCO for objects and a conventional BDI+FPC algorithm for arrays.",
      "Metadata encoding the compression algorithm used is encoded in the pointers.",
      "Evaluation  Zippads and COCO are evaluated on a mix of array-based and object-based workloads, including eight Java and two C/C++ benchmarks.",
      "Zippads achieves the best compression ratios across all of the Java benchmarks.",
      "In the figure below, CMH is a state-of-art compressed memory hierarchy, hotpads is plain hotpads (no compression) and a three-level cache hierarchy, Zippads-BF is Zippads without COCO, and Zippads is the full Zippads with dynamic selection of compression algorithm including COCO.",
      "Zippads also has the best reduction in main memory traffic : halving the amount of traffic compared to the baseline.",
      "Not only that, but Zippads improves performance over the baseline by 30% as well (and by 17% when compared against CMH).",
      "As the following plots show, it\u2019s also highly effective on C/C++ benchmarks.",
      "The bottom line:  Zippads + COCO improves compression ratio over a combination of state-of-the-are techniques by up to 2x and by 1.63x on average.",
      "It also reduces memory traffic by 56% and improves performance by 17%."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://people.csail.mit.edu/poantsai/papers/2019.zippads.asplos.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 36051826
  },
  {
    "blog_id": "on-the-information-bottleneck-theory-of-deep-learning",
    "summary": [
      "On the information bottleneck theory of deep learning Anonymous et al., ICLR\u201918 submission  Last week we looked at the Information bottleneck theory of deep learning paper from Schwartz-Viz & Tishby ( Part I , Part II ).",
      "I really enjoyed that paper and the different light it shed on what\u2019s happening inside deep neural networks.",
      "Sathiya Keerthi got in touch with me to share today\u2019s paper, a blind submission to ICLR\u201918, in which the authors conduct a critical analysis of some of the information bottleneck theory findings.",
      "It\u2019s an important update pointing out some of the limitations of the approach.",
      "Sathiya gave a recent talk summarising results on understanding optimisation and generalisation, \u2018 Interplay between Optimization and Generalization in DNNs ,\u2019 which is well worth checking out if this topic interests you.",
      "Definitely some more papers there that are going on my backlog to help increase my own understanding!",
      "Let\u2019s get back to today\u2019s paper!",
      "The authors start out by reproducing the information plane dynamics from the Schwartz-Viz & Tishby paper, and then go on to conduct further experiments: replacing the tanh activation with ReLU to see what impact that has; exploring the link between generalisation and compression; investigating whether the randomness is important to compression during training; and studying the extent to which task-irrelevant information is also compressed.",
      "The short version of their findings is that the results reported by Schwartz-Viz and Tishby don\u2019t seem to generalise well to other network architectures: the two phases seen during training depend on the choice of activation function; there is no evidence of a causal connection between compression and generalisation; and that when compression does occur, it is not necessarily dependent on randomness from SGD.",
      "Our results highlight the importance of noise assumptions in applying information theoretic analyses to deep learning systems, and complicate the IB theory of deep learning by demonstrating instances where representation compression and generalization performance can diverge.",
      "The quest for deeper understanding continues!",
      "The impact of activation function choice  The starting point for our analysis is the observation that changing the activation function can markedly change the trajectory of a network in the information plane.",
      "The authors used code supplied by Schwartz-Vis and Tishby to first replicate the results that we saw last week (Fig 1A below), and then changed the network to use ReLU instead \u2014 rectified linear activation functions  .",
      "The resulting information plane dynamics are show in Fig 1B.",
      "The phase shift that we saw with the original tanh activation functions disappears!",
      "The mutual information with the input monotonically increases in all ReLu layers, with no apparent compression phase.",
      "Thus, the choice of nonlinearity substantively affects the dynamics in the information plane.",
      "Using a very simple three neuron network, the authors explore this phenomenon further.",
      "A scalar Gaussian input distribution  is fed through a scalar first layer weight  and passed through a neural nonlinearity  to yield hidden unit activity  .",
      "In order to calculate the mutual information, the hidden unit activity  is binned into 30 uniform bins, to yield the discrete variable  .",
      "With the tanh nonlinearity, mutual information first increases and then decreases.",
      "With the ReLU nonlinearity it always increases.",
      "What\u2019s happening is that with large weights, the tanh function saturates, falling back to providing mutual information with the input of approximately 1 bit (i.e, the discrete variable concentrates in just two bins around 1 and -1).",
      "With the ReLU though, half of the inputs are negative and land in the bin around 0, but the other half are Gaussian distributed and have entropy that increases with the size of weight.",
      "So it turns out that this double saturating nature of tanh is central to the original results.",
      "\u2026 double-saturating nonlinearities can lead to compression of information about the input, as hidden units enter their saturation regime, due to the binning procedure used to calculate mutual information.",
      "We note that this binning procedure can be viewed as implicitly adding noise to the hidden layer activity: a range of X values map to a single bin, such that the mapping between X and T is no longer perfectly invertible.",
      "The binning procedure is crucial for the information theoretic analysis, \u201chowever, this noise is not added in practice either during training or testing in these neural networks.\u201d  The saturation of tanh explains the presence of the compression period where mutual information decreases, and also explains why training slows down as tanh networks enter their compression phase: some fraction of inputs have saturated the nonlinearities, reducing backpropagated error gradients.",
      "Generalisation independent of compression  Next the authors use the information plane lens to further study the relationship between compression and generalisation.",
      "\u2026 we exploit recent results on the generalization dynamics in simple linear networks trained in an student-teacher setup (Seung et al., 1992; Advani & Saxe, 2017).",
      "This setting allows exact calculation of the generalization performance of the network, exact calculation of the mutual information of the representation (without any binning procedure), and, though we do not do so here, direct comparison to the IB bound which is already known for linear Gaussian problems.",
      "No compression is observed in the information plane (panel D in the figure above), although the network does learn a map that generalise well on the task and shows minimal overtraining.",
      "Experimentation to force varying degrees of overfitting shows networks with similar behaviour in the information plane can nevertheless have differing generalisation performance.",
      "This establishes a dissociation between behavior in the information plane and generalization dynamics: networks that compress may or may not generalize well, and that networks that do not compress may or may not generalize well.",
      "Does randomness help compression?",
      "Next the authors investigate what contributes to compression in the first place, looking at the differences in the information plane between stochastic gradient descent and batch gradient descent.",
      "Whereas SGD takes a sample from the dataset and calculates the error gradient with respect to it, batch gradient descent uses the total error across all examples \u2014 \u201cand crucially, therefore has no randomness or diffusion-like behaviour in its updates.\u201d  Both tanh and linear networks are trained with both SGD and BGD, and the resulting information plane dynamics look like this:  We find largely consistent information dynamics in both instances, with robust compression in tanh networks for both methods.",
      "Thus randomness in the training process does not appear to contributed substantially to compression of information about the input.",
      "This finding is consistent with the view presented in \u00a72 that compression arises predominantly from the double saturating nonlinearity.",
      "(Which seems to pretty much rule out the hope from the Schwartz-Viz & Tishby paper that we would find alternatives to SGD that support better diffusion and faster training).",
      "The compression of task-irrelevant information  A final experiment partitions the input X into a set of task-relevant inputs and a known to be task_irrelevant_ inputs.",
      "The former contribute signal therefore, while the latter only contribute noise.",
      "Thus good generalisation would seem to require ignoring the noise.",
      "The authors found that information for the task-irrelevant subspace does compress, at the same time as fitting is occurring for the task-relevant information, even though overall there is no observable compression phase.",
      "The bottom line  Our results suggest that compression dynamics in the information plane are not a general feature of deep networks, but are critically influenced by the nonlinearities employed by the network\u2026 information compression may parallel the situation with sharp minima; although empirical evidence has shown a correlation with generalization error in certain settings and architectures; further theoretical analysis has shown that sharp minima can in fact generalize well."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://openreview.net/pdf?id=ry_WPG-A-",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 34958521
  },
  {
    "blog_id": "progressive_growing_of_gans",
    "summary": [
      "They suggest a new method to train GANs.",
      "They start training them at low resolution (4x4), wait until \"convergence\", then add more convolutions to the existing model to generate and discriminate higher resolutions.",
      "Each new block of convolutions is slowly blended in, instead of being added from one batch to the next.",
      "Combined with two new normalization techniques, they get good-looking images at up to 1024x1024 on their new CelebA-HQ dataset (CelebA in high resolution).",
      "They also suggest a new scoring method based on the approximated Wasserstein distance between real and generated image patches.",
      "According to that score, their progressive training method improves results significantly.",
      "What  They suggest a new, progressive training method for GANs.",
      "The method enables the training of high resolution GANs (1024x1024) that still produce good-looking, diverse images.",
      "They also introduce two new normalization techniques.",
      "They also suggest a new method to estimate/score the quality of the generated images.",
      "They introduce CelebA-HQ, a variation of CelebA containing high resolution images.",
      "How  Progressive growing/training  They train their GANs resolution by resolution, starting with 4x4 and going up to 1024x1024 (a bit similar to LAPGAN).",
      "Visualization:  Initially, their generator produces 4x4 images and the discriminator receives 4x4 images.",
      "Once training at 4x4 does not improve any more (measured by their new score, see below), they add an upscaling module (to 8x8) to the generator and add a downscaling one to the discriminator.",
      "They don't switch to the added convolutions instantly/suddenly, but give the model a grace period during which the upscaled features are computed from (1-alpha)*A + alpha*B, where A are the features after just upscaling, B are the features after upscaling AND the convolutions and alpha is the overlay factor, which is gradually increased over time.",
      "This is done for both the generator and the discriminator and at all resolutions.",
      "Visualization:  Note that all layers are always trained (after they were added to the models).",
      "Training for the earlier layers does not stop.",
      "Training in this way focuses most of the computation on the earlier resolutions.",
      "It also seems to increase stability, as the model does not have to learn all features of all resolutions at the same time.",
      "Minibatch Standard Deviation  They try to improve diversity by adding a method very similar to minibatch discrimination.",
      "They compute the standard deviation of each feature per spatial location (for one of the disciminator's last layers).",
      "They do this per example in each minibatch, resulting in B*H*W*C standard deviations.",
      "(B = batch size, H = height, W = width, C = channels/filters)  They average these values to one value, then replicate them to size H*W and concatenate that to the layer's output.",
      "This adds a channel with one constant value to each example in the minibatch.",
      "The value is the same for all examples.",
      "Equalized Learning Rate  They use Adam for their training.",
      "Adam updates weights roughly based on mean(gradient)/variance(gradient) (per weight).",
      "They argue that this has the downside of equalizing all weight's stepsizes.",
      "But some weights might require larger stepsizes and other smaller ones (large/small \"dynamic range\").",
      "As a result, the learning rate will be too small for some weights and too large for others.",
      "To evade this problem, they first stop using modern weight initialization techniques and instead simply sample weights from the standard normal distribution N(0,1).",
      "Then, they rescale each weight w_i continuously during runtime to w_i/c, where c is the per-layer normalization from He's initializer.",
      "(TODO exact formula for c?)",
      "(This looks an aweful lot like weight normalization .)",
      "Using simpler weight initialization equalizes the dynamic range of parameters.",
      "Doing the normalization then fixes problems related to the simpler weight initialization.",
      "Pixelwise Feature Vector Normalization in the Generator  They argue that collapses in GANs come from the discriminator making some temporary error, leading to high gradients, leading to bad outputs of the generator, leading to more problems in the discriminator and ultimately making both spiral out of control.",
      "They fix this by normalizing feature vectors in the generator, similar to local response normalization.",
      "They apply the following equation in the generator (per spatial location (x, y) with N = number of filters):  Scoring Images  They suggest a new method to score images generated by the generator.",
      "They perform the following steps:  Sample 16384 images from the generator and the dataset.",
      "Build a Laplacian Pyramid of each image.",
      "It begins at a 16x16 resolution of the image and progressively doubles that until the final image resolution.",
      "Each level of the pyramid only contains the difference between the sum of the previous scales and the final image (i.e. each step is a difference image, containing a frequency band).",
      "Sample per image 128 7x7 neighbourhoods/patches (randomly?)",
      "from each pyramid level.",
      "Per image set (generator/real) and pyramid level, compute the mean and standard deviations of each color channels of the sampled patches.",
      "Normalize each patch with respect to the computed means and standard deviations.",
      "Use Sliced Wasserstein Distance (SWD) to compute the similarity between the image sets (generator/real).",
      "The result is one value.",
      "Lower values are better.",
      "CelebA-HQ  They derive from CelebA images a new dataset containing 30k 1024x1024 images of celebrity faces.",
      "They use a convolutional autoencoder to remove JPEG artifacts from the CelebA images.",
      "They use an adversarially-trained superresolution model to upscale the images.",
      "They crop faces from the dataset based on their facial landmarks, so that each final face has a normalized position and rotation.",
      "They rescale the images to 1024x1024 using bilinear sampling and box filters.",
      "They manually select the 30k best looking images.",
      "Other stuff  They use Adam for training (alpha=0.001, beta1=0, beta2=0.99).",
      "They use the WGAN-WP method for training, but LSGAN also works.",
      "They set gamma to 750 (from 1) for CIFAR-10, incentivizing fast transitions.",
      "They also add regularization loss on the discriminator, punishing outputs that are very far away from 0.",
      "Their model for CelebA-HQ training is similar to a standard DCGAN model.",
      "The generator uses two convolutions after each upscaling, the discriminator analogously two convolutions after each downscaling.",
      "They start with 512 filters in the generator and end in 16 (before the output) - same for the discriminator.",
      "They use leaky ReLUs in the generator and discriminator.",
      "They remove batch normalization everywhere.",
      "Results  Scores  Results, according to their new scoring measure (Sliced Wasserstein Distance) and MS-SSIM measure:  So progressive growing (b) significantly improves results.",
      "Same -- to a smaller degree -- for minibatch standard deviation (e), equalized learning rate (f) and pixelwise normalization (g).",
      "Minibatch discrimination worsened the results.",
      "Using small batch sizes also worsened the results.",
      "In (d) they \"adjusted the hyperparameters\" (??)",
      "and removed batch normalization.",
      "They generate 1024x1024 CelebA images, while maintaining pixelwise quality compared to previous models.",
      "They achieve an Inception Score of 8.80 on CIFAR-10.",
      "Images look improved.",
      "CelebA-HQ example results:  LSUN dining room, horse, kitchen, churches:"
    ],
    "author_id": "ALEJU",
    "pdf_url": "https://arxiv.org/pdf/1710.10196.pdf",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 25710556
  },
  {
    "blog_id": "the-truth-the-whole-truth-and-nothing-but-the-truth-a-pragmatic-guide-to-assessing-empirical-evaluations",
    "summary": [
      "The truth, the whole truth, and nothing but the truth: A pragmatic guide to assessing empirical evaluations Blackburn et al. ACM Transactions on Programming Languages and Systems 2016  Yesterday we looked at some of the ways analysts may be fooled into thinking they\u2019ve found a statistically significant result when in fact they haven\u2019t.",
      "Today\u2019s paper choice looks at what can go wrong with empirical evaluations.",
      "Let\u2019s start with a couple of definitions:  An evaluation is either an experiment or an observational study, consisting of steps performed and data produced from those steps.",
      "A claim is an assertion about the significance and meaning of an evaluation; thus, unlike a hypothesis, which precedes an evaluation, a claim comes after the evaluation.",
      "A sound claim is one where the evaluation provides all the evidence necessary to support the claim, and does not provide any evidence that contradicts the claim.",
      "Assuming honest researchers, things can go wrong in one of two basic ways: sins of reasoning are those where the claim is not supported by the evaluation; and sins of exposition are those where the description of either the evaluation or the claim is not sufficient for readers to evaluate and/or reproduce the results.",
      "As the authors show, it\u2019s not easy to avoid these traps even when you are doing your best.",
      "We\u2019ll get into the details shortly, but for those authoring (or reviewing) evaluations and claims, here are five questions to help you avoid them:  Has all of the data from the evaluation been considered, and not just the data that supports the claim?",
      "Have any assumptions been made in forming the claim that are not justified by the evaluation?",
      "If the experimental evaluation is compared to prior work, is it an apples-to-oranges comparison?",
      "Has everything essential for getting the results been described?",
      "Has the claim been unambiguously and clearly stated?",
      "The two sins of exposition  The sin of inscrutability occurs when poor exposition obscures a claim.",
      "That is, we can\u2019t be sure about what is really being claimed!",
      "The most basic form of inscrutability is simply omission \u2013 leaving the reader to figure out what the claims might be by themselves!",
      "Claims may also be ambiguous \u2013 for example, \u201cimproves performance\u201d by 10% (throughput or latency?",
      "), or \u201creduces latency by 10%\u201d (at what percentile and under what load?).",
      "Distorted claims are clear from the reader\u2019s perspective, but don\u2019t actually capture what the author intended.",
      "The sin of irreproducability occurs when poor exposition obscures an evaluation: the evaluation steps, the data, or both.",
      "The evaluation may not be reproducible because key information is omitted, imprecise language and lack of detail may lead to ambiguity.",
      "One important kind of omission that is hard to guard against is an incomplete understanding of the factors that are relevant to the evaluation.",
      "Experience shows that whole communities can ignore important, though non-obvious, aspects of the empirical environment, only to find out their significance much later, throwing into question years of published results.",
      "This situation invites two responses: (i) authors should be held to the community\u2019s standard of what is known about the importance of the empirical environment, and (ii) the community should intentionally and actively improve this knowledge, for example, by promoting reproduction studies.",
      "A good example of an unknown significant factor turns out to be the size of environment variables when measuring speed-ups of gcc in Linux!",
      "The size of the environment affects memory layout, and thus the performance of the program.",
      "An evaluation only exploring one (unspecified) environment size leads to the sin of irreproducibility.",
      "Researchers knew of course that memory layout affects performance, but it wasn\u2019t obvious that environment variables would affect it enough to make a difference\u2026 until someone measured it.",
      "The three sins of reasoning  The three sins of reasoning are the sin of ignorance, the sin of inappropriateness, and the sin of inconsistency.",
      "The sin of ignorance occurs when a claim is made that ignores elements of the evaluation supporting a contradictory alternative claim (i.e., selecting data points to substantiate a claim while ignoring other relevant points).",
      "In our experience, while the sin of ignorance seems obvious and easy to avoid, in reality, it is far from it.",
      "Many factors in the evaluation that seem irrelevant to a claim may actually be critical to the soundness of the claim.",
      "Consider the following latency histogram for a component of Gmail that has both a fast-path and a slow-path through it.",
      "The distribution is bimodal, which means that commonly used statistical techniques making assumptions the data is normally distributed do not apply.",
      "If a claim is made on the basis of such techniques, it is unsound and we have committed the sin of ignorance.",
      "This is easy to see when you have the histogram, but suppose the experiment calculated mean and standard-deviation on the fly \u2013 then it may not be so obvious to us at all!",
      "The sin of inappropriateness occurs when a claim is made that is predicated on some fact that is absent from the evaluation.",
      "In our experience, while the sin of inappropriateness seems obvious and easy to avoid, in reality, it is far from it.",
      "Many factors that may be unaccounted for in evaluation may actually be important to derive a sound claim.",
      "Consider an evaluation trying to measure energy consumption.",
      "Often execution time is measured as a proxy for this, but it turns out that other factors can also affect energy consumption, and thus execution time does not support a claim about energy consumption.",
      "Knowing that execution time is not always an accurate proxy for energy consumption is not obvious \u2013 until it was pointed out in a 2001 paper.",
      "As another example, before it was widely understood that heap sizes have a big impact on garbage collector performance, many papers derived a claim from an evaluation on only one heap size, and did not report it.",
      "The sin of inconsistency is a sin of faulty comparison \u2013 two systems are compared, but each is evaluated in a way that is inconsistent with the other.",
      "In our experience, while the sin of inconsistency seems obvious and easy to avoid, in reality, it is far from it.",
      "Many artifacts that seem comparable may actually be inconsistent.",
      "Consider the use of hardware performance counters to evaluate performance\u2026  \u2026the performance counters that are provided by different vendors may vary, and even the performance counters provided by the same vendor may vary across different generations of the same architecture.",
      "Comparing data from what may seem like similar performance counters within an architecture, across architectures, or between generations of the same architecture may result in the sin of inconsistency, because the hardware performance counters are counting different hardware events.",
      "Comparing against inconsistent workloads would be another way of falling foul of this sin: for example, testing one algorithm during the first half of a week, and another algorithm during the second half of a week, when the underlying distribution of workload is not uniform across the week."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://kar.kent.ac.uk/55171/1/Blackburn+2016TOPLAS.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 90421095
  },
  {
    "blog_id": "snorkel-rapid-training-data-creation-with-weak-supervision",
    "summary": [
      "Snorkel: rapid training data creation with weak supervision Ratner et al., VLDB\u201918  Earlier this week we looked at Sparser, which comes from the Stanford Dawn project , \u201ca five-year research project to democratize AI by making it dramatically easier to build AI-powered applications.\u201d Today\u2019s paper choice, Snorkel, is from the same stable.",
      "It tackles one of central questions in supervised machine learning: how do you get a large enough set of training data to power modern deep models?",
      "\u2026deep learning has a major upfront cost: these methods need massive training sets of labeled examples to learn from \u2013 often tens of thousands to millions to reach peak predictive performance.",
      "Such training sets are enormously expensive to create\u2026  Snorkel lets you throw everything you\u2019ve got at the problem.",
      "Heuristics, external knowledge bases, crowd-sourced workers, you name it.",
      "These are known as weak supervision sources because they may be limited in accuracy and coverage.",
      "All of these get combined in a principled manner to produce a set of probability-weighted labels.",
      "The authors call this process \u2018data programming\u2019.",
      "The end model is then trained on the generated labels.",
      "Snorkel is the first system to implement our recent work on data programming\u2026 While programming weak supervision seems superficially similar to feature engineering, we observe that users approach the two processes very differently.",
      "Our vision \u2013 weak supervision as the sole port of interaction for machine learning \u2013 implies radically different workflows\u2026  The big picture  There are three main stages in the Snorkel workflow:  Instead of hand-labelling large quantities of training data, users write labelling functions which capture patterns and heuristics, connect with external knowledge bases (distant supervision), and so on.",
      "A labelling function is a Python method which given an input can either output a label or abstain.",
      "Snorkel also includes a number of declarative labelling functions that can be used out of the box.",
      "Snorkel learns a generative model over all of the labelling functions, so that it can estimated their accuracies and correlations.",
      "\u201cThis step uses no ground-truth data, learning instead from the agreements and disagreements of the labeling functions.\u201d  Snorkel outputs a set of probabilistic labels which can then be used to train a wide variety of machine learning models.",
      "( Enlarge )  While the generative model is essentially a re-weighted combination of the user-provided labeling functions \u2013 which tend to be precise but low coverage \u2013 modern discriminative models can retain this precision while learning to generalize beyond the labelling functions, increasing coverage and robustness on unseen data.",
      "Labelling functions  Say we\u2019re interested in a binary classifier text-relation extraction task, in which a (chemical, disease) input tuple maps to true iff the chemical causes the disease.",
      "Snorkel breaks input documents (PubMed abstracts) down into a context hierarchy made up of context types.",
      "The set of context types that make sense will be data dependent.",
      "Here we might extract documents, sentences, spans, and entities.",
      "Tuples of relevant entities are then passed to labelling functions as candidates.",
      "Writing in Python, a labelling function encoding the heuristic that the word \u2018causes\u2019 in-between a chemical and a disease indicates a causal relationship would look like this:  For simple cases, there are built-in declarative labelling functions.",
      "In this case, we could have used a pattern-based function instead of writing our own:  Labeling function generators create multiple labelling functions from a single resource.",
      "We could use the Comparative Toxicogenomics Database as a distant supervision source for example, and label candidates as \u2018true\u2019 if they appear in the \u201cCauses\u201d subset, and \u2018false\u2019 if they appear in the \u201cTreats\u201d subset.",
      "One neat example in the evaluation is using a set of crowdworkers to crowdsource annotations, and then representing each crowdworker as a distinct labelling function.",
      "Snorkel will automatically learn to adapt to the different skill levels and accuracy of the workers.",
      "The generative model  Once we have a collection of labelling functions, an obvious thing to do would be to ask each function to label a candidate and use majority voting to determine the resulting label.",
      "In fact, in situations where we don\u2019t have many votes on an input (e.g., most of the labelling functions abstain), and in situations where we have lots of votes, then majority voting works really well.",
      "But in-between these two extremes, taking a weighted vote based on modelling labelling function accuracy works better.",
      "Snorkel uses a heuristic based on the ratio of positive to negative labels for each data point to decide whether to use majority voting or to build a generative model of function accuracy in order to perform weighted voting.",
      "Essentially, we are taking the expected counts of instances in which a weighted majority vote could possibly flip the incorrect predictions of unweighted majority vote under best case conditions, which is an upper bound for the expected advantage.",
      "When a generative model is called for it is built as a factor graph, applying all labelling functions to the unlabelled data points and capturing the labelling propensity, accuracy, and pairwise correlations of the functions.",
      "The details of learning the model are given in an earlier paper, \u2018 Learning the structure of generative models without labeled data .\u2019  Dealing with correlated labels  Often the provided labelling functions are not independent.",
      "For example functions could be simple variations of each other, or they could depend on a common source of distant supervision.",
      "If we don\u2019t account for the dependencies between labelling functions, we can get into all sorts of trouble:  Getting users to somehow indicate dependencies by hand is difficult and error-prone.",
      "We therefore turn to our method for automatically selecting which dependencies to model without access to ground truth (See \u2018 Learning the structure of generative models without labeled data .\u2019 It uses a pseudo-likelihood estimator, which does not require any sampling or other approximations to compute the objective gradient exactly.",
      "It is much faster than maximum likelihood estimation, taking 15 seconds to select pairwise correlations to be modeled among 100 labeling functions with 10,000 data points.",
      "The estimator does rely on a hyperparameter  though, which trades-off between predictive performance and computational cost.",
      "With large values of  no correlations are included and as we reduce the value progressively more correlations are added, starting with the strongest.",
      "The following plots show examples of the numbers of correlations added for different values of the correlation threshold  in three different tasks.",
      "Generally, the number of correlations grows slowly at first, then hits an \u201celbow point\u201d beyond which the number explodes\u2026 setting  to this elbow point is a safe tradeoff between predictive performance and computational cost.",
      "Snorkel in action  Snorkel is evaluated across six different applications and in a user study to determine how quickly subject-matter experts could learn to write labelling functions.",
      "In the user study, participants were given 4.5 hours of instruction on how to use and evaluate models developed using Snorkel, and then had 2.5 hours to write labelling functions, for a total time invested of 7 hours.",
      "(Workshop materials are available at  [url]"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://arxiv.org/pdf/1711.10160",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 202387
  },
  {
    "blog_id": "snuba",
    "summary": [
      "Snuba: automating weak supervision to label training data Varma & R\u00e9, VLDB 2019  This week we\u2019re moving on from ICML to start looking at some of the papers from VLDB 2019.",
      "VLDB is a huge conference, and once again I have a problem because my shortlist of \u201cthat looks really interesting, I\u2019d love to read it\u201d papers runs to 54 long at the moment!",
      "As a special bonus for me, I\u2019m actually going to be at VLDB this year, where no doubt I\u2019ll learn about even more interesting things!",
      "By the time you get to read this, it should be the first (workshop) day of the conference\u2026  The conference may have changed, but to bridge from ICML to VLDB I\u2019m going to start with a paper on very much the same theme as we\u2019ve been dipping into over the past couple of weeks: how to combine and learn from multiple noisy sources of data and labels.",
      "Snuba is from the same Stanford line as Snorkel which we looked at last year.",
      "It\u2019s tackling the same fundamental problem: how to gather enough labeled data to train a model, and how to effectively use it in a weak supervision setting (supervised learning with noisy labels).",
      "In Snorkel human experts write (noisy) labelling functions, aka heuristics, but in Snuba the system itself generates its own heuristics!",
      "Here\u2019s the setup:  We have a small labeled dataset, but not big enough to learn an accurate classifier  We use the labeled dataset to learn classifiers that have good-enough accuracy over subsets of the data (features)  We use the learned classifiers to predict labels for a much larger unlabelled dataset  We train a final model on the now noisily labelled large dataset, and this model shows increased performance  It took me quite a while to get my head around this!",
      "We don\u2019t have enough labeled data to learn a good classifier, but we end up learning a good classifier anyway.",
      "What magic is this?",
      "The secret is in the selection, application, and aggregation of those intermediate learned heuristics.",
      "Snuba automatically generates heuristics that each labels the subset of the data it is accurate for, and iteratively repeats this process until the heuristics together label a large portion of the unlabeled data\u2026 Users from research labs, hospitals and industry helped us design Snuba such that it outperforms user-defined heuristics and crowdsourced labels by up to 9.74 F1 point and 13.80 F1 points in terms of end model performance.",
      "Compared to Snorkel of course, \u201cthe key challenge in automating weak supervision lies in replacing the human reasoning that drives heuristic development.\u201d  The big picture  Snuba has three main components: the synthesiser, the pruner, and the verifier.",
      "It maintains a committed set of heuristics which will be used in labelling, and in each iteration it synthesises new candidate heuristics, selects from among them to add to the committed set, and then verifies the results.",
      "The synthesiser uses the small labelled dataset to generate new candidate heuristics.",
      "It associates each heuristic with a labelling pattern to assign labels only where the heuristic has high confidence.",
      "Allowing heuristics to abstain from labelling datapoints where they have low confidence is a key part of how Snuba achieves its final performance.",
      "Compared to noisily labelling everything the result is a smaller labelled dataset once the heuristic has been applied to the unlabelled data, but with higher confidence.",
      "Not every candidate heuristic generated by the synthesiser ends up being used.",
      "It\u2019s the job of the pruner to select a heuristic to add from among them.",
      "We want the heuristics in the committed set to be diverse in terms of the datapoints in the unlabeled set they label, but also ensure that it performs well for the datapoints it labels in the labeled dataset.",
      "A diverse heuristic is defined as one that labels points that have never received a label from any other heuristics.",
      "The pruner selects for diversity by measuring the Jaccard distance 1 between the set of datapoints labelled by a candidate heuristic and the set of datapoints labelled so far.",
      "The verifier uses a generative model to learn the accuracies of the heuristics in the committed set and produce a single probabilistic training label for each data point in the unlabeled dataset.",
      "Generative models are a popular approach to learn and model the accuracies of different labeling sources like user-defined heuristics and knowledge bases when data is labeled by a variety of sources.",
      "The generative model assumes that each heuristic performs better than random.",
      "For user-defined heuristics that\u2019s a reasonable assumption, but with machine-generated heuristics that assumption could be violated.",
      "Snuba use the small labeled dataset to indirectly determine whether the generated heuristics are likely to be worse than random on the unlabeled dataset.",
      "Snuba keeps iterating (adding new heuristics) until the estimate of the accuracy of the new heuristic suggests it performs worse than random.",
      "Supported heuristic models  Users can plug in their own heuristic models for heuristic generation, all that\u2019s required is that the model generates heuristics that output a probabilistic label over a subset of the data.",
      "Snuba comes with three different models out of the box:  Decision stumps are decision trees limited to a certain depth  Logistic regressors learn a single linear decision boundary  K-nearest neighbour relies on the distribution of the labeled datapoints to decide decision boundaries, with confidence based on the distance of an unlabelled point from a cluster.",
      "Why does the end model performed better than a simple ensemble of the selected heuristics?",
      "At the end of this process, we have an aggregation of heuristics that assign probabilistic labels to a large portion of the unlabelled dataset.",
      "Why not just use this aggregation as the final model??",
      "One of the motivations for designing Snuba is to efficiently label enough training data for training powerful, downstream machine learning models like neural networks.",
      "Heuristics from Snuba are not used directly for the classification task at hand because (1) they may not label the entire dataset due to abstentions, and (2) they are based only on the user-defined primitives and fail to take advantage of the raw data representation.",
      "For example, heuristics may be based on features such as measurements of a tumor (images), bag-of-words representations (text), or bounding box coordinates.",
      "An end model can operate over the entire raw image, sentence, or representation.",
      "Evaluation  The evaluation demonstrates the following:  Training labels from Snuba outperform labels from automated baseline methods by up to 14.35 F1 points, and to transfer learning from the small labelled dataset by up to 5.74 F1 points  Training labels from Snuba outperform those from user-developed heuristics by up to 9.74 F1 points.",
      "The heuristics developed by users have very high precision, but Snuba claws back its advantage by improving recall through its diversity measures.",
      "Each component of Snuba plays its part in boosting overall system performance.",
      "Experiments are conducted over a variety of different applications and datasets:  In image-based tasks Snuba generated heuristics that used at most 4 primitives, while for text-based tasks it generated heuristics that relied on only a single primitive.",
      "Our work suggests that there is potential to use a small amount of labeled data to make the process of generating training labels much more efficient.",
      "The Jaccard distance between two sets A and B is given by  .",
      "\u21a9"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://www.vldb.org/pvldb/vol12/p223-varma.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 57114772
  },
  {
    "blog_id": "prudent-cryptography",
    "summary": [
      "Prudent Engineering Practice for Cryptographic Protocols \u2013 Abadi & Needham, 1994  Prudent engineering practice for cryptographic protocols for most of us is not to design cryptographic protocols!",
      "Today\u2019s paper serves to highlight how even the experts can get it wrong, and presents 11 design principles for cryptographic protocols \u2013 some of which may be useful in the design of other kinds of protocols too.",
      "We present principles for the design of cryptographic protocols.",
      "The principles are not necessary for correctness, nor are they sufficient.",
      "They are however helpful, in that adherence to them  would have contributed to the simplicity of protocols and avoided a considerable number of published confusions and mistakes.",
      "We arrived at our principles by noticing some common features among protocols that are difficult to analyze.",
      "If these features are avoided, it becomes less necessary to resort to formal tools- and also easier to do so if there is good reason to.",
      "The principles themselves are informal guidelines, and useful independently of any logic.",
      "The authors observe that formal techniques can help to prove a design is correct, but give you no guidance in coming up with a correct design in the first place.",
      "A protocol in this paper is simply defined to be a set of rules or conventions defining an exchange of messages among a set of two or more partners.",
      "Of the 11 principles discussed in the paper, the first two are set above the rest as overarching principles.",
      "Principle 1  Every message should say what it means:  the interpretation of the message should depend only on its content.",
      "It should be possible to write down a straightforward English  sentence describing the content-though if  there is a suitable formalism available that  is good too.",
      "In other words, there is to be no implied context in which the message is to be evaluated.",
      "Principle 2  The conditions for a message to be acted upon should be clearly set out so that someone reviewing a design may see whether they are acceptable or not.",
      "For a message to be acted on, you have to trust it.",
      "What does trust mean?",
      "You should at least clearly set out what needs to be trusted in order to act on a message.",
      "Principle 3  If the identity of a principal is essential to  the meaning of a message, it is prudent to  mention the principal\u2019s name explicitly in  the message.",
      "This follows from the first principle.",
      "It is \u2018obvious and simple, but commonly ignored.\u2019  Principle 4  Be clear as to why encryption is being done.",
      "Encryption is not wholly cheap, and not asking precisely why it is being done can lead to redundancy.",
      "Encryption is not synonymous  with security, and its improper use can lead  to errors.",
      "There are at least four different reasons the authors cite for why a protocol may be using encryption in a particular step.",
      "Protocol authors should be clear what the intended purpose is at each stage.",
      "Is it:  To preserve confidentiality?",
      "To guarantee authenticity (that a message came from a particular sender)?",
      "To bind together the parts of a message?",
      "While encryption guarantees confidentiality  and authenticity, it also serves in binding  together the parts of a message: receiving { X ,Y }K is not always the same as receiving { X }K and { Y }K.  When encryption is  used only to bind parts of a message, signature is sufficient.",
      "The meaning attached  to this binding is rather protocol-dependent,  and often subtle.",
      "To produce random numbers?",
      "Principle 5  Don\u2019t confuse the fact that a party signed an encrypted message, with the fact that a party knows the content of an encrypted message\u2026  When a principal signs material that has already been encrypted, it should not be inferred that the principal knows the content  of the message.",
      "On the other hand, it is  proper to infer that the principal that signs  a message and then encrypts it for privacy  knows the content of the message.",
      "Principle 6  Be clear what properties you are assuming  about nonces.",
      "What may do for ensuring  temporal succession may not do for ensuring  association-and perhaps association is best  established by other means.",
      "Nonces are often used to avoid message replay attacks, as part of a challenge-response exchange.",
      "\u201cA message is sent which leads to a reply  which could only have been produced in knowl-edge of the first message.",
      "The objective is to  guarantee that the second message is made after  the first was sent, and sometimes to bind the two together.",
      "There is sometimes confusion about nonces-are they guaranteed new, random, unpredictable?\u201d  It is not necessary for nonces to be unpredictable (you could use a counter).",
      "However, predictable nonces should be used with caution.",
      "This leads to principle 7\u2026  Principle 7  The use of a predictable quantity (such as the value of a counter) can serve in  guaranteeing newness, through a challenge- response exchange.",
      "But if a predictable  quantity is to be effective, it should be protected so that an intruder cannot simulate a  challenge and later replay a response.",
      "Principle 8  If timestamps are used as freshness guarantees by reference to absolute time, then  the difference between local clocks at various machines must be much less than the  allowable age of a message deemed to be  valid.",
      "Furthermore, the time maintenance  mechanism everywhere becomes part of the  trusted computing base.",
      "Principle 9  A key may have been used recently, for ex-ample to encrypt a nonce, yet be quite old,  and possibly compromised.",
      "Recent use does  not make the key look any better than it  would otherwise.",
      "In other words, don\u2019t confuse recency of use with recency of generation.",
      "Principle 10  If an encoding is used to present the meaning of a message, then it should be possible  to tell which encoding is being used.",
      "In the  common case where the encoding is protocol  dependent, it should be possible to deduce  that the message belongs to this protocol,  and in fact to a particular run of the protocol, and to know its number in the protocol.",
      "If you choose to use a compact encoding, make sure there is no loss of information and no chance of confusing messages from different instances of the protocol.",
      "Principle 11  The final principle builds on the second one:  The protocol designer should know which trust relations his protocol depends on, and  why the dependence is necessary.",
      "The reasons for particular trust relations being acceptable should be explicit though they will  be founded on judgement and policy rather  than on logic.",
      "An example is given with respect to timestamps:  The use of timestamps makes explicit for the first  time a question of trust.",
      "When can a principal  A rely on another principal B putting a correct  timestamp in a message?",
      "The answer usually  given is that this is acceptable if A trusts B in  relation to timestamps\u2026  Which leads to the general rule\u2026  We may simply say that A trusts  B in regard to some function if a loss of security  to A could follow from B not behaving in the  specified way; it is usually difficult or impossible  for A to verify B\u2019s good behavior.",
      "There is some measure of trust involved whenever one principal acts on the content of a message from another.",
      "It is essential that this trust  be properly understood.",
      "See the full paper for a more detailed exposition of the principles, and for examples in each case of protocols that violated the principle and hence ended up in trouble."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://www.cs.utexas.edu/~shmat/courses/cs6431/prudent.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 97381495
  },
  {
    "blog_id": "moving-fast-with-software-verification",
    "summary": [
      "Moving Fast with Software Verification \u2013 Calcagno et al. 2015  This is a story of transporting ideas from recent theoretical research in reasoning about programs into the fast-moving engineering culture of Facebook.",
      "The context is that most of the authors landed at Facebook in September of 2013, when we brought the INFER static analyser with us from the verification startup Monoidics.",
      "INFER itself is based on recent academic research in program analysis, which applied a relatively recent development in logics of programs, separation logic.",
      "As of this writing INFER is deployed and running continuously to verify select properties of every code modification in Facebook\u2019s mobile apps; these include the main Facebook apps for Android and iOS, Facebook Messenger, Instagram, and other apps which are used by over a billion people in total.",
      "How do you mesh formal verification \u201cproponents of which sometimes even used to argue that programs should be developed only after a prior specifications had been written down,\u201d with a continuous delivery model?",
      "This strikes me as very similar to the problem of integrating security into a continuous delivery pipeline too.",
      "On the web, Facebook pushes new changes to code twice a day \u2013 but mobile platforms are now even more important than the web.",
      "With the mobile platforms (iOS and Android), you can\u2019t just push new features and bug fixes the minute they are ready \u2013 Facebook can only distribute a new version to the Apple App Store or Google Play but the user controls if/when they update.",
      "This difference has dramatic impact on bug fixes.",
      "On the web when a bug is discovered a fix can be shipped to the servers as part of a periodic release or in exceptional cases immediately via a \u201chotfix\u201d.",
      "And on the web mechanisms exist to automatically update the JavaScript client software running in the browser, allowing fixes to quickly and automatically be deployed as soon as they have been developed.",
      "On current mobile platforms updates must typically be explicitly authorised by the device owner, so there is no guarantee that a fix will be deployed in a timely manner, if ever, once it is developed.",
      "Furthermore mobile platforms have relatively low fault tolerance so a runtime error can often cause termination of the entire app.",
      "Thus mobile development at Facebook presents a strong dichotomy: on one hand it employs continuous development; on the other hand it could benefit from techniques like formal verification to prevent bugs before apps are shipped.",
      "The first safety properties the INFER team decided to tackle were the implicit safety properties that null pointer exceptions and resource leaks cannot occur in Android-based code, and additionally memory leaks in iOS code.",
      "The formal verification is integrated with regression testing as part of the continuous delivery pipeline.",
      "The authors identify four (challenging) requirements for verification in such a scenario:  It must be fully automated and integrated  It must scale to millions of lines of code  It must give precise and useful results: \u201ca developer\u2019s time is an importance resource, an imprecise tool providing poor results would be seen as a waste of that resource.\u201d  It must generate results quickly: \u201cit has to report in minutes, before programmers commit or make further changes.\u201d  These requirements are challenging.",
      "In our context we are talking about analysis of large Android and iPhone apps (millions of lines of code are involved in the codebases).",
      "The analysis must be able to run on thousands of code diffs in a day, and it should report in under 10 minutes on average to fit in with the developer workflow.",
      "There are intra-procedural analyses and linters which fit these scaling requirements, and which are routinely deployed at Facebook and other companies with similar scale codebases and workflows.",
      "But if an analysis is to detect or exclude bugs involving chains of procedure calls, as one minimally expects of verification techniques, then an inter-procedural analysis is needed, and making inter-procedural analyses scale to this degree while maintaining any degree of accuracy has long been a challenge.",
      "To address these challenges INFER exploits several recent advances in automatic verification:  It\u2019s underlying formalism is separation logic.",
      "It implements a compositional, bottom-up variant of the classic RHS inter-procedural analysis algorithm based on procedure summaries.",
      "There are two main novelties.",
      "First, it uses compact summaries, based on the ideas of footprints and frame inference from separation logic, to avoid the need for huge summaries that explicitly tabulate most of the input-output possibilities.",
      "Second, it uses a variation on the notion of abductive inference to discover those summaries.",
      "The overall development process works as follows:  The programmer creates a diff  The diff goes through a phase of peer-reviews: \u201ca loop of interactions aimed at making the code change robust and efficient as well as being understandable, readable, and maintainable by others.\u201d  When the reviewers are satisfied they accept the code-change and the diff is pushed to the main code-base.",
      "Every two weeks a version of the code is frozen into the release candidate.",
      "This goes into a testing period where it is available to Facebook\u2019s employees for internal use.",
      "After two weeks of internal use, the release candidate is rolled out to Facebook users.",
      "During phase 2, regression tests are automatically run and before accepting any code change a reviewer requires that all the tests pass.",
      "Tests run asynchronously and the results are automatically available in the collaboration tool phabricator used for peer review.",
      "INFER is run at phase 2.",
      "The process is completely automatic.",
      "Once the code is submitted for peer review, an analysis is run asynchronously in one of Facebook\u2019s datacenters and results are reported on phabricator in the form of comments.",
      "To be able to comment on a diff within 10 minutes, INFER uses incremental analysis and a caching system for analysis results.",
      "The full Android and iOS code bases are fully analysed nightly as well.",
      "This full analysis can take over 4 hours, and produces the cache used when processing diffs.",
      "Ultimately, one of the biggest challenges we faced was a social challenge: to get programmers to react to bugs reported by the tool and fix genuine errors.",
      "Programmers need to accumulate trust in the analyser and they should see it as something helping them to build better software rather than something slowing them down  To build this trust, the team started conservatively concentrating on out-of-memory errors and null pointer exceptions and training INFER on the existing database of crashes and bugs to target false positives and negatives.",
      "\u201cHaving a dedicated static analysis team within Facebook helps tremendously with the social challenge.",
      "INFER is a good start, but the authors believe there is much more the research community can do as a whole:  Finally, although there have been some successes, we should say that from an industrial perspective advanced program analysis techniques are generally under- developed.",
      "Simplistic techniques based on context insensitive pattern matching (\u201clinters\u201d) are deployed often and do provide value, and it is highly non-trivial to determine when or where many of the ingenious ideas being proposed in the scientific literature can be deployed practically.",
      "Part of the problem, we suggest, is that academic research has focused too much on whole-program analysis, or on specify-first, both of which severely limit the number of use cases.",
      "There are of course many other relevant problem areas \u2013 error reporting, fix suggestion, precision of abstract domains, to name a few \u2013 but we believe that automatic formal verification techniques have the potential for much greater impact if compositional analyses can become better developed and understood."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://research.fb.com/wp-content/uploads/2016/11/publication00124_download0001.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 35928796
  },
  {
    "blog_id": "deep-code-search",
    "summary": [
      "Deep code search Gu et al., ICSE\u201918  The problem with searching for code is that the query, e.g. \u201cread an object from xml,\u201d doesn\u2019t look very much like the source code snippets that are the intended results, e.g.",
      ":  *  That\u2019s why we have Stack Overflow!",
      "Stack Overflow can help with \u2018how to\u2019 style queries, but it can\u2019t help with searches inside codebases you care about.",
      "For example, \u201cwhere in this codebase are events queued on a thread?\u201d  \u2026an effective code search engine should be able to understand the semantic meanings of natural language queries and source code in order to improve the accuracy of code search.",
      "DeepCS is just such a search engine for code, based on the CODEnn (Code-Description Embedding Neural Network) network model.",
      "During training, it takes code snippets (methods) and corresponding natural language descriptions (from the method comments) and learns a joint-embedding.",
      "I.e., it learns embeddings such that a method description and its corresponding code snippet are both mapped to a similar point in the same shared embedding space.",
      "Then given a natural language query, it can embed the query in vector space and look for nearby code snippets.",
      "Compared to Lucene powered code search tools, and the recently proposed state-of-the-art CodeHow search tool, CODEnn gives excellent results.",
      "Embeddings  One of the most popular posts over time on this blog has been \u2018 The amazing power of word vectors ,\u2019 which describes the process of turning words into vectors.",
      "To turn a sequence of words into a vector, one common approach is to use an RNN (e.g., LSTM).",
      "For example, given a sentence such as \u2018parse xml file,\u2019 the RNN reads the first word, \u2018parse,\u2019 maps it into a vector  , and then computes the RNN hidden state  using  .",
      "Then it reads the second word, \u2018xml,\u2019 maps that into word vector  and updates the hidden state  to  .",
      "We k keep going in this manner until we reach the end of the sentence, and then use the final hidden state (  in this example) as the sentence embedding.",
      "We don\u2019t want just any old embedding though.",
      "We want to learn embeddings such that code snippets (think of them like \u2018code sentences\u2019 for now, we\u2019ll get to the details shortly) and their corresponding descriptions have nearby embeddings.",
      "Joint Embedding, also known as multi-model embedding, is a technique to jointly embed/correlate heterogeneous data into a unified vector space so that semantically similar concepts across the two modalities occupy nearby regions of the space.",
      "When we\u2019re training, we use a similarity measure (e.g. cosine) in the loss function to encourage the joint mapping.",
      "CODEnn high level architecture  CODEnn has three main components: a code embedding network to embed source code snippets in vectors; a description embedding network to embed natural language descriptions into vectors; and a similarity module (cosine) to measure the degree of similarity between code and descriptions.",
      "At a high level it looks like this:  If we zoom in one level, we can start to see the details of the network constructions:  The code embedding network  A code snippet (a method) is turned into a tuple (M,A,T) where M is a sequence of camelCase split tokens in the method name, A is an API sequence (the method invocations made by the method body), and T is the set of tokens in the snippet.",
      "The method name and API invocation sequences are both embedded (separately) into vectors using RNNs with maxpooling.",
      "The tokens have no strict order, so they are embedded using a conventional multilayer perceptron.",
      "The three resulting vectors are then fused into one vector through a fully connected layer.",
      "The description embedding network  The description embedding network uses an RNN with maxpooling to encode the natural language description from the (first sentence of) the method comment.",
      "Training  During training, training instances are given as triples (C, D+, D-), where C is a code snipped, D+ is the correct (actual) description of C, and D- is an incorrect description, chosen randomly from the pool of all descriptions.",
      "The loss function seeks to maximise the cosine similarity between C and D+, and make the distance between C and D- as large as possible.",
      "The training corpus is based on open-source Java projects on GitHub, resulting in a body of just over 18M commented Java methods.",
      "For each Java method, we use the method declaration as the code element and the first sentence of its documentation comment as its natural language description.",
      "According to the Javadoc guidance, the first sentence is usually a summary of a method.",
      "The method bodies are parsed using the Eclipse JDT compiler.",
      "(Note, some of the tools from Source{d} may also be useful here).",
      "All the RNNs are embodied as LSTMs with 200 hidden units in each direction, and word embeddings have 100 dimensions.",
      "Indexing and querying  To index a codebase, DeepCS embeds all code snippets in the codebase into vectors using offline processing.",
      "Then during online searching DeepCS embeds the natural language user query, and estimates the cosine similarities between the query embedding and pre-computed code snippet embeddings.",
      "The top K closest code snippets are returned as the query results.",
      "Evaluation  Evaluation is done using a search codebase constructed from ~10 thousand Java projects on GitHub (different to the training set), with at least 20 stars each.",
      "All code is indexed (including methods without any comments), resulting in just over 16M indexed methods.",
      "We build a benchmark of queries from the top 50 voted Java programming questions in Stack Overflow.",
      "To achieve so, we browse the list of Java-tagged questions in Stack Overflow and sort them according to the votes that each one receives.",
      "To qualify, the questions must be about a concrete Java programming task and include a code snippet in the accepted answer.",
      "The 50 such resulting questions are shown in the following table.",
      "In the right-hand column we can see the FRank (rank of the first hit result in the result list) for DeepCS, CodeHow, and a conventional Lucene-based search tool.",
      "Here are some representative query results.",
      "\u201cqueue an event to be run on the thread\u201d vs \u201crun an event on a thread queue\u201d (both have similar words in the query, but are quite different).",
      "\u201cget the content of an input stream as a string using a specified character encoding\u201d  \u201cread an object from an xml file\u201d (note that the words xml, object, and read don\u2019t appear in the result, but DeepCS still finds it)  \u201cplay a song\u201d (another example of associative search, where DeepCS can recommend results with semantically related words such as audio).",
      "DeepCS isn\u2019t perfect of course, and sometimes ranks partially relevant results higher than exact matching ones.",
      "This is because DeepCS ranks results by just considering their semantic vectors.",
      "In future work, more code features (such as programming context) could be considered in our model to further adjust the results."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://guxd.github.io/papers/deepcs.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 51254358
  },
  {
    "blog_id": "smoke-fine-grained-lineage-at-interactive-speed",
    "summary": [
      "Smoke: fine-grained lineage at interactive speed Psallidas et al., VLDB\u201918  Data lineage connects the input and output data items of a computation.",
      "Given a set of output records, a backward lineage query selects a subset of the output records and asks \u201cwhich input records contributed to these results?\u201d A forward lineage query selects a subset of the input records and asks, \u201cwhich output records depend on these inputs?\u201d.",
      "Lineage-enabled systems capture record-level relationships throughout a workflow and support lineage queries.",
      "Data lineage is useful in lots of different applications; this paper uses as its main example interactive visualisation systems.",
      "This domain requires fast answers to queries and is typically dominated by hand-written implementations.",
      "Consider the two views in the figure below.",
      "When the user selects a set of marks in  , marks derived from the same records are highlighted in  (linked brushing).",
      "A typical visualisation system implements this manually, but it can equally be viewed as a backward lineage query from the selection points in  , followed by a forward lineage query from the resulting input records to  .",
      "(See \u2018 Explaining outputs in modern data analytics\u2019 which we looked at last year for an introduction to lineage and provenance principles.",
      "Chotia et al. use a shadow dataflow graph mapping the original computation but flowing in the opposite direction\u2026).",
      "Challenges with existing lineage capture systems  We have the usual space/time trade-offs to consider.",
      "We can slow down the base query in order to capture lineage information during query execution (and store that information somewhere).",
      "This speeds up answering lineage queries later on.",
      "Or we can keep base queries fast and lazily materialize lineage information later when lineage queries are asked (making them slower).",
      "As data processing engines become faster, an important question \u2014and the main focus of this paper\u2014 is whether it is possible to achieve the best of both worlds: negligible lineage capture overhead, as well as fast lineage query execution.",
      "Smoke\u2019s four principles  Smoke employs four central design principles to try and pull off this trick.",
      "Tight integration of lineage capture into query execution itself, using write-efficient data structures.",
      "Where apriori knowledge of lineage queries is available (e.g., the set of explorations supported by a visualisation tool), this information is used to minimise the amount of lineage that needs to be materialized.",
      "Again, if we know lineage queries in advance, and those queries are only interested in aggregated information then we can potentially materialize aggregate statistics as we process queries, and prune or re-partition lineage indices.",
      "Wherever possible, data structures constructed during normal operation execution are augmented and reused for lineage purposes rather than introducing separated dedicated structures.",
      "\u2026Smoke is an in-memory query compilation database engine that tightly integrates the lineage capture logic within query execution and uses simple, write-efficient lineage indexes for low-overhead lineage capture.",
      "In addition, Smoke enables workload-aware optimizations that prune captured lineage and push the logic of lineage consuming queries down into the lineage capture phase.",
      "Lineage capture  Smoke uses read- and write-efficient index structures based on row ids to capture lineage information.",
      "1-N relationships (between input and output tuples) are represented as inverted indexes.",
      "The index\u2019s ith entry corresponds to the ith output group, and points to a row id array containing the ids of all input records that belong to the group.",
      "1-1 relationships between input and output are represented as a single array.",
      "These indices are populated through a tight integration of lineage capture and relation operator logic to avoid additional API calls, and to facilitate co-optimisation.",
      "There are two basic strategies depending on the operator and circumstances: defer and inject.",
      "Inject strategies incur the full cost of index generation during base query execution, whereas defer strategies defer (portions of) the lineage capture until after the operation execution.",
      "Consider selection: both forward and backward lineage capture use row-id arrays, with the forward one pre-allocated based on the cardinality of the input relation.",
      "While iterating over the relation evaluating the predicate, the inject strategy adds two counters to track the row ids of the current input and output and uses these to update the indices when an output row is emitted.",
      "There is no defer strategy for selection, because it is strictly inferior to inject.",
      "As another example consider hash joins.",
      "Smoke will generate both backward row id arrays, and forward row-id indexes.",
      "Under the inject strategy a build phase augments each hash table entry with a row id array containing the input row ids for that entry\u2019s join key.",
      "The probe phase tracks the row id for each output record and populates the forward and backward indexes.",
      "One drawback of this strategy is that we might trigger multiple re-allocations (growing the size) of the forward indexes if an input record has many matches.",
      "The defer strategy for hash joins takes advantage of the fact that we know the size of the forward indexes after the probe phase.",
      "The build phase maintains an additional output row id list, storing the first output record for each match (output records are emitted contiguously).",
      "After the probe phase, the forward and backward indexes can be pre-allocated and populated in a final scan of the hash table.",
      "For multi-operator plans Smoke propagates lineage information through plan execution so that only a single set of lineage indexes connecting the input and final output relations are emitted.",
      "It also takes advantage of pipelines (that merge multiple operators into a single pipeline as part of normal query execution) to eliminate intermediate lineage materialization points where possible.",
      "Workload-aware optimisations  When the set of lineage queries is known in advance, Smoke can go further than the baseline lineage capture describe above and also apply instrumentation pruning and optimisation push-down.",
      "Instrumentation pruning disables lineage capture for lineage indexes that will not be used by the workload.",
      "Lineage queries that apply summarisation / aggregation before presenting results provide an opportunity to push-down optimisations.",
      "For example, selection push-down when using a static predicate, and partitioning of index arrays by predicate attributes for dynamic selections.",
      "Group-by aggregation can also be pushed down into lineage capture.",
      "We observe that popular provenance semantics (e.g. which and why provenance) can be expressed as lineage consuming queries and pushed down using the above optimizations.",
      "In other words, Smoke can operate as a system with alternative provenance semantics depending on the given lineage consuming query.",
      "Evaluation  Smoke is compared to state-of-the-art logical and physical lineage capture and query approaches using a combination of microbenchmarks, TPC-H queries, and two real-world applications.",
      "Smoke\u2019s lineage capture techniques outperform both logical and physical approaches by up to two orders of magnitude.",
      "For example:  Smoke also sped-up lineage query evaluation by multiple orders of magnitude, especially for low-selectivity lineage queries.",
      "The two real-world applications used in the evaluation were Crossfilter visualisation and data profiling with UGuide.",
      "The results show that :  Lineage can express many real-world tasks from visualisation and data profiling, that are currently implemented by hand in ad-hoc ways, and  the lineage capture mechanism is fast enough to avoid sacrificing performance vs the hand implementations, and in many cases may even perform better.",
      "Smoke illustrates that it is possible to both capture lineage with low overhead and enable fast lineage query performance\u2026 Our capture techniques and workload-aware optimization make Smoke well-suited for online; adaptive; and offline physical database design settings."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://www.vldb.org/pvldb/vol11/p719-psallidas.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 66357293
  },
  {
    "blog_id": "gnn-explainer-a-tool-for-post-hoc-explanation-of-graph-neural-networks",
    "summary": [
      "Graph Neural Network (GNN) is a family of powerful machine learning (ML) models for graphs that can combine node information with the structural information.",
      "One downside of GNNs is that their predictions are hard to interpret.",
      "The paper proposes GNN Explainer model for solving the problem of interpretability.",
      "Paper  Desiderata for GNN explanations  Local edge fidelity - identify the subgraph structure (ideally the smallest) that significantly affected the predictions of the GNN.",
      "ie identify the important edges in the graph (for a given prediction).",
      "Local node fidelity - identify the import node features and correlations in the features of the neighboring nodes.",
      "Single instance and multi-instance explanations - Support both single instance prediction tasks and multi-instance prediction tasks.",
      "Model Agnostic - Support a large family of models (ideally all)  Task Agnostic - Support a large family of tasks (ideally all)  Approach  I first describe the single instance prediction case and use that as the base to describe the multiple instance prediction cases.",
      "All the discussion in this section assumes a single instance prediction task.",
      "Input: Trained GNN, a single instance whose prediction is to be explained.",
      "Task: Identify the small subgraph and the small subset of features that explain the prediction.",
      "Idea: Maximize the mutual information (MI) between the GNN and the explanation by learning a graph mask which can be used for selecting the relevant subgraph (from the GNN\u2019s computational graph) and features (from all layers of the GNN).",
      "Computational graph of GNN (corresponding to a node) refers to the approx L-hop neighborhood of the node in the graph ie the subgraph formed by nodes and edges whose representation affected the representation of the given node.",
      "Single-Instance Explanations  For a node v, the information used to predict its label y is completely described by its computation graph Gc(v) and the associated feature set Xc(v).",
      "The feature set includes the features of all the nodes in the computation graph.",
      "When constructing the explaination, only Gc(v) and Xc(v) are used.",
      "The task can be reformulated as identifying a subgraph GS (subset of Gc(v)) with associated features XS which are important when predicting the label y for node v.  \u201cImportance\u201d is measured in terms of MI  MI(Y, (GS, XS)) = H(Y) - H(Y | G = GS, X = XS) where H is the entropy and Y is a random variable representing the prediction.",
      "A further constraint, | GS| < k is imposed to obtain consise explaintations.",
      "Since H(Y) is fixed (recall that the network has already been trained and is now being used in the inference mode), maximizing MI is equivalent to minimizing the conditional entropy H(Y | G = GS, X = XS)  This is equivalent to selecting the subgraph that minimizes the uncertainty in the prediction of y when the computational graph is Gc(v)  Optimiation Process  Given the exponentially large number of possible subgraphs, we can not directly optimize the given equation.",
      "A \u201crelaxed\u201d-adjacency matrix (whose values are real numbers in the range 0 to 1) is introduced where each element of this fractional adjacency matrix is smaller than the corresponding element of the original adjacency matrix.",
      "Gradient descent can be performed on this adjacency matrix.",
      "The \u201crelaxed\u201d GS can be interpreted as a variational approximation of the subgraph distributions of Gc(v) and the objective can be written as min EGSH(Y | G = GS, X = XS)  Now the paper makes a big approximation that the GNN is convex so as to leverage the Jensen inequality and push the expectation inside the entropy term to get an upper bound and then minimize that ie min H(Y | G = Es[GS], X = XS)  The paper reports that the convexity approximation (along with discreteness constraint) works in practice.",
      "Next, mean field approximation is used to decompose P(GS) as a multivariate Bernoulli distrbitution ie product of AS(i, j) for all (i, j) belonging to Gc(v).",
      "AS can be optimized directly and its values represent the expectation of the Bernoulli distrbitution on wether the edge ei, j exists.",
      "Given the constraints on AS, it is easier to learn a mask matrix M and optimize that such that AS = M * Ac* Additionally, the sigmod operator can be applied on M.  Once M is learned, only the top k values are retained.",
      "Including Node Features in the Explanation  Similar to the previous approach, another feature mask is learned (either one for entire GNN or one per node of the GNN) and is used as a feature selector.",
      "The mask could either be learned such that same set of node features (in terms of dimensions) are selected or a different set of features are selected per node.",
      "The paper uses the former as it is more straightforward.",
      "Just like before, a \u201crelaxed\u201d mask MT is trained to select features as MT * XS.",
      "One tricky case is where one feature is important but its value is set to 0.",
      "In the case, the value will be masked even though it should not be  The workaround is to use Monte Carlo (MC) estimates of marginals of the missing features.",
      "This gives a way to assign importance scores to each feature dimension and a form of reparameterization trick is used to perform end-to-end learning.",
      "Masks are encouraged to be discrete by regularizing their element-wise entropy.",
      "Resulting computation graph is valid as in it allows message passing towards the central node v.  Multi-Instance Explanations  Given a set of nodes (having the label say y),  the task is to obtain a global explanation of the predictions.",
      "For the given class, a prototypical reference node is chosen by computing the mean of embeddings of all the nodes in the class and then selecting the node which is closest to the mean.",
      "Now, compute the important computational graph corresponding to this node and align the computational subgraphs of all the other nodes (in the given class) to reference.",
      "Let A* be the adjacency matrix and X* be the feature matrix for the explanation corresponding to the reference node.",
      "Let Av and Xv be the adjacency matrix and feature matrix of the to-ber-aligned computational graph.",
      "A relaed alignment matrix P is optimized to align the nodes and features in the two graphs ie we minimize |PTAvP - A*| + *|PTXvP - X*|  Choosing concise explanations helps in efficient graph matching.",
      "For GNNs that compute attention over the entire graph, edges with low attention weight can be pruned to increase efficiency.",
      "Experiments  Datasets  Node classification: BA-Shapes, BA-Community, Tree-Cycles, Tree-Grid  Graph classification: MUTAG, Reddit-Binary  Baselines  GRAD - Compute the gradient of the model loss with respect to the adjacency matrix and the node features to be classified and fix the edges with the highest absolute gradient.",
      "GAT - Graph Attention Network  The proposed model seems to outperform the baselines both qualitatively and quantitatively.",
      "But the results should be taken with a grain of salt as only 2 baselines are considered."
    ],
    "author_id": "shugan",
    "pdf_url": "https://www.mitpressjournals.org/doi/pdf/10.1162/089976602753712972",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 65756728
  },
  {
    "blog_id": "a-new-metaheuristic-bat-inspired-algorithm",
    "summary": [
      "A New Metaheuristic Bat-Inspired Algorithm \u2013 Xin-She Yang, 2010  Today it\u2019s the turn of bats!",
      "The bat algorithm is an attempt to combine some of the major advantages of previous algorithms such as the firefly algorithm and harmony search (inspired by music composition).",
      "It is based on the echo-location behaviour of bats.",
      "Bats are fascinating animals.",
      "They are the only mammals with wings and they also have advanced capability of echolocation.",
      "It is estimated that there are about 996 different species which account for up to 20% of all mammal species.",
      "Their size ranges from the tiny bumblebee bat (of about 1.5 to 2g) to the giant bats with wingspan of about 2m and weight up to about 1 kg.",
      "Microbats typically have forearm length of about 2.2 to 11cm.",
      "Most bats uses echolocation to a certain degree; among all the species, microbats are a famous example as microbats use echolocation extensively while megabats do not.",
      "(I think they missed a trick by not calling the megabats monoliths ;) ).",
      "Microbats use a type of sonar, called, echolocation, to detect prey, avoid obstacles, and locate their roosting crevices in the dark.",
      "These bats emit a very loud sound pulse and listen for the echo that bounces back from the surrounding objects.",
      "Their pulses vary in properties and can be correlated with their hunting strategies, depending on the species.",
      "Most bats use short, frequency-modulated signals to sweep through about an octave, while others more often use constant-frequency signals for echolocation.",
      "Their signal bandwidth varies depends on the species, and often increased by using more harmonics.",
      "Microbats emit about 10-20 sound bursts per second.",
      "When hunting this can go up to about 200 pulses a second as they get near their prey.",
      "\u201cSuch short sound bursts imply the fantastic ability of the signal processing power of bats.",
      "In fact, studies shows the integration time of the bat ear is typically about 300 to 400 \u00b5s.\u201d The wavelengths are in the same order as their prey sizes.",
      "Fortunately these pulses are in the ultrasonic region because they are astonishingly loud \u2013 up to about 110 dB.",
      "(That\u2019s roughly the same volume as a live rock concert, or a jackhammer).",
      "The loudness also varies from the loudest when searching for prey and to a quieter base when homing towards the prey.",
      "The travelling range of such short pulses are typically a few metres, depending on the actual frequencies.",
      "Microbats can manage to avoid obstacles as small as thin human hairs.",
      "Studies show that microbats use the time delay from the emission and detection of the echo, the time difference between their two ears, and the loudness variations of the echoes to build up three dimensional scenario of the surrounding.",
      "They can detect the distance and orientation of the target, the type of prey, and even the moving speed of the prey such as small insects.",
      "Indeed, studies suggested that bats seem to be able to discriminate targets by the variations of the Doppler effect induced by the wing-flutter rates of the target insects.",
      "Bats are pretty amazing really!",
      "By equating the echo-location behaviour of bats with an objective function to be optimised, we can formulate new optimisation algorithms.",
      "We will use the following rules/simplifications for our idealized bats:  Echolocation is used to sense distance (ignore any eyesight etc.).",
      "Bats fly randomly with a velocity vi , at position xi.",
      "They emit pulses at a fixed wavelength \u03bb, with varying frequency f and loudness A to search for prey.",
      "The rate of pulse emission r \u2208 [0,1] varies depending on the proximity of the target  Loudness varies from a large positive A0 to a minimum constant value Amin  Another obvious simplification is that no ray tracing is used in estimating the time delay and three dimensional topography (!).",
      "Though this might be a good feature for the application in computational geometry, however, we will not use this as it is more computationally extensive in multidimensional cases.",
      "Starting out with an initial population of bats spread over the solution space, the algorithm proceeds in iterations.",
      "Each bat is randomly assigned a start frequency drawn from [fmin,fmax] \u2013 for example, 0..100.",
      "Each bat is also given a random initial loudness and pulse emission rate r \u2208 [0,1] close to zero.",
      "For each iteration, we update the frequency, position and velocity of each bat in the search space as follows.",
      "Draw a random number between 0 and 1, if it is less than or equal to the current rate of bat i, then update the bat\u2019s location using:  fi = fmin + \u03b2(fmax \u2013 fmin) , where \u03b2 is a random number between 0 and 1.  vi = vi + fi(xi \u2013 xG),  where xG is the current global best location (solution)  xi = xi + vi  However, if the random number is greater than ri, then instead move the bat to a new location generated by taking a random walk from the current best solution:  xi = xG + \u03b5A , where A is the average loudness of all bats at this time, and \u03b5 is a random number drawn from [-1,1].",
      "Now we evaluate the objective function at the new location for the bat.",
      "If the fitness has improved and a random number drawn between A0 and Amin is less than the current loudness for the bat, then we accept (move it to) the the location.",
      "Finally we update the loudness and rate of pulse emission for the bat:  Ai = \u03b1Ai  rit+1 = ri0[1 \u2013 exp(-\u03bbt)], where t represents the time step (iteration)  \u03b1 and \u03bb are constants, both set to 0.9 in the simulation in the paper.",
      "At the end of our chosen number of iterations, the bat at the best location (solution) is the winner.",
      "Note that the algorithm description given in the paper I found quite confusing, and ended up piecing together the above from a combination of the paper and the pseudo-code in Xin-She Yang\u2019s book .",
      "[In tests,] the Bat Algorithm is much superior to other algorithms [Genetic Algorithms and PSO] in terms of accuracy and efficiency.",
      "This is not surprising as the aim of developing the new algorithm was to try to use the advantages of existing algorithms and other interesting feature inspired by the fantastic behaviour of echolocation of microbats.",
      "The Bat Algorithm subsumes (is more powerful than) PSO and Harmony Search:  If we replace the variations of the frequency fi by a random parameter and setting Ai = 0 and ri= 1, the bat algorithm essentially becomes the standard Particle Swarm Optimization (PSO).",
      "Similarly, if we do not use the velocities, but we use fixed loudness and rate: Ai and ri \u2013 for example, Ai = ri= 0.7 \u2013 this algorithm is virtually reduced to a simple Harmony Search (HS) as the frequency/wavelength change is essentially the pitch adjustment, while the rate of pulse emission is similar to the harmonic acceptance rate (here with a twist) in the harmony search algorithm.",
      "The current studies implies that the proposed new algorithm is potentially more powerful and thus should be investigated further in many applications of engineering and industrial optimization problems."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://arxiv.org/pdf/1004.4170v1.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 66112363
  },
  {
    "blog_id": "a-static-verification-framework-for-message-passing-in-go-using-behavioural-types",
    "summary": [
      "A static verification framework for message passing in Go using behavioural types Lange et al., ICSE 18  With thanks to Alexis Richardson who first forwarded this paper to me.",
      "We\u2019re jumping ahead to ICSE 18 now, and a paper that has been accepted for publication there later this year.",
      "It fits with the theme we\u2019ve been exploring this week though, so I thought I\u2019d cover it now.",
      "We\u2019ve seen verification techniques applied in the context of Rust and JavaScript , looked at the integration of linear types in Haskell , and today it is the turn of Go!",
      "Despite its popularity, the Go programming ecosystem offers little to no support for guaranteeing the correctness of message-passing concurrent programs.",
      "This work proposes a practical verification framework for message passing concurrency in Go\u2026  Go\u2019s channel-based concurrency model is inspired by process calculi.",
      "There is a rich body of work on process calculi-based verification for reasoning about safety and liveness properties of interactive systems.",
      "However, Go itself only enforces that messages exchange via communication channels adhere to the declared payload types, and at runtime offers just a \u201ctoy global deadlock detector.\u201d Can we apply more of the process calculi based reasoning in the context of Go?",
      "It turns out that yes, we can.",
      "The Godel Checker enables verification that Go programs are free of global deadlocks, as well as several Go specific safety properties (including channel safety).",
      "There is also support for detecting potentially problematic loops and partial deadlocks.",
      "At the core of the system is a translation from Go source code to a behavioural type model of the program.",
      "A model checker mCRL2 and termination checker (based on KiTTeL ) can then be applied to the extracted behavioural types.",
      "Verification of key safety and liveness properties for a variety of programs shows that Godel Checker can complete its analysis in just a few seconds for smaller programs, and just over a minute for the larger code bases tackled (up to 16 kloc).",
      "As a user, what\u2019s nice about all this is that you don\u2019t have to get involved with any of the formal machinery yourself, just supply the source code!",
      "( Enlarge )  Common concurrency errors in Go programs  Godel Checker address three sources of common concurrency errors in Go programs: channel safety errors, global deadlocks, and partial deadlocks.",
      "After a channel is closed, receive actions always succeed but any send or close actions raise a runtime error.",
      "Hence, \u201cchannels should be closed at most once and no message should be sent on closed channels.\u201d  Go does have a built-in global deadlock detector that will signal at runtime if all goroutines in a program are stuck.",
      "We\u2019d like to find out about the possibility (or hopefully, the absence of the possibility) of global deadlocks ahead of time.",
      "Moreover, when certain common libraries are imported, the global deadlock detector is silently disabled and hence global deadlocks are just ignored.",
      "Then there\u2019s the case when a program communication cannot progress even though only some of its goroutines are stuck.",
      "\u201cThis is known as a partial deadlock or as a failure of liveness.\u201d Consider the following program:  Because ch1 is passed as both arguments to Consumer on line 16 the resulting system is not live: the second producer is not interacting with the consumer and its outputs will never be matched with their respective inputs.",
      "From Go to Behavioural types  Behavioural types are a typing discipline in which types express the possible actions of a program in a fine-grained way.",
      "When applied to communication and concurrency, behavioural types act as an abstract specification of all communication actions that may be performed in a program.",
      "Moreover, behavioural types are an executable specification.",
      "They have a natural operational meaning and evolve throughout program execution.",
      "For the program we saw above, the behavioural type looks like this:  Imperative control structures are transformed into recursive definitions, and data elements are erased.",
      "In terms of types, global deadlock freedom (GDF) requires that if a communication action is available to fire, the type can always make progress.",
      "Thus a type as a whole is never globally stuck.",
      "Liveness, or partial deadlock freedom, is a stronger condition (every live type is also global deadlock free).",
      "Liveness states that all communications that can become enabled in a type can always eventually fire.",
      "(Replacing the call to cons(ch1,ch1) with cons(ch1,ch2) makes the type main() satisfy liveness).",
      "In order to infer behavioural types from Go source code, the source is first converted to a static single assignment (SSA) intermediate representation (IR).",
      "The SSA IR conversion takes a Go program such as this:  And produces:  The main SSA instructions used in the IR are shown in the following table:  Given the SSA form, the next step is to soundly approximate the communication behaviour.",
      "A type signature is generated for every SSA block.",
      "The details of the algorithm are in section 3.2 of the paper.",
      "For our purposes we mostly just need to know that it is possible.",
      "At the end of this process, for Listing 1 above its SSA representation, the inferred behavioural type looks like this:  Model checking  We have our behavioural type model, and now we can proceed to verify its properties:  We proceed in three steps: (1) we generate a (finite) labelled transition system (LTS) for the types from a set of operational semantics rules; (2) we define properties of the states of the LTS in terms of the immediate actions behavioural types can take; and (3) we give safety and liveness properties expressed in the modal \u03bc-calculus.",
      "Finiteness is defined by the restriction that types cannot feature parallel composition or channel creation under recursion.",
      "Semantics for the types follow definitions from CCS (concurrent communication systems) and CSP (communicating sequential processes).",
      "A labelled transition system is built for the entry point type (it basically tells you how to move between states in the system).",
      "Given this representation, we can encode (and hence check) a number of useful liveness and safety properties in the \u03bc-calculus.",
      "They look like this:  You\u2019ll find a very concise guide to decoding those symbols in section 4 of the paper!",
      "Defined this way, the properties can be verified using the mCRL2 model checker.",
      "When extracting behavioural types, conditionals are abstracted as a non-deterministic choice between the alternate behaviours in the then and else branches.",
      "This means that any data dependencies in the conditionals (e.g., testing the value of a variable) are not captured.",
      "This coarse abstraction introduces a subtle interaction between non-terminating program behaviour and data-dependent communication wrt.",
      "liveness.",
      "To address this, an additional termination analysis of loops is done using the KITTeL termination analyser.",
      "KITTeL actually targets C programs, but the syntax of Go is close enough to make it work with a translation to C functions.",
      "The analysis checks that the loop parameters are sufficient to make each loop eventually terminate.",
      "\u201c_This enables us to pinpoint program locations where the liveness of types may not entail the analogue property in the program \u2013 if the termination analysis identifies the program as terminating, the liveness properties on types and programs coincide.\u201d"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://mrg.doc.ic.ac.uk/publications/a-static-verification-framework-for-message-passing-in-go-using-behavioural-types/draft.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 25766140
  },
  {
    "blog_id": "mask_r-cnn",
    "summary": [
      "What  They suggest a variation of Faster R-CNN.",
      "Their network detects bounding boxes (e.g. of people, cars) in images and also segments the objects within these bounding boxes (i.e.",
      "classifies for each pixel whether it is part of the object or background).",
      "The model runs roughly at the same speed as Faster R-CNN.",
      "How  The architecture and training is mostly the same as in Faster R-CNN:  Input is an image.",
      "The backbone network transforms the input image into feature maps.",
      "It consists of convolutions, e.g. initialized with ResNet's weights.",
      "The RPN (Region Proposal Network) takes the feature maps and classifies for each location whether there is a bounding box at that point (with some other stuff to regress height/width and offsets).",
      "This leads to a large number of bounding box candidates (region proposals) per image.",
      "RoIAlign: Each region proposal's \"area\" is extracted from the feature maps and converted into a fixed-size 7x7xF feature map (with F input filters).",
      "(See below.)",
      "The head uses the region proposal's features to perform  Classification: \"is the bounding box of a person/car/.../background\"  Regression: \"bounding box should have width/height/offset so and so\"  Segmentation: \"pixels so and so are part of this object's mask\"  Rough visualization of the architecture:  RoIAlign  This is very similar to RoIPooling in Faster R-CNN.",
      "For each RoI, RoIPooling first \"finds\" the features in the feature maps that lie within the RoI's rectangle.",
      "Then it max-pools them to create a fixed size vector.",
      "Problem: The coordinates where an RoI starts and ends may be non-integers.",
      "E.g. the top left corner might have coordinates (x=2.5, y=4.7).",
      "RoIPooling simply rounds these values to the nearest integers (e.g. (x=2, y=5)).",
      "But that can create pooled RoIs that are significantly off, as the feature maps with which RoIPooling works have high (total) stride (e.g. 32 pixels in standard ResNets).",
      "So being just one cell off can easily lead to being 32 pixels off on the input image.",
      "For classification, being some pixels off is usually not that bad.",
      "For masks however it can significantly worsen the results, as these have to be pixel-accurate.",
      "In RoIAlign this is compensated by not rounding the coordinates and instead using bilinear interpolation to interpolate between the feature map's cells.",
      "Each RoI is pooled by RoIAlign to a fixed sized feature map of size (H, W, F), with H and W usually being 7 or 14.",
      "(It can also generate different sizes, e.g. 7x7xF for classification and more accurate 14x14xF for masks.)",
      "If H and W are 7, this leads to 49 cells within each plane of the pooled feature maps.",
      "Each cell again is a rectangle -- similar to the RoIs -- and pooled with bilinear interpolation.",
      "More exactly, each cell is split up into four sub-cells (top left, top right, bottom right, bottom left).",
      "Each of these sub-cells is pooled via bilinear interpolation, leading to four values per cell.",
      "The final cell value is then computed using either an average or a maximum over the four sub-values.",
      "Segmentation  They add an additional branch to the head that gets pooled RoI as inputs and processes them seperately from the classification and regression (no connections between the branches).",
      "That branch does segmentation.",
      "It is fully convolutional, similar to many segmentation networks.",
      "The result is one mask per class.",
      "There is no softmax per pixel over the classes, as classification is done by a different branch.",
      "Base networks  Their backbone networks are either ResNet or ResNeXt (in the 50 or 102 layer variations).",
      "Their head is either the fourth/fifth module from ResNet/ResNeXt (called C4 (fourth) or C5 (fifth)) or they use the second half from the FPN network (called FPN).",
      "They denote their networks via backbone-head, i.e. ResNet-101-FPN means that their backbone is ResNet-101 and their head is FPN.",
      "Visualization of the different heads:  Training  Training happens in basically the same way as Faster R-CNN.",
      "They just add an additional loss term to the total loss (L = L_classification + L_regression + L_mask).",
      "L_mask is based on binary cross-entropy.",
      "For each predicted RoI, the correct mask is the intersection between that RoI's area and the correct mask.",
      "They only train masks for RoIs that are positive (overlap with ground truth bounding boxes).",
      "They train for 120k iterations at learning rate 0.02 and 40k at 0.002 with weight decay 0.0002 and momentum 0.9.",
      "Test  For the C4-head they sample up to 300 region proposals from the RPN (those with highest confidence values).",
      "For the FPN head they sample up to 1000, as FPN is faster.",
      "They sample masks only for the 100 proposals with highest confidence values.",
      "Each mask is turned into a binary mask using a threshold of 0.5.",
      "Results  Instance Segmentation  They train and test on COCO.",
      "They can outperform the best competitor by a decent margin (AP 37.1 vs 33.6 for FCIS+++ with OHEM).",
      "Their model especially performs much better when there is overlap between bounding boxes.",
      "Ranking of their models: ResNeXt-101-FPN > ResNet-101-FPN > ResNet-50-FPN > ResNet-101-C4 > ResNet-50-C4.",
      "Using sigmoid instead of softmax (over classes) for the mask prediction significantly improves results by 5.5 to 7.1 points AP (depending on measurement method).",
      "Predicting only one mask per RoI (class-agnostic) instead of C masks (where C is the number of classes) only has a small negative effect on AP (about 0.6 points).",
      "Using RoIAlign instead of RoIPooling has significant positive effects on the AP of around 5 to 10 points (if a network with C5 head is chosen, which has a high stride of 32).",
      "Effects are smaller for small strides and FPN head.",
      "Using fully convolutional networks for the mask branch performs better than fully connected layers (1-3 points AP).",
      "Examples results on COCO vs FCIS (note the better handling of overlap):  Bounding-Box-Detection  Training additionally on masks seems to improve AP for bounding boxes by around 1 point (benefit from multi-task learning).",
      "Timing  Around 200ms for ResNet-101-FPN.",
      "(M40 GPU)  Around 400ms for ResNet-101-C4.",
      "Human Pose Estimation  The mask branch can be used to predict keypoint (landmark) locations on human bodies (i.e. locations of hands, feet etc.).",
      "This is done by using one mask per keypoint, initializing it to 0 and setting the keypoint location to 1.",
      "By doing this, Mask R-CNN can predict keypoints roughly as good as the current leading models (on COCO), while running at 5fps.",
      "Cityscapes  They test their model on the cityscapes dataset.",
      "They beat previous models with significant margins.",
      "This is largely due to their better handling of overlapping instances.",
      "They get their best scores using a model that was pre-trained on COCO.",
      "Examples results on cityscapes:"
    ],
    "author_id": "ALEJU",
    "pdf_url": "https://arxiv.org/pdf/1703.06870",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 12239056
  },
  {
    "blog_id": "mining-high-speed-data-streams",
    "summary": [
      "Mining High-Speed Data Streams \u2013 Domingos & Hulten 2000  This paper won a \u2018test of time\u2019 award at KDD\u201915 as an \u2018outstanding paper from a past KDD Conference beyond the last decade that has had an important impact on the data mining community.\u2019  Here\u2019s what the test-of-time committee have to say about it:  This paper proposes a decision tree learner for data streams, the Hoeffding Tree algorithm, which comes with the guarantee that the learned decision tree is asymptotically nearly identical to that of a non-incremental learner using infinitely many examples.",
      "This work constitutes a significant step in developing methodology suitable for modern \u2018big data\u2019 challenges and has initiated a lot of follow-up research.",
      "The Hoeffding Tree algorithm has been covered in various textbooks and is available in several public domain tools, including the WEKA Data Mining platform.",
      "The goal is to create a knowledge discovery system that can cope with large volumes of data (perhaps an unbounded stream) without needing to fit everything in memory (40MB was the allotted amount used in their evaluation tests \u2013 remember this was 2000).",
      "Ideally, we would like to have KDD systems that operate continuously and indefinitely, incorporating examples as they arrive, and never losing potentially valuable information.",
      "Such desiderata are fulfilled by incremental learning methods (also known as online, successive or sequential methods), on which a substantial literature exists.",
      "However, the available algorithms of this type have significant shortcomings from the KDD point of view.",
      "Some are reasonably efficient, but do not guarantee that the model learned will be similar to the one obtained by learning on the same data in batch mode.",
      "They are highly sensitive to example ordering, potentially never recovering from an unfavorable set of early examples.",
      "Others produce the same model as the batch version, but at a high cost in efficiency, often to the point of being slower than the batch algorithm.",
      "Based on a statistical result known as the Hoeffding bound, the authors show how to create Hoeffding (decision) trees and build a Very Fast Decision Tree (VFDT) system based on them.",
      "A key property of the Hoeffding tree algorithm is that it is possible to guarantee under realistic assumptions that the trees it produces are asymptotically arbitrarily close to the ones produced by a batch learner (i.e., a learner that uses all the examples to choose a test at each node).",
      "In other words, the incremental nature of the Hoeffding tree algorithm does not significantly affect the quality of the trees it produces.",
      "In a classification problem, a set of N training examples of the form (x\u20d7,y) is given, where y is a discrete class label and x\u20d7 is a vector of d attributes.",
      "From these examples we need to produce a model y = f(x\u20d7) that will predict the class of future examples x\u20d7 with high accuracy.",
      "Decision tree learners create models in the form of decision trees, where each node contains a test on an attribute, each branch corresponds to a possible outcome of the test, and each leaf contains a class prediction.",
      "To learn a decision tree you recursively replace leaves by test nodes, starting at the root.",
      "Our goal is to design a decision tree learner for extremely large (potentially infinite) datasets.",
      "This learner should require each example to be read at most once, and only a small constant time to process it.",
      "This will make it possible to directly mine online data sources (i.e., without ever storing the examples), and to build potentially very complex trees with acceptable computational cost.",
      "In Hoeffding trees, in order to find the best attribute to test at a given node, only a small subset of the training examples that pass through that node are used.",
      "The key of course, is to determine how small that subset can be, and what guarantees we can give concerning it.",
      "Thus, given a stream of examples, the first ones will be used to choose the root test; once the root attribute is chosen, the succeeding examples will be passed down to the corresponding leaves and used to choose the appropriate attributes there, and so on recursively.",
      "We solve the difficult problem of deciding exactly how many examples are necessary at each node by using a statistical result known as the Hoeffding bound (or additive Chernoff bound).",
      "Given a real-valued random variable r with range R (e.g. 0-1 for a probability), and n independent observations of the variable, we can compute the mean of those observations, r\u0304.",
      "The Hoeffding bound tells us that with probability 1 \u2013 \u03b4, the true mean of the variable is at least r\u0304 \u2013 \u03b5, where:  The Hoeffding bound has the very attractive property that it is independent of the probability distribution generating the observations.",
      "The price of this generality is that the bound is more conservative than distribution-dependent ones (i.e., it will take more observations to reach the same \u03b4 and \u03b5).",
      "If G(Xi) is the heuristic used to choose test attributes, then we want to ensure with high probability the attribute chosen using n examples (where n is as small as possible) is the same that would be chosen using infinite examples.",
      "Suppose that we\u2019ve seen n examples so far, and the best attribute predicted by G is Xa and the second best is Xb.",
      "Call the difference between the observed heuristic values of Xa and Xb \u0394G\u0304  Now, given a desired \u03b4, the Hoeffding bound tells us that Xa is the correct choice with probability 1 \u2013 \u03b4 if n examples have been seen at this node and \u0394G\u0304 > \u03b52.",
      "Thus a node needs to accumulate examples from the streamuntil \u03b5 becomes smaller than \u2206G.",
      "(Notice that \u03b5 is a monotonically decreasing function of n.) At this point the node can be split using the current best attribute, and succeeding examples will be passed to the new leaves.",
      "Pseudo-code for a Hoeffding tree algorithm based on this is given in table 1 of the paper.",
      "The VFDT system was built using this algorithm, and included a number of additional optimisations:  When two or more attributes have very similar scores, lots of examples may be needed to decide between them with confidence.",
      "But if they are very similar, it probably doesn\u2019t matter too much which one we choose, so let\u2019s just pick one after we reach some user-defined threshold and move on\u2026  We don\u2019t need to recompute G after every example since it is unlikely the decision to split will be made at that specific point.",
      "So we can micro-batch and accept a minimum number of new examples before recomputing G.  Under memory pressure, VFDT deactivates the least promising leaves in order to make room for new ones.",
      "Likewise VFDT can also drop early on attributes that do not look promising.",
      "VFDT can be initialised with a tree produced offline by a traditional batch learner.",
      "(Trained on a subset of the overall data).",
      "VFDT can rescan previously seen examples if desired.",
      "This option can be activated if either the data arrives slowly enough that there is time for it, or if the dataset is finite and small enough that it is feasible to scan it multiple times.",
      "This means that VFDT need never grow a smaller (and potentially less accurate) tree than other algorithms because of using each example only once."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://homes.cs.washington.edu/~pedrod/papers/kdd00.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 4830217
  },
  {
    "blog_id": "faster_r-cnn",
    "summary": [
      "What  R-CNN and its successor Fast R-CNN both rely on a \"classical\" method to find region proposals in images (i.e. \"Which regions of the image look like they might be objects?\").",
      "That classical method is selective search.",
      "Selective search is quite slow (about two seconds per image) and hence the bottleneck in Fast R-CNN.",
      "They replace it with a neural network (region proposal network, aka RPN).",
      "The RPN reuses the same features used for the remainder of the Fast R-CNN network, making the region proposal step almost free (about 10ms).",
      "How  They now have three components in their network:  A model for feature extraction, called the \"feature extraction network\" (FEN).",
      "Initialized with the weights of a pretrained network (e.g. VGG16).",
      "A model to use these features and generate region proposals, called the \"Region Proposal Network\" (RPN).",
      "A model to use these features and region proposals to classify each regions proposal's object and readjust the bounding box, called the \"classification network\" (CN).",
      "Initialized with the weights of a pretrained network (e.g. VGG16).",
      "Usually, FEN will contain the convolutional layers of the pretrained model (e.g. VGG16), while CN will contain the fully connected layers.",
      "(Note: Only \"RPN\" really pops up in the paper, the other two remain more or less unnamed.",
      "I added the two names to simplify the description.)",
      "Rough architecture outline:  The basic method at test is as follows:  Use FEN to convert the image to features.",
      "Apply RPN to the features to generate region proposals.",
      "Use Region of Interest Pooling (RoI-Pooling) to convert the features of each region proposal to a fixed sized vector.",
      "Apply CN to the RoI-vectors to a) predict the class of each object (out of K object classes and 1 background class) and b) readjust the bounding box dimensions (top left coordinate, height, width).",
      "RPN  Basic idea:  Place anchor points on the image, all with the same distance to each other (regular grid).",
      "Around each anchor point, extract rectangular image areas in various shapes and sizes (\"anchor boxes\"), e.g. thin/square/wide and small/medium/large rectangles.",
      "(More precisely: The features of these areas are extracted.)",
      "Visualization:  Feed the features of these areas through a classifier and let it rate/predict the \"regionness\" of the rectangle in a range between 0 and 1.",
      "Values greater than 0.5 mean that the classifier thinks the rectangle might be a bounding box.",
      "(CN has to analyze that further.)",
      "Feed the features of these areas through a regressor and let it optimize the region size (top left coordinate, height, width).",
      "That way you get all kinds of possible bounding box shapes, even though you only use a few base shapes.",
      "Implementation:  The regular grid of anchor points naturally arises due to the downscaling of the FEN, it doesn't have to be implemented explicitly.",
      "The extraction of anchor boxes and classification + regression can be efficiently implemented using convolutions.",
      "They first apply a 3x3 convolution on the feature maps.",
      "Note that the convolution covers a large image area due to the downscaling.",
      "Not so clear, but sounds like they use 256 filters/kernels for that convolution.",
      "Then they apply some 1x1 convolutions for the classification and regression.",
      "They use 2*k 1x1 convolutions for classification and 4*k 1x1 convolutions for regression, where k is the number of different shapes of anchor boxes.",
      "They use k=9 anchor box types: Three sizes (small, medium, large), each in three shapes (thin, square, wide).",
      "The way they build training examples (below) forces some 1x1 convolutions to react only to some anchor box types.",
      "Training:  Positive examples are anchor boxes that have an IoU with a ground truth bounding box of 0.7 or more.",
      "If no anchor point has such an IoU with a specific box, the one with the highest IoU is used instead.",
      "Negative examples are all anchor boxes that have IoU that do not exceed 0.3 for any bounding box.",
      "Any anchor point that falls in neither of these groups does not contribute to the loss.",
      "Anchor boxes that would violate image boundaries are not used as examples.",
      "The loss is similar to the one in Fast R-CNN: A sum consisting of log loss for the classifier and smooth L1 loss (=smoother absolute distance) for regression.",
      "Per batch they only sample examples from one image (for efficiency).",
      "They use 128 positive examples and 128 negative ones.",
      "If they can't come up with 128 positive examples, they add more negative ones.",
      "Test:  They use non-maximum suppression (NMS) to remove too identical region proposals, i.e. among all region proposals that have an IoU overlap of 0.7 or more, they pick the one that has highest score.",
      "They use the 300 proposals with highest score after NMS (or less if there aren't that many).",
      "Feature sharing  They want to share the features of the FEN between the RPN and the CN.",
      "So they need a special training method that fine-tunes all three components while keeping the features extracted by FEN useful for both RPN and CN at the same time (not only for one of them).",
      "Their training methods are:  Alternating traing: One batch for FEN+RPN, one batch for FEN+CN, then again one batch for FEN+RPN and so on.",
      "Approximate joint training: Train one network of FEN+RPN+CN.",
      "Merge the gradients of RPN and CN that arrive at FEN via simple summation.",
      "This method does not compute a gradient from CN through the RPN's regression task, as that is non-trivial.",
      "(This runs 25-50% faster than alternating training, accuracy is mostly the same.)",
      "Non-approximate joint training: This would compute the above mentioned missing gradient, but isn't implemented.",
      "4-step alternating training:  Clone FEN to FEN1 and FEN2.",
      "Train the pair FEN1 + RPN.",
      "Train the pair FEN2 + CN using the region proposals from the trained RPN.",
      "Fine-tune the pair FEN2 + RPN.",
      "FEN2 is fixed, RPN takes the weights from step 2.",
      "Fine-tune the pair FEN2 + CN.",
      "FEN2 is fixed, CN takes the weights from step 3, region proposals come from RPN from step 4.",
      "Results  Example images:  Pascal VOC (with VGG16 as FEN)  Using an RPN instead of SS (selective search) slightly improved mAP from 66.9% to 69.9%.",
      "Training RPN and CN on the same FEN (sharing FEN's weights) does not worsen the mAP, but instead improves it slightly from 68.5% to 69.9%.",
      "Using the RPN instead of SS significantly speeds up the network, from 1830ms/image (less than 0.5fps) to 198ms/image (5fps).",
      "(Both stats with VGG16.",
      "They also use ZF as the FEN, which puts them at 17fps, but mAP is lower.)",
      "Using per anchor point more scales and shapes (ratios) for the anchor boxes improves results.",
      "1 scale, 1 ratio: 65.8% mAP (scale 128*128, ratio 1:1) or 66.7% mAP (scale 256*256, same ratio).",
      "3 scales, 3 ratios: 69.9% mAP (scales 128*128, 256*256, 512*512; ratios 1:1, 1:2, 2:1).",
      "Two-staged vs one-staged  Instead of the two-stage system (first, generate proposals via RPN, then classify them via CN), they try a one-staged system.",
      "In the one-staged system they move a sliding window over the computed feature maps and regress at every location the bounding box sizes and classify the box.",
      "When doing this, their performance drops from 58.7% to about 54%."
    ],
    "author_id": "ALEJU",
    "pdf_url": "https://arxiv.org/pdf/1506.01497",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 72529720
  },
  {
    "blog_id": "columnstore-and-b-tree-are-hybrid-physical-designs-important",
    "summary": [
      "Columnstore and B+ tree \u2013 are hybrid physical designs important?",
      "Dziedzic et al., SIGMOD\u201918  Earlier this week we looked at the design of column stores and their advantages for analytic workloads.",
      "What should you do though if you have a mixed workload including transaction processing, decision support, and operational analytics?",
      "Microsoft SQL Server supports hybrid physical design combining both column store and B+ tree indexes in the same database.",
      "It is generally understood that columnstores are crucial to achieving high performance for analytic queries and that B+ tree indexes are key to supporting transactional workloads efficiently.",
      "However, it is not well understood whether hybrid physical designs \u2013 both columnstore and B+ tree indices on the same database and potentially the same table \u2013 are important for any of the above workloads.",
      "Through a series of benchmarks the authors show that hybrid physical designs can result in more than an order of magnitude lower execution costs for many workloads when compared to alternatives using B+ tree-only or columnstore-only.",
      "The Database Engine Tuning Advisor (DTA) for SQL Server is extended to analyze and recommend the appropriate indices for a given workload.",
      "Support for columnstore indices and the new DTA functionality was released in January 2017 as part of the Community Technology Preview release for Microsoft SQL Server 2017.",
      "Physical design options in SQL Server  RDBMSs have supported B+ trees and heap files for several decades.",
      "With the advent of columnstores, which significantly outperform B+ trees for data analysis workloads, many commercial RDBMS vendors have added support for columnstore indexes (CSI)\u2026  In SQL Server columnstores are treated as indexes.",
      "They can be primary (the main storage for all columns of the table) or secondary (e.g., just a subset of columns).",
      "You can have any combination of primary and secondary indexes on the same table.",
      "The primary can be a heap file, B+ tree, or a columnstore, and secondaries can be B+ trees or columnstore.",
      "At most one columnstore index is supported per table though.",
      "SQL Server columnstores support vectorised operations and compression using run-length encoding and dictionary encoding.",
      "Within a columnstore, column data is held in column segments, each with data for 100K-1M rows.",
      "Inserts are handled using delta stores implemented as B+ trees.",
      "Secondary columnstores, optimised for operational analytics, used a B+ tree delete buffer for logical deletion of rows.",
      "This is periodically compressed into a delete bitmap storing the physical identifiers of the deleted rows.",
      "Primary columnstores don\u2019t support delete buffers and work directly with delete bitmaps instead.",
      "Microbenchmarks  The authors conducted a series of microbenchmarks as follows:  scans with single predicates with varying selectivity to study the trade-off between the range scan of a B+ tree vs a columnstore scan  sort and group-by queries to study the benefit of the sort order supported by B+ trees (columnstores in SQL Server are not sorted).",
      "update statements with varying numbers of updated rows to analyze the cost of updating the different index types  mixed workloads with different combinations of reads and updates  The key findings are summarised in the table below.",
      "In a nutshell, B+ tree indexes are suitable for short range scans where the index allows efficient point and short range lookups.",
      "B+ trees are also the cheapest to update.",
      "On the other hand, primary CSIs are most suitable for large scans and bulk updates typical in data warehousing and analysis workloads.",
      "Secondary CSIs can provide significant speed-up for operational analytics on the same database where the OLTP application generating the data also runs.",
      "The basic workload axes can be combined in a variety of ways where a mix of the basic physical design axes are needed for optimal performance.",
      "Well, surprise!",
      "B+ trees are good for OLTP, and columstores are good for analytics.",
      "Where things get interesting though, is when we combine the two for certain workloads\u2026.",
      "Recommending hybrid designs  The Database Engine Tuning Advisor (DTA) recommends B+ tree indexes (primary and/or secondary), materialized views, and partitioning.",
      "We extended DTA to analyze the combined space of B+ tree and columnstore indexes.",
      "By analyzing the workload, DTA is now capable of recommending B+ tree indexes only, columnstore indexes only, or a combination.",
      "The high level architecture of DTA is shown in the figure below.",
      "DTA begins with a local per-query analysis stage called candidate selection, which determines the optimal set ofd indexes for each query.",
      "Then it proceeds to global analysis, starting out by considering the potential to merge indexes on the same table.",
      "Once this is done DTA selects a final set of indexes to minimise the total cost of the workload subject to specified constraints.",
      "DTA uses a cost-base search, which means it needs to estimate the costs using some indexes it hasn\u2019t actually built yet.",
      "The \u201cwhat-if\u201d API is used to simulate such hypothetical indexes.",
      "During the candidate selection phase DTA considers columnstore indexes on the tables referenced in the query.",
      "Given the constraint in SQL Server of only one columnstore index per table DTA chooses to include all columns with data types suitable for columnstore indexes.",
      "(And if the table includes a column with an unsupported data type, then a primary columnstore index is ruled out).",
      "During merging there\u2019s not much extra that can be done \u2013 only one columstore index is supported per table, and we can\u2019t merge a columnstore index with a B+ tree index.",
      "During the final selection phase it is necessary to estimated per-column sizes for cost estimation.",
      "For hypothetical indexes, we need to do this without actually building the index.",
      "DTA uses block-level sampling coupled with techniques to estimate the impact of compression.",
      "The effectiveness of run-length encoding depends on the number of runs in the column and the length of each run\u2026 SQL Server uses a greedy strategy that picks the next column to sort by based on the column with the fewest runs; we mimic this approach in our [estimation] technique.",
      "The GEE estimator is used to estimate the number of distinct values for a set of columns.",
      "Evaluation  The paper closes with an evaluation of both industry-standard benchmarks and real-world customer workloads to see how well the hybrid physical designs suggested by DTA improve query performance.",
      "Read-only workloads are based on the TPC-DS benchmark and five real customer workloads.",
      "For mixed workloads, the CH benchmark (an extension of TPC-C) is used.",
      "Figure 9 below shows the results for the read-only workloads.",
      "In each chart the blue bars show the speed-up obtained by the hybrid design vs CSI-only, and the green bars show the speed-up versus a B+ tree only design.",
      "For example, on TPC-DS, 46 queries were sped-up by a factor of 1.2x when comparing the hybrid design to a CSI only design.",
      "( Enlarge )  \u2026 hybrid leverages the best of columnstores and B+ tree across several workloads.",
      "For each workload, there are several queries for which a hybrid physical design results in more than an order of magnitude improvement in execution cost.",
      "In some cases, the improvement is 2-3 orders of magnitude.",
      "An example of a TCP-DC query that really benefits from the hybrid design is query #54.",
      "It references several large fact tables and as well as many dimension tables.",
      "The predicates on the dimension tables are selective enough that B+ trees have a significant advantage.",
      "Other tables have columnstore indexes.",
      "A similar pattern emerges with the workload of customer four where the optimiser uses an index seek on the fact table(s) followed by a scan of the columnstore on the dimensions, joining the tables with a hash join.",
      "Here are the results for the CH workload:  The hybrid design significantly speeds up the H (analytic) queries, while resulting in a moderate slow-down for the C (OLTP) queries \u2013 mostly the write transactions NewOrder and Payment."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://adam-dziedzic.github.io/static/assets/papers/dziedzic-sigmod2018-recommend-hybrid-designs.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 74156567
  },
  {
    "blog_id": "rpcvalet",
    "summary": [
      "RPCValet: NI-driven tail-aware balancing of \u00b5s-scale RPCs Daglis et al., ASPLOS\u201919  Last week we learned about the [increased tail-latency sensitivity of microservices based applications with high RPC fan-outs.",
      "Seer uses estimates of queue depths to mitigate latency spikes on the order of 10-100ms, in conjunction with a cluster manager.",
      "Today\u2019s paper choice, RPCValet, operates at latencies 3 orders of magnitude lower, targeting reduction in tail latency for services that themselves have service times on the order of a small number of \u00b5s (e.g., the average service time for memcached is approximately 2\u00b5s).",
      "The net result of rapid advancements in the networking world is that inter-tier communications latency will approach the fundamental lower bound of speed-of-light propagation in the foreseeable future.",
      "The focus of optimization hence will completely shift to efficiently handling RPCs at the endpoints as soon as they are delivered from the network.",
      "Furthermore, the evaluation shows that \u201cRPCValet leaves no significant room for improvement\u201d when compared against the theoretical ideal (it comes within 3-15%).",
      "So what we have here is a glimpse of the limits for low-latency RPCs under load.",
      "When it\u2019s no longer physically possible to go meaningfully faster, further application-level performance gains will have to come from eliminating RPCs altogether.",
      "RPCValet balances incoming RPC requests among the multiple cores of a server.",
      "Consider for example a Redis server maintaining a sorted array in memory\u2026  \u2026 an RPC to add a new entry may incur multiple TLB misses, stalling the core for a few \u00b5s while new translations are installed.",
      "While this core is stalled on the TLB miss(es), it is best to dispatch RPCs to other available cores on the server.",
      "In theory, how fast could we go?",
      "Consider a 16-core server handling 16 requests.",
      "We could put anywhere from 1 to 16 queues in front of those cores.",
      "At one extreme we have a \u201816 x 1\u2019 architecture with 16 queues each with one associated processing unit.",
      "At the other extreme is a \u20181 x 16\u2019 architecture with one shared queue serving all 16 processing units.",
      "Or we could have e.g. a \u20184 x 4\u2019 with 4 queues each serving 4 units, and so on\u2026  If you model that out with Poisson arrivals and variety of different service time distributions (fixed, uniform, exponential, and generalised extreme value, GEV) what you\u2019ll find is that the \u20181\u00d716\u2019 architecture performs the best, and the \u201816\u00d71\u2019 architecture performs the worst.",
      "1 x 16 significantly outperforms 16 x 1.",
      "16 x 1\u2019s inability to assign requests to idle cores results in higher tail latencies and a peak throughput 25-73% lower than 1 x 16 under a tail latency SLO at 10x the mean service time\u2026 The theoretical results suggest that systems should implement a queueing configuration that is as close as possible to a single queue (1 x 16) configuration.",
      "In practice those models failed to account for a significant source of overhead: synchronizing across multiple cores on the single queue.",
      "When you take synchronisation into account, a dedicated queue per core starts to look like a good idea again.",
      "NICs supporting Receive-Side Scaling (RSS) can push messages into each core\u2019s queue, but while RSS can help to achieve load distribution, it can\u2019t truly achieving load balancing.",
      "Any resulting load imbalance after applying these [RSS] rules must be handled by system software, introducing unacceptable latency for the most latency-sensitive RPCs with \u00b5s-scale service times.",
      "Introducing RPCValet  RPCValet uses a push-based model, while also taking into account the current loading of the cores.",
      "It\u2019s designed for \u201cemerging architectures featuring fully integrated NIs and hardware-terminated transport protocols.\u201d.",
      "The key hardware feature is that the network interface has direct access to the server\u2019s memory hierarchy, eliminating round trips over e.g. PCIe.",
      "Servers register a part of their DRAM into a partitioned global address space (PGAS), where every server can read and write memory in RDMA fashion.",
      "(We\u2019re not given any details on system reconfiguration etc.).",
      "An integrated NI can, with proper hardware support, monitor each core\u2019s state and steer RPCs to the least loaded cores.",
      "Such monitoring is implausible without NI integration, as the latency of transferring load information over an I/O bus (e.g. ~ 1.5 \u00b5s for a 3-hop posted PCIe transaction) would mean that the NI will make delayed\u2014hence sub-optimal, or even wrong\u2014 decisions until the information arrives.",
      "With N participants in the system, RPCValet first writes every incoming message into a single PGAS-resident message buffer of NxS slots.",
      "Then it notifies the selected core to process the request.",
      "Message arrival and memory location are thus decoupled from the assignment of a core for processing.",
      "We have the synchronization-free zero-copy behaviour of a partitioned multi-queue architecture, together with the load-balancing flexibility of a single queue system.",
      "In the implementation each node maintains a send and a receive buffer.",
      "Send buffers contain a valid bit indicating whether they are currently being used, and a pointer to a buffer in local memory containing the payload.",
      "Receive buffers on the other hand have slots that are size to accommodate message payloads directly.",
      "Overall, the messaging mechanism\u2019s memory footprint is 32 x N x S + (max_msg_size + 64) x N x S bytes.",
      "That is, a few tens of MB at most.",
      "The implementation uses a simple scheme to estimate core loads.",
      "RPCValet simply keeps track of the number of outstanding send requests assigned to each core.",
      "Allowing only one inflight request per core corresponds to a true single-system queue behaviour, but introduces a small inefficiency waiting for the notification of completed request processing.",
      "A practical compromise is to allow two outstanding requests per core.",
      "This results in marginal performance gains over the single request design for ultra-fast RPCs with service times of a few nanoseconds.",
      "All IN backends independently handle incoming network packets and access memory directly, but they hand off to a single NI dispatcher (over the on-chip interconnect) that is statically assigned to dispatch requests to cores for servicing.",
      "\u2026 for modern server processor core counts, the required dispatch throughput should be easily sustainable by a single centralized hardware unit, while the additional latency due to the indirection from any NI backend to the NI dispatcher is negligible (just a few ns).",
      "Evaluation  The evaluation is conducted using against three different service implementations: a synthetic service that emulates different service time distributions; the HERD key-value store, and the Masstree data store.",
      "An SLO of less than or equal to 10x the mean service time is assumed in each case, and configurations are evaluated in terms of throughput under SLO.",
      "Compared to a software implementation, which requires a synchronisation primitive, the hardware implementation delivers 2.3x-2.7x higher throughput under SLO.",
      "The following plots show the performance of different queueing arrangements under the three workloads, with the \u20181\u00d716\u2019 arrangement that RPCValet simulates performing the best as expected.",
      "( Enlarge )  Compared to a theoretically optimal 1 x 16 model, RPCValet gets within 3-15% (depending on the service time distribution).",
      "\u201cWe attribute the gap between the implementation and the model to contention that emerges under high load in the implemented systems, which is not captured by the model.\u201d  At the end of the day though:  RPCValet leaves no significant room for improvement; neither centralizing dispatch nor maintaining private request queues per core introduces performance concerns.",
      "Throughput under tight tail latency goals is improved by up to 1.4x, and tail latency before saturation is reduced by up to 4x."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://www.cc.gatech.edu/~adaglis3/files/papers/RPCValet_asplos19.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 85505359
  },
  {
    "blog_id": "the-art-of-testing-less-without-sacrificing-quality",
    "summary": [
      "The Art of Testing Less Without Sacrificing Quality \u2013 Herzig et al. 2015  Why on earth would anyone want to test less?",
      "Maybe if you could guarantee the same eventually quality, and save a couple of million dollars along the way\u2026  By  nature, system and compliance tests are complex and time-consuming although they rarely find a defect.",
      "Large complex  software products tend to run on millions of configuration in the  field and emulating these configurations requires multiple test  infrastructures and procedures that are expensive to run in terms  of cost and time.",
      "Making tests faster is desirable but usually  requires enormous development efforts.",
      "Simply removing tests  increases the risk of expensive bugs being shipped as part of the final product\u2026 At the same time, long running  test processes increasingly conflict with the need to deliver  software products in shorter periods of time while maintaining  or increasing product quality and reliability.",
      "Increasing  productivity through running less tests is desirable but threatens  product quality, as code defects may remain undetected.",
      "So long as you run every test eventually you\u2019re guaranteed to catch all defects that would have been caught.",
      "That means there\u2019s a trade-off you can make between the costs of executing a test now, and the (increased) costs of possibly finding a defect later in the cycle.",
      "Herzig et al. present THEO, which evaluates these costs and chooses the path with the lower expected cost.",
      "The evaluation is done in the context of Microsoft\u2019s Office, Windows, and Dynamics products,  which means we also get some really interesting data on these costs at Microsoft.",
      "The results (in simulation) cover more than 26 months of industrial product execution and more than 37 million test executions, and the savings are impressive:  THEO would have reduced the number of test executions by up  to 50% cutting down test time by up to 47%.",
      "At the same time,  product quality was not sacrificed as the process ensures that all  tests are ran at least once on all code changes.",
      "Removing tests  would result in between 0.2% and 13% of defects being caught  later in the development process, thus increasing the cost of  fixing those defects.",
      "Nevertheless simulation shows that THEO  produced an overall cost reduction of up to $2 million per development year, per product.",
      "Through reducing the overall  test time, THEO would also have other impacts on the product  development process, such as increasing code velocity and  productivity.",
      "These improvements are hard to quantify but are likely to increase the cost savings estimated in this paper.",
      "The technique and results described in this paper have  convinced an increasing number of product teams, within  Microsoft, to provide dedicate resources to explore ways to  integrate THEO into their actual live production test  environments.",
      "Let\u2019s take a look at the model THEO uses.",
      "THEO does not require any additional test case instrumentation, and uses only the test name, time taken for the test to run, and test result as input.",
      "This data is already collected by test runners.",
      "The test result could be a pass or a fail, and if it\u2019s a fail it could be because of  a genuine defect or because of a problem with the test case itself (false positive).",
      "By tying failed tests back to raised bug reports, THEO is able to distinguish between these two scenarios .",
      "If the bug report results in a (non test-)code fix the test is presumed to have found  a code defect, otherwise it is assumed to have been a false alarm.",
      "If the failure was not investigated the outcome is \u2018unknown.\u2019 Tests run in different execution contexts (and may have different probabilities of finding bugs in different contexts).",
      "An execution context is a collection of properties \u2013 for the study BuildType, Architecture, Language, and Branch were used.",
      "This is a crucial point as a test may show different  execution behaviors for different execution contexts.",
      "For  example, a test might find more issues on code of one branch  than another depending on the type of changes performed on that  branch.",
      "For example, tests cases testing core functionality might  find more defects on a branch containing kernel changes than on  a branch managing media changes.",
      "Thus, our approach will not  only differentiate between test cases, but also bind historic defect  detection capabilities of a test to its execution context.",
      "At the core of the model are estimates of the probability that a given test execution will find a genuine defect (true positive), and that it will raise a false alarm (false positive).",
      "Pdefect(test,context) = #detectedDefects(test,context) / #executions(test,context)  PfalsePositive(test,context) = #falseAlarms(test,context) / #executions(test,context)  All pretty straightforward\u2026  Both probability measurements  consider the entire history from the beginning of monitoring  until the moment the test is about to be executed.",
      "Consequently,  probability measures get more stable and more reliable the more  historic information we gathered for the corresponding test.",
      "Given these probabilities, all that remains is to estimate the associated costs.",
      "If the estimated cost of skipping a test, Costskip, is less than the estimated cost of executing a test, Costexec then THEO will skip the test (with the proviso that every test eventually gets executed).",
      "The cost of executing a test is captured as a function of the machine time spent on test execution and time wasted on investigating false positives:  Costexec = Costmachine + (PfalsePositive * Costinspect)  For the Microsoft development environment, Costmachine was estimated at $0.03/hour, corresponding roughly to the cost of a memory intense Azure image including power and hardware consumption as well as maintenance.",
      "Costinspect at Microsoft is $9.60.",
      "\u201cIt considers the size of the test  inspection teams, the number of inspections performed and the  average salary of engineers on the team.\u201d  The cost of skipping a test is captured as a function of the probability that the test would have found a genuine defect, the (increased) cost of fixing a defect found later in the cycle, the delay before the defect is found, and the number of engineers affected.",
      "Costskip = Pdefect * Costescaped * Timedelay*#Engineers  The constant Costescaped represents the average cost of an escaped defect.",
      "This cost depends on the number of people that  will be affected by the escaped defect and the time duration the  defect remains undetected.",
      "We used a value of $4.20 per  developer and hour of delay for Costescaped.",
      "This value  represents the average cost of a bug elapsing within Microsoft.",
      "Depending on the time the defect remains undetected and the  number of additional engineers affected, elapsing a defect from  a development branch into the main trunk branch in Windows  can cost tens of thousands of dollars.",
      "The #Engineers is determined by counting the number of engineers whose code changes pass the code branch.",
      "Timedelay is the average timespan required to fix historic defects on the corresponding branch.",
      "The final piece of the puzzle is ensuring that every test gets executed eventually:  To ensure this happens we use two separate criteria,  depending on the development process:  Option 1: For single branch development processes, e.g. Microsoft Office, we enforce each test to execute at least  every third day.",
      "Since all code changes are applied to the  same branch, re-execution of each test for each execution  context periodically ensures that each code change has to go  through the same verification procedures as performed  originally.",
      "Option 2: For multi-branch development processes, e.g. Microsoft Windows, we enforce to execute a combination of  test and execution context on the branch closest to trunk on which the test had been executed originally."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://research.microsoft.com/pubs/238350/The%20Art%20of%20Testing%20Less%20without%20Sacrificing%20Quality.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 24169489
  },
  {
    "blog_id": "automated-localization-for-unreproducible-builds",
    "summary": [
      "Automated localization for unreproducible builds Ren et al., ICSE\u201918  Reproducible builds are an important component of integrity in the software supply chain.",
      "Attacks against package repositories and build environments may compromise binaries and produce packages with backdoors (see this report for a recent prominent example of compromised packages on DockerHub).",
      "If the same source files always lead to the same binary packages, then an infected binary can be much more easily detected.",
      "Unfortunately, reproducible builds have not traditionally been the norm.",
      "Non-determinism creeping into build processes means that rebuilding an application from the exact same source, even within a secure build environment, can often lead to a different binary.",
      "Due to the significant benefits, many open-source software repositories have initiated their validation processes.",
      "These repositories include GNU/Linux distributions such as Debian and Guix, as well as software systems like Bitcoin.",
      "If you have a non-reproducible build, finding out why can be non-trivial.",
      "It takes time and a lot of effort to hunt down and eradicate  the causes.",
      "For example, Debian unstable for AMD64 still had 2,342 packages with non-reproducible builds as of August 2017.",
      "(The number today as I\u2019m writing this is 2,826).",
      "You can see a stubbornly persistent group of unreproducible builds in this screen grab from tests.reproducible-builds.org :  screenshot  RepLoc is a tool for highlighting the files within a package which are the likely root cause of non-reproducible builds.",
      "Tested against 671 unreproducible Debian packages, it achieves a Top-1 accuracy rate of 47.09%, and at Top-10 accuracy rate of 79.28%.",
      "That\u2019s a significant aid to developers looking at cryptic messages from the builds of packages that may include many hundreds of files.",
      "With the help of RepLoc, the authors also identified and fixed non-reproducibility issues in 6 packages from Debian and Guix.",
      "Detecting and diagnosing unreproducible builds  The Debian workflow for testing build reproducibility looks like this:  The source is built in two different environments, deliberately constructed with different environment variables and software configurations.",
      "If the two resulting binaries are not identical, the package is flagged for human inspection, a process called localization which seeks to localise the causes of unreproducible builds.",
      "One of the major inputs to this process is the diff logs, as generated by diffoscope .",
      "Those logs produce output that looks like this:  Here we can see diffoscope highlighting a difference in libcompat.a.",
      "In this case, the root cause is in the Makefile:  Can you spot it?",
      "The root issue is the unstable ordering of files passed to the ar utility to generate libcompat.a (wildcard libcompat/*.c).",
      "Here\u2019s a patch that fixes the issue.",
      "In general there are many possible causes of unreproducible builds including timezones (making e.g. anything that uses the __DATE__ macro to be unreproducible ) and locale settings (making e.g.",
      "capture of output text unreproducible).",
      "Introducing RepLoc  RepLoc begins with the build logs and diff log created by diffoscope, and seeks to automatically highlight the most likely files in which the root cause can be found.",
      "RepLoc\u2019s Query Augmentation (QA) component uses information from the logs to refine queries that help to pinpoint likely causes.",
      "The Heuristic Filtering component embodies 14 hand-coded heuristic rules that further help to highlight possible causes.",
      "The combined outputs are passed to ranking component to produce the list of ranked likely-culprit source files as the final output.",
      "Query Augmentation  The files names highlighted in the diff log are used as the initial seed query set.",
      "The build logs contain additional information relating to these files, for example:  The QA component splits the build logs into directory sections passed on the \u2018Entering / Leaving directory\u2019 messages in the logs.",
      "Each directory section thus contains a set of commands, which is denoted a \u2018command file\u2019 in the paper.",
      "TF/IDF is used to assign a weight value to each command file to assess it\u2019s relevance to the initial seed queries.",
      "The commands from the most relevant command files are then added to the query set, to produce an augmented query set.",
      "(In the example above, the ar cru bin-x86_64/libcompat.a... command causes us to add the content of this command file).",
      "Heuristic filtering  The authors extract fourteen heuristic rules from Debian\u2019s documentation .",
      "These rules are encoded as Perl regular expressions(!",
      "), as summarised in the table below.",
      "The time and date rules look for uses of the __TIME__ and __DATE__ macros.",
      "The gzip rule (3) looks for uses of gzip without the -n argument (in which case gzip embeds timestamps in the resulting archive).",
      "The date cmd rule (4) looks for capture of the current date using the date shell command  PL_LOCALTIME looks for Perl scripts capturing date and time  DATE_IN_TEX looks for date embedding in tex files  SORT_IN_PIPE captures cases where sort is used without a local set  TAR_GZIP_PIPE looks for tar and gzip executed in a pipeline  PL_UNSORTED_KEY catches traversal of unsorted hash keys in Perl  LS_WITHOUT_LOCALE captures cases where ls is used without a locale  UNSORTED_WILDCARD looks for the use of wildcard in Makefiles without sorting  With the rules in hand, it\u2019s a simple matter of running grep over the source tree.",
      "Heuristic filtering has good recall, but poor precision (i.e., it can produce a lot of false positives).",
      "Ranking  We can compute the cosine similarity (using TF/IDF) between each package file and the augmented queries to produce a ranked list of candidate files from the QA module.",
      "These are then combined with the files highlighted by the HF module to give a simple weighted score:  Where QA(f) is the similarity score produced by the QA module, and HF(f) is 1 if the HF module flagged the file, and 0 otherwise.",
      "\u03b1 is a configurable parameter to tune the balance between the two terms.",
      "RepLoc in action  At the time of the study, Debian had 761 packages with accompanying patches that turned unreproducible builds into reproducible ones.",
      "This constitutes the ground truth dataset for RepLoc.",
      "The dataset is further divided into four subsets based on the major causes of non-reproducibility.",
      "The table below summarises how well RepLoc gets on.",
      "Concentrate on the bottom line (RepLoc) in each row (the other lines show how RepLoc behaves with different subsets of its modules).",
      "A@N is a top-N accuracy rate score, defined as the percentage of top-N ranked file lists produced by RepLoc that contain at least one problematic file (from the patches).",
      "P@N is a top-N precision score, defined as the percentage of files reported in a top-N list that are genuinely problematic  R@N is at top-N recall score, defined as the percentage of all problematic files that are successfully identified in a top-N list.",
      "Overall, RepLoc achieves an average accuracy score of 79% for Top-10 files.",
      "I.e., if you examine the first ten files RepLoc highlights, you have a 79% chance of finding an issue causing an unreproducible build in at least one of them.",
      "You will also find on average 75% of all the files with reproducibility problems by the time you have worked through that top-10 list.",
      "The authors then used RepLoc to see if they could find the root causes of unreproducible builds for packages where no ground truth was available (i.e., there was no-known reproducible build process for them).",
      "Three packages from Debian are fixed (regina-rexx, fonts-uralic, and manpages-tr).",
      "The problematic files are right at the top of list produced by RepLoc.",
      "Three packages from Guix are also fixed (libjpeg-turbo, djvulibre, and skalibs).",
      "Once more the problematic files are right at the top of the list produced by RepLoc.",
      "Future work  For the future work, we are interested in the localization of problematic files for tool-chain related issues.",
      "Also, inspired by record-and-play techniques from crash reproduction based debugging research, it would be interesting to leverage these techniques to detect more accurate correspondence between the build commands executed and the built binaries."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://arxiv.org/pdf/1803.06766",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 16027177
  },
  {
    "blog_id": "fail-slow-at-scale-evidence-of-hardware-performance-faults-in-large-production-systems",
    "summary": [
      "Fail-slow at scale: evidence of hardware performance faults in large production systems Gunawi et al., FAST\u201918  The first thing that strikes you about this paper is the long list of authors from multiple different establishments.",
      "That\u2019s because it\u2019s actually a study of 101 different fail-slow hardware incidents collected across large-scale cluster deployments in 12 different institutions.",
      "Last year we looked at \u2018 Gray failure: the Achilles\u2019 heel of cloud-scale systems ,\u2019 in which partial failures and system components running in degraded mode were revealed to be involved in many production incidents, and difficult to detect and diagnose with today\u2019s systems.",
      "In \u2018Fail-slow at scale\u2019 we have a catalog of ways in which hardware has been shown to run in degraded mode, causing all sorts of problems.",
      "Fail-slow hardware, like file system faults and network partitions , should be added to the list of things that occur much more frequently than we\u2019d like to think, and cause a lot of damage when they do occur.",
      "This paper highlights an under-studied \u201cnew\u201d failure type: fail-slow hardware, hardware that is still running and functional but in a degraded mode, slower than its expected performance.",
      "We found that all major hardware components can exhibit fail-slow faults.",
      "These faults may be comparatively rare, but they can trigger chains of cascading events, resulting visible failures some distance from the original cause.",
      "Perhaps because of this, and because the partial failures are often deliberately masked in the hardware without enough metrics to surface them, troubleshooting fail-slow hardware based incidents can be very time-consuming:  The fail-slow hardware incidents in our report took hours or even months to detect (pinpoint).",
      "More specifically, 1% of the cases are detected in minutes, 13% in hours, 13% in days, 11% in weeks, and 17% in months (and unknown time in 45%).",
      "One of the conclusions is that many modern deployed systems do not anticipate this failure mode.",
      "The authors offer a set of suggestions to hardware vendors, operators, and systems designers to improve the state of affairs.",
      "Guidelines for vendors  Instead of implicitly masking errors whenever possible, consider throwing explicit error signals when error rates far exceed the expected rate.",
      "Expose device-level performance statistics at fine enough granularity to be able to pinpoint device degradation.",
      "(The information from S.M.A.R.T.",
      "was deemed insufficient to act upon).",
      "Guidelines for operators  Enable online (in-situ) diagnosis, since many problems cannot be reproduced offline (back in the office).",
      "Monitor all hardware components.",
      "Capture full-stack performance data and use statistical approaches to pinpoint and isolate root causes.",
      "Guidelines for system designers  Make implicit error masking explicit (this is the software equivalent of the advice given above to hardware vendors).",
      "\u201cSoftware systems should not just silently work around fail-slow hardware, but need to expose enough information to help troubleshooting.\u201d  Consider converting fail-slow faults to fail-stop.",
      "One example given is skipping a caching layer altogether when SSDs are misbehaving.",
      "Another example is to fail-stop after sufficient recurrence of fail-slow incidents (e.g., tripping a circuit breaker after too many retries).",
      "Doing this requires an ability to shut-off devices at a fine-grained level.",
      "Use fault-injection to explore fail-slow scenarios:  \u2026 we strongly believe that injecting root causes reported in this paper will reveal many flaws in existing systems.",
      "Furthermore, all forms of fail-slow hardware such as slow NIC\u2019s, switches, disks, SSD, NVDIMM, and CPUs need to be exercised as they lead to differrent symptoms.",
      "The challenge is then to build future systems that enable various fail-slow behaviors to be injected easily.",
      "Let\u2019s now take a look at some of the characteristics and causes of fail-slow incidents.",
      "Root causes  There can be a variety of root causes \u2014 these can be internal to the hardware component (for example, device wear or firmware issues), or external (for example, configuration, environment, temperature or power issues).",
      "Section 4 in the paper describes the myriad ways that SSDs, Disks, Memory, and Network components can suffer from internal failures.",
      "And even when a component would otherwise function just fine, section 5 catalogues a long list of external factors that have been known to cause partial failures:  Temperature: clogged air filters, cold-air-under-floor systems, broken fans, buggy fan firmware, poor assembly/design such that heat-sinks are ineffective.",
      "For example, \u201cthere was a case where a fan in a compute node stopped working, and to compensate this failing fan, fans in other compute nods started to operate at their maximal speed, which then generated heavy noise and vibration that degraded the disk performance.\u201d  Power: reduced power can easily trigger fail-slow hardware faults.",
      "Causes include insufficient capacitors, PCU firmware bugs, fail-partial power supplies, power hungry neighbours (drawing too much power under periods of heavy load), and faulty motherboard sensors.",
      "Enviroment: altitude (failure to provide enough cooling for high altitude), cosmic events, loose interconnects, vibrations (e.g. from fans).",
      "Configuration errors due to buggy BIOS firmware and simple human mistakes (e.g., in mapping PCIe cards to slots).",
      "Fault conversion  Fail-stops in some components can cause others to exhibit fail-slow behaviour:  For example, a dead power supply throttled the CPUs by 50% as the backup supply did not deliver enough power; a single bad disk exhausted the entire RAID card\u2019s performance; and a vendor\u2019s buggy firmware made a batch of SSDs stop for seconds, disabling the flash cache layer and making the entire storage stack slow.",
      "Likewise frequent transient failures can result in fail-slow behaviour due to the overheads of error masking (retries, repairs, etc..).",
      "When errors ceases to be rare, the masking overhead becomes the normal case performance.",
      "Partial internal failures whereby only some part of the device is unusable can also lead to fail-slow behaviour.",
      "Symptoms  Fail-slow conditions can manifest in several different ways.",
      "The slowdown may be permanent, or the device may fluctuate between normal and degraded performance.",
      "Some parts of the device may continue to offer full performance while others are degraded, and in other situations a device may periodically reboot itself, leading to periods of complete unavailability.",
      "Cascading failures  \u2026between the actual root cause and the hardware\u2019s fail-slow symptom, there is a chain of cascading root causes\u2026 the fail-slow symptom then creates cascading impacts to the high-level software stack, and potentially to the entire cluster.",
      "We\u2019ve already seen the example of the failing fan leading to a cascade of events ending in degraded disk performance.",
      "Another example is a faulty sensor in a motherboard reporting a false value to the OS, thus making the CPUs run slower in energy saving mode.",
      "Fail-slow hardware problems can cascade into the software stack \u2013 for example, in an HBase deployment a memory card at 25% of normal speed caused backlogs, out-of-memory errors, and crashes.",
      "The last word  The paper is packed with anecdotes that I didn\u2019t have space to cover here.",
      "It\u2019s well worth a read to get a finer appreciation of the kinds of hazards that await when building and operating systems at scale.",
      "We believe fail-slow hardware is a fundamentally harder problem to solve [than fail-stop].",
      "It is very hard to distinguish such cases from ones that are caused by software performance issues.",
      "It is also evident that many modern, advanced deployed systems do not anticipate this failure mode.",
      "We hope that our study can influence vendors, operators, and system-designers to treat fail-slow hardware as a separate class of failures and start addressing them more robustly in future system."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://www.usenix.org/system/files/conference/fast18/fast18-gunawi.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 78412080
  },
  {
    "blog_id": "the-log-structured-merge-tree-lsm-tree",
    "summary": [
      "The Log-Structured Merge-Tree (LSM Tree) \u2013 O\u2019Neil et al. \u201996.",
      "Log-Structured Merge is an important technique used in many modern data stores (for example, BigTable, Cassandra, HBase, Riak, \u2026).",
      "Suppose you have a hierarchy of storage options for data \u2013 for example, RAM, SSDs, Spinning disks, with different price/performance characteristics.",
      "Furthermore, you have a large dataset experiencing frequent writes, and need to maintain an index on that dataset that can be queried at any time \u2013 how can you best exploit the tiers of storage available to you to make keeping this up-to-date as efficient as possible?",
      "For example:  High-performance transaction system applications typically insert rows in a History table to provide an activity trace;  at the same time the transaction system generates log records for purposes of system recovery.",
      "Both types of generated information can benefit from efficient indexing.",
      "The LSM tree is a data structure designed to provide low-cost indexing for files experiencing a high rate of inserts (and deletes) over an extended period.",
      "A good modern example might be incoming streaming data being written to a table.",
      "LSM trees cascade data over time from smaller, higher performing (but more expensive) stores to larger less performant (and less expensive) stores.",
      "The LSM-tree uses an algorithm that defers and batches index changes, cascading the changes from a memory-based component through one or more disk components in an efficient manner reminiscent of merge sort.",
      "\u2026 it is most useful in applications where index inserts are more common than finds that retrieve the entries.",
      "This seems to be a common property for History tables and log files, for example.",
      "The only thing that is required for LSM trees to give an advantage is a high update rate compared to the retrieval rate.",
      "Although many examples involve time-series data, this is not a necessary feature.",
      "The high update:retrieval rate ratio makes the efficiency of maintaining the index paramount.",
      "At the same time find access needs to be  frequent enough that an index of some kind must be maintained, because a sequential search through all the records would be out of the question.",
      "An LSM-tree is composed of two or more tree-like component data structures.",
      "For example,  a two component LSM-tree has a smaller component which is entirely memory resident, known as the C0 tree (or C0 component), and a larger component which is resident on disk, known as the C1 tree (or C1 component).",
      "Note that although the C1 tree resides on disk, frequently referenced page nodes will still reside in memory in memory buffers.",
      "Dath is first inserted into C0, and from there it migrates to C1.",
      "The index entry for [an insert] is inserted into the memory resident C0 tree, after which it will in time migrate out to the C1 tree on disk;  any search for an index entry will look first in C0 and then in C1.",
      "There is a certain amount of latency (delay) before entries in the C0 tree migrate out to the disk resident C1 tree, implying a need for recovery of index entries that don\u2019t get out to disk prior to a crash.",
      "It\u2019s very cheap to insert an entry into the memory-resident C0 tree, but  the cost / capacity of memory compared to disk limits the size of what it makes sense to keep in memory.",
      "At the heart of the LSM algorithm is a rolling merge process:  We need an efficient way to migrate entries out to the C1 tree that resides on the lower cost disk medium.",
      "To achieve this, whenever the C0 tree as a result of an insert reaches a threshold size near the maximum allotted, an ongoing rolling merge  process serves to delete some contiguous segment of entries from the C0 tree and merge it into the C1 tree on disk.",
      "The rolling merge proceeds one block at at time from the downstream tree (C1 in our example).",
      "A block is read in and  entries from the upstream tree (C0) are merged with it.",
      "This reduces the size of the C0 tree, and creates a new merged C1 block that is written out to disk.",
      "The rolling merge acts in a series of merge steps.",
      "A read of a multi-page block containing leaf nodes of the C1 tree makes a range of entries in C1 buffer resident.",
      "Each merge step then reads a disk page sized leaf node of the C1 tree buffered in this block, merges entries from the leaf node with entries taken from the leaf level of the C0 tree, thus decreasing the size of C0, and creates a newly merged leaf node of the C1 tree.",
      "Newly merged blocks are written to new disk positions, so that the old blocks will not be over-written and will be available for recovery in case of a crash\u2026.",
      "We picture the rolling merge process in a two component LSM-tree as having a conceptual cursor which slowly circulates in quantized steps through equal key values of the C0 tree and C1 tree components, drawing indexing data out from the C0 tree to the C1 tree on disk.",
      "The best efficiency gains over B-tree based systems (the prior art) come when reads and writes are in multi-page blocks thus eliminating seek time.",
      "Finds simply need to work through the trees in order:  When an exact-match find or range find requiring immediate response is performed through the LSM-tree index, first the C0 tree and then the C1 tree is searched for the value or values desired.",
      "We can generalize from a two-component LSM tree to one with k components:  \u2026we define a multi component LSM-tree as having components C0, C1, C2, .",
      ".",
      ".,   CK-1 and CK, indexed tree structures of increasing size, where C0 is memory resident and all other components are disk resident.",
      "There are asynchronous rolling merge processes in train between all component pairs (Ci-1, Ci) that move entries out from the smaller to the larger component each time the smaller component, Ci-1, exceeds its threshold size.",
      "Section 3 of the paper contains an analysis of the cost-performance of LSM trees.",
      "This is based on the cost per unit of storage in each component (i.e. memory in C0, and disk in C1), and the cost per unit of I/O in each component.",
      "The Five-Minute Rule , which we examined earlier in the series (together with its updates at 10 and 20 years later) determines the inflection points where the dominant factors change.",
      "This section also shows how to calculate the optimal threshold sizes for the various components in an LSM tree.",
      "The answer turns out to be to arrange things in a geometric progression whereby the ratio of the size of component(i) to the size of component(i+1) is a fixed value r for all adjacent components in the tree.",
      "The three variables ultimately affecting overall cost are therefore r, the size of component 0, and the number of components, k.  there are costs associated with increasing the number of components:  a CPU cost to perform the additional rolling merges and a memory cost to buffer the nodes of those merges (which will actually swamp the memory cost of C0 in common cost regimes).",
      "In addition, indexed finds requiring immediate response will sometimes have to perform retrieval from all component trees.",
      "These considerations put a strong constraint on the appropriate number of components, and three components are probably the most that will be seen in practice.",
      "There\u2019s plenty more in the paper (which runs to 32 pages) that we haven\u2019t had space to cover here, including an analysis of the concurrency and recovery implications of LSM-trees."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://paperhub.s3.amazonaws.com/18e91eb4db2114a06ea614f0384f2784.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 45601881
  },
  {
    "blog_id": "popularity-predictions-of-facebook-videos-for-higher-quality-streaming",
    "summary": [
      "Popularity prediction of Facebook videos for higher quality streaming Tang et al., USENIX ATC\u201917  Suppose I could grant you access to a clairvoyance service, which could make one class of predictions about your business for you with perfect accuracy.",
      "What would you want to know, and what difference would knowing that make to your business?",
      "(For example, in the VC world you\u2019d want to know which companies are going to make it big \u2014 that\u2019s a hard one!).",
      "In many cases though, although perfect clairvoyance isn\u2019t achievable, with some care and attention to data collection and modelling, you can get predictions that are useful.",
      "Today\u2019s paper looks at the problem of predicting the popularity of videos on Facebook.",
      "Why does that matter?",
      "As we saw yesterday, videos can be encoded at multiple different bitrates.",
      "Having a broader choice of bitrates means a better overall experience for clients across a range of bandwidths, at the expense of more resources consumed on the server side in encoding.",
      "In addition, Facebook\u2019s QuickFire engine can produce versions of a video with the same quality but approximately 20% smaller than the standard encoding.",
      "It uses up to 20x the computation to do so though!",
      "Since video popularity on Facebook follows a power law, accurate identification of e.g. the top 1% of videos would cover 83% of the total video watch time, and allow us to expend server side effort where it will have the most impact.",
      "The challenge in exploiting this insight is in scalably and accurately predicting the videos that will become popular.",
      "State of the art popularity prediction algorithms are accurate but do not scale to handle the Facebook video workload\u2026 Simple heuristics that exploit information from the social network scale, but are not accurate.",
      "Facebook\u2019s solution to this problem is CHESS: a Constant History, Exponential kernels, and Social Signals popularity prediction algorithm.",
      "Impressively, CHESS requires only four machines to provide popularity prediction for all of Facebook\u2019s videos.",
      "Compared to the heuristic currently used in production, CHESS improves the watch time coverage of QuickFire by 8%-30% using the same CPU resources for re-encoding.",
      "To cover 80% of watch time, CHESS reduces the overhead from 54% to 17%.",
      "CHESS aligns with another theme of our times too.",
      "Whereas previous work on video popularity considered popularity on a daily (batch) basis, CHESS is able to make predictions quickly (on the order of minutes).",
      "This matters because if prediction takes longer than the interval between when a video is uploaded and when it peaks then much of the watch time will already be in the past by the time we figure out it would have been worth optimising.",
      "So in summary we have scalable near-real time predictions to guide business decisions.",
      "Although CHESS has been applied here in a video context, the fundamental approach should generalize to predict popularity in other settings too.",
      "Let\u2019s jump straight into looking at the details of the CHESS prediction algorithm.",
      "How the CHESS prediction algorithm works  A common theme in popularity prediction is exploiting past access patterns.",
      "The state of the art approaches do so by modeling the behavior as a self-exciting process that predicts future accesses based on all past accesses.",
      "A past access at time  is assumed to provide some influence on future popularity at time  , as modeled by a kernel function  .",
      "Such self-exciting processes are based on the sum of the influence off all past requests from the current time to infinity.",
      "Power-law based kernels provide high accuracy predictions but require each new prediction to compute over all past accesses.",
      "Which in turn means linear growth of storage and computation with respect to past accesses.",
      "At the heart of CHESS is a new exponentially decaying kernel:  where  represents a time window modelling how far into the future the influence of past requests extends.",
      "Such a kernel allows us to simplify the computation of a new prediction to only require the last prediction,  , and its timestamp,  , which drastically reduces the space and time overhead.",
      "Such an exponentially decayed watch time (EDWT) kernel is efficiently computable, but a weaker predictor than self-exciting processes with more powerful kernels.",
      "We overcome this limitation of EDWTs with the second key insight in the CHESS design: combining many weak, but readily computable, signals through a learning framework achieves high accuracy while remaining efficient.",
      "The learning framework is a neural network, and it\u2019s refreshingly simple!",
      "The authors use a 2-layer network with 100 hidden nodes.",
      "Compared to linear models, using such a network reduces prediction error by 40%, but there was no further gain in moving to more complex network models.",
      "The inputs to the network are a combination of static and dynamic features.",
      "The static (stateless) features do not change dramatically during the life-cycle of a video \u2013 for example, the number of friends of the video owner, as well as things like the length of the video.",
      "The dynamic (stateful) features do change, and therefore we need state to track them.",
      "This include things such as the number of comments, likes, shares, saves, and so on.",
      "To keep the state constant per video, four exponential kernels with different time windows are used: 1, 4, 16, and 64 hours.",
      "Values for some of the features can vary over a very wide scale: from 10-10^8 depending on video popularity.",
      "Although linear scaling, in the form of standardization, is the commonly used method in statistical learning, we find that logarithmic scaling, i.e.,  , delivers much better performance for our workload.",
      "Logarithmic scaling improves the coverage ratio of QuickFire by as much as 6% over linear scaling.",
      "To generate training examples the authors use an example queue.",
      "The current state of a video is appended to the queue when it is accessed, and while it remains in the queue the watch time and feature values are tracked.",
      "After a prediction horizon amount of time has expired, the video is evicted from the queue again.",
      "A prediction horizon of six days is empirically found to give a good tradeoff between high accuracy and low overhead.",
      "To avoid the example queue being flooded by data points from the most popular videos, an additional constraint is that an example for a video is only admitted to the queue if at least D = 2 hours of time has elapsed singe the last example of the same video.",
      "Putting it all together: the CHESS video prediction service  The CHESS video prediction service (CHESS-VPS) uses 8 workers distributed across 4 machines to generate predictions on the full Facebook workload.",
      "Facebook video accesses are logged to Scribe, and from there they are streamed to CHESS-VPS and sharded based on video ID into eight shards.",
      "Each worker queries TAO for the additional features that are needed.",
      "Queries are batched, and the results are cached for 10 minutes to reduce load.",
      "Workers maintain tables with their most recent predictions for the top 10 million most popular videos in its shard.",
      "Thus across the 8 shards there are 80 million videos that make up actively accessed video working set.",
      "Every ten minutes each worker scans its table and sorts videos based on their predictions.",
      "An aggregator in each machine collects the top 1 million videos from the workers on that machine, and broadcasts these to all aggregators and waits for their predictions.",
      "When all the predictions have been received these are merged and sorted.",
      "These predictions are then used to answer queries for the next ten minutes.",
      "Each video has 12 stateless features and 7 stateful features.",
      "These features, associated metadata, and current popularity prediction add up to a storage overhead of ~250 bytes per video.",
      "Thus, all 80 million videos use ~20GB RAM in total to maintain.",
      "This results in a total memory overhead of ~44GB RAM from models and metadata, or only ~11GB RAM per machine."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://www.usenix.org/system/files/conference/atc17/atc17-tang.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 4536127
  },
  {
    "blog_id": "slog",
    "summary": [
      "SLOG: serializable, low-latency, geo-replicated transactions Ren et al., VLDB\u201919  SLOG is another research system motivated by the needs of the application developer (aka, user!).",
      "Building correct applications is much easier when the system provides strict serializability guarantees.",
      "Strict serializability reduces application code complexity and bugs, since it behaves like a system that is running on a single machine processing transactions sequentially.",
      "The challenge with strict serializability (or even just serializability on a regular DBMS) is that it requires coordination, and as we know, coordination kills performance.",
      "Weaker consistency models can give better performance yet \u201cexpose applications to potential race condition bugs, and typically require skilled application programmers.\u201d But developers are the kingmakers (I think it\u2019s been enough time now that we can drop the \u2018new\u2019 in that phrase??",
      ";) ), and thus:  \u2026 the demand for systems that support strict serializability has only increased.",
      "So starting with strict serializability as a given, how do we claw back some of that performance?",
      "That\u2019s where SLOG (Serializable LOw-latency, Geo-replicated transactions) comes in.",
      "SLOG achieves high throughput, strictly serializable ACID transactions at geo-replicated scale for all transactions submitted across the world, all the while achieving low latency for transactions that initiate from a location close to the home region for data they access.",
      "Coordination overhead is bad for performance, but coordination across regions is bad for performance on a whole other level!",
      "So the central idea behind SLOG is to take advantage of region affinity \u2014 the notion that e.g. data related to a user is likely to accessed in their home region\u2014 to handle as many transactions as possible using only intra-region coordination.",
      "Cross-region coordination on every write is not necessary to guarantee strict serializability.",
      "If every read of a data item is served from the location of the most recent write to that data item, then there is no need to synchronously replicate writes across regions.",
      "Is my data at home?",
      "Data is replicated across regions, but for every data item one of these regions is designated as its home replica.",
      "All writes and linearizable reads of a data item are directed to that replica.",
      "Transactions can be single homed (all the data items they access have the same home region), or multi-homed (data items from more than one home region).",
      "Within a region there is a local input log maintained across multiple servers using Paxos.",
      "Regions periodically send their log updates to other regions, which enables local snapshots reads in remote (non-home) regions, and faster remastering (re-homing) if a data item needs to be migrated.",
      "If data turns out not to be in the best location, it can be migrated.",
      "SLOG borrows PNUTs heuristic for when to remaster data: 3 accesses in a row not from the home region (see \u00a73.4 in the paper for details on how migrations are made safe).",
      "Let\u2019s make a plan  When data for single home transactions is processed within a region, SLOG works similarly to Calvin with data partitioned across servers.",
      "Key to good intra-region performance is the deterministic nature of the processing.",
      "All nodes involved in processing a transaction\u2026 run an agreement protocol prior to processing a batch of transactions that plans out how to process the batch.",
      "This plan is deterministic in the sense that all replicas that see the same plan must have only one possible final state after processing transactions according to this plan.",
      "Once all parties agree to the plan, processing occurs (mostly) independently on each node, with the system relying on the plan\u2019s determinism in order to avoid replica divergence.",
      "To make the plan we need to know quite a bit about the transactions within a batch: \u201cthis makes deterministic database systems a poor fit for ORM tools and other applications that submit transactions to the database in pieces.\u201d SLOG giveth and SLOG taketh away!",
      "If a transaction that SLOG thought was going to be single-homed turns out not to be (e.g. , data migrated), then that will be detected at runtime, the transaction is aborted, and SLOG restarts it as a multi-home transaction instead.",
      "Are you feeling lucky?",
      "SLOG has two different models of operation: SLOG-B and SLOG-HA.",
      "In SLOG-B the only synchronous replication is internally within a home region.",
      "If a region fails, data may become unavailable.",
      "In SLOG-HA data is synchronously replicated to one or more nearby regions.",
      "Unlike Spanner, Cosmos DB, Aurora, or other previously cited systems that support synchronous cross-region replication, SLOG\u2019s deterministic architecture ensures that its throughput is unaffected by the presence of synchronous cross-region replicas.",
      "Multi-home transactions  The most technically challenging problem in the design of LOG is the execution of multi-home transactions.",
      "The goal is to maintain strict serializability guarantees and high throughput in the presence of potentially large numbers of multi-home transactions, even though they may access contended data, and even though they require coordination across physically distant regions.",
      "For single-homed transactions we know we can end up with a global serializable schedule without coordination.",
      "But all multi-homed transactions need to be ordered with respect to each other.",
      "Any strategy that produces a global ordering would do here (e.g. Paxos) \u2013 the current implementation achieves this goal by sending all multi-home transactions to the same region to be ordered by the local log there.",
      "(So in the current implementation, SLOG-HA is region fault-tolerant unless the region that dies happens to also be the nominated global ordering region??).",
      "Within each region involved in processing a multi-home transaction, a local LockOnlyTxn is generated which locks the local reads and writes for that region.",
      "LockOnlyTxns are placed in the local log like any other transaction,  securing an ordering relative to other single-home transactions at the same region.",
      "Evaluation  SLOG is compared to Calvin (a deterministic system, but without the notion of region affinities), to a design based on NuoDB (has region affinities, but is not deterministic, uses 2PL for serializability), and to Cloud Spanner using transactional YCSB and TPC-C NewOrder benchmarks.",
      "Under low contention (mostly single-home transactions) throughput  is similar across SLOG, Calvin, and NuoDB-2PL, but as the number of multi-home transactions in the mix increases, Calvin shows an advantage.",
      "SLOG pays an extra cost over Calvin for its LockOnlyTxns, but still does better than NuoDB-2PL.",
      "However, Calvin\u2019s advantage is reduced somewhat when multi-partition transactions (which could still be single-homed) are in play.",
      "\u2026 we expect most workloads to be in one of two categories: either it is mostly single-partition and mostly single-home, or otherwise both multi-partition and multi-home transactions are common.",
      "So hold on a minute, did I bring you all this way just to tell you that Calvin is better?!",
      "Not at all!",
      "You see, SLOG\u2019s slight disadvantage on multi-home transaction throughput is more than made up for by its latency figures:  \u2026SLOG\u2019s ability to commit single-home transactions as soon as they are processed at their home region allows SLOG to dramatically reduce its latency relative to Calvin.",
      "Compared to Spanner, SLOG has a very different performance profile.",
      "In particular, Spanner\u2019s throughput decreases rapidly under contention since it doesn\u2019t allow conflicting transactions to run during 2PC and the Paxos-implemented geo-replication.",
      "The last word  Current state-of-the-art geo-replicated systems force their users to give up one of: (1) strict serializability, (2) low-latency writes, (3) high transactional throughput.",
      "Some widely used systems force their users to give up two of them\u2026 SLOG leverages physical region locality in an application workload in order to achieve all three, while also supporting online consistent dynamic \u201cre-mastering\u201d of data as application patterns change over time."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://www.vldb.org/pvldb/vol12/p1747-ren.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 82626355
  },
  {
    "blog_id": "scalable-atomic-visibility-with-ramp-transactions",
    "summary": [
      "Scalable Atomic Visibility with RAMP Transactions \u2013 Bailis et al. 2014  RAMP transactions came up last week as part of the secret sauce in Coordination avoidance in database systems that contributed to a 25x improvement on the TPC-C benchmark.",
      "So what exactly are RAMP transactions and why might we need them?",
      "As soon as you partition your database across multiple servers, things start to get interesting.",
      "We\u2019d like to maintain atomic isolation \u2013 either all of a transaction\u2019s effects are visible or none are \u2013 for transactions that span partitions\u2026  The status quo for these multi-partition atomic transactions provides an uncomfortable choice between algorithms that are fast but deliver inconsistent results and algorithms that deliver consistent results but are often slow and unavailable under failure.",
      "A lot of implemented systems have chosen to go with the fast-and-furious option resulting in incorrect behaviour for cases where atomic visibility matters.",
      "The RAMP (Read Atomic Multiple Partition) transaction models introduced in this paper show that you can have performance and scalability of transactions spanning multiple partitions with atomic visibility.",
      "\u2026data stores like Bigtable, Dynamo, and many popular \u201cNoSQL\u201d and even some \u201cNewSQL\u201d stores do not provide transactional guarantees for multi-item operations.",
      "The designers of these Internet-scale, real-world systems have made a conscious decision to provide scalability at the expense of multi-partition transactional semantics.",
      "Our goal with RAMP transactions is to preserve this scalability but deliver correct, atomically visible behavior for the use cases we have described.",
      "Under evaluation, the RAMP algorithms did not degrade substantially under contention, and scaled linearly to over 7.1 million operations per second on 100 servers.",
      "Bad things that can happen when you don\u2019t have atomic multi-partition isolation  Without atomic isolation foreign key constraints, secondary indexing, and materialized view maintenance can all break!",
      "Data models often represent bi-directional relationships as two distinct uni-directional relationships.",
      "\u201cFor example, in TAO, a user performing a \u2018like\u2019 action on a Facebook page produces updates to both the LIKES and LIKED_BY associations.\u201d  These applications require foreign key maintenance and often, due to their unidirectional relationships, multi-entity update and access.",
      "Without atomic isolation broken bi-directional relationships, and dangling or incorrect references can surface.",
      "With data partitioned across servers by primary key, access by secondary attributes becomes more challenging.",
      "There are two dominant strategies for distributed secondary indexing.",
      "First, the local secondary index approach co-locates secondary indexes and primary data, so each server contains a secondary index that only references (and indexes) data stored on its server.",
      "This allows easy, single-server updates but requires contacting every partition for secondary attribute lookups (write-one, read-all), compromising scalability for read-heavy workloads.",
      "Alternatively, the global secondary index approach locates secondary indexes (which may be partitioned, but by a secondary attribute) separately from primary data.",
      "This alternative allows fast secondary lookups (read-one) but requires multi-partition update (at least write-two)  Real-world services tend to use either local secondary indexing (non-scalable but correct), or non-atomic (scalable but incorrect) global indexes.",
      "In the latter cases queries involving the secondary attributes can return records that shouldn\u2019t match, and omit ones that should.",
      "Without atomic isolation, materialized views can diverge from the base data.",
      "For example, a count may become inaccurate.",
      "With RAMP transactions, base data and views can be updated atomically.",
      "The physical maintenance of a view depends on its specification, but RAMP transactions provide appropriate concurrency control primitives for ensuring that changes are delivered to the materialized view partition.",
      "For select-project views, a simple solution is to treat the view as a separate table and perform maintenance as needed: new rows can be inserted/deleted according to the specification, and, if necessary, the view can be (re-)computed on demand (i.e., lazy view maintenance).",
      "For more complex views, such as counters, users can execute RAMP transactions over specialized data structures such as the CRDT G-Counter.",
      "Scalability Requirements  Consider databases that are partitioned over multiple servers.",
      "Each item has a single logical copy stored on one of those partitions, which one can be calculated using the item itself (e.g. primary key).",
      "In order to achieve scalability the author\u2019s identify two key properties that must be preserved: synchronization independence, and partition independence.",
      "Synchronization independence ensures that one client\u2019s transactions cannot cause another client\u2019s to block and that, if a client can contact the partition responsible for each item in its transaction, the transaction will eventually commit (or abort of its own volition).",
      "(Also known as transactional availability).",
      "Partition independence ensures that, in order to execute a transaction, a client never has to contact partitions that its transaction does not access.",
      "Thus, a partition failure only affects transactions that access items contained on the partition.",
      "This also reduces load on servers not directly involved in a transaction\u2019s execution.",
      "In the distributed systems literature, partition independence for replicated data is called replica availability or genuine partial replication.",
      "A third constraint is that the metadata required to achieve synchronization and partition independence is not too large: \u201cthere are many potential solutions for providing atomic visibility that rely on storing prohibitive amounts of state.\u201d  The RAMP transaction algorithms  You may be wondering why I keep referring to algorithms (plural).",
      "This is because the authors actually define three RAMP variants: RAMP-Fast, RAMP-Small, and RAMP-Hybrid.",
      "These trade-off between performance and the amount of metadata that needs to be kept.",
      "At a high level, RAMP transactions allow reads and writes to proceed concurrently.",
      "This provides excellent performance but, in turn, introduces a race condition: one transaction might only read a subset of another transaction\u2019s writes, violating RA (i.e., fractured reads might occur).",
      "Instead of preventing this race (hampering scalability), RAMP readers autonomously detect the race (using metadata attached to each data item) and fetch any missing, in-flight writes from their respective partitions.",
      "To make sure that readers never have to block for writes to arrive at a partition, writers use a two-phase (atomic commitment) protocol that ensures that once a write is visible to readers on one partition, any other writes in the transaction are present on and, if appropriately identified by version, readable from their respective partitions.",
      "RAMP-Fast stores metadata in the form of write sets (thus the overhead is linear in transaction size), and has one RTT for reads in the best case (two in the worst case).",
      "RAMP-Small uses constant size metadata (it only stores the transaction timestamp) but always requires two RTT for reads.",
      "RAMP-Hybrid takes the same write set information as RAMP-Fast, but encodes it in a Bloom filter.",
      "With no false positives from the filter, Ramp-Hybrid would therefore behave as RAMP-Fast.",
      "And with all false positives, it behaves as RAMP-Small.",
      "All of the variants require two RTTs/transaction for writes.",
      "The two-phase atomic commitment protocol used by RAMP ensures readers never block waiting for writes to arrive.",
      "It is known that every atomic commitment protocol may block during failures.",
      "Blocked writes instead act as \u201cresource leaks\u201d on partitions: partitions will retain prepared versions indefinitely unless action is taken.",
      "To \u201cfree\u201d these leaks, RAMP servers can use the Cooperative Termination Protocol (CTP).",
      "CTP can always complete the transaction except when every partition has performed PREPARE but no partition has performed COMMIT\u2026 Compared to alternatives (e.g. replicating clients), we have found CTP to be both lightweight and effective.",
      "There is of course much more detail in the full paper, which I encourage you to go on and read.",
      "Section 6 on Related Work contains a nice short summary of isolation guarantees in the wild.",
      "\u201cIn recent years, many \u2018NoSQL\u2019 designs have avoided cross-partition transactions entirely, effectively providing Read Uncommitted isolation\u2026\u201d"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://www.bailis.org/papers/ramp-sigmod2014.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 87411149
  },
  {
    "blog_id": "watching-for-software-inefficiencies-with-witch",
    "summary": [
      "Watching for software inefficiencies with Witch Wen et al., ASPLOS\u201918  (The link above is to the ACM Digital Library, if you don\u2019t have membership you should still be able to access the paper pdf by following the link from The Morning Paper blog post directly.)",
      "Inefficiencies abound in complex, layered software.",
      "These inefficiencies can arise during design (poor choice of algorithm), implementation, or translation (e.g., compiler optimisations or lack thereof).",
      "At the level of the hardware, inefficiencies involving the memory subsystem are some of the most costly\u2026  Repeated initialization, register spill and restore on hot paths, lack of inlining hot functions, missed optimization opportunities due to aliasing, computing and storing already computed or sparingly changing values, and contention and false sharing (in multi-threaded codes), are some of the common prodigal uses of the memory subsystem.",
      "Coarse grained profilers (e.g., gprof) have comparatively little overhead and can detect hotspots, but fail to distinguish between efficient and inefficient resource usage.",
      "Fine-grained profilers (e.g. DeadSpy) can detect inefficiencies, but typically introduce high overheads (10-80x slowdown and 6-100x extra memory).",
      "These high overheads prevent such tools from being widely used.",
      "Witch is a fine-grained inefficiency detection framework with low overhead, a trick it pulls off by sampling hardware PMUs (performance monitoring units) to gather its data.",
      "Our key observation is that an important class of inefficiency detection schemes, explored previously via fine-grained profilers, requires monitoring consecutive accesses to the same memory location.",
      "For example, detecting repeated initialization\u2014 a dead write\u2014 requires monitoring store after store without an intervening load to the same location.",
      "Witch is implemented as part of the open source HPCToolkit performance analysis tools suite.",
      "It comprises the base framework, on top of which various detection tools can be implemented.",
      "Three such tools are described in the paper:  DeadCraft detects dead stores, a store followed by another store to the same address with on intervening load  SilentCraft detects stores that update a location with a value already present at the location (i.e., useless stores).",
      "LoadCraft detects loads followed by another load from the same location where the value remains unchanged between the two loads.",
      "Not every instance of these situations is a problem of course, but they can be very useful in pointing developers in the right direction.",
      "Let\u2019s look at an example applying SilentCraft to the Caffe deep learning framework.",
      "SilentCraft reports that 25% of all memory stores in a key loop of the pooling and normalisation layers are redundant.",
      "This is a clue to look at the code and see what is going on.",
      "Investigation reveals that a large portion of elements in top_diff are zeroes, effectively leading to execution of bottom_diff[index] += 0 in line 8.",
      "Adding a zero check for the value in top_diff can eliminate a division, an addition, and a memory store.",
      "This change speeds up the pooling layer by 1.16x, and the normalization layers by 1.34x.",
      "Using a near-zero check (less than 1e-7) gives a 2% accuracy loss, but yields 1.16x and 2.23x speedups for pooling and normalization respectively.",
      "(A 6% speedup over the program as a whole).",
      "The NWChem computational chemistry package provides an illustration of the benefits of detecting dead stores with DeadCraft.",
      "DeadCraft reveals that 60% of memory stores are dead,  with 94% of those dead stores due to one store pair in the call of the dfill function.",
      "More than 200K calls to dfill are made, writing 500GB of data that is never used.",
      "Analysis revealed that the size of the work2 array was larger than necessary, and that the zero initialisation (the cause of all those stores) was also unnecessary.",
      "Eliminating it lead to a 1.43x speedup.",
      "For a LoadCraft example we can turn to GNU Binutils-2.27.",
      "LoadCraft identifies 96% of the loads in this program as loading the same value from the same location.",
      "The culprit is a linear scan over addresses in a function table.",
      "Replacing the linked list with a sorted array and using a binary search instead sped up execution by 10x.",
      "We\u2019ve seen how Witch can help programmers focus their attention on interesting parts of the codebase.",
      "Now let\u2019s take a look at how it works under the covers\u2026  PMU Sampling  Hardware performance monitoring units in CPUs offer a programmable way to count hardware events such as loads and stores, and hardware debug registers can trap execution when the PC reaches an address, or an instruction accesses a designated address (a watchpoint).",
      "Linux offers a standard interface to program and sample PMUs.",
      "PMU samples that include the effective address accessed in a sample provide the knowledge of the addresses accessed in an execution.",
      "Given this effective address, a hardware debug register allows us to keep an eye on (watch) a location and recognize what the program subsequently does to such a location.",
      "The following figure shows how all the pieces fit together in the context of dead store detection (the DeadCraft client):  Intervening accesses  We can only monitor a small number of locations at a time (e.g., four hardware debug registers), so reservoir sampling allows us to monitor a subset of previously seen addresses without any bias.",
      "Two accesses to the same memory address, separated by many PMU samples in the intervening time, present an issue.",
      "Once all the watchpoints are in use, a simple \u2018replace the oldest watchpoint\u2019 scheme will most likely not detect e.g. dead stores, separated by such a distance.",
      "Monitoring a new sample may help detect a new, previously unseen problem whereas continuing to monitor an old, already-armed address may help detect a problem separated by many intervening operations.",
      "We should detect both.",
      "The solution uses reservoir sampling and relies on multiple unbiased samples taken over a repetitive execution to capture both scenarios.",
      "Since only counts of previous samples are maintained the technique needs only O(1) memory.",
      "In practice, the \u201cblindspot window\u201d (number of consecutive unmonitored PMU samples) for many applications is very short \u2013 e.g., less than 0.02% of the total samples in the SPEC CPU2006 benchmarks.",
      "Proportional attribution  Consider the following code fragment:  There are many dead stores in the i-loop (line 3) due to the overwriting j-loop (line 11).",
      "However, only a few watchpoints survive between these two loops because of all the watchpoints consumed in the middle loop (lines 7-8).",
      "Without correcting for this sampling imbalance, a disproportionately high dead write count will be recorded for the line pairs (7,8) and (8,7) compared to the rest.",
      "We solve this problem with a context-sensitive approximation.",
      "The code behaviour is typically the same in a calling context; hence, an observation made by monitoring an address accessed in a calling context can approximately represent other unmonitored samples occurring in the same calling context.",
      "If in a sequence of N samples occurring in a calling context C, only one sample is monitored through a debug register, we scale the observation made for the monitored sample by N to approximated the behaviour of the remaining N-1 unmonitored samples taken at C.  Accuracy and overheads  The following charts show the accuracy of DeadCraft, SilentCraft, and LoadCraft at different sampling rates, as compared to ground truth exhaustive monitoring.",
      "Clearly, the sampling rate, when chosen with some care, does not significantly affect the results.",
      "Unsurprisingly, sampling also has much lower overheads than exhaustive monitoring, as shown in this table.",
      "( Enlarge )  Perhaps more interesting is the overhead compared to normal (i.e., non-instrumented) execution, which is typically less than 5%:"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3173162.3177159?download=true",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 92615357
  },
  {
    "blog_id": "exploring-complex-networks",
    "summary": [
      "Exploring Complex Networks \u2013 Strogatz 2001  Network anatomy is important to characterize because structure always affects function\u2026  Written in 2001, this article \u2013 recently recommended by Werner Vogels in his \u2018 Back-to-Basics \u2018 series \u2013 explores the topic of complex networks.",
      "It turns out that the behaviour of individual nodes, and the way that we connect them together, tells us a lot about the emergent properties of the resulting system.",
      "Non-linear dynamics primer  Let the state of some system be modelled by a vector of state variables, x, with x(t) representing the state at time t. Let v(x) be a vector of functions that encode the dynamics of the system (how it changes over time).",
      "The behaviour of the resulting dynamical system can be modelled by differential equations dx/dt = v(x).",
      "As the system evolves x(t) flows through state space, guided by the \u2018velocity\u2019 field dx/dt = v(x) like a speck carried along in a steady, viscous fluid\u2026  There are three possibilities:  If x(t) comes to rest at some point, then the velocity there must be zero and we call it a fixed point and it corresponds to an equilibrium state for the system.",
      "If small disturbances from this point \u2018damp out\u2019, then it is known as a stable fixed point.",
      "x(t) may flow towards a closed loop and eventually circulate around it forever.",
      "This is known as a limit cycle.",
      "x(t) might settle onto a strange attractor \u2013 a set of states on which it wanders forever, never stopping or repeating.",
      "If two nearby states flow away from each other exponentially fast we call the system chaotic.",
      "Networks of dynamical systems  Consider a network in which each node is a dynamical system.",
      "The long-term behaviour of each node in isolation will be given by stable fixed points, limit cycles, or attractors.",
      "But what can we say about their collective behaviour when we couple them together?",
      "If the dynamical system at each node has stable fixed points and no other attractors, the network tends to lock into a static pattern.",
      "Many such patterns may coexist, especially if the nodes have competing interactions.",
      "In that case the network may become frustrated and display enormous numbers of locally stable equilibria.",
      "This kind of complex static behaviour is seen in models of spin glasses, associative memory neural networks and combinatorial optimization problems.",
      "If each node has a chaotic attractor, then there are few rules.",
      "But one curious phenomenon exists for networks of identical chaotic systems:  It is known that networks of identical chaotic systems can synchronize their erratic fluctuations\u2026 For a wide range of network topologies, synchronized chaos requires that the coupling be neither too weak nor too strong; otherwise spatial instabilities are triggered.",
      "The intermediate case, in which each node has a stable limit cycle, is more fruitful.",
      "When nodes are coupled by smooth interactions similar to diffusion then:  Arrays of identical oscillators often synchronize, or else form patterns that depend on the symmetry of the underlying network.",
      "Other common modes of organization are travelling waves in one spatial dimension, rotating spirals in two dimensions and scroll waves in three dimensions.",
      "For fully connected networks where each node is coupled equally to all the others, the completely synchronized state becomes likely.",
      "Instead of smooth interactions though, \u201cMany biological oscillators communicate by sudden impulses: a neuron fires, a firefly flashes, a cricket chirps\u2026\u201d \u2013 and many computer networks communicate by sending messages.",
      "Peskin investigated this scenario.",
      "If each oscillator is connected to all others, they end up firing in unison.",
      "It is conjectured that this is also true if the oscillators are not quite identical.",
      "With weaker coupling than in the fully connected model, Winfree discovered that there is a \u2018temporal analogue of a phase transition.\u2019 With a network of weakly coupled, nearly identical limit-cycle oscillators the system behaves incoherently\u2026  As the coupling is increased, the incoherence persists until a certain threshold is crossed \u2013 then a small cluster of oscillators suddenly \u2018freezes\u2019 into synchrony.",
      "For still greater coupling, all the oscillators become locked in phase and amplitude.",
      "This model was further refined by Kuramoto.",
      "Network architectures  In a simple random graph with n nodes and m links the expected structure of the graph changes as a function of m.  Erd\u00f6s and R\u00e9nyi studied how the expected topology of this random graph changes as a function of m. When m is small, the graph is likely to be fragmented into many small clusters of nodes, called components.",
      "As m increases, the components grow, at first by linking to isolated nodes and later by coalescing with other components.",
      "A phase transition occurs at m=n/2, where many clusters crosslink spontaneously to form a single giant component.",
      "For m > n/2, this giant component contains on the order of n nodes (more precisely, its size scales linearly with n, as n \u2192 \u221e), while its closest rival contains only about log n nodes.",
      "Furthermore, all nodes in the giant component are connected to each other by short paths: the maximum number of \u2018degrees of separation\u2019 between any two nodes grows slowly, like log n  Many real world networks contain a mixture of order and randomness.",
      "Watts and Strogatz studied this model by starting with a regular lattice and then replacing the original links with random ones with some probability p.  They found that the slightest bit of rewiring transforms the network into a \u2018small world,\u2019 with short paths between any two nodes, just as in the giant component of a random graph.",
      "Yet the network is much more highly clustered than a random graph, in the sense that if A is linked to B and B is linked to C, there is a greatly increased probability that A will also be linked to C.  The short path and high clustering properties also hold for many natural and technological networks.",
      "Furthermore, they conjectured that dynamical systems coupled in this way would display enhanced signal propagation speed, sychronizability and computational power, as compared with regular lattices of the same size.",
      "The intuition is that the short paths could provide high-speed communication channels between distant parts of the system, thereby facilitating any dynamical process (like synchronization or computation) that requires global coordination and information flow.",
      "In real networks, some nodes are more highly connected than others.",
      "The degree of a node is the number of links that it has.",
      "Many networks have a degree distribution that is highly skewed and decays as a power law.",
      "For example, the world-wide web has a small number of nodes with many links, and a long tail of nodes with very few links.",
      "Albert and Jeong have dubbed these networks \u2018scale-free,\u2019 by analogy with fractals, phase transitions, and other situations where power laws arise and no single characteristic scale can be defined.",
      "An interesting property of such networks is their resistance to random failures (e.g. of nodes on AWS!).",
      "Albert, Jeong and Barab\u00e1si suggested that scale-free networks are resistant to random failures because a few hubs dominate their topology Any node that fails probably has small degree (like most nodes) and so is expendable.",
      "The flip side is that such networks are vulnerable to deliberate attacks on the hubs.",
      "These intuitive ideas have been confirmed numerically and analytically by examining how the average path length and size of the giant component depend on the number and degree of the nodes removed.",
      "Let the power law governing degree distribution be pk ~ k-\u03b3.",
      "Then if \u03b3 < 3.47 (which holds for most scale-free networks measured so far) then a giant component will exist.",
      "A component is a set of connected nodes with a path from each node to every other node.",
      "If \u03b3 < 1 then the network forms one huge connected piece.",
      "For the world wide web (at least as of 2001) \u03b3 is approximately 2.2.",
      "If this topic interests you, I can also recommend Matthew Jackson\u2019s book on Social and Economic Networks from 2010."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://www.nature.com/nature/journal/v410/n6825/pdf/410268a0.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 36197895
  },
  {
    "blog_id": "keeping-master-green-at-scale",
    "summary": [
      "Keeping master green at scale Ananthanarayanan et al., EuroSys\u201919  This paper provides a fascinating look at a key part of Uber\u2019s software delivery machine.",
      "With a monorepo, and many thousands of engineers concurrently committing changes, keeping the build green, and keeping commit-to-live latencies low, is a major challenge.",
      "This paper introduces a change management system called SubmitQueue that is responsible for continuous integration of changes into the mainline at scale while always keeping the mainline green.",
      "The challenge: build fails at scale  Each individual submitted change will have passed all local tests, but when you put large numbers of concurrent changes together conflicts can still happen.",
      "Finding out what\u2019s gone wrong is a tedious and error-prone task often requiring human intervention.",
      "Meanwhile, new features are blocked from rolling out.",
      "So the goal is to keep it green:  \u2026the monorepo mainline needs to remain green at all times.",
      "A mainline is called green if all build steps (e.g., compilation, unit tests, UI tests) can successfully execute for every commit point in the history.",
      "Keeping the mainline green allows developers to (i) instantly release new features from any commit point in the mainline, (ii) roll back to any previously committed change, and not necessarily to the last working version, and (iii) always develop against the most recent and healthy version of the monorepo.",
      "Here\u2019s 9 months of data for the Uber iOS and Android repos, showing the probability of conflicts as the number of concurrent changes increases:  At \u2018only\u2019 16 concurrent and potentially conflicting changes, there\u2019s a 40% chance of a problem.",
      "Thus, \u201cdespite all efforts to minimize mainline breakages, it is very likely that the mainline experiences daily breakages due to the sheer volume of everyday code changes committed to a big monorepo.\u201d  And that\u2019s exactly what Uber saw.",
      "Here\u2019s a one week view of the iOS mainline prior to the introduction of SubmitQueue.",
      "The mainline was green only 52% of the time.",
      "(Since the introduction of SubmitQueue over a year ago, mainlines have remained green at all times).",
      "To keep the mainline green we need to totally order changes and only apply patches to mainline HEAD if all build steps succeed.",
      "The simplest solution to keep the mainline green is to enqueue every change that gets submitted to the system.",
      "A change at the head of the queue gets committed into the mainline if its build steps succeed.",
      "For instance, the rustproject uses this technique to ensure that the mainline remains healthy all the time.",
      "This approach does not scale as the number of changes grows.",
      "For instance, with a thousand changes per day, where each change takes 30 minutes to pass all build steps, the turnaround time of the last enqueued change will be over 20 days.",
      "20 day turnarounds clearly is not going to lead to a high performing organisation!",
      "One possible solution to reduce the latency is batching changes, but then we\u2019re back at the problem of conflicts and complex manual resolution if we\u2019re not careful.",
      "Another tactic is optimistic execution \u2013 given enough compute we can start builds in parallel on, with the assumption that all pending changes submitted will succeed.",
      "This approach suffers from high failure rates and turnaround times still though as failure of a change can abort many optimistically executing builds.",
      "SubmitQueue  Uber\u2019s solution to these challenges is SubmitQueue.",
      "SubmitQueue guarantees an always green mainline by providing the illusion of a single queue where every change gets enqueued, performs all its build steps, and ultimately gets merged with the mainline branch if all build steps succeed.",
      "Developers create changes, which pending a review process are packaged into a revision.",
      "Revisions are submitted to the SubmitQueue for integration into the monorepo.",
      "SubmitQueue\u2019s planner engine orchestrates executions of ending changes.",
      "In order to scale to thousands of changes per day while ensuring serializability, the planner engine speculates on outcomes of pending changes using a speculation engine, and executes their corresponding builds in parallel by using a build controller.",
      "The planner periodically asks the speculation engine for the builds most likely to succeed.",
      "The speculation engine in turn uses a probabilistic model to compute the likelihood of a given build passing.",
      "At each epoch the planner schedules execution of the selected builds and stops execution of any currently running builds not included in the new schedule.",
      "Once it is safe to do so, the planner commits change patches to the monorepo.",
      "When distributing work among worker nodes, the planner tries to ensure a uniform distribution.",
      "To this end, it keep a history of build steps performed together with their average build durations.",
      "The key challenge is to determine which set of builds we need to run in parallel, in order to improve turnaround time and throughput, while ensuring an always green mainline.",
      "To this end, the speculation engine builds a binary decision tree, called a speculation tree, annotated with prediction probabilities for each edge.",
      "The model selects builds based on their predicted value \u2013 which is a combination of likelihood of success and change priority (e.g. , security patches may have higher values).",
      "In the current implementation, all builds are given the same priority (benefit) value.",
      "When we include independent changes in the mix, the speculation tree can become a speculation graph.",
      "This enables independent changes to be committed in parallel.",
      "To determine independence, we need to know if changes conflict with each other.",
      "In order to build a conflict graph among pending changes, the conflict analyzer relies on the build system.",
      "A build system partitions the code into smaller entities called targets\u2026 Roughly speaking, two changes conflict if they both affect a common set of build targets.",
      "Every build target is associated with a unique target hash that represents its current state (a bit like a Merkle tree, this is the result of combining the hashes of all the inputs to the build of that target).",
      "Predicting success  We trained our success prediction models in a supervised manner using logistic regression.",
      "We selected historical changes that went through SubmitQueue along with their final results for this purpose.",
      "We then extracted around 100 handpicked features.",
      "The trained model achieved 97% accuracy.",
      "The features with the highest positive correlation scores were:  The number of successful speculations so far  Revision and revert test plans included as part of the submission  The number of initial tests that succeeded before submitting a change  The strongest negative correlations were with the number of failed speculations, and the number of times changes were submitted to a revision.",
      "We also note that while developer features such as the developer name had high predictive power, the correlation varied based on different developers.",
      "Evaluation  Taken in isolation, an iOS or Android build at Uber takes around 30-60 minutes:  When considering concurrent changes, and given an Oracle able to make perfect predictions, the turnaround times for builds looks like this:  (Each plot line shows a different number of changes per hour coming into the system).",
      "With n changes per hour, and n worker nodes available, SubmitQueue can achieve a turnaround time with 1.2x of the Oracle.",
      "Future work  The current version of SubmitQueue respects the order in which changes are submitted to the system.",
      "Thus small changes can be backed up behind larger ones.",
      "Future work will include re-ordering of non-independent changes to improve throughput.",
      "Another optimisation to be explored is batching independent changes expected to succeed together before running their build steps.",
      "This will enable Uber to make trade-offs between cost and turnaround time."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3302424.3303970?download=true",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 23313716
  },
  {
    "blog_id": "cornernet",
    "summary": [
      "What  Current object detectors follow either a two-stage or single-stage approach.",
      "Two-stage: Accurate, but rather slow as they use two different networks to predict RoIs and then classify them.",
      "Single-stage  Almost as accurate as two-stage detectors nowadays and faster than them.",
      "Single-stage detectors place a dense grid of anchor boxes on the image and classify for each of them whether they match an object in the image.",
      "There has to be a large number of anchor boxes for different object sizes (and also possible more when the detectors works on multiple scales).",
      "This increases memory demands, decreases performance and introduces difficult hyperparameter choices (number of anchors boxes and their sizes).",
      "They develop an object detector that is single-stage and free of anchor boxes.",
      "Their system predicts bounding boxes by localizing the top left and bottom right corner of an object.",
      "They argue that predicting corners is easier than the anchor-based approach.",
      "There are essentially exactly two points to predict per bounding box, while there can be many more valid anchors per object (that are than fine-tuned via regression to fit the object).",
      "They also propose a corner pooling layer that complements their technique.",
      "Pools along the horizontal/vertical axis.",
      "This is useful for objects where no object parts are locally in the corners (e.g. imagine a frontal-view on a human with stretched out arms).",
      "They argue that this pooling layer encodes prior knowledge of the prediction task.",
      "How  Corner Detection  They predict for each object class two heatmaps: One for top-left corners and one for bottom-right corners.",
      "The ground truth heatmaps contain positive values at the locations of corners.",
      "Gaussian Ground Truth  They argue that corners that are slightly off are still good hits reaching high IoUs.",
      "They set the desired target IoU to t=0.7 and derive from that per corner pair k a radius r_k describing how far the corners can be off to still fulfill t.  They place on each ground truth location an unnormalized 2D gaussian e^(-(x^2 + y^2)/2sigma^2), where sigma=r_k/3.",
      "Corner Position Loss  They use a variant of Focal Loss, applied once for the top-left corner heatmaps and once for bottom-right corners.",
      "where  y_cij is ground truth corner heatmap for class c at location y=i and x=j.",
      "p_cij is the predicted heatmap.",
      "alpha=2, beta=4 are focal loss hyperparameters.",
      "N is the number of objects in one image.",
      "Offsets  Predicted heatmaps are often downsampled compared to the ground truth heatmaps.",
      "That can lead to predicted locations being slightly off compared to true locations.",
      "They compensate for that by letting the network learn that offset (per spatial location).",
      "The ground truth for the offset value is:  where n is the downsampling factor.",
      "They apply a smooth L1 loss to the offset predictions.",
      "They apply the loss only at the ground truth corner locations.",
      "Grouping Corners  Top-left and bottom-right corners have to be matched in order to create a full bounding box.",
      "They do this by predicting embedding vectors for each top-left and bottom-right corner.",
      "They train these to be similar for corners belonging to the same objects.",
      "They train these to be different for corners belonging to different objects.",
      "For (1) they use a pull loss and for (2) a push loss:  where  e_t_k is the embedding of the top left corner of object k.  e_b_k is analogously is the embedding of the bottom right corner.",
      "e_k is the average of e_t_k and e_b_k.",
      "Delta=1.",
      "Note that there is no ground truth here, because only the distances matter.",
      "They apply these losses only at the ground truth corner locations.",
      "Corner Pooling  In many cases, there is no local evidence for an object around its top-left or bottom-right corners.",
      "But there is evidence around its top and left sides (analogously bottom and right sides).",
      "Visualization of the problem:  They compensate for that by max-pooling along the sides of each potential object.",
      "E.g. for the top-left corner they would max-pool along the corner's row and to the right, as well as along the corner's column und to the bottom.",
      "They sum both of these results.",
      "Visualization:  Architecture  They use stacked hourglass networks as their backbone (with minor modifications, e.g. striding instead of max-pooling).",
      "They place a corner pooling layer in residual fashion on top of the backbone.",
      "They then apply three branches:  Corner heatmaps branch (predicts 2*C channels for C classes).",
      "Embeddings branch (predicts ?",
      "*C channels).",
      "Offsets branch (predicts 2*C channels).",
      "Visualization:  Bounding box extraction  To get bounding boxes out of their corner predictions, they first apply non-maximum suppression to their corner heatmaps via a 3x3 max pooling layer.",
      "Then they extract the top 100 top-left and bottom-right corners over all classes.",
      "They shift them by the predicted offsets.",
      "They pair per class the top-left and bottom-right corners with the most similar embeddings, rejecting anything with an L1 distance above 0.5.",
      "To these bounding box candidates they apply soft-NMS to remove strongly overlapping bounding boxes.",
      "Results  Loss weightings: They weight their corner heatmaps loss with 1.0, the offset loss also with 1.0 and the pull and push losses for the embeddings with 0.1 each.",
      "They train on 10 PASCAL Titan X.",
      "For inference they zero-pad images to the desired input size (511x511) and feed the padded image as well as its horizontally flipped version through the network.",
      "They need about 244ms per image for inference.",
      "They train and test on COCO.",
      "Corner Loss  Corner Pooling is essential for the performance of the network.",
      "It improves AP by about 2 percentage points.",
      "It is especially important for large objects (+3.7 AP), not for small objects (+0.1 AP).",
      "Location Penalty (via gaussians)  They investigate whether it is necessary to reduce the location penalty in the corner location heatmaps by using gaussians.",
      "They compare using  no penalty reduction (just set corner locations to 1, everything else to 0),  placing gaussians with a fixed radius of 2.5 and  placing gaussians with object-dependent radii.",
      "Option (3) performs best, (1) worst.",
      "(2) is in between the two options, usually half-way from (1) to (3).",
      "They observe increases of AP between 5 and 6 percentage points when using (3) as opposed to (1).",
      "They difference is more pronounced for large objects (about 12 points) as opposed to small objects (2.3 points).",
      "Importance of each branch  They evaluate which branch (corner location heatmaps, offsets, embeddings) has most influence on the AP.",
      "They replace the predicted corner location heatmaps with ground truth heatmaps and increase AP by about 35 points (to 74.0%), suggesting that their corner heatmap prediction is the main bottleneck.",
      "They then add ground truth offsets and improve by 13.1 points (to 87.1%), suggesting that the offset prediction still has a significant impact on overall AP.",
      "This leaves 12.9 points for the other components (embedding prediction, bounding box extraction).",
      "Final results  They reach 42.1 AP in a multi-scale approach.",
      "(I guess they feed the images in at multiple scales?",
      "Not really explained.",
      "In previous chapters they write specifically they don't use multi-scale feature maps, but only the final feature map.)",
      "They beat the best multi-scale competitor (RefineDet512) by 0.3 points.",
      "They reach 40.5 AP in a single-scale approach.",
      "They beat the best single-scale competitor (RetinaNet800) by 1.4 points.",
      "Example predictions and extracted bounding boxes (each left: top-left corner, each right: bottom-right):"
    ],
    "author_id": "ALEJU",
    "pdf_url": "http://openaccess.thecvf.com/content_ECCV_2018/papers/Hei_Law_CornerNet_Detecting_Objects_ECCV_2018_paper.pdf",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 91200961
  },
  {
    "blog_id": "slim-os-kernel-support-for-a-low-overhead-container-overlay-network",
    "summary": [
      "Slim: OS kernel support for a low-overhead container overlay network Zhuo et al., NSDI\u201919  Container overlay networks rely on packet transformations, with each packet traversing the networking stack twice on its way from the sending container to the receiving container.",
      "There are CPU, throughput, and latency overheads associated with those traversals.",
      "In this paper, we ask whether we can design and implement a container overlay network, where packets go through the OS kernel\u2019s network stack only once.",
      "This requires us to remove packet transformation from the overlay network\u2019s data-plane.",
      "Instead, we implement network virtualization by manipulating connection-level metadata at connection setup time, saving CPU cycles and reducing packet latency.",
      "Slim comes with some caveats: it requires a kernel module for secure deployment, has longer connection establishment times, doesn\u2019t fit with packet-based network policies, and only handles TCP traffic.",
      "For UDP, ICMP, and for its own service discovery, it also relies on an existing container overlay network ( Weave Net ).",
      "But for longer lasting connections managed using connection-based network policies it delivers some impressive results:  memcached throughput up by 71%, with latency reduced by 42%, and CPU utilisation reduced by 56%.",
      "Nginx CPU utilisation reduced by 22-24%  PostgreSQL CPU utilisation reduced by 22%  Apache Kafka CPU utilisation reduced by 10%  Since Slim both builds on and compares to Weave Net, I should say at this point that Weave Net was the very first open source project from Weaveworks , the \u201cGitOps for Kubernetes\u201d company.",
      "Accel is an investor in Weaveworks, and I am also a personal investor.",
      "If you\u2019re using Kubernetes, you should definitely check them out.",
      "Anyway, on with the show\u2026  Container networking  In theory there are four possible modes for container networking: a bridge mode for containers on the same host; host mode in which containers use the IP address of their host network interface; macvlan mode (or similar hardware mechanisms) to give each container its own IP address; and overlay mode in which each container is given its own own virtual network interface and each application has its own network namespace.",
      "In practice, there are management and deployment challenges with the bridge, host, and macvlan approaches, so overlay networks such as Weave Net are the preferred solution.",
      "Overlay packets are encapsulated with host network headers when routed on the host network.",
      "This lets the container overlay network have its own IP address space and network configuration that is disjoint from that of the host network; each can be managed completely independently.",
      "Many container overlay network solutions are available today\u2014 such as Weave, Flannel, and Docker Overlay\u2014 all of which share similar internal architectures.",
      "Overlay network overheads  The overheads in overlay networking come from the per-packet processing inside the OS kernel: delivering a packet on the overlay network requires one extra traversal of the network stack and also packet encapsulation and decapsulation.",
      "Here are some test measurements comparing Weave Net in fast dataplane mode to host networking to give an example:  In this test, compared to direct host networking, for two containers on the same host (Intra) the overlay network reduces throughput by 23% and increases latency by 34%.",
      "For containers communicating across hosts (Inter), throughput reduces by 48% and latency increases by 85%.",
      "The overheads are lower when communicating on the same host since packet encapsulation is not required.",
      "Compared to host networking, the CPU utilisation also increases by 93%.",
      "There are several known techniques to reduce the data plane overhead.",
      "Packet steering creates multiple queues, each per CPU core, for a network interface and uses consistent hashing to map packets to different queues.",
      "In this way, packets in the same network connection are processed only on a single core.",
      "Different cores therefore do not have access to the same queue, removing the overhead due to multi-core synchronization.",
      "The authors integrated the above Receive Packet Steering (RPS), and also an enhancement called Receive Flow Steering (RFS\u2014 which further ensures that interrupt processing occurs on the same core as the application\u2014 into Weave Net.",
      "With this enhancement, throughput is within 9% of that achieved with host networking, but it makes almost no difference to latency.",
      "Introducing Slim  The big idea in Slim is to reduce CPU utilisation and latency overheads by having packets traverse the network stack only once.",
      "That means you can\u2019t do per-packet processing.",
      "Instead, Slim works at the connection level.",
      "Slim virtualizes the network by manipulating connection-level metadata.",
      "SlimSocket exposes the POSIX socket interface to application binaries to intercept invocations of socket-related system calls.",
      "When SlimSocket detects an application is trying to set up a connection, it sends a request to SlimRouter.",
      "After SlimRouter sets up the network connection, it passes access to the connection as a \ufb01le descriptor to the process inside the container.",
      "The application inside the container then uses the host namespace \ufb01le descriptor to send/receive packets directly to/from the host network.",
      "Because SlimSocket has the exact same interface as the POSIX socket, and Slim dynamically links SlimSocket into the application, the application binary need not be modi\ufb01ed.",
      "Given that Slim is out of the picture once the connection is established, a separate mechanism is needed to support control plane and data plane policies.",
      "SlimRouter stores control plane policies and enforces them at connection setup time.",
      "If the policy changes, SlimRouter scans existing connections and removes the file descriptors for any connection violating the new policy.",
      "This requires the support of a kernel module, SlimKernModule.",
      "To avoid containers learning the IP addresses of host machines, SlimKernModule (in secure mode) also prohibits unsafe system calls on file descriptors (e.g. getpeername).",
      "Existing kernel mechanisms are used to enforce data plane policies.",
      "This is what it looks like when Slim is used with blocking I/O:  Calls to the POSIX socket interface are intercepted by the SlimSocket shim and forward to the SlimRouter.",
      "For non-blocking I/O (e.g., select, epoll) Slim also intercepts these API calls and maintains mappings for epoll file descriptor sets.",
      "The SlimRouter needs to know the IP address and port mappings in order to establish connections.",
      "It does this using a Weave Net overlay network!",
      "When the client calls connect, it actually creates an overlay network connection on the standard container overlay network.",
      "When the server receives an incoming connection on the standard overlay network, SlimSocket queries SlimRouter for the physical IP address and port and sends them to the client side inside the overlay connection.",
      "In secure mode (\u00a74.3), the result queried from SlimRouter is encrypted.",
      "SlimSocket on the client side sends the physical IP address and port (encrypted if in secure mode) to its SlimRouter and the SlimRouter establishes the host connection.",
      "This means connection setup time is longer in Slim than that on container overlay networks based on packet transformation.",
      "Weave Net is also used for packets that require data plane handling such as ICMP and UDP.",
      "Evaluation  Microbenchmarks compare Slim to Weave Net with RFS.",
      "Creating on a TCP connection with Weave Net takes 270 \u00b5s.",
      "With Slim it takes 556\u00b5s (440\u00b5s in insecure mode).",
      "For applications with persistent connections, this additional overhead will not be significant.",
      "Compared to Weave Net, Slim reduces CPU overhead by 22-41%.",
      "Slim and Weave Net are then further compared on four application workloads based on Memcached, Nginx, PostgreSQL, and Apache Kafka respectively.",
      "For Memcached, Slim improves throughput by 71% and reduces latency by 42%, with 25% lower CPU utilisation.",
      "For Nginx, PostgreSQL the main advantage of Slim is reduced CPU utilisation (around 22% reduction).",
      "For Kafka the CPU utilisation reduction is around 10%, but latency is also reduced by 28%.",
      "Slim\u2019s source code is available at  [url]"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://www.usenix.org/system/files/nsdi19-zhuo.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 46507565
  },
  {
    "blog_id": "three-years-of-the-right-to-be-forgotten",
    "summary": [
      "Three years of the Right To Be Forgotten Bertram et al., 2018  With thanks to Elie Bursztein for bringing this paper to my attention.",
      "See also Elie\u2019s blog post \u2018 Insights about the first three years of the Right To Be Forgotten requests at Google .\u2019  Following on from the GDPR we looked at yesterday, and which comes into force in May of this year, I thought it would be interesting to take a look at another right to be forgotten (RTBF) that has been in force since May 2014.",
      "Today\u2019s paper choice is a retrospective study from Google of the 2.4M URLs that were requested for delisting from the Google search engine over the last 3 and a half years.",
      "This particular right to be forgotten enables individuals to request that search engines delist URLs containing \u201cinaccurate, inadequate, irrelevant or excessive\u201d information surfaced by queries containing the name of the requestor.",
      "Critically, the ruling requires that search engine operators make the determination for whether an individual\u2019s right to privacy outweighs the public\u2019s right to access lawful information when delisting URLs.",
      "That\u2019s a lot of responsibility to place with private groups within search engine companies!",
      "Google formed an advisory council drawn from academic scholars, media producers, data protection authorities, civil society, and technologists to establish decision criteria for \u201cparticularly challenging delisting requests.\u201d  Google make a RTBF submission form available online.",
      "Requestors must verify their identity and provide a list of URLs they would like to delist along with the search queries leading to those URLs and a short comment about how the URLs relate to the requestor.",
      "Every submission is manually reviewed \u2013 there is no automation in the decision process.",
      "The reviewers consider four criteria, designed to weigh public interest against the requestor\u2019s personal privacy:  The validity of the request: is it actionable (e.g., specifies exact URLs) and is the requestor connected with an EU/EEA country.",
      "The identity of the requestor: for example, if the requestor is a public figure there may be heightened public interest.",
      "Other categories of interest include minors, politicians, and professionals.",
      "The content referenced by the URL.",
      "\u201cFor example, information related to a requestor\u2019s business may be of public interest for potential customers.",
      "Similarly, content related to a violent crime may be of interest to the general public\u2026 \u201c  The source of the information: e.g., government site, news site, blog, or forum.",
      "For the period between May 30th 2014, and December 31st 2017, Google received requests to delist almost 2.4M URLs, from 399,779 unique requestors.",
      "Only 43% of these URLs were ultimately delisted.",
      "From January 22nd 2016, requested URLs were additionally annotated with categorical data for purposes of improving transparency around RTBF requests.",
      "Applying judgement  The paper contains a sprinkling of anonymous requests and the decisions reached, which provide good insight into the challenges the reviewers face.",
      "Here are some examples:  \u201c\u2026 an individual, who was convicted for two separate instances of domestic violence within the previous five years, sent Google a delisting request focusing on the fact that their first conviction was \u2018spent\u2019 under local law.",
      "The requestor did not disclose that the second conviction was not similarly spent, and falsely attributed all the pages sought for delisting to the first conviction.",
      "Reviewers discovered this as part of the review process and the request was ultimately rejected.\u201d  \u201cIn another case, reviewers first delisted 150 URLs submitted by a businessman who was convicted for benefit fraud, after they provided documentation confirming their acquittal.",
      "When the same person later requested the delisting of URLs related to a conviction for manufacturing court documents about their acquittal, reviewers evaluated the acquittal documentation sent to Google, found it to be a forgery, and reinstated all previously delisted URLs\u201d.",
      "\u201c\u2026a requestor who held a significant position at a major company sought to delist an article about receiving a long prison sentence for attempted fraud.",
      "Google rejected this request due to the seriousness of the crime and the professional relevance of the content.\u201d  \u201c\u2026an individual sought to delist an interview they conducted after surviving a terrorist attack.",
      "Despite the article\u2019s self-authored nature given the requestor was interviewed [note the assumption here that the journalist gave a fair impression of what the interviewee actually said!",
      "], Google delisted the URL as the requestor was a minor and because of the sensitive nature of the content.\u201d  \u201c\u2026 a requestor sought to delist a news article about their acquittal for domestic violence on the grounds that no medical report was presented to the judge confirming the victim\u2019s injuries.",
      "Given that the requestor was acquitted, Google delisted the article.",
      "**\u201d  Welcome to the court of social reputation!",
      "How often are requests made and who makes them?",
      "Overall, the number of RTBF requests per month has been slowly declining after an initial peak when the facility was first launched.",
      "The number of previously unseen requestors per month is also declining.",
      "The most requests originate in France, Germany, and the United Kingdom, though if we look at per capita rates Estonia tops the table.",
      "Most interestingly, reputation management is clearly a growing business (see also Black Mirror: \u2018nosedive\u2019):  The top thousand requesters (0.25% of all requesters) generated 14.6% of requests and 20.8% of delistings.",
      "These mostly included law firms and reputation management agencies, as well as some requestors with a sizable online presence\u2026 while hundreds of thousands of Europeans rely on the RTBF to delist a handful of URLs, there are thousands of entities using the RTBF to alter hundreds of URLs about them or their clients that appear in search results.",
      "Most requested for delisting, by an order of magnitude,  are URLs concerning private individuals:  Requests predominantly come from private individuals (88%).",
      "Politicians and government officials requested delisting of 33,937 URLs, and non-governmental public figures another 41,213 URLs.",
      "Over 77% of requests to delist URLs rooted in a country code top-level domain come from requestors in the same country.",
      "Requests now take a median of 4 days to process.",
      "What content do people request delisting for?",
      "The major categories of sites that contains URLs targeted for delisting are social media sides, directory sites aggregating contact details and personal content, and news sites.",
      "It seems that people don\u2019t want you to read bad things about them on Facebook!",
      "Here are the most requested sites by category:  Different countries show different characteristics, which can be explained by, for example, the character of the news organisations in those countries, and the role of the government in publishing information.",
      "In Italy and the UK, requestors target news media much more than in Germany and France for example.",
      "Journalists in the former countries are prone to reveal the identity of individuals, whereas those in the latter tend to anonymise parties in their coverage of crimes.",
      "Requestors in France and Germany target social media and directory services more than average.",
      "In Spain there is a higher proportion of requests targeting government records.",
      "Spanish law requires the government to publish \u2018edictos\u2019 and \u2018indultos.\u2019 \u201c_The former are public notifications to inform missing individuals about a government decision that directly affects them; the latter are government decisions to absolve an individual from a criminal sentence or to commute to a lesser one.\u201d  Looking at the content at the URLs requested for delisting, we find that most pages contain professional information, though it\u2019s interesting to see self-authored content in the number two spot!",
      "The most commonly requested content related to professional information, which rarely met the criteria for delisting (16.7%).",
      "Many of these requests pertain to information which is directly relevant or connected to the requestor\u2019s current profession and is therefore in the public interest to have indexed in Google Search."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://www.elie.net/static/files/three-years-of-the-right-to-be-forgotten/three-years-of-the-right-to-be-forgotten-paper.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 13793499
  },
  {
    "blog_id": "adding-concurrency-to-smart-contracts",
    "summary": [
      "Adding concurrency to smart contracts Dickerson et al., PODC\u201917  Yesterday we looked at how analogies from concurrent objects could help us understand smart contract behaviour.",
      "In today\u2019s paper choice from PODC\u201917 (which also has one Maurice Herlihy on the author list) we get to borrow some ideas from concurrent objects to increase the concurrency of smart contracts.",
      "Back in 2008 Herlihy & Koskinen published a paper on \u2018 Transactional boosting: a methodology for highly-concurrent transactional objects.",
      "\u2018 In the context of software transactional memory (STM), transactional boosting showed how to transform highly-concurrent base objects implemented without any notion of transactions into equally concurrent transaction objects that can safely be used with STM.",
      "Taking some ideas from transaction boosting, \u2026  \u2026 This paper presents a novel way to permit miners and validators to execute smart contracts in parallel, based on techniques adapted from software transactional memory.",
      "Miners execute smart contracts speculatively in parallel, allowing non-conflicting contracts to proceed concurrently, and \u201cdiscovering\u201d a serialized concurrent schedule for a block\u2019s transactions.",
      "Why do we want more concurrency?",
      "When a miner creates a block, includes a sequence of transactions, computing the new state for the transactions\u2019 smart contracts serially, in the order in which they appear in the block.",
      "If the block is subsequently successfully appended to the blockchain, that block\u2019s transactions are re-executed by every node to confirm that the state transitions were computed honestly and correctly.",
      "To summarize, a transaction is executed in two contexts: once by miners before attempting to append a block to the blockchain, and many times afterward by validators checking that each block in the blockchain is honest.",
      "In both contexts, each block\u2019s transactions are executed sequentially in block order.",
      "Miners are rewarded for blocks that they successfully append to the blockchain, so they have an incentive to increase throughput by parallelizing smart contract executions.",
      "But simply executing contracts in parallel without any special precautions won\u2019t work as the contracts may perform conflicting accesses to shared data leading to an inconsistent final state.",
      "Validators on the other hand end up performing the vast majority of contract executions and parallel execution here could bring significant benefits if it could be done safely.",
      "Speculative smart contracts  We\u2019ve seen previously that writing bug free smart contracts is hard.",
      "Clearly, even sequential smart contracts must be written with care, and introducing explicit concurrency to contract programming languages would only make the situation worse.",
      "We conclude that concurrent smart contract executions must be serializable: indistinguishable, except for execution time, from a sequential execution.",
      "Smart contracts read and modify shared storage, and they are written in Turing-complete languages \u2013 so it is impossible in the general case to determine statically whether or not contracts have data conflicts.",
      "Instead, contracts can be instrumented to detect synchronization conflicts at runtime, in a manner similar to that done in transaction boosting.",
      "Contracts are executed speculatively, and if a conflict does occur at runtime the conflict is resolved either by delaying one contract until the other completes, or rolling back and restarting one of the conflicting executions.",
      "Speculation is controlled by two run-time mechanism, invisible to the programmer, and managed by the virtual-machine: abstract locks and inverse logs.",
      "Storage operations are protected by abstract locks.",
      "If two storage operations map to distinct locks, then they must commute.",
      "Or to put it another way, operations that don\u2019t commute must be protected by the same lock.",
      "It wasn\u2019t clear to me from reading the paper whether this mapping of operations to locks can be performed automatically, or whether it requires human intervention.",
      "Before executing the operation, a thread must acquire the associated lock.",
      "When the lock is acquired, it records an inverse operation in a log (think undo log), and then proceeds with the operation.",
      "If the action commits, its abstract locks are released and its log is discarded.",
      "If the action aborts, the inverse log is replayed, most recent operations first, to undo the effects of that speculative action.",
      "When the replay is complete, the actions\u2019 abstract locks are released.",
      "The advantage of combining abstract locks with inverse logs is that the virtual machine can support very fine-grained concurrency.",
      "If one contract calls another, a nested speculative action is created.",
      "At the end of this process, the miner will have discovered a concurrent schedule for a block\u2019s transactions, that is equivalent to some sequential schedule, only faster.",
      "Validation  So far so good for the miners, but not so great for the validators.",
      "The problem is that the validators need to produce the same or an equivalent schedule of execution to that discovered by the  miner.",
      "The solution is for miners to produce and publish extra information concerning the constraints discovered during execution.",
      "Why would miners make this available?",
      "\u2026 that block [produced by the miner] may be competing with other blocks produced at the same time, and the miner will be rewarded only if the other miners choose to build on that block.",
      "Publishing a block with a parallel validation schedule makes the block more attractive for validation by other miners.",
      "Here\u2019s how it works:  Each lock includes a use counter keeping track of the number of times it has been released by a committing action during the construction of the current block.",
      "When a speculative action commits, it increments the counters for each of the locks it holds, and registers a lock profile with the VM recording the abstract locks and their counter values.",
      "When all the actions have committed, the common schedule can be reconstructed by comparing lock profiles.",
      "It is these profiles that the miner includes in the blockchain along with the usual information.",
      "For example, consider three committed speculative actions, A, B, and C. If A and B have no abstract locks in common, they can run concurrently.",
      "If an abstract lock has counter value 1 in A\u2019s profile and 2 in C\u2019s profile, then C must be scheduled after A.",
      "Using the algorithm below, a validator can construct a simple fork-join program that deterministically reproduces the miner\u2019s original speculative schedule.",
      "Using a work-stealing scheduler, the validator can exploit whatever degree of parallelism it has available.",
      "The validator keeps a thread-local trace of the abstract locks each thread would have acquired.",
      "If these traces don\u2019t match the lock profiles provided by the miner the block is rejected.",
      "Is it safe?",
      "Correctness is argued by appeal to the analogy with transactional boosting, where serial equivalence has been proven.",
      "Experimental results  The authors built an implementation based on the JVM for experimental purposes, using the Scala STM library for speculative action execution.",
      "Examples of smart contracts were translated from Solidity into Scala, the modified to use the concurrency libraries.",
      "Each function from the Solidy contract is turned into a speculative transaction by wrapping its contents with a ScalaSTM atomic section.",
      "Solidity mapping objects are implemented as boosted hashtables, where key values are used to index abstract locks.",
      "The benchmarks are based on Ballot, SimpleAuction, and EtherDoc contracts, as well as a workload mixing all three.",
      "The experiments used only three concurrent threads, but this was still sufficient to show a benefit:  The charts below give more detail of the speedups obtained at different conflict levels.",
      "Our proposal for miners only is compatible with current smart contract systems such as Ethereum, but our overall proposal is not, because it requires including scheduling metadata in blocks and incentivizing miners to publish their parallel schedules.",
      "It may well be compatible with a future \u201csoft fork\u201d (backward compatible change), a subject for future research."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3087801.3087835?download=true",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 17754953
  },
  {
    "blog_id": "learning-to-count-objects-in-natural-images-for-visual-question-answering",
    "summary": [
      "Most of the visual question-answering (VQA) models perform poorly on the task of counting objects in an image.",
      "The main reasons are:  Most VQA models use a soft attention mechanism to perform a weighted sum over the spatial features to obtain a single feature vector.",
      "These aggregated features helps in most category of questions but seems to hurt for counting based questions.",
      "For the counting questions, we do not have a ground truth segmentation of where the objects to be counted are present on the image.",
      "This limits the scope of supervision.",
      "Additionally, we need to ensure that any modification in the architecture, to enhance the performance on the counting questions, should not degrade the performance on other classes of questions.",
      "The paper proposes to overcome these challenges by using the attention maps (and not the aggregated feature vectors) as input to a separate count module.",
      "Notes  The basic idea is quite intuitive: when we perform weighted averaging based on different attention maps, we end up averaging the features corresponding to the difference instances of an object.",
      "This makes the feature vectors indistinguishable from the scenario where we had just one instance of the object in the image.",
      "Even multiple glimpses (multiple attention steps) can not resolve this problem as the weights given to one feature vector would not depend on the other feature vectors (that are attended to).",
      "Hard attention could be more useful than soft-attention but there is not much empirical evidence in support of this hypothesis.",
      "The proposed count module is a separate pipeline that can be integrated with most of the existing attention based VQA models without affecting the performance on non-count based questions.",
      "The inputs to the count module are the attention maps and the object proposals (coming from some pre-trained model like the RCNN model) and the output is an count-feature vector which is used to answer the count based question.",
      "The top level idea is the following - given the object proposals and the attention maps, create a graph where nodes are objects (object proposals) and edges capture how similar two object proposals are (how much do they overlap).",
      "The graph is transformed (by removing and scaling edges) so that the count of the object can be obtained easily.",
      "To explain their methodology, the paper simplifies the setting by making two assumptions:  The first assumption is that the attention weights are either 1 (when the object is present in the proposal) or 0 (when the object is absent from the proposal).",
      "The second assumption is that any two object proposals either overlap completely (in which case, they are corresponding to the exact same object and hence receive the exact same weights) or the two proposals have zero overlap (in which case, they must be corresponding to completely different objects).",
      "These simplifying assumptions are made only for the sake of exposition and do not limit the capabilities of the count module.",
      "Given the assumptions, the task of the count module is to handle the exact duplicates to prevent double-counting of objects.",
      "As the first step, the attention weights (a) are used to generate an attention matrix (A) by performing an outer product between a and aT.",
      "This corresponds to the step of creating a graph from the input.",
      "A corresponds to the adjacency matrix of that graph.",
      "The attention weight for the ith proposal corresponds to the ith node in the graph and the edge between the nodes i and j has the weight ai*aj.",
      "Also note that the graph is a weighted directed graph and the subgraph of vertices satisfying the condition ai = 1 is a complete directed graph with self-loops.",
      "Given such a graph, the number of vertices, V = sqrt(E) where E could be computed by summing over the adjacency matrix.This implies that if the proposals are distinct, then the count can be obtained trivially by performing a sum over the adjacency matrix.",
      "The objective is now to eliminate the edges such that the underlying objects are the vertices of a complete subgraph.",
      "This requires removing two type of duplicate edges - intra-object edges and inter-object edges.",
      "Intra-object edges can be removed by computing a distance matrix, D, defined as 1 - IoU, where IoU matrix corresponds to the Intersection-over-Union matrix.",
      "A modified adjacency matrix A\u2019 is obtained by performing the element-wise product between f1(A) and f2(D) where f1 and f2 are piece-wise linear functions that are learnt via backpropogation.",
      "The inter-object edges are removed in the following manner:  Count the number of proposals that correspond of each instance of an object and then scale down the edges corresponding to the different instances by that number.",
      "This creates the effect of reducing the weights of multiple proposals equivalent to a single proposal.",
      "The number of proposals corresponding to an object is not available as an annotation in the training pipeline and is estimated based on the similarity between the different proposals (measured via the attention weights a, adjacency matrix A and distance matrix D).",
      "The matrix corresponding to the similarity between proposals  (simi, j) is transformed into a vector corresponding to the scaling factor of each node (si)  s can be converted into a matrix (by doing outer-product with itself) so as to scale both the incoming and the outgoing edges.",
      "The self edges (which were removed while computing A\u2019 are added back (after scaling with s) to obtain a new transformed matrix C.  The transformed matrix C is a complete graph with self-loops where the nodes corresponds to all the relevant object instances and not to object proposals.",
      "The actual count can be obtained from C by performing a sum over all its values as described earlier.",
      "The original count problem was a regression problem but it is transformed into a classification problem to avoid scale issues.",
      "The network produces a k-hot n-dimensional vector called o where n is the number of object proposals that were feed into the module (and hence the upper limit on upto how large a number could the module count).",
      "In the ideal setting, k should be one, as the network would produce an integer value but in practice, the network produces a real number so k can be upto 2.",
      "If c is an exact integer, the output is a 1-hot vector with the value in index corresponding to c set to 1.",
      "If c is a real number, the output is a linear interpolation between two one-hot vectors (the one-hot vectors correspond to the two integers between  which c lies).",
      "count module supports computing the confidence of a prediction by defining two variables pa and pD which compute the average distance of f6(a) and $f7(D) from 0.5.",
      "The final output o\u2019 is defined as f8(pa + pD) .",
      "o  All the different f functions are piece wise linear functions and are learnt via backpropagation.",
      "Experiments  The authors created a new category of count-based questions by filtering the number-type questions to remove questions like \u201cWhat is the time right now\u201d.",
      "These questions do have a neumerical answer but do not fall under the purview of count based questions and hence are not targeted by the count model.",
      "The authors augmented a state of the art VQA model with their count module and show substantial gains over the count-type questions for the VQA-v2 dataset .",
      "This augmentation does not drastically impact the performance on non-count questions.",
      "The overall idea is quite crisp and intutive and the paper is easy to follow.",
      "It would be even better if there were some more abalation studies.",
      "For example, why are the piece-wise linear functions assumed to have 16 linear components?",
      "Would a smaller or larger number be better?"
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1802.05766",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 11112277
  },
  {
    "blog_id": "measuring-the-tendency-of-cnns-to-learn-surface-statistical-regularities",
    "summary": [
      "Measuring the tendency of CNNs to learn surface statistical regularities Jo et al., arXiv\u201917  With thanks to Cris Conde for bringing this paper to my attention.",
      "We\u2019ve looked at quite a few adversarial attacks on deep learning systems in previous editions of The Morning Paper.",
      "I find them fascinating for what they reveal about the current limits of our understanding.",
      "\u2026humans are able to correctly classify the adversarial image with relative ease, whereas the CNNs predict the wrong label, usually with very high confidence.",
      "The sensitivity of high performance CNNs to adversarial examples casts serious doubt that these networks are actually learning high level abstract concepts.",
      "This begs the following question: How can a network that is not learning high level abstract concepts manage to generalize so well?",
      "In this paper,  Jo and Bengio conduct a series of careful experiments to try and discover what\u2019s going on.",
      "The initial hypothesis runs like this:  There are really only two ways we could be seeing the strong generalisation performance that we do.",
      "Either (a) the networks are learning high level concepts, or (b) there may be a number of superficial cues in images that are shared across training and test datasets, and the networks are learning these instead.",
      "We have reason to doubt scenario (a) because the success of adversarial images  We have some reasons to believe scenario (b) given results in the computer vision literature that show a strong statistical relationship between image statistics and visual understanding.",
      "These suggest that computer vision algorithms may \u201c\u2026lean heavily on background features to perform categorization.\u201d (For example, cars tend to be on roads).",
      "If scenario (b) is true, then we ought to see a drop in generalisation performance if we manipulate the image statistics in such a way that they are appreciably different, and yet humans can still easily recognise the target object.",
      "When the training and the test set share similar image statistics, it is wholly possible for a machine learning model to learn superficial cues and generalize well, albeit in a very narrow sense as they are highly dependent on the image statistics.",
      "Adversarial examples would be destroying the superficial cues.",
      "We believe that this is precisely how deep CNNs can attain record breaking generalization performance on all sorts of natural image tasks, and yet can be so sensitive to adversarial perturbations.",
      "To gather evidence in favour of this hypothesis, we need to find a perturbation function over a dataset such that:  Object recognisability is preserved from the perspective of a human  The perturbed images exhibit qualitatively different image statistics.",
      "When trained on either (but not both!)",
      "of the original or perturbed images, and then tested against both the original and perturbed images, we see a non-trivial generalisation gap between the generalisation capability on the test images with similar statistics, and the the generalisation capability on the test images with different statistics.",
      "Conditions (1) and (2) together guarantee that the original and perturbed datasets share the same high level abstractions but exhibit different superficial cues.",
      "Fourier filtering to the rescue  While natural images are known to exhibit a huge variance in the raw pixel space, it has been shown that the Fourier image statistics of natural images obey a power law decay\u2026 An immediate takeaway is that natural images tend to have the bulk of their Fourier spectrum concentrated in the low to medium range frequencies.",
      "Due to this power law concentration of energy in the frequency space, it is possible to perform certain types of Fourier filtering and preserve much of the perceptual content of an image.",
      "The authors experiment with two different kinds of Fourier filters: a low-frequency filter that uses a radial mask in the Fourier domain to set higher frequency modes to zero, and a random filter that uses a uniformly random mask to set a Fourier mode to zero with probability p.  Here are some example images from the SVHN dataset (top row), and the results of applying the radial and random masking filters respectively:  And this is the same thing, but for the CIFAR-10 dataset:  The filters do introduce artifacts into the images, but these don\u2019t really interfere with human perception, and tend to occur in the background of the image.",
      "Running the experiment  For both the CIFAR-10 and SVHN datasets the authors use some established high-performance CNN architectures (Preact ResNet).",
      "A model is trained on one of the unfiltered, radial mask, or random mask datasets, and then tested across all three test groups (unfiltered, radial, random).",
      "This enables us to measure the test gap or generalisation gap as the maximum difference in accuracy across the test sets.",
      "For the SVHN source dataset, the three figures below show the generalisation results after training on (a) unfiltered, (b) random, and (c) radial.",
      "And here\u2019s the same thing for CIFAR-10:  With SVHN the largest generalisation gap (7.89%) occurs when training on randomly filtered data, and testing on the radially filtered dataset.",
      "Training on the radially filtered dataset actually turned out to improve generalisation performance on the unfiltered test set by 1.5%.",
      "The authors put this down to a regularisation effect.",
      "Changing the network depth seemed to have little impact on the generalisation gap.",
      "That is, there is no evidence of an ability to more successfully learn higher level abstractions when we add more data.",
      "With CIFAR-10 the generalisation gaps are bigger: 28% when trained on randomly filtered data tested on radially filtered.",
      "Changing the depth has little impact in this case too.",
      "Discussion  In addition to still being recognisable to the human eye (a subjective measure), we know also know that networks trained on the filtered datasets actually generalised quite well to the unfiltered test set (off by 1-2% of the best unfiltered accuracy).",
      "This provides further evidence that the choice of Fourier filtering schemes is producing datasets that are perceptually not too far off from the original unfiltered dataset.",
      "We see that deep CNNs trained on a unfiltered natural image dataset exhibit a tendency to latch onto the image statistics of the training set, yielding a non-trivial generalization gap\u2026  When training on all three training sets (unfiltered, radial, and random) there is an improvement in the generalisation gap.",
      "\u201cHowever, we cast doubt on the notion that this sort of data augmentation scheme is sufficient to learn higher level semantic features in the dataset.",
      "Rather it is far more likely that the CNNs are learning a superficial robustness to the varying image statistics.\u201d  Overall, the data seems to support the initial hypothesis: \u201cthe current incarnation of deep neural networks have a tendency to learn surface statistical regularities as opposed to high level abstractions.\u201d  What are we to do?",
      "I\u2019m reminded of the brightly coloured word books we teach our children with \u2013 often with a single strong cartoon-like image of an object, on a plain white background, and the label (word) underneath.",
      "By seeing real world objects with movement (either of our own point of view, or because they have independent motion) we are also forced to more strongly separate object from background.",
      "The authors cite six pieces of work which they believe represent promising new directions:  \u201c Independently controllable factors \u201d and \u201c Reinforcement learning with unsupervised auxiliary tasks \u201d aim to learn good disentangled feature representations by combining unsupervised and reinforcement learning.",
      "\u201c SCAN: learning abstract hierarchical compositional visual concepts \u201d uses a variational setup.",
      "\u201c Discovering objects and their relations from untangled scene representations \u201d aims to learn abstract relations between objects in natural scene images  \u201c The consciousness prior \u201d moves away from making predictions in the perceptual space, and instead operates in the higher-order abstract space."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://arxiv.org/pdf/1711.11561",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 23942242
  },
  {
    "blog_id": "formal-requirements-for-virtualizable-third-generation-architectures",
    "summary": [
      "Formal Requirements for Virtualizable Third Generation Architectures \u2013 Popek & Goldberg 1974.",
      "With thanks to Alfred Bratterud for pointing me at this paper.",
      "What exactly is a virtual machine?",
      "What does a virtual machine monitor do?",
      "And how do we now whether a given piece of hardware can support virtualization or not?",
      "In today\u2019s paper choice, Popek and Goldberg set out the answers for us \u2013 and by the way, they had all this figured out way back in 1974!",
      "What is a Virtual Machine?",
      "There are currently (1974) a number of viewpoints suggesting what a virtual machine is, how it ought to be constructed, and what hardware and operating system implications result\u2026  Here\u2019s a very simple definition of a virtual machine:  A virtual machine is taken to be an efficient, isolated duplicate of the real machine.",
      "Though of course we need to dig further and understand what is implied by the three words efficient, isolated, and duplicate.",
      "To explain these, the authors introduce the notion of a virtual machine monitor\u2026  What is a Virtual Machine Monitor?",
      "A virtual machine monitor (VMM) does three things:  It provides a duplicate, or essentially identical to the original machine, environment for programs.",
      "\u201cAny program run under the VMM should exhibit an effect  identical with that demonstrated if the program had been run  on the original machine directly, with the possible exception of differences caused by the availability of system resources and differences caused by timing dependencies.\u201d  It does so efficiently, requiring \u201ca statistically dominant subset of the virtual processor\u2019s instructions be executed directly by the real processor, with no software intervention by the VMM.",
      "This statement rules out traditional emulators and complete software interpreters (simulators) from the virtual machine umbrella.\u201d Thus programs that run in this environment show only minor decreases in speed.",
      "It is in complete control of system resources (memory, peripherals, and the like).",
      "This requires two conditions: (i) it must not be possible for a program running in the created environment to access any resource not allocated to it (isolation), and (ii) it is possible under certain circumstances to regain control of resources already allocated.",
      "A virtual machine is the environment created by the virtual machine monitor.",
      "Does my Hardware Support Virtualization?",
      "This is the question the vast majority of the paper is dedicated to.",
      "Before we can get to the answer (which is actually a very simple and easy test), we need to understand what the authors mean by a \u2018third generation architecture\u2019 (per the paper title).",
      "Examples of a third generation architecture machine are the IBM 360, Honeywell 6000, or DEC PDP-10.",
      "Such machines have a processor, and linear uniformly addressable memory.",
      "The processor can operate in supervisor mode, or in user mode.",
      "Some instructions are only available in supervisor mode.",
      "Memory addressing is done relative to a relocation register R=(l,b) which is always active.",
      "The location parameter l gives the absolute address that corresponds to the apparent address zero, and the bounds parameter b gives the absolute size of the virtual memory.",
      "Suppose an instruction produces some address a, we check and then find the true address as follows:  if  a + l >= total-real-memory-size then       // out of real memory bounds     memorytrap  else if a >= b then     // out of virtual memory bounds    memorytrap else     use address a+l  The program status word PSW is a triplet (mode \u2013 user/supervisor, program counter, relocation register), and the overall state S of the machine can be modeled as (E,PSW) where E is the executable storage.",
      "E[0] and E[1] are used to store an old-PSW and fetch a new PSW respectively.",
      "Instructions  are simply modeled as a function from State -> State.",
      "In this model, for simplicity, we have departed slightly from most common relocation systems by assuming it to be active in the supervisor as well as user mode.",
      "This difference will not be important to the proof of our result.",
      "Note also that all references made by the processor to memory are assumed to be relocated.",
      "A trap, such as the memorytrap above, automatically saves the current state of the machine and passes control to a pre-specified control routine by changing the PSW to the values specified in E[1].",
      "Key to understand whether or not it is possible to virtualize a given piece of hardware is to divide the instructions into groups.",
      "In particular, privileged instructions are those that do not trap when the processor is in supervisor mode, but do trap (a privileged instruction trap) when in user mode.",
      "Privileged instructions are independent of the virtualization process.",
      "They are merely characteristics of the machine which may be determined from reading the principles of operation.",
      "Note, however, that the way we have defined privileged instructions requires them to trap.",
      "Merely NOPing the instruction without trapping is insufficient.",
      "The latter case should not be called a privileged instruction; maybe \u201cuser mode NOP\u201d would be more accurate.",
      "Sensitive instructions may be either control sensitive, or behaviour sensitive.",
      "Control sensitive instructions are those that affect or can affect control over system resources \u2013 in our simplified model the only such resource is memory.",
      "A control sensitive instruction attempts to change the amount of resource (memory) available, or change processor mode, without going through a memory trap.",
      "A behaviour sensitive instruction is one whereby the effect of its execution is dependent on the value of the relocation bounds register (location in real memory) or processor mode.",
      "An instruction that is not sensitive is innocuous.",
      "A virtual machine monitor is a control program comprising a dispatcher, an allocator, and a set of interpreters, one per privileged instruction.",
      "The location of the control program (dispatcher) is placed in the program counter at E[1], it directs execution to the allocator or interpreters as needed.",
      "The allocator decides what system resources are to be provided (e.g. keeping the VMM and VM memory separate).",
      "The allocator will be invoked by the dispatcher whenever an attempted execution of a privileged instruction in a virtual machine environment occurs which would have the effect of changing the machine resources associated with that environment.",
      "Attempting to reset the R (relocation-bounds) register is the primary example in our skeletal model.",
      "If the processor were to be treated as a resource, a halt would be another.",
      "The job of the interpreters is to simulate the instruction that trapped.",
      "A virtual machine monitor [that satisfies the three properties of efficiency, resource control, and equivalence] may be constructed if the set of sensitive instructions for that computer is a subset of the privileged instructions.",
      "The proof of this statement is given in the paper and the appendices \u2013 it rests on showing a one-one homomorphism f between real machine states and virtual machine states, and that if the real machine halts in state S, then the virtual machine halts in state f(S).",
      "The final step is an existence argument (i.e. there is at least one way) to build the privileged instruction interpreters \u2013 using a lookup table.",
      "Furthermore, recursive virtualization (a VM that runs a copy of itself under the VMM) is possible if (a) a VMM can be constructed for the hardware as above, and (b) the VMM does not have any timing dependencies.",
      "The theorem provides a fairly simple condition sufficient to guarantee virtualizability, assuming, of course, that the requisite features of \u201cconventional third generation machines\u201d are present.",
      "However, those features which have been assumed are fairly standard ones, so the relationship between the sets of sensitive and privileged instructions is the only constraint.",
      "It is a very modest one, easy to check.",
      "Further, it is also a simple matter for hardware designers to use as a design requirement."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://www.princeton.edu/~rblee/ELE572Papers/Fall04Readings/secureOS/popek_virtualizable.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 17349212
  },
  {
    "blog_id": "protecting-user-privacy-an-approach-for-untraceable-web-browsing-history-and-unambiguous-user-profiles",
    "summary": [
      "Protecting user privacy: an approach for untraceable web browsing history and unambiguous user profiles Beigi et al., WSDM\u201919  Maybe you\u2019re reading this post online at The Morning Paper , and you came here by clicking a link in your Twitter feed because you follow my paper write-up announcements there .",
      "It might even be that you fairly frequently visit paper write-ups on The Morning Paper.",
      "And perhaps there are several other people you follow who also post links that appear in your Twitter feed, and occasionally you click on those links too.",
      "Given your \u2018anonymous\u2019 browsing history I could probably infer that you\u2019re likely to be one of the 20K+ wonderful people with a wide-ranging interest in computer science and the concentration powers needed to follow longer write-ups that follow me on Twitter.",
      "You\u2019re awesome, thank you!",
      "Tying other links in the browsing history to other social profiles that have promoted them, we might be able to work out who else our mystery browser probably follows on social media.",
      "It won\u2019t be long before you\u2019ve been re-identified from your browsing history.",
      "And that means everything else in that history can be tied back to you too.",
      "(See \u2018 De-anonymizing web browsing data with social networks \u2019).",
      "In addition to the web browser, users\u2019 browsing histories are recorded via third-party trackers embedded on web pages to help improve online advertising and web surfing experience.",
      "Moreover, Internet Service Providers (ISPs) such as AT&T and Verizon, have full access to individuals\u2019 web browsing histories\u2026 FCC\u2019s Internet privacy protection has also been removed in late March of 2017.",
      "This new legislation allows ISPs to monitor, collect, share and sell their customer\u2019s behavior online such as detailed Web browsing histories without their consent and any anonymization.",
      "(Thank goodness for the GDPR in Europe!)",
      "Given the lack of protection, it\u2019s now up to individual users to protect their browsing history.",
      "It\u2019s not possible to remove entries from the history, so the only thing we can do is add noise.",
      "Adding noise may make it harder to de-anonymise a profile, and it also gives plausible deniability that you actually visited any given link that appears in the history.",
      "This is a tough one.",
      "I don\u2019t know about you, but I feel a little uncomfortable with the thought of random links (for who knows what content!)",
      "appearing in my browsing history.",
      "Try the opposite thought experiment though, and imagine e.g. a browser plugin that posts every URL you visit to your social media feed.",
      "I don\u2019t think many people would be comfortable with that either!",
      "The second issue with polluting your history is that some of the personalisation that goes on is actually useful.",
      "So you might start seeing content that is less relevant and interesting to you.",
      "At the heart of today\u2019s paper is an exploration of this privacy-utility trade-off through a system called PBooster.",
      "In this paper, we aim to study the following problem: how many links and what links should be added to a user\u2019s browsing history to boost user privacy while retaining high utility.",
      "For privacy without caring about utility, just adding random links is the best strategy (though PBooster gets close).",
      "The random link strategy seriously degrades the utility of the history though (we\u2019ll get into how that\u2019s measured next), whereas PBooster is able to retain much more utility.",
      "Measuring privacy and utility  From a privacy perspective, the more uniformly distributed a history is over a set of topics, the harder it will be to infer which topics are of true interest.",
      "Thus,  \u2026 we leverage the entropy of the user\u2019s browsing history distribution over a set of pre-defined topics as a measure of privacy.",
      "Given some set of m topics, we assign each link in the browsing history to one of those topics, sum counts when grouping by topic, and then normalise them to give the topic probability distribution.",
      "Using j to range over topics,  to represent the topic probability distribution for a user  , and  to represent the topic probability for user  and topic  , then have have:  The higher this metric is, the greater the privacy.",
      "For utility, we want the opposite: a similar distribution between the modified history,  , and the true history  .",
      "For some similarity function sim, the formula used is:  For this work cosine similarity is used:  Navigating the privacy-utility trade-off with PBooster  PBooster seeks to optimise an objective function  , where  controls the desired privacy/utility trade-off.",
      "It is worthwhile to mention that the search space for this problem is exponential to  , where N is the maximum number of links with respect to a topic.",
      "That\u2019s about as bad as it gets!",
      "The solution is to break the search down into two subproblems: first select a subset of topics, and calculate how many links should be added to each (the topic selection phase, and then select the links for each topic (link selection phase).",
      "An analysis in section 4.4 of the paper shows that the topic selection process is also NP-hard, but there exists a greedy local search algorithm which can achieve at least 1/3 of the value of the optimal solution.",
      "The algorithm alternates between incrementing the link counts of topics and decrementing them, until no further improvement is possible.",
      "We emphasize that according to [13], there is no efficient algorithm which could select the best set of links to maximize aggregation of both privacy and utility in polynomial time\u2026 the proposed greedy algorithm can select a set with a lower bound of 1/3 of the optimal solution, providing the maximum user privacy and utility in polynomial time.",
      "When it comes to selecting actual links to hit the desired topic counts, PBooster needs access to the user\u2019s friend list on social media.",
      "It randomly selects a social media profile from outside of your friend list, and simulates a browsing history from that profile for links in the desired topic areas.",
      "If the chosen social media profile does not contain enough such links, another profile is chosen at random until all link count targets are hit.",
      "Evaluation  The evaluation compares PBooster against a strategy of adding the total number of links recommended by the topic selection phase but at random with no consideration for topics, a \u2018JustFriends\u2019 strategy that adds links from the profiles of friends during the link selection phase, and an \u2018ISPPolluter\u2019 strategy that adds 20,000 links randomly to the history.",
      "As we saw previously, since random strategies pay no attention to utility, they can achieve the best overall privacy:  Interestingly, ISPPolluter turned out not to work well in practice as a defense against attacks that leverage information from social media feeds.",
      "For intuition, consider that even adding 20,000 random links, a history with say 50 visits to The Morning Paper is still going to be pretty unique (that is very unlikely to happen by chance).",
      "Whereas when links are added from a social media profile, this kind of behaviour is generated by design.",
      "However, as I understand it the end result of using PBooster will be a history that instead of containing influences from n social media profiles, will now contain influences from n+m profiles, where m is the number of profiles used during the link selection phase.",
      "The measure used for privacy only looks at topic distribution, not the end number of influencing profiles.",
      "I\u2019d really like to see some analysis of this aspect (for example, if m is small compared to n, then the user is probably still very identifiable, so how should we think about m as it relates to n?).",
      "When it comes to balancing the trade-off between utility and privacy as measured though, PBooster unsurprisingly comes out best.",
      "Our experiments demonstrate the efficiency of the proposed model by increasing user privacy and preserving utility of browsing history for future applications."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://arxiv.org/pdf/1811.09340",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 84480736
  },
  {
    "blog_id": "algorithmic-glass-ceiling-in-social-networks-the-effects-of-recommendation-on-social-diversity",
    "summary": [
      "Algorithmic glass ceiling in social networks: the effects of social recommendations on network diversity Stoica et al., WWW\u201918  (If you don\u2019t have ACM Digital Library access, the paper can be accessed either by following the link above directly from The Morning Paper blog site, or from the WWW 2018 proceedings page).",
      "Social networks were meant to connect us and bring us together.",
      "This paper shows that while they might be quite successful at doing this in the small, on a macro scale they\u2019re actually doing the opposite.",
      "Not only do they reinforce and sustain disparities among groups, but they actually reinforce the rate at which disparity grows.",
      "I.e., they\u2019re driving us apart.",
      "This happens due to the rich-get-richer phenomenon resulting from friend/follow recommendation algorithms.",
      "\u2026 we find that prominent social recommendation algorithms can exacerbate the under-representation of certain demographic groups at the top of the social hierarchy\u2026 Our mathematical analysis demonstrates the existence of an algorithmic glass ceiling that exhibits all the properties of the metaphorical social barrier that hinders groups like women or people of colour from attaining equal representation.",
      "Organic growth vs algorithmic growth  \u201cIn the social networks now governing the knowledge, jobs and deals one can seek, what matters most today is one\u2019s position in the graph of advantageous connections.\u201d It takes time and effort to build and maintain  your connections.",
      "One of the key tools that social networks provide to help with this is algorithm recommendation of connections.",
      "I.e., \u201cpeople you may know\u201d on Facebook, \u201cwho to follow\u201d on Twitter, and \u201csuggested accounts\u201d on Instagram.",
      "Unsurprisingly, and by design, these suggestions influence the networks that people end up building.",
      "Homophily \u2014 a tendency of individuals to favour interactions with similar peers \u2014 influences even organic connection growth (i.e., connections that are built up in the absence of algorithmic recommendations).",
      "When we combine homophily with algorithmic connection growth (i.e., connections that are built up in the presence of algorithmic recommendations), the advantage of the majority group is exacerbated.",
      "We build on a growing body of evidence that online services (including Twitter, TaskRabbit, and Airbnb) can reproduce well-know prejudices against historically under-represented groups.",
      "Issues raised include disparate treatment and evidence of a metaphorical glass ceiling.",
      "The latter denotes an invisible barrier preventing given demographics (most commonly females) from reaching superior levels of recognition.",
      "The conditions for declaring the presence of a glass ceiling impacting a given demographic group are:  The chances of advancement to a higher level are uneven for members of that group  The disparity is not explained by task-relevant characteristics  The inequality increases for higher and higher levels.",
      "Consider the following chart, concentrating just on the blue circles to start with.",
      "You\u2019re looking at Instagram data collected over multiple months of 2014 and 2015, before Instagram\u2019s \u201csuggested accounts\u201d feature was introduced.",
      "That is, it\u2019s the result of organic growth.",
      "The horizontal dashed line shows the fraction of female users in the overall dataset (54.43%).",
      "On the x-axis we have increasing degree, and on the y-axis the percentage of female users among nodes with that degree.",
      "Women appear under-represented at the top of the hierarchy, but they are not completely excluded.",
      "The paper just rolls straight past this factual observation, since the authors are mostly interested in the comparison to algorithmic growth, but it\u2019s worth pausing for a moment to think about what\u2019s causing this result: it\u2019s not what you\u2019d expect from a straightforward \u2018rich get richer\u2019 model.",
      "It\u2019s due to differential homophily between men and women.",
      "Men show a stronger bias towards connecting to other men than women do for connecting to other women.",
      "This happens even when controlling for content productivity.",
      "Now take a second look at the chart, this time focusing on the blue and green circles.",
      "The x-axis is now representing the frequency with which a node is recommended as a connection, under approximations to two common recommendation strategies (the Adamic-Adar index and a random walk of length 2).",
      "There\u2019s a much sharper drop-off for recommendations of female users under these algorithmic growth conditions.",
      "You can see it even more clearly in this log-log scale plot:  This gap, exhibited to grow in log-log scale, is a sign of different power coefficients governing the statistical chance to reach at least x recommendations, depending on gender.",
      "As one progressively selects to retain only the most successful individuals, the aforementioned effect translates into a sudden and accelerating drop of the observed fraction of females.",
      "Again, simple homophily would suggest that females should be recommended more throughout, but differential homophily coupled with recommendation can rapidly overturn and invert the inequality.",
      "(Of course, if females showed stronger homophily than males, then the effect would be to even more rapidly increase the female advantage instead).",
      "Modelling growth  Consider a network with red and blue labelled nodes.",
      "In the organic growth model we have the following rules;  A new node  enters the network and receives label R with probability r, and B with probability 1-r, where r is less than 0.5.",
      "Then repeat the following steps until an edge is formed:  With probability  , the new node  choose an existing node  at random.",
      "With probability  ,  choose a node uniformly at random and copy one of its edges.",
      "This model the \u2018rich get richer\u2019 effect.",
      "If the new node has a different label than the node it chooses to connect to, the connection is accepted with probability  .",
      "(If it has the same label, it is always accepted).",
      "For algorithmic growth we make the following changes.",
      "Once a new node has entered the network as above, it connects through organic growth with probability  .",
      "With probability  it will instead select a node  uniformly at random and follow a random walk of length 2 to choose a node.",
      "The chosen node is accepted if it is the same colour, and accepted with probability  if it is a different colour.",
      "( Enlarge )  A power inequality effect exists for the red nodes if the average degree of a red node ends up lower than the average degree of a blue node.",
      "A tail glass ceiling effect exists if there exists an increasing function k(n) for short k such that:  The algorithmic glass ceiling  \u2026 the algorithms we analyzed do not create bias ex nihilo, but simply amplify bias in networks where it exists.",
      "In section 5 of the paper the authors show the following under the organic model:  The rate of growth of edges towards the red population converges towards a constant  .",
      "The in-degree distributions of the two populations follow power laws with different coefficients  And under the algorithmic model:  The sum of degrees of the red nodes converges to a constant that is smaller than r, and even smaller than in the organic case: $latex \\alpha_2 < \\alpha \u2026 the sharp amplification of the glass ceiling effect by an algorithm is an entirely new result, which has no equivalent that we know of.",
      "It is a special case of a widely open problem: how to correct a seemingly neutral algorithm when the structure it exploits is not fair to begin with\u2026 Unfortunately, without a deep understanding of the cause and reverberation of bias, any post-hoc correction can be harshly debated.",
      "Our paper offers an alternative way: identify some structural causes of the emerging unfairness, and require algorithms to be designed in a way that leverages structure while carefully avoiding those biases in the presence of the above conditions."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3178876.3186140?download=true",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 28727146
  },
  {
    "blog_id": "race-trees",
    "summary": [
      "Boosted race trees for low energy classification Tzimpragos et al., ASPLOS\u201919  We don\u2019t talk about energy as often as we probably should on this blog, but it\u2019s certainly true that our data centres and various IT systems consume an awful lot of it.",
      "So it\u2019s interesting to see a paper using nano-Joules per prediction as an evaluation metric.",
      "The goal is to produce a low-energy hardware classifier for embedded applications doing local processing of sensor data.",
      "To get there, the authors question a whole bunch of received wisdom, beginning with this: do we really need to convert the analog sensor data into a digital signal?!",
      "Here\u2019s another fun one: what if instead of being something you worked hard to avoid, you had to build your whole application based on the outcomes of data races??!",
      "Typically, a sensor gathers analog information from the physical world and then converts it into a conventional digital signal\u2026 While this binary-represented integer is perfectly efficient for storage as bits in memory and for typical general purpose computing operations, it is unclear that this is the most efficient for our target application.",
      "One such possible representation is pure analog signalling.",
      "Of course analog signalling comes with a whole bunch of challenges of its own, which is one of the reasons we tend to convert to digital.",
      "But here the authors seem to have found a perfect marriage of a class of logic called race logic, a natural encoding for sensor data, and the classification use case.",
      "We argue that race logic fits nicely with temporally-coded sensing systems,, such as 3D depth perception systems and dynamic vision sensors, where the time that a signal gets \u201cexcited\u201d to a logic-level \u201c1\u201d depends on the magnitude of the sensor reading.",
      "Furthermore, we demonstrate that the structure of race logic makes it a natural encoding for decision tree-based approaches.",
      "The resulting system can integrate seamlessly into a scikit-learn based development process, and dramatically reduces the total energy usage required for classification with very low latency.",
      "Introducing race logic  Race logic encodes values by delaying signals.",
      "Signals can still be low or high (0 or 1), but it\u2019s the timing of the transition between those states which is all important.",
      "For example, a value of \u20182\u2019 can be represented by a signal that is low until time 2 and then high from then on.",
      "Race logic has four primary operations that are easy to implement in hardware: MAX, MIN, ADD-CONSTANT, and INHIBIT.",
      "For MAX the output of a gate should go high only when all of the inputs have arrived.",
      "A single AND gate between two wires is therefore all we need to implement MAX in the race domain.",
      "(That\u2019s kind of cool isn\u2019t it :) ).",
      "MIN is actually pretty straightforward as well \u2013 here we want the first signal to arrive so a single OR gate does the job.",
      "Since values are encoded by the time of the rising edge from 0 to 1, adding a constant value to a signal (ADD-CONSTANT) amounts to introducing the proportionate amount of delay to the signal.",
      "One efficient way of doing that in analog hardware is the use of current-starved inverters.",
      "INHIBIT takes two inputs: an inhibiting signal and a data signal.",
      "If the inhibiting signal arrives first, the output is prevented from ever going high.",
      "If the data signal arrives at the same time as, or before, the inhibiting signal, it is allowed to pass through unchanged.",
      "An INHIBIT function can be encoded using a single PMOS pass gate.",
      "Together this set of four operations allow us to deliberately engineer \u201crace conditions\u201d in a circuit to perform computation at low energy.",
      "Encoding decision trees using race logic  Take a decision tree, and turn it upside down so that instead of computation starting at the root and proceeding to an ultimately selected leaf, we start at the leaves and work our way to the root.",
      "One idea is to assign a unique delay-encoded label to each leaf, and then let the labels race each other on their journey to the root.",
      "The first one to arrive at the root is the winner, i.e., the correct label for this classification.",
      "Another way of looking at the problem is that each path from the tree root to a leaf corresponds to a unique conjunction of attribute tests.",
      "We can execute those tests in parallel rather than sequentially.",
      "When it comes to making a decision at a node in the tree, each node has a fixed threshold t, learned during the training phase, against which it compares the input value.",
      "So decision trees are essentially networks of threshold functions.",
      "This fits rather nicely with an INHIBIT gate.",
      "An end-to-end architecture  It\u2019s now time to put our race-logic encoded decision trees in the context of an end-to-end system.",
      "The first stage of analog-to-digital converters (ADC) often involves converting the analog signal into the temporal domain.",
      "\u201cSuch an approach leverages the small delays and high temporal resolution of nano-meter scale devices to get superior performance over their voltage domain counterparts.\u201d For our use case, we simply don\u2019t need the rest of the conversion (temporal to digital, TDC).",
      "Examples of systems providing directly time-encoded outputs without TDCs, range from visual sensors, such as Dynamic Vision Sensors (DVS), Asynchronous Time-based Image Sensors (ATIS), Time to First Spike (TTFS) and Time-of-Flight (ToF) cameras, to sound sensors; e.g. the AER (Address Event Representation) Ear, which is a silicon-based cochlea that converts auditory signals from various frequency bands to precise temporally coded outputs\u2026.",
      "the presented architecture can work with any temporally-coded input provided to it.",
      "A programmable race-logic accelerator is used for the decision trees, with threshold values encoded using a shift register.",
      "A decoder reads the race-tree output and turns it into a one-hot encoded memory address.",
      "For an ensemble learner, we have an ensemble of such trees, with a weighted voting scheme based on general binary adders.",
      "Integration with scikit-learn  The Race Trees accelerator is fully integrated into a sci-kit learn based development flow.",
      "Once the model is trained, our tool analyzes the importance of input features, explores the learners performance against lower resolution data, proceeds with votes (content of tree leaves) quantization.",
      "Finally, the user has to choose one of the following options: (a) the generation of a configuration file for the crossbar networks of a programmable Race Trees accelerator, or (b) the generation of a custom RTL-level design with hardwired units.",
      "Performance evaluation  The evaluation is performed using the MNIST dataset, since that has the most results available in the literature for comparison.",
      "Here\u2019s how Race Trees (green dots in the top LH corner) compare against other state-of-the-art low power classifiers when we look at energy vs accuracy:  An open-source implementation of race-logic primitives and example Race Trees can be found on GitHub.",
      "Using GradientBoosting, a classifier consisting of 1,000 race trees of depth 6 gets 97.45% accuracy at 31.35nJ/prediction.",
      "Using just 200 race trees it is still possible to achieve 95.7% accuracy, but now at just 7.8nJ/prediction.",
      "A comparison of Race Trees to other machine learning implementations in an accuracy vs energy-delay product scatter plot is presented in Figure 14 (below), and shows that RaceTrees achieve both lower delay and energy per operation than their counterparts.",
      "The paper\u2019s conclusion lists a number of areas for future work which indicate there\u2019s still plenty of room for further exploration and improvement, including for example the integration of other circuits such as vector-matrix multipliers that operate purely in the time domain.",
      "The quest for energy-efficient machine learning systems looks like it may take us to some very interesting places indeed!"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://sites.cs.ucsb.edu/~sherwood/pubs/ASPLOS-19-racetree.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 66912872
  },
  {
    "blog_id": "obfuscated-gradients-give-a-false-sense-of-security-circumventing-defenses-to-adversarial-examples",
    "summary": [
      "Obfuscated gradients give a false sense of security: circumventing defenses to adversarial examples Athalye et al., ICML\u201918  There has been a lot of back and forth in the research community on adversarial attacks and defences in machine learning.",
      "Today\u2019s paper examines a number of recently proposed defences and shows that most of them rely on forms of gradient masking.",
      "The authors develop attack techniques to overcome such defences, and 9 analyse defences from ICLR 2018 claiming to protect against white-box attacks.",
      "7 of these turn out to rely on obfuscated gradients, and 6 of these fall to the new attacks (and the other one partially succumbs).",
      "Athalye et al. won a best paper award at ICML\u201918 for this work.",
      "One of the great things about work on adversarial attacks and defences, as we\u2019ve looked at before, is that they illuminate the strengths and weaknesses of current technology.",
      "Depending on the threat model you choose, for my own part I\u2019m currently of the opinion that we\u2019re unlikely to find a robust adversarial defence without a more radical re-think of how we\u2019re doing image classification.",
      "If we\u2019re talking about the task of \u2018find an image that doesn\u2019t fool a human, but does fool a neural network\u2019 then I think there will always be gaps to exploit until we incorporate more human-like (e.g., higher level semantic understanding) reasoning into our classifiers.",
      "Not that the human system itself can\u2019t be fooled of course (e.g., optical illusions).",
      "Breaking gradient descent  A defense is said to cause gradient masking if it \u201cdoes not have useful gradients\u201d for generating adversarial examples; gradient masking is known to be an incomplete defense to adversarial examples.",
      "Despite this, we observe that 7 of the ICLR 2018 defenses rely on this effect.",
      "Some defenses break gradient descent deliberately, others may do it unintentionally.",
      "Here are five clues that something isn\u2019t right:  One-step attacks perform better than iterative attacks.",
      "Iterative attacks are strictly stronger, so this shouldn\u2019t be the case.",
      "If single-step methods are working better, it\u2019s a sign the iterative attack is becoming stuck at a local minimum.",
      "Black-box attacks work better than white-box attacks.",
      "The black-box threat model is a strict subset of white-box attacks, so white-box attacks should perform better.",
      "When a defense obfuscates gradients, then black-box attacks (which don\u2019t use it) often perform better.",
      "(Since practical black-box attacks are possible and we can also find e.g. universal adversarial perturbations the utility of a defense that excludes such attack modes seems rather limited anyway to me).",
      "Unbounded attacks do not reach 100% success.",
      "With unbounded distortion, any classifier should eventually fail.",
      "An attack that doesn\u2019t achieve this should be improved (i.e., it\u2019s a weak attack, not necessarily a strong defense).",
      "Random sampling finds adversarial examples (where a gradient-based attack does not).",
      "Increasing the distortion bound does not increase success.",
      "We expect a monotonically increasing attack success rate with increasing distortion bound.",
      "Three ways that a defense might break gradient descent are shattering, stochastic gradients, and exploding & vanishing gradients:  Shattered gradients are caused when a defense is \u201cnon-differentiable, introduces number instability, or otherwise causes a gradient to be nonexistent or incorrect.\u201d  Stochastic gradients are a result of randomization \u2013 either introduced in the network itself, or in the inputs being fed to the network  Exploding and vanishing gradients are often caused by defenses that consist of multiple iterations of neural network evaluation, feeding the output of one computation into the next.",
      "Overcoming masked gradients  Shattered gradients can be overcome using a technique the authors call \u2018Backward Pass Differentiable Approximation\u2019 (BPDA).",
      "Think of a secured neural network (i.e., one that has been hardened against adversarial attacks) as being a composition of some hardening function  and a regular pre-trained classifier  such that the hardened classifier  .",
      "For example,  might be a denoising function designed to remove perturbations introduced by an adversary.",
      "If  itself is smooth and differentiable we can often compute gradients through the combined network  .",
      "Even if it isn\u2019t differentiable, we know that by construction in a local area it is close to the identify function (i.e. ).",
      "So we conduct an attack by performing forward propagation through the neural network as usual, but replacing  with the identity function (or even simpler, just the evaluation of  ) during backpropagation.",
      "The full (general) version of BPDA operates by finding and replacing any non-differentiable layer  with a differentiable approximation  such that  .",
      "As before, we use the vanilla network on the forward pass, and substitute  on the backward pass.",
      "To attack a network that relies on stochastic gradients, it is necessary to estimate the gradient of the stochastic function.",
      "\u2018 Expectation over Transformation \u2019 (EOT) can be used to compute the gradient over an expected transformation to an input.",
      "Given a transformation function  EOT optimises the expectation over  .",
      "\u201cThe optimization problem can be solved through gradient descent\u2026 differentiating through the classifier and transformation, and approximating the expectation with samples at each gradient descent step.\u201d  Vanishing or exploding gradients can be addressed through reparameterization.",
      "Suppose differentiating  leads to a problem.",
      "We can introduce a new differentiable function  such that  and  for all  .",
      "We can now compute gradients through  and circumvent the defense.",
      "(\u00a74.3).",
      "How the ICLR 2018 defenses stack up  Section 5 of the paper works through nine defenses from ICRL 2018 that claim robustness in a white-box threat model.",
      "Seven of the nine turn out to rely on some form of obfuscated gradients.",
      "For these the authors use the techniques described above to successfully attack them.",
      "Thermometer encoding is an example of a defense that relies on gradient shattering at its core.",
      "Image pixels are mapped to l-dimensional vectors such as \u2018111110000\u2019, where the number of leading 1\u2019s indicates the temperature (colour value) of a pixel.",
      "A BPDA attack reduces model accuracy to 30% (worse than if thermometer encoding were not used at all!).",
      "Examples of transformation based defenses include image cropping and rescaling, bit-depth reduction, JPEG compression, randomly dropping pixels and replacing them using total variance minimization, and image quilting (reconstructing images by replaces small patches with patches taken from \u2018clean\u2019 images).",
      "It is possible to bypass each defense independently (and ensembles of defenses usually are not much stronger than the strongest sub-component)\u2026 with our attack, we achieve 100% targeted attack success rate and accuracy drops to 0% for the strongest defense under the smallest perturbation budget\u2026  Image cropping and rescaling are circumvented using EOT.",
      "Bit depth reduction and JPEG compression are circumvented using BPDA and the identity function on the backward pass.",
      "Random pixel replacement and quilting are defeated using a combination of EOT and BPDA.",
      "PixelDefend and Defense-GAN both use generators to project input images back onto a data manifold before feeding them into a classifier.",
      "Informally, this is intended to \u2018purify\u2019 them by avoiding low-probability regions in the data distribution.",
      "PixelDefend can be defeated using BPDA, and DefenseGAN can also be evaded with BPDA, though only at a 45% success rate.",
      "See section 5 in the paper for more details and discussion of the other defenses.",
      "\u2026 we hope that future work will be able to avoid relying on obfuscated gradients (and other methods that only prevent gradient descent-based attacks) for perceived robustness, and use our evaluation approach to detect when this occurs."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://arxiv.org/pdf/1802.00420",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 87208689
  },
  {
    "blog_id": "chimera",
    "summary": [
      "Chimera: Large-Scale Classification Using Machine Learning, Rules, and Crowdsourcing \u2013 Sun et al. 2014 (WalmartLabs)  Large-scale classification, where we need to classify hundreds of thousands or millions of items into thousands of classes, is becoming increasingly common in this age of Big Data\u2026 So far, however, very little has been published on how large-scale classification has been carried out in practice, even though there are many interesting questions about such cases.",
      "Today\u2019s paper is a case study on large-scale classification of products at Walmart.",
      "The requirement is to classify 10M+ products into 5000+ categories based on fairly minimal product descriptions.",
      "Oh, and new products turn up all the time, and the set of categories is continuously evolving.",
      "Many learning solutions assume that we can take a random sample from the universe of items, manually label the sample to create training data, then train a classifier.",
      "At this scale, however, we do not even know the universe of items, as product descriptions keep \u201ctrickling in\u201d, a few tens of thousands or hundreds of thousands at a time\u2026 concept drift becomes common (e.g., the notion \u201ccomputer cables\u201d keeps drifting because new types of computer cables keep appearing).",
      "Walmart vendors submit product descriptions for their new products continuously.",
      "The way in which they fill-in the product detail information varies wildly from vendor to vendor, and asking them to be more thorough or detailed doesn\u2019t really work.",
      "So the only field that classification really can rely on is \u2018Title\u2019.",
      "Creating training data manually is time-consuming \u2013 a good analyst can classify about 100 items a day.",
      "(Matches to the correct category can take some research \u2013 where do you put a \u2018Dynomax Exhaust 17656 Thrush Welded Muffler\u2019 when you have 150 automotive categories and none of them are \u2018Mufflers\u2019?).",
      "Given the rate of 100 product items per day per analyst, it would take 200 days for a team of 5 analysts to manually classify 100,000 items.",
      "Thus, asking analysts to manually classify incoming product items is clearly not a practical solution.",
      "To label just 200 items per product category as training data would require labeling 1M product items.",
      "Outsourcing (in-place of using in-house analysts) is prohibitively expensive \u2013 at $10/hour it would cost Walmart $770K to get 1 million items classified, \u201can unacceptable cost to us.\u201d Crowdsourcing (E.g. Mechanical Turk) doesn\u2019t work very well because the classification task is too complex compared to the preferred micro tasks on those platforms that take a few tens of seconds to answer yes or no.",
      "If you can\u2019t get enough training data, perhaps a rules-based approach will work instead?",
      "\u201cBut writing rules to cover all 5000+ product types is a very slow and daunting process.",
      "In fact, we did not find it to be scalable.\u201d  Since none of the approaches (manual classification, machine learning, and rules) can solve the problem in isolation, Walmarts Chimera system uses all three in combination:  Chimera uses a combination of machine learning, hand-crafted rules, developers, analysts, and crowd workers to form a solution that continuously improves over time, and that keeps precision high while trying to improve recall.",
      "Chimera is initialised using a basic set of training data and hand-crafted rules supplied by analysts.",
      "The learning-based classifiers are trained using this initial data,  and the system then proceeds to iterate as follows:  Given a set of incoming product items, classify them, then use crowdsourcing to continuously evaluate the results and flag cases judged incorrect by the crowd.",
      "The analysts examine the flagged cases, and fix them by writing new rules, relabeling certain items, and alerting the developers.",
      "The newly created rules and the relabeled items are incorporated into the system, and the developers may fine tune the underlying automatic algorithm.",
      "For those items that the system refuses to classify (due to a lack of confidence), the analysts examine them, then create hand-crafted rules as well as training data (i.e., labeled items) to target those cases.",
      "The newly created rules and training data are incorporated into the system, and it is run again over the product items.",
      "Through this loop, the system continuously improves its performance.",
      "The big picture looks like this:  A new product item is first examined by the GateKeeper.",
      "If the GateKeeper can trivially classify it with high confidence (for example, it\u2019s an exact match to training data) then the result is immediately sent to the results processing phase (for sampling & possible evaluation).",
      "Otherwise the item is passed to three different classification systems in parallel:  A rules-based system that uses white-list and black-list regexs on product title text.",
      "(Yes, the thought of thousands of regexs sounds a handful, but each regex is quite simple based on the examples shown.",
      "For example: \u2018wedding bands?",
      "-> rings).",
      "An attribute and value-based classifier that examines attributes and uses a rules-based approach to classify based on them.",
      "For example, if the product has an ISBN, classify it as a book.",
      "A machine learning ensemble using naive-Bayes, K-Nearest Neighbours, and Perceptron classifiers.",
      "Given an item, all classifiers will make predictions (each prediction is a set of product types optionally with weights).",
      "The Voting Master and the Filter combine these predictions into a final prediction.",
      "The pair (product item, final predicted product type) is then added to the result set.",
      "Why is there a need for a filter step after the voting master?",
      "Once the voting master has produced a combined ranked list of product types, the filter applies a set of rules (created by the analysts) to exert a final control over the output types.",
      "Note that analysts can already control the rule-based classifiers.",
      "But without the filter, they do not have a way to control the output of the learning-based classifier as well as the voting master, and this can produce undesired cases.",
      "For example, learning-based classifiers may keep misclassifying \u201cnecklace pendant\u201d as of type \u201cnecklace\u201d (because we have many items of type \u201cnecklace\u201d in the training data that do contain the word \u201cnecklace\u201d).",
      "As a result, the voting master may produce \u201cnecklace\u201d as the output type for \u201cnecklace pendant\u201d.",
      "The analysts can easily address this case by adding a rule such as \u201cpendant \u2192 NOT necklace\u201d to the filter.",
      "Items that could not be classified with sufficient confidence are passed to the in-house analysts.",
      "Items that have been classified with sufficient confidence are randomly sampled to be sent to crowdsourced workers for category verification.",
      "If the crowd indicates an item was wrongly classified it is sent to the analysts who then create rules to correctly classify it in the future.",
      "As of March 2014, the system had been stable for about 6 months and classified about 2.5M items from marketplace vendors.",
      "Overall we managed to classify more than 90% of them with 92% precision.",
      "Chimera has also been applied to 14M items from walmart.com.",
      "Overall it classified 93% of them with 93% precision.",
      "The authors summarise six main lessons learned while building Chimera:  Things break down at large scale  Both learning and hand-crafted rules are critical  Crowdsourcing is critical, but must be closely monitored  Crowdsourcing must be coupled with in-house analysts and developers  Outsourcing does not work at a very large scale  Hybrid human-machine systems are here to stay\u2026  While academia has only recently started exploring such systems, largely in the context of crowdsourcing, they have been used for years in industry, as evidenced by Chimera and other systems that we know.",
      "Such systems use not just crowd workers, but also analysts, and developers, and treat them as \u201cfirst-class citizens\u201d.",
      "They have been quite successful, and deserve much more attention and closer studies."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://pages.cs.wisc.edu/~anhai/papers/chimera-vldb14.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 52464539
  },
  {
    "blog_id": "the-paradigms-of-programming",
    "summary": [
      "The paradigms of programming Floyd, CACM 1979  (Also available in  )  A couple of weeks ago we looked at Dan Bernstein\u2019s very topical \u201c thoughts on security after ten years of qmail 1.0 .\u201d From the general reaction I can tell that lots of you enjoyed reading that paper, but in the discussions that I saw, no-one was picking up on what I see as the real underlying secret to Bernstein\u2019s success and progression as a software engineer.",
      "(Perhaps because it is one level of indirection away from the main topic of security in that paper).",
      "Here is my favourite extract again:  For many years I have been systematically identifying error-prone programming habits \u2014 by reviewing the literature, analyzing other people\u2019s mistakes, and analyzing my own mistakes \u2014 and redesigning my programming environment to eliminate those habits.",
      "In today\u2019s paper choice we\u2019ll be looking at some other ways of systematically improving your skills over time (along with quite a few other gems).",
      "In 1978 Professor Robert Floyd was presented with the ACM Turing Award for \u201chelping to found the following important subfields of computer science: the theory of parsing, the semantics of programming languages, automatic program verification, automatic program synthesis, and analysis of algorithms.\u201d Not a bad list!",
      "\u201cThe paradigms of programming\u201d is his acceptance speech.",
      "Today I want to talk about the paradigms of programming, how they affect our success as designers of computer programs, how they should be taught, and how they should be embodied in our programming languages.",
      "Dominant at the time was the idea of structured programming (whose ideas are still very much with us today of course).",
      "The notion of starting with a top-down, stepwise refinement of the problem, and then building upwards from the primitives of the underlying machine to \u2018meet in the middle\u2019 with a set of more abstract modules and functions to be used by the top-down design.",
      "See e.g. \u2018 Program development by stepwise refinement \u2019, and \u2018 On the criteria to be used in decomposing systems into modules \u2019.",
      "The structured programming paradigm is useful, says Floyd, but it\u2019s not the only one.",
      "Programming paradigms are at the heart of this paper \u2013 and a reasonable interpretation of what Floyd means by paradigm here is, I think, \u2018a strategy or tactic for solving a class of problems.\u2019 That sounds a bit like a design pattern when I say it that way, but the examples Floyd gives us are at a slightly more fundamental level than those the phrase \u2018design patterns\u2019 conjures in my mind.",
      "Far more powerful than how many languages you know (in terms of syntax), is how many paradigms you are fluent with.",
      "I believe that the current state of the art of computer programming reflects inadequacies in our stock of paradigms, and in the way our programming languages support, or fail to support, the paradigms of their user communities.",
      "Computer science quickly breaks down into communities each with its own languages and dominant paradigms.",
      "The problem of falling into one of these and not escaping is that it becomes hard to see the fundamentals afresh and discover new approaches.",
      "Quoting from Kuhn in \u2018The Structure of Scientific Revolutions\u2019 :  The study of paradigms, including many that are far more specialized than those named illustratively above, is what mainly prepares the student for membership in the particular scientific community with which he will later practice.",
      "Because he there joins men who learned the bases of their field from the same concrete models, his subsequent practice will seldom evoke overt disagreement over fundamentals.",
      "John Cocke invented the dynamic programming paradigm to solve a problem with the efficient parsing of context-free languages.",
      "Floyd discovered recursive coroutines as a structure while building hierarchical top-down parsers.",
      "John Cocke\u2019s experience and mine illustrate the likelihood that continued advance in programming will require the continuing invention, elaboration, and communication of new paradigms.",
      "On developing as a programmer  So much for the advancement of the field, what about developing your own skills?",
      "If the advancement of the general art of programming requires the continuing invention and elaboration of paradigms, advancement of the art of the individual programming requires that he expand his repertory of paradigms.",
      "Here\u2019s the technique that Floyd used to expand his own capabilities.",
      "After solving a challenging problem, I solve it again from scratch, retracing only the insight of the earlier solution.",
      "I repeat this until the solution is as clean and direct as I can hope for.",
      "Then I look for a general rule for attacking similar problems, that would have led me to approach the given problem in the most efficient way the first time.",
      "Often, such a rule is of permanent value.",
      "It can be hard to gain exposure to new paradigms from within your own immediate environment, because it\u2019s likely your colleagues are all working within the same local paradigm set \u2014 witness job advertisements that specify the desired programming language (\u201cThe rules of Fortran can be learned within a few hours; the associated paradigms take much longer,  both to learn and unlearn.\u201d).",
      "Floyd writes of an eye-opening experience of visiting MIT and seeing the power of Lisp first-hand (as someone grown up more in the tradition of Algol-like languages).",
      "\u2026 my message to the serious programmer is to spend a part of your working day examining and refining your own methods.",
      "Even though programmers are always struggling to meet some future or past deadline, methodological abstraction is a wise long term investment.",
      "On designing (and evaluating) programming languages  Everyone wants to design a new programming language.",
      "Bah!",
      "Floyd doesn\u2019t find much satisfaction in the incremental extensions to existing languages (example: adding variant records to Pascal).",
      "Instead, it\u2019s far more important to look at the paradigms a language supports.",
      "I believe that the continued advance of programming as a craft requires the development and dissemination of languages which support the major paradigms of their user\u2019s communities.",
      "The design of a language should be preceded by enumeration of those paradigms, including a study of the deficiencies in programming caused by discouragement of unsupported paradigms\u2026 If there is ever a science of programming language design, it will probably consist largely of matching languages to the design methods they support.",
      "It\u2019s not just the programming language itself of course, \u201cthe entire environment in which we program, diagnostic systems, files systems, editors, and all, can be analyzed as supporting or failing to support the spectrum of methods for design of programs.\u201d  To persuade me of the merit of your language, you must show me how to construct programs in it.",
      "I don\u2019t want to discourage the design of new languages; I want to encourage the language designer to become a serious student of the details of the design process.",
      "On teaching programming  We have an unfortunate obsession with form over content (Floyd is speaking in 1978 remember, not a lot has changed in the intervening 40 years!).",
      "You can feel Floyd\u2019s heart sink in the following exchange:  If I ask another professor what he teaches in the introductory programming course, whether he answers proudly \u201cPascal\u201d or diffidently \u201cFORTRAN,\u201d I know that he is teaching a grammar, a set of semantic rules, and some finished algorithms, leaving the students to discover, on their own, some process of design.",
      "We would do better to explicitly teach a set of systematic methods for all levels of program design.",
      "Students trained this way \u201chave a large head start over those conventionally taught.\u201d  To the teacher of programming\u2026 I say: identify the paradigms you use, as fully as you can, then teach them explicitly.",
      "They will serve your students when Fortran has replaced Latin and Sanskrit as the archetypal dead language.",
      "How many paradigms do you have in your toolbox?"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://pdfs.semanticscholar.org/a57d/cde5113855aec888b2a4e1fdd6e3956ce2e6.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 75962230
  },
  {
    "blog_id": "reactive-vega",
    "summary": [
      "Reactive Vega: A Streaming Dataflow Architecture for Declarative Interactive Visualization \u2013 Satyanarayan et al. 2015  Today\u2019s paper choice combines Event-driven FRP (E-FRP) with dataflow and stream management techniques from the database community to implement declarative interactive visualisations on top of the existing Vega declarative visualisation grammar and supporting runtime .",
      "As a good example of what\u2019s possible, take a look at this interactive visualization of US airports in the Live Vega Editor :  (Note the \u201csignals\u201d section in the mark-up).",
      "In contrast with existing reactive visualization toolkits where only interaction events are modeled as time-varying, Reactive Vega features a unified data model in which the input data, scene graph elements, and interaction events are all treated as first-class streaming data sources.",
      "Functional Reactive Programming (FRP) models mutable values as continuous, time-varying data streams.",
      "We focus on a discrete variant called Event-Driven FRP (E-FRP).",
      "To capture value changes as they occur, E-FRP provides streams, which are infinite time-ordered sequences of discrete events.",
      "Streams can be composed into signals to build expressions that react to events.",
      "The E-FRP runtime constructs the necessary dataflow graph such that, when a new event fires, it propagates to corresponding streams.",
      "Dependent signals are evaluated in a two-phase update: signals reevaluated in the first phase use prior computed values of their dependents, which are subsequently updated in the second phase.",
      "To efficiently support relational data, Reactive Vega integrates methods from the streaming database literature (Aurora, Eddies, STREAM, TelegraphCQ, Borealis).",
      "And to support streaming hierarchical data, Reactive Vega\u2019s dataflow graph dynamically rewrites itself at runtime, instantiating new branches to process nested relations.",
      "In Vega\u2019s declarative visualization design, visual encodings are defined by composing graphical primitives called marks (arcs, bars, lines, symbols and text for example).",
      "Marks are associated with datasets, and their specifications describe how tuple values map to visual properties such as position and color.",
      "Scales and guides (i.e., axes and legends) are pro- vided as first-class primitives for mapping a domain of data values to a range of visual properties.",
      "Special group marks serve as containers to express nested or small multiple displays.",
      "Child marks and scales can inherit a group mark\u2019s data, or draw from independent datasets.",
      "Here\u2019s a declarative specification for a brushing interaction:  Our approach draws on Event- Driven Functional Reactive Programming (E-FRP) to abstract input events as time-varying streaming data.",
      "An event selector syntax facilitates composing and sequencing events together, for example \u2018[mousedown, mouseup] > mousemove\u2019 is a single stream of mousemove events that occur between a mousedown and mouseup (i.e., \u201cdrag\u201d events).",
      "Event streams are modeled as first-class data sources and can thus drive visual encoding primitives, or be run through the full gamut of data transformations.",
      "For added expressivity, event streams can be composed into reactive expressions called signals.",
      "Signals can be used directly to specify visual primitive properties.",
      "For example, a signal can dynamically determine a mark\u2019s fill color or a scale\u2019s domain.",
      "Signals can also parameterize interactive selection rules for visual elements called predicates.",
      "Predicates define membership within the selection (e.g., by specifying the conditions that must hold true) and can be used within sequences of production rules to drive conditional visual encodings.",
      "Under the Covers  Dataflow operators are instantiated and connected by the Reactive Vega parser, which traverses a declarative specification containing definitions for input datasets, visual encoding rules, and interaction primitives as described in \u00a7 3.",
      "When data tuples are observed, or when interaction events occur, they are propagated (or \u201cpulsed\u201d) through the graph with each operator being evaluated in turn.",
      "Propagation ends at the graph\u2019s sole sink: the renderer.",
      "As each dataset definition is parsed, a corresponding branch in the dataflow graph is constructed.",
      "The branches contain input and output nodes connected by a pipeline of data transformation operators.",
      "Input nodes receive raw tuples as a linear stream (tree and graph structures are supported via parent-child or neighbor pointers, respectively).",
      "Upon data source updates, tuples are flagged as either added, modified, or removed, and each tuple is given a unique identifier.",
      "Data transformation operators use this metadata to perform targeted computation and, in the process, may derive new tuples from existing ones.",
      "Derived tuples retain access to their \u201cparent\u201d via prototypal inheritance.",
      "This relieves operators of the burden of propagating unrelated upstream changes.",
      "For every low-level event type required by the visualization (e.g. mousedown events) Vega instantiates an event listener node in the dataflow graph and directly connects it to dependent signals.",
      "In the case of ordered selectors (e.g., a \u201cdrag\u201d event specified by \u2018[mousedown, mouseup] > mousemove\u2019), each constituent event is connected to an automatically created anonymous signal; an additional anonymous signal connects them to serve as a gatekeeper, and only propagates the final signal value when appropriate.",
      "Individual signals can be dependent on multiple event nodes and/or other signals, and value propagation follows E-FRP\u2019s two-phase update.",
      "Generated scene graph elements are themselves modeled as data tuples, and thus can serve as the input data for further downstream visual encoding primitives.",
      "This enables higher-level layout algorithms to be expressed in a fully declarative fashion.",
      "The authors describe this as reactive geometry.",
      "Glitches are avoided through the use of a centralized dataflow graph scheduler that dispatches changesets to appropriate operators in topological order, thus ensuring that an operator is only evaluated after all of its dependencies are up to date.",
      "This centralization also allows more aggressive pruning of unnecessary computation:  (a) As the scheduler ensures a topological propagation ordering, a branch can be safely pruned for the current propagation if it has already been reflowed\u2026 (b) Skipping unchanged operators: Operators identify their dependencies\u2014including signals, data fields, and scale functions\u2014and changesets maintain a tally of updated dependencies as they flow through the graph.",
      "The scheduler skips evaluation of an individual operator if it is not responsible for deriving new tuples, or if a changeset contains only modified tuples and no dependencies have been updated.",
      "Both push- and pull-models are used to flow data.",
      "When an edge connects operators that work with the same data (e.g. a pipeline of data transformations for the same data source) then changesets are pushed along the edge.",
      "When an edge connects operators with external dependencies such as other data sources, signals, or scale functions then these edges are flagged as reflow changesets.",
      "External dependencies are connected to Collector nodes along these reflow changeset edges.",
      "Collectors propagate tuples forward to their dependents, which then request (pull) the latest versions of their dependencies from the scheduler.",
      "To support streaming nested data structures, operators can dynamically restructure the graph at runtime by extending newbranches, or pruning existing ones, based on observed data.",
      "These dataflow branches model their corresponding hierarchies as standard relations, thereby enabling subsequent operators to remain agnostic to higher-level structure.",
      "For example, a Facet operator partitions tuples by key fields; each partiion then propagates down a unique, dynamically-constructed dataflow branch, which can include other operators such as Filter or Sort.",
      "In order to maintain interactive performance, new branches are queued for evaluation as part of the same propagation in which they were created.",
      "To ensure changeset propagation continues to occur in topological order, operators are given a rank upon instantiation to uniquely identify their place in the ordering\u2026 The most common source of restructuring operations are scene graph operators, as building a nested scene graph is entirely data-driven.",
      "Reactive Vega is open source and has been merged with the existing Vega project.",
      "It is available at  [url]"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://idl.cs.washington.edu/files/2015-ReactiveVega-InfoVis.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 39674358
  },
  {
    "blog_id": "can-you-trust-the-trend-discovering-simpsons-paradoxes-in-social-data",
    "summary": [
      "Can you trust the trend?",
      "Discovering Simpson\u2019s paradoxes in social data Alipourfard et al., WSDM\u201918  In \u2018 Same stats, different graphs ,\u2019 we saw some compelling examples of how summary statistics can hide important underlying patterns in data.",
      "Today\u2019s paper choice shows how you can detect instances of Simpson\u2019s paradox, thus revealing the presence of interesting subgroups, and hopefully avoid drawing the wrong conclusions.",
      "For the evaluation part of the work, the authors look at question-answering on Stack Exchange (Stack Overflow, for many readers of this blog I suspect).",
      "We investigate how Simpson\u2019s paradox affects analysis of trends in social data.",
      "According to the paradox, the trends observed in data that has been aggregated over an entire population may be different from, and even opposite to, those of the underlying subgroups.",
      "Let\u2019s jump straight to an example.",
      "In Stack Exchange someone posts a question and other users of the system post answers (ignoring the part about the question first being deemed worthy of the forum by the powers-that-be).",
      "Users can vote for answers that they find helpful, and the original poster of the question can accept one of the answers as the best one.",
      "What factors influence whether or not a particular answer is accepted as the best one?",
      "One of the variables that has been studied here is when in a user\u2019s session an answer is posted.",
      "Suppose I\u2019m feeling particularly helpful today, and I log onto to Stack Overflow and start answering questions to the best of my ability.",
      "I answer one question, then another, then another.",
      "Is my answer to the first question more or less likely to be accepted as a best answer than my answer to the third question?",
      "If we look at the data from 9.6M Stack Exchange questions, we see the following trend:  It seems that the more questions I\u2019ve previously answered in a given session, the greater the probability my next answer will be accepted as the best one!",
      "That seems a bit odd.",
      "Do question answerers get into the flow?",
      "Do they start picking \u2018easier\u2019 questions (for them) as the session goes on?",
      "Here\u2019s another plot of exactly the same dataset, but with the data disaggregated by session length.",
      "Each different coloured line represents a session of a different length (i.e., sessions where the user answered only one question, sessions where the user answered two questions, and so on).",
      "When we compare sessions of the same length, we clearly see exactly the opposite trend: answers later in a session tend to fare worse than earlier ones!",
      "The truth is that \u201ceach successive answer posted during a session by a user on Stack Exchange is shorter, less well documented with external links and code, and less likely to be accepted by the asker as the best answer.\u201d  When measuring how an outcome changes as a function of an independent variable, the characteristics of the population over which the trend is measured may change as a function of the independent variable due to survivor bias.",
      "To illustrate, here\u2019s an example of that Stack Exchange data broken down by session length.",
      "Note the massive class imbalances (many more sessions of length one than length eight for example).",
      "When calculating acceptance probability for the aggregate data, consider the case of computing the probability that the third answer in a session is accepted as the best one.",
      "Of the 12.8M total data points, 9.6M of them aren\u2019t eligible to contribute to this analysis (sessions of length one or two).",
      "Thus there is a survivorship bias \u2013 when we get to the third answer, the first two have already failed to be accepted, indicating that perhaps the new answer is facing weaker competition.",
      "This increases the probability that the third answer will be accepted as the best.",
      "(And so on, as session lengths get longer and longer).",
      "\u2026 despite accumulating evidence that Simpson\u2019s paradox affects inference of trends in social and behavioral data, researchers do not routinely test for it in their studies.",
      "Identifying Simpson\u2019s paradoxes  We\u2019d like to know if a Simpson\u2019s paradox exists so that we don\u2019t draw the wrong conclusions, and also because it normally suggests something interesting happening in the data: subgroups of the population which differ in their behaviour in ways which are significant enough to affect aggregate trends.",
      "We propose a method to systematically uncover Simpson\u2019s paradox for trends in data.",
      "Let Y be the outcome being measured (e.g., the probability than an answer is accepted as the best one), and  be the set of m independent variables or features (e.g, the reputation of the answering user, the number of answers so far, and so on).",
      "The method finds pairs of variables  such that a trend in Y as a function of  disappears or reverses when the data is disaggregated by conditioning on  .",
      "If  is categorical, then we can simply group data by the unique values.",
      "For continuous various (or discrete variables with a very large range), you can bin the elements.",
      "The experiments in the paper used bins of fixed size, but other binning strategies are available.",
      "A trend in Y as a function of  can be expressed as  And the reverse trend when conditioned on  can be expressed as:  We\u2019re looking for pairs where both equation (1) and (2) are true simultaneously.",
      "The process starts out by fitting linear models.",
      "Let the relationship between Y and  be modelled by  (Here  is just the intercept of the regression function, and the trend parameter  quantifies the effect of  on Y).",
      "We can use a similar linear model (with different values of alpha and beta) for the conditioned expectation:  When fitting linear models  we have not only fitted a trend parameter  but also a p-value which gives the probability of finding an intercept  [  ??? ]",
      "at least as extreme as the fitted value under the null hypothesis  .",
      "From this, we have three possibilities:  is not statistically different from zero  is statistically different from zero and positive  is statistically different from zero and negative  By comparing the sign of  from the aggregated fit with the signs of the  s from the disaggregated fits we can test for Simpson\u2019s paradox.",
      "Although [our equations] state that the signs from the disaggregated curves should all be different from the aggregrated curve, in practice this is too strict, especially as human behavioral data is noisy.",
      "Thus, we compare the sign of the fit to aggregated data to the simple average of the signs of fits to disaggregated data.",
      "Here\u2019s the algorithm pseudocode:  Using the Stack Exchange data, the authors used this algorithm to find several instances of Simpson\u2019s paradox:  We looked at one of these earlier.",
      "Here\u2019s a breakdown of the paradox regarding acceptance probability versus the total number of answers posted by a user in their account lifetime.",
      "An analysis of the mathematical formulation of Simpson\u2019s paradox presented above also reveals two necessary conditions for a paradox to arise:  The distribution of the conditioning variable  must be dependent on  (i.e., as  changes, so does the distribution of values of  ).",
      "The expectation of  , conditioned on  , must not be independent of  .",
      "(I.e., for a given value of  , as  changes, so does the expected value of Y).",
      "The last word  Since social data is often generated by a mixture of subgroups, existence of Simpson\u2019s paradox suggests that these subgroups differ systematically and significantly in their behavior.",
      "By isolating important subgroups in social data, our method can yield insights into their behaviors."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://arxiv.org/pdf/1801.04385",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 24824894
  },
  {
    "blog_id": "information-flow-analysis-of-android-applications-in-droidsafe",
    "summary": [
      "Information-Flow Analysis of Android Applications in DroidSafe \u2013 Gordon et al. 2015  This is the first of three papers we\u2019ll be looking at this week from the NDSS\u201915 conference that took place earlier this month.",
      "DroidSafe is a tool that looks for potential leaks of sensitive information in Android applications.",
      "And it works incredibly well!",
      "DroidSafe detects all malicious information flow leaks inserted into 24 real-world Android applications by three independent, hostile Red-Team organizations.",
      "The previous state-of-the art analysis, in contrast, detects less than 10% of these malicious flows.",
      "The definition of sensitive data includes the unique device ID, sensor data (location, acceleration etc.",
      "), file data, image data and meta-data, email and SMS messages, passwords, network traffic and screen-shots.",
      "DroidSafe is a static analysis framework that analyzes the application before it executes.",
      "Given the size, richness, and complexity of the Android API and runtime (about 1.3 million lines of code) this is a significant challenge.",
      "It\u2019s important to detect leaks, and to be practical it\u2019s also important not to generate too many false positives.",
      "The Android API version 4.4.3 includes over 3,500 classes visible to an application developer.",
      "Analyzing the complete source code for the API is exceedingly difficult because it is implemented over multiple languages and some of the implementation is device-specific.",
      "Thus, static analysis frameworks rely on modelling the Android API semantics.",
      "Beyond the sheer scale of the challenge, event dispatching, callbacks, and inter-component communication all add to the difficulty.",
      "Event dispatching can lead to many different orderings of events, and event handlers are not called directly in application code.",
      "Callback handlers can include arguments passed by the runtime to the application for processing \u2013 which could include data from the application (tainted data), depending on the execution sequence prior to the event.",
      "Inter-component communication (ICC) is via Intent objects \u2013 and the resolution of an Intent destination is complex and may be dynamically determined.",
      "Starting with the Android Open Source Project (AOSP) source code, \u201cit quickly became apparent that the size and complexity of the Android environment made it necessary to develop the model and the analysis together as an integrated whole, with the design decisions in the model and the analysis working together synergistically to enable an effective solution to the Android static information-flow analysis problem.\u201d Stubs were developed to cover code outside of the AOSP Java codebase:  Examples of semantics missing in the AOSP and added via accurate analysis stubs include native methods; event callback initiation with accurate context; component life-cycle events; and hidden state maintained by the Android runtime and accessible to the application only via the Android API.",
      "For the information flow analysis, 4051 sensitive source methods and 2116 sensitive sink methods were manually identified and classified.",
      "In addition the implementation of 117 classes in the Java standard library and Android library were carefully simplified to increase precision and decrease analysis time (on the order of 5 to 10 minutes).",
      "At the core of the model are 550 Android classes that account for over 98.1% of the total calls made by over 95K applications downloaded from the Google Play Store.",
      "These were manually reviewed to confirm that the implementation fully covered semantics for data flow, object instantiation and aliasing, and that the event callbacks defined are called explicitly by the model with the proper context.",
      "At the core of the approach is an analysis method called \u2018Points-to\u2019 analysis:  Points-to analysis (PTA) is a foundational static program analysis that computes a static abstraction of all the heap locations that a pointer (reference) variable may point to during program execution.",
      "In addition to the points-to relation, points-to analysis also constructs a call graph as modern languages re-quire points-to results to calculate targets for dynamic dispatch and functional lambda calculations.",
      "The PTA implementation implements object sensitivity (identifying flows that originate in different object instances).",
      "\u201cObject sensitivity is notoriously difficult to understand and implement.\u201d It also requires large amounts of memory.",
      "Prior to optimising, several applications could not be analysed even with 64GB of heap memory.",
      "With optimisations, all tested applications now fit in under 34GB.",
      "The optimisation involved analysing a suite of Android applications to determine an appropriate context-depth (from 0 to 4) for each API class.",
      "(Traditional approaches use a fixed context depth).",
      "For inter-component communication, string analysis is performed to work out possible runtime targets using the JSA String Analyzer.",
      "This creates a regular expression representing the possible values of the string value.",
      "After JSA is run, we replace resolved string values in the application code with constants representing their computed regular expression, and perform a pass of our points-to analysis such that these values can be propagated globally.",
      "Additional transformations are made for ICC initiation calls and Android Service components to improve model precision.",
      "The resulting information flow analysis is built on top of the Soot Java Analysis framework and comprises approximately 70Kloc of Java code.",
      "Our information-flow analysis computes an over-approximation of all the memory states that occur during the execution of a program.",
      "The analysis is designed as a forward data-flow analysis.",
      "For each type of statement, we define a transfer function in terms of how it changes the state of memory.",
      "We divide memory into four separate areas that store local variables, instance fields, static fields, and arrays, reflecting the semantics of the Java programming language.",
      "Each of the memory areas is modelled as a function whose codomain consists of a set of information values.",
      "An information value is a tuple of the type of information and the source code location where the information was first injected.",
      "Our analysis can identify not only the kind of information being exfiltrated but the code location of the source.",
      "Results  We evaluate DroidSafe on 24 complete real-world Android applications that, as part of the DARPA Automated Program Analysis for Cybersecurity (APAC) program, have been augmented with malicious information flow leaks by three hostile Red Team organizations.",
      "The goal of these organizations was to develop information leaks that would either evade detection by static analysis tools or overwhelm static analysis tools into producing unacceptable results (by, for example, manipulating the tool into reporting an overwhelming number of false positive flows).",
      "DroidSafe accurately detects all of the 69 malicious flows in these applications (while reporting a manageable total number of flows).",
      "A current state-of-the-art Android information-flow analysis system, Flow-Droid + IccTA, in contrast, detects only 6 of the 69 malicious flows, and has a larger ratio of total flows reported to true malicious flows reported.",
      "DroidSafe was also evaluated against the DroidBench suite and gave the highest reported accuracy and highest precision for the suite to date at 94.3% and 87.6% respectively.",
      "Unsurprisingly, DroidSafe gets 100% accuracy and precision on its own test suite \u2013 but the next best tool could only achieve 34.9% accuracy and 79.9% precision.",
      "As these results illustrate, DroidSafe implements an analysis of unprecedented accuracy and precision.",
      "To the best of our knowledge, DroidSafe provides the first usable information-flow analysis for Android applications.",
      "The Secret Sauce  Our experience developing DroidSafe shows that 1) there is no substitute for an accurate and precise model of the application environment, and 2) using the model to drive the design decisions behind the analysis and supporting techniques (such as accurate analysis stubs) is one effective but (inevitably) labor-intensive way to obtain an acceptably precise and accurate analysis.",
      "As long as there are complex application frameworks, we anticipate that making an appropriate set of design decisions (such as the use of a scalable flow insensitive analysis) to successfully navigate the trade-off space that the application framework implicitly presents will be a necessary prerequisite for obtaining acceptable accuracy and precision."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://www.internetsociety.org/sites/default/files/02_1_2.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 50524744
  },
  {
    "blog_id": "beyond-news-contents-the-role-of-social-context-for-fake-news-detection",
    "summary": [
      "Beyond news contents: the role of social context for fake news detection Shu et al., WSDM\u201919  Today we\u2019re looking at a more general fake news problem: detecting fake news that is being spread on a social network.",
      "Forgetting the computer science angle for a minute, it seems intuitive to me that some important factors here might be:  what is being said (the content of the news), and perhaps how it is being said (although fake news can be deliberately written to mislead users by mimicking true news)  where it was published (the credibility / authority of the source publication).",
      "For example, something in the Financial Times is more likely to be true than something in The Onion!",
      "who is spreading the news (the credibility of the user accounts retweeting it for example \u2013 are they bots??)",
      "Therefore I\u2019m a little surprised to read in the introduction that:  The majority of existing detection algorithms focus on finding clues from the news content, which are generally not effective because fake news is often intentionally written to mislead users by mimicking true news.",
      "(The related work section does however discuss several works that include social context.).",
      "So instead of just looking at the content, we should also look at the social context: the publishers and the users spreading the information!",
      "The fake news detection system developed in this paper, TriFN considers tri-relationships between news pieces, publishers, and social network users.",
      "\u2026 we are to our best knowledge the first to classify fake news by learning the effective news features through the tri-relationship embedding among publishers, news contents, and social engagements.",
      "And guess what, considering publishers and users does indeed turn out to improve fake news detection!",
      "Inputs  We have  publishers,  social network users, and  news articles.",
      "Using a vocabulary of t words, we can compute an  bag-of-word feature matrix.",
      "For the m users, we can have an m x m adjacency matrix  , where  is 1 if i and j are friends, and 0 otherwise.",
      "We also know which users have shared which news pieces, this is encoded in a matrix  .",
      "The matrix  similarly encodes which publishers have published which news pieces.",
      "For some publishers, we can know their partisan bias.",
      "In this work, bias ratings from mediabiasfactcheck.com are used, taking just the \u2018Left-Bias\u2019, \u2018Least-Bias\u2019 (neutral) and \u2018Right-Bias\u2019 values (ignoring the intermediate left-center and right-center values) and encoding these as -1, 0, and 1 respectively in a publisher partisan label vector,  .",
      "Not every publisher will have a bias rating available.",
      "We\u2019d like to put \u2018-\u2019 in the entry for that publisher in  but since we can\u2019t do that, the separate vector  encodes whether or not we have a bias rating available for publisher p.  There\u2019s one last thing at our disposal: a labelled dataset for news articles telling us whether they are fake or not.",
      "(Here we have just the news article content, not the social context).",
      "The Tri-relationship embedding framework  TriFN takes all of those inputs and combines them with a fake news binary classifier.",
      "Given lots of users and lots of news articles, we can expect some of the raw inputs to be pretty big, so the authors make heavy use of dimensionality reduction using non-negative matrix factorisation to learn latent space embeddings (more on that in a minute!)",
      "TriFN combines:  A news content embedding  A user embedding  A user-news interaction embedding  A publisher-news interaction embedding, and  The prediction made by a linear classifier trained on the labelled fake news dataset  Pictorially it looks like this (with apologies for the poor resolution, which is an artefact of the original):  News content embedding  Let\u2019s take a closer look at non-negative matrix factorisation (NMF) to see how this works to reduce dimensionality.",
      "Remember the bag-of-words sketch for news articles?",
      "That\u2019s an n x t matrix where n is the number of news articles and t is the number of words in the vocabulary.",
      "NMF tries to learn a latent embedding that captures the information in the matrix in a much smaller space.",
      "In the general form NMF seeks to factor a (non-negative) matrix M into the product of two (non-negative) matrices W and H (or D and V as used in this paper).",
      "How does that help us?",
      "We can pick some dimension d (controlling the size of the latent space) and break down the  matrix into a d-dimension representation of news articles  , and a d-dimension representation of words in the vocabulary,  .",
      "That means that  has shape  and so  ends up with the desired shape  .",
      "Once we\u2019ve learned a good representation of news articles,  we can use those as the news content embeddings within TriFN.",
      "We\u2019d like to get  as close to  as we can, and at the same time keep  and  \u2018sensible\u2019 to avoid over-fitting.",
      "We can do that with a regularisation term.",
      "So the overall optimisation problem looks like this:  User embedding  For the user embedding there\u2019s a similar application of NMF, but in this case we\u2019re splitting the adjacency matrix  into a user latent matrix  , and a user correlation matrix  .",
      "So in this case we\u2019re using NMF to learn  which has shape mxd .",
      "dxd .",
      "dxm, resulting in the desired mxm shape.",
      "There\u2019s also a user-user relation matrix  which  controls the contribution of  .",
      "The basic idea is that any given user will only share a small fraction of news articles, so a positive case (having shared an article) should have more weight than a negative case (not having shared).",
      "User-news interaction embedding  For the user-news interaction embedding we want to capture the relationship between user features and the labels of news items.",
      "The intuition is that users with low credibility are more likely to spread fake news.",
      "So how do we get user credibility?",
      "Following \u2018 Measuring user credibility in social media \u2019 the authors base this on similarity to other users.",
      "First users are clustered into groups such that members of the same cluster all tend to share the same news stories.",
      "Then each cluster is given a credibility score based on its relative size.",
      "Users take on the credibility score of the cluster they belong to.",
      "It all seems rather vulnerable to the creation of large numbers of fake bot accounts that collaborate to spread fake news if you ask me.",
      "Nevertheless, assuming we have reliable credibility scores then we want to set things up such that the latent features of high-credibility users are close to true news, and the latent features of low-credibility users are close to fake news.",
      "Publisher-news embeddings  Recall we have the matrix  encoding which publishers have published which news pieces.",
      "Let  be the normalised version of the same.",
      "We want to find  , a weighting matrix mapping news publisher\u2019s latent features to the corresponding partisan label vector  .",
      "It looks like this:  Semi-supervised linear classifier  Using the labelled data available, we also learn a weighting matrix  mapping news latent features to fake news labels.",
      "Putting it all together  The overall objective becomes to find matrices  using a weighted combination of each of the above embedding formulae, and a regularisation term combining all of the learned matrices.",
      "It looks like this:  and it\u2019s trained like this:  Evaluation  TriFN is evaluated against several state of the art fake news detection methods using the FakeNewsNet BuzzFeed and PolitiFact datasets.",
      "It gives the best performance on both of them:  ( Enlarge )"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://www.public.asu.edu/~skai2/files/wsdm_2019_fake_news.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 68340528
  },
  {
    "blog_id": "tensorflow-a-system-for-large-scale-machine-learning",
    "summary": [
      "TensorFlow: A system for large-scale machine learning Abadi et al. (Google Brain) OSDI 2016  This is my last paper review for 2016!",
      "The Morning Paper will be taking a two week break for the holidays, resuming again on the 2nd January.",
      "Sometime inbetween I\u2019ll do a short retrospective on the year.",
      "It seems fitting to finish the year with a software system that was released as OSS just over a year ago and has since gathered a lot of mindshare and attention: Google\u2019s TensorFlow.",
      "A large number of groups at Google have deployed TensorFlow in production, and TensorFlow is helping our research colleagues to make new new advances in machine learning.",
      "Since we released TensorFlow as open-source software, more than 14,000 people have forked the source code repository, the binary distribution has been downloaded over one million times, and dozens of machine learning models that use TensorFlow have been published.",
      "TensorFlow essentials  A tensor is simply a multi-dimensional array of primitive types.",
      "A machine learning system in TensorFlow is represented by a dataflow graph with operators and state at the nodes in the graph, and tensors flowing on the edges between them.",
      "This explicit representation of the computation and the communication between stages makes it easy to partition computation across devices, and to execute independent computations in parallel.",
      "At the system level, all tensors are treated as dense.",
      "If you want to model a sparse tensor therefore, you need to encode it somehow at the application level.",
      "One option is to encode the data into variable-length string elements in a dense tensor; another option is to use a tuple of dense tensors, the first carrying coordinates, and the second the (non-zero) values at those coordinates.",
      "Operations take one or more tensors as input and produce one or more tensors as output.",
      "The attributes of an operation at compile-time determine both the expected types and the arity of inputs and outputs.",
      "Operations may contain mutable state that is read and/or written each time it executes.",
      "The special Variable operation simply owns a mutable buffer that may be used to store the shared parameters of a model as it is trained.",
      "In this way, parameters may be contained within the dataflow itself, rather than being \u2018outside\u2019 of the system in a parameter server.",
      "A second type of stateful operator is a queue operator.",
      "Queues support more advanced forms of coordination.",
      "The simplest queue is FIFOQueue, which owns an internal queue of tensors, and allows concurrent access in first-in first-out order.",
      "Other types of queues dequeue tensors in random and priority orders, which ensure that input data are sampled appropriately.",
      "Advanced machine learning algorithms may contain conditional and iterative control flow (e.g., RNNs).",
      "Given that expressing everything in the dataflow graph is a fundamental tenet of TensorFlow, this control flow also needs to be expressed in the graph.",
      "TensorFlow borrows Switch and Merge operations from traditional dynamic dataflow architectures to implement conditionals.",
      "Enter, Exit, and Next Iteration operators are used to support looping.",
      "The execution of iterations can overlap, and TensorFlow can also partition conditional branches and loop bodies across multiple devices and processes.",
      "The partitioning step adds logic to coordinate the start and termination of each iteration on each device, and to decide the termination of the loop.",
      "It\u2019s the job of the TensorFlow runtime to place operations on devices within task processes.",
      "TensorFlow supports CPUs, GPUs, and Google\u2019s own custom ASIC TPUs \u2013 Tensor Processing Units.",
      "TPUs give an order of magnitude improvement in performance-per-watt compared to the alternative state-of-the-art.",
      "The placement algorithm computes a feasible set of devices for each operation, calculates the set of operations that must be colocated, and selects a satisfying device for each colocation group.",
      "An operation may have multiple kernels registered for it, with specialized implementations for particular devices or data types.",
      "When submitting a graph to the TensorFlow runtime, the user can specify zero or more edges to feed input tensors into the dataflow, and one or more edges to fetch output tensors from.",
      "The distributed master prunes the graph to support just what is needed for the given inputs and outputs, partitions it into subgraphs for each participating device, and caches them for reuse in subsequent steps.",
      "Since the master sees the overall computation for a step, it applies standard optimizations such as common subexpression elimination and constant folding; pruning is a form of dead code elimination.",
      "It then coordinates execution of the optimized subgraphs across a set of tasks.",
      "For transfers between task processes, TensorFlow can take advantage of multiple protocols including gRPC over TCP, and RDMA over converged Ethernet (RoCE).",
      "TensorFlow differs from standard batch dataflow systems in that:  the model supports multiple concurrent executions on overlapping subgraphs of the overall graph  Individual vertices may have mutable state that can be shared between different executions of the graph  Naiad with its Differential dataflow support seems to come close to many of the general dataflow requirements of TensorFlow (without having the specialized operators etc.",
      "for ML).",
      "Since Amazon have just blessed MXNet as their deep learning system of choice , it\u2019s interesting to see what the TensorFlow authors have to say about it:  MXNet is perhaps the closest system in design to TensorFlow.",
      "It uses a dataflow graph to represent the computation at each worker, and uses a parameter server to scale training across multiple machines.",
      "The MXNet parameter server exports a key-value store interface that supports aggregating updates sent from multiple devices in each worker, and using an arbitrary user-provided function to combine incoming updates with the current value.",
      "The MXNet key-value store interface does not currently allow sparse gradient updates within a single value, which are crucial for the distributed training of large models, and adding this feature would require modifications to the core system.",
      "History and design rationale  TensorFlow is a successor to a previous Google system called DistBelief which used a parameter server architecture.",
      "One of its key goals was to provide much more flexibility to users and hence support rapid experimentation with new algorithms etc..  Making everything part of a dataflow makes it easier for users to compose novel layers using just a high-level scripting interface.",
      "Having state in the dataflow graph enables experimentation with different update rules.",
      "Having global information about the computation enables optimization of the execution phase \u2013 for example, TensorFlow achieves high GPU utilization by using the graph\u2019s dependency structure to issue a sequence of kernels to the GPU without waiting for intermediate results.",
      "Allowing operations to have multiple kernels enables exploitation of special-purpose accelerators when they are available.",
      "This enable a TensorFlow program, for example, to be deployed to a cluster of GPUs for training, a cluster of TPUs for serving, and a cellphone for mobile inference.",
      "Where next?",
      "TensorFlow is a work in progress.",
      "Its flexible dataflow representation enables power users to achieve excellent performance, but we have not yet determined default policies that work well for all users.",
      "Further research on automatic optimization should bridge this gap.",
      "On the system level, we are actively developing algorithms for automatic placement, kernel fusion, memory management, and scheduling.",
      "Fault-tolerance today is supported by user-level checkpointing operations (Save and Restore).",
      "A typical configuration connects each Variable in a task to the same Save operation to maximize I/O bandwidth to a distributed file system.",
      "While the current implementations of mutable state and fault tolerance suffice for applications with weak consistency requirements, we expect that some TensorFlow applications will require stronger consistency, and we are investigating how to build such policies at user-level.",
      "TensorFlow was originally designed to support asynchronous training , but new research suggests in some configurations synchnronous training may be faster to get to a certain quality level than asynchronous training, thus the team have begun experimenting with synchronous methods.",
      "Finally, some users have begun to chafe at the limitations of a static dataflow graph, especially for algorithms like deep reinforcement learning.",
      "Therefore, we face the intriguing problem of providing a system that transparently and efficiently uses distributed resources, even when the structure of the computation unfolds dynamically."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 263861
  },
  {
    "blog_id": "why-should-i-trust-you-explaining-the-predictions-of-any-classifier",
    "summary": [
      "\u201cWhy Should I Trust You?",
      "Explaining the Predictions of Any Classifier Ribeiro et al., KDD 2016  You\u2019ve trained a classifier and it\u2019s performing well on the validation set \u2013 but does the model exhibit sound judgement or is it making decisions based on spurious criteria?",
      "Can we trust the model in the real world?",
      "And can we trust a prediction (classification) it makes well enough to act on it?",
      "Can we explain why the model made the decision it did, even if the inner workings of the model are not easily understandable by humans?",
      "These are the questions that Ribeiro et al. pose in this paper, and they answer them by building LIME \u2013 an algorithm to explain the predictions of any classifier, and SP-LIME, a method for building trust in the predictions of a model overall.",
      "Another really nice result is that by explaining to a human how the model made a certain prediction, the human is able to give feedback on whether the reasoning is \u2018sound\u2019 and suggest features to remove from the model \u2013 this leads to classifiers that generalize much better to real world data.",
      "Consider two classifiers (Algorithm 1 and Algorithm 2 in the figure below) both trained to determine whether a document is about Christianity or atheism.",
      "Algorithm 2 performs much better in hold-out tests, but when we see why it is making its decisions, we realise it is actually much worse\u2026  Magenta words are those contributing to the atheism class, green for Christianity.",
      "The second algorithm is basing its decision on \u201cPosting\u201d, \u201cHost\u201d, \u201cRe\u201d and \u201cnntp\u201d \u2013 words that have no connection to either Christianity or atheism, but happen to feature heavily in the headers of newsgroup postings about atheism in the training set.",
      "What makes a good explanation?",
      "It must be easily understandable by a human!",
      "For example, if hundreds or thousands of features significantly contribute to a prediction, it is not reasonable to expect any user to comprehend why the prediction was made, even if individual weights can be inspected.",
      "And it must meaningfully connect input variables to the response:  ..which is not necessarily tue of the features used by the model, and thus the \u201cinput variables\u201d in the explanation may need to be different than the features.",
      "Furthermore, an explanation must have local fidelity: it should correspond to how the model behaves in the vicinity of the instance being predicted.",
      "The ideal explainer, should also be able to explain any model, and thus be model-agnostic.",
      "A key insight \u2013 local interpretation  Creating a globally faithful interpreter of a model\u2019s decisions might require a complete description of the model itself.",
      "But to explain an individual decision we only need to understand how it behaves in a small local region.",
      "The idea reminds me a little bit of differentiation \u2013 overall the shape of the curve may be very complex, but if we look at just a small part we can figure out the gradient in that region.",
      "Here\u2019s a toy example from the paper \u2013 the true decision boundary in the model is represented by the blue/pink background.",
      "In the immediate vicinity of the decision (the bold red cross) though we can learn a much simpler explanation that is locally faithful even if not globally faithful.",
      "The LIME algorithm produces Local Interpretable Model-agnostic Explanations.",
      "The overall goal of LIME is to identify an interpretable model over the interpretable representation that is locally faithful to the classifier.",
      "For text classification, an interpretable representation could be a vector indicating the presence or absence of a word, even though the classifier may use more complex word embeddings.",
      "For image classification an interpretable representation might be an binary vector indicating the \u2018presence\u2019 or \u2018absence\u2019 of a contiguous patch of similar pixels.",
      "LIME works by drawing samples in the vicinity of the input to be explained and learning a linear classifier using locally weighted square loss, with a limit K set on the number of interpretable features.",
      "Since [the algorithm] produces an explanation for an individual prediction, its complexity does not depend on the size of the dataset, but instead on time to compute f(x) [a model prediction] and on the number of samples N. In practice, explaining random forests with 1000 trees using scikit-learn on a laptop with N = 5000 takes under 3 seconds without any optimizations such as using gpus or parallelization.",
      "Explaining each prediction of the Inception network for image classification takes around 10 minutes.",
      "From local explanation to model trust  The central idea here is that if we understand and trust the reasoning behind an individual prediction, and we repeat this process for a number of predictions that give good coverage of the input space, then we can start to build global trust in the model itself.",
      "We propose to give a global understanding of the model by explaining a set of individual instances.",
      "This approach is still model agnostic, and is complementary to computing summary statistics such as held-out accuracy.",
      "Even though explanations of multiple instances can be insightful, these instances need to be selected judiciously, since users may not have the time to examine a large number of explanations.",
      "We represent the time/patience that humans have by a budget B that denotes the number of explanations they are willing to look at in order to understand a model.",
      "Given a set of instances X, we define the pick step as the task of selecting B instances for the user to inspect.",
      "Examining the instances X, we know the features that are locally important in making the prediction at X.",
      "Features that are locally important for many instances are globally important.",
      "Instances B are picked so as to cover the globally important features first, and to avoid redundancy in explanation between them.",
      "With a little help from my friends  Using human subjects recruited via Amazon Mechanical Turk \u2013 by no means machine learning experts, but with a basic knowledge of religion \u2013 the team provided explanations for the predictions of two different models classifying documents as atheist or Christian and asked the subjects which would generalize better (perform the best in the real world).",
      "Using LIME coupled with the mechanism just described to create representative instances, the human subjects were able to choose the correct model 89% of the time.",
      "A second experiment asked Amazon Mechanical Turk users to identify which words from the explanations should be removed from subsequent training, for the worst classifier.",
      "If one notes that a classifier is untrustworthy, a common task in machine learning is feature engineering, i.e. modifying the set of features and retraining in order to improve generalization.",
      "Explanations can aid in this process by presenting the important features, particularly for removing features that the users feel do not generalize.",
      "The users are not ML experts, and don\u2019t know anything about the dataset.",
      "Starting with 10 users, 10 classifiers are trained (one for each subject, with their suggested words removed).",
      "These are presented to five users each, resulting in another 50 classifiers.",
      "Each of these are presented to five users, giving 250 final models.",
      "It is clear\u2026 that the crowd workers are able to improve the model by removing features they deem unimportant for the task\u2026 Each subject took an average of 3.6 minutes per round of cleaning, resulting in just under 11 minutes to produce a classifier that generalizes much better to real world data.",
      "High agreement among users on the words to be removed indicated that users are converging to similar correct models.. \u201cThis evaluation is an example of how explanations make it easy to improve an untrustworthy classifier \u2013 in this case easy enough that machine learning knowledge is not required.\u201d"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 96747724
  },
  {
    "blog_id": "towards-a-theory-of-software-development-expertise",
    "summary": [
      "Towards a theory of software development expertise Baltes et al., ESEC/FSE\u201918  This is the last paper we\u2019ll be looking at this year, so I\u2019ve chosen something a little more reflective to leave you with (The Morning Paper will return on Monday 7th January, 2019).",
      "The question Baltes and Diehl tackle is this: \u201cHow do you get better as a software developer?\u201d What does expert performance look like?",
      "We present a first conceptual theory of software development expertise that is grounded in data from a mixed-methods survey with 335 software developers and in literature on expertise and expert performance\u2026.",
      "[the theory] describes central properties of software development expertise and important factors influencing its formation.",
      "In essence, ask a bunch of practitioners what they think, use a disciplined coding scheme to interpret the answers (a \u201cgrounded theory\u201d), and then layer in what we know about expertise and expert performance in general.",
      "The end result is a \u201cconceptual theory\u201d that shows the various contributors to expert performance and the relationships between them.",
      "\u201cSoftware Development\u201d in the current work is synonymous with \u201cprogramming.\u201d  To make the paper come alive you need to engage with it a little: Does the theory developed by the authors make sense to you?",
      "What\u2019s missing?",
      "How would you weight the various factors?",
      "How could you apply this on a personal level in 2019?",
      "How could this be applied in your team or organisation to raise the collective level of expertise next year?",
      "Software developers can use our results to see which properties are distinctive for experts in their field, and which behaviors may lead to becoming a better software developer\u2026.",
      "Employers can learn what typical reasons for demotivation among their employees are, and how they can build a work environment supporting the self-improvement of their staff.",
      "A grounded theory  The first phase involved sending a questionnaire to users active on both GitHub and StackOverflow between Jan 2014 and October 2015.",
      "The questionnaire was sent to 1,000 individuals, and received 122 responses.",
      "( Enlarge )  The grounded theory (GT) coding exercise was then used to generate a theory from the qualitative data:  \u2026 the process of coding assigns \u201csummative, salient, essence-capturing\u201d words or phrases to portions of the unstructured data.",
      "Those codes are iteratively and continuously compared, aggregrated, and structured into higher levels of abstractions, the categories and the concepts.",
      "This iterative process is called constant comparison.",
      "(Aside: it strikes me that the body of work on grounded theory development might be very interesting to study from the perspective of domain-driven design and the building of a ubiquitous language.)",
      "After much distillation, the model comes out looking like this:  The grounded theory describes software development expertise as a combination of a certain quantity and quality of knowledge and experience, both general and for a particular language.",
      "The work context, behavior, character traits, and skills influence the formation of expertise, which can be observed when experts write well-structured, readable, and maintainable source code.",
      "You\u2019ll know an expert programmer by the quality of the code that they write.",
      "Experts have good communication skills, both sharing their own knowledge and soliciting input from others.",
      "They are self-aware, understanding the kinds of mistakes they can make, and reflective.",
      "They are also fast (but not at the expense of quality).",
      "Experience should be measured not just on its quantity (i.e., number of years in the role), but on its quality.",
      "For example, working on a variety of different code bases, shipping significant amounts of code to production, and working on shared code bases.",
      "The knowledge of an expert is T-shaped with depth in the programming language and domain at hand, and a broad knowledge of algorithms, data structures, and programming paradigms.",
      "A preliminary conceptual theory  The next phase was to take the grounded theory and embed it within the existing literature on expertise and expert performance, for which the main resource used was \u2018 The Cambridge Handbook of Expertise and Expert Performance \u2019.",
      "This handbook is the first, and to the best of our knowledge most comprehensive, book summarizing scientific knowledge on expertise and expert performance.",
      "The result of this process is a preliminary conceptual theory that looks like this:  Acquiring expertise is not exclusively a cognitive matter, personality and motivation influence behaviours that may or may not lead to improvements of expertise.",
      "The work context, including team members, managers, and customers, can also influence the behaviour of a developer, and this can also vary according to the type of task being undertaken.",
      "Reaching true experts levels requires deliberate practice combined with monitoring, feedback, and self-reflection.",
      "Deliberate practice  Having more experience with a task does not automatically lead to better performance.",
      "Research has shown that once an acceptable level of performance has been attained, additional \u201ccommon\u201d experience has only a negligible effect, in many domains the performance even decreases over time.",
      "The length of experience has been found to be only a weak correlate of job performance after the first two years.",
      "Deliberate practice is required to become an expert: prolonged efforts to improve performance while continuously increasing the difficulty and centrality of development tasks.",
      "\u2026studies have shown that deliberate practice is necessary but not sufficient to achieve high levels of expert performance\u2014 individual differences also play an important role.",
      "Monitoring, feedback, and self-reflection  Deliberate practice requires a way of monitoring performance, which could be e.g. from a teacher, coach, mentor, or peer: \u201cthe more channels of accurate and helpful feedback we have access to, the better we are likely to perform.\u201c.",
      "Monitoring and self-reflection also influence motivation and consequently behaviour.",
      "The full conceptual theory  For the third and final phase the authors sampled two additional programmer populations, active Java developers, and very experienced developers, with the goal of further elaborating and refining the categories and relationships in the theory.",
      "The final resulting model looks like this:  ( Enlarge )  The most frequently cited tasks that an expert should be good at were designing software architecture, writing source code, and analysing and understanding requirements.",
      "Within the software architecture task, understanding modularisation and decomposition were frequently mentioned.",
      "In terms of personality traits, experts should be open minded and curious, be team players, and pay attention to detail.",
      "Patience and self-reflection were also cited.",
      "In terms of general skills, \u201cproblem solving\u201d came top of the list under which analytical thinking, logical thinking, and abstraction/decomposition all feature.",
      "Another important skill is being to assess trade-offs.",
      "Mentors should be guiding, patient, and open-minded.",
      "Participants were most motivated by mentors that posed challenging tasks.",
      "To facilitate continuous development of their employee\u2019s software development skills, (employees suggested that) employers should:  Encourage learning (e.g. training courses, conference attendance, and access to a good analog or digital library)  Encourage experimentation (e.g.",
      "through side projects and by building a work environment that is open to new ideas and technologies)  Improve information exchange between development teams, departments, and even companies.",
      "E.g. lunch and learn sessions, rotation between teams, pairing, mentoring, and code reviews.",
      "Grant freedom (primarily in the form of less time pressure) to allow developers to invest in learning new technologies or skills.",
      "In contrast, non-challenging or routine tasks result in demotivation.",
      "Other causes of performance decline over time are lack of a clear vision or direction, absence of reward for quality work, stress in the work environment, and bad management or team structure.",
      "Your turn  How will you ensure that in 2019 you grow your expertise, and not simply add another year of (the same or similar) \u2018experience\u2019 ?",
      "See you in January!",
      "Thanks, Adrian."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://arxiv.org/pdf/1807.06087",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 23675750
  },
  {
    "blog_id": "uncertainty-propagation-in-data-processing-systems",
    "summary": [
      "Uncertainty propagation in data processing systems Manousakis et al., SoCC\u201918  When I\u2019m writing an edition of The Morning Paper, I often imagine a conversation with a hypothetical reader sat in a coffee shop somewhere at the start of their day.",
      "There are three levels of takeaway from today\u2019s paper choice:  If you\u2019re downing a quick espresso, then it\u2019s good to know that uncertainty can creep into our data in lots of different ways, and if you compute with those uncertain values as if they were precise, errors can compound quickly leading to incorrect results or false confidence.",
      "If you\u2019re savouring a cortado, then you might also want to dip into the techniques we can use to propagate uncertainty through a computation.",
      "If you\u2019re lingering over a latte, then the UP (Uncertainty Propagation) framework additionally shows how to integrate these techniques into a dataflow framework.",
      "We implement this framework in a system called UP-MapReduce, and use it to modify ten applications, including AI/ML, image processing, and trend analysis applications to process uncertain data.",
      "Our evaluation shows that UP-MapReduce propagates uncertainties with high accuracy and, in many cases, low performance overheads.",
      "Are you sure?",
      "Uncertainty can arise from a number of different sources including probabilistic modelling, machine learning, approximate computing, imprecise sensor data, and such like.",
      "For many applications, uncertain data should be represented as probability distributions or estimated values with error bounds rather than exact values.",
      "Failure to properly account for this uncertainty may lead to incorrect results.",
      "For example, Bornholt et al. have shown that computing speeds from recorded GPS positions can lead to absurd values (e.g., walking speeds above 30mph) when ignoring uncertainties in the recordings.",
      "If you have a dataflow system with computation based on a DAG, then uncertainty in upstream data values needs to flow through the computation.",
      "For example, consider a simple 2-node DAG where an approximate query is used to produce an approximate count of the number of customers in different age groups (e.g., using BlinkDB ), and then we take a weighted average of those groups.",
      "The second node will by default produce a single value, but in reality it should result in a distribution.",
      "There may be meaningful parts of that distribution where the outcome would be disadvantageous (for example), but the probability of this is completely lost when reporting a single value.",
      "Uncertainty propagation  Our method offers, to the best of our knowledge, the only known computationally tractable (and as our evaluation will show, potentially with low overheads) large-scale uncertainty propagation.",
      "Consider a function  , where  is an arbitrary function without side-effects representing the computation at a node in a dataflow,  is a set of random variables representing inputs with uncertainties, and  is a set of random variables representing outputs with uncertainties.",
      "Depending on the nature of  , we can use different statistical methods to approximate the mean and variance of each variable in the output.",
      "When  is a continuous differentiable function we can use first-order Differential Analysis:  The general strategy is to compute  by approximating  using its first-order Taylor series at the expected value of  .",
      "This approximation is accurate if  is roughly linear around the support (in other words, neighborhood) of  \u2026  When there are multiple inputs and multiple outputs, the calculation also needs to take into account the covariances between the outputs.",
      "When  is a semi-continuous function we have two possibilities.",
      "If the support of each input mostly or entirely falls within a continuous differentiable part of the function then we can use Differential Analysis (DA) as before.",
      "If it spans a discontinuity then we have to use Monte Carlo simulation.",
      "For example, consider the function  when  , and  otherwise.",
      "If each input is greater than  then we can use DA.",
      "We use Monte Carlo simulation to approximate  for functions  that do not meet (or the developers do not know whether they meet) the requirements for DA.",
      "is evaluated on  randomly drawn samples of the input, and the outputs are used as an approximation of  .",
      "To generate accurate samples, one must know the joint density of  and pay the heavy computational cost of any rejection-sampling algorithm.",
      "Unfortunately that cost grows exponentially with an increasing size of  and thus we resort to two approximations:  Given input distributions, generate samples accordingly and ignore covariances  In the absence of full distributional information, assume that each input is normally distributed with the same mean and covariance matrix as the unknown distribution.",
      "(This approximation works because the mean and variance estimation of Y depends solely on the mean and variance of  ).",
      "Uncertainty propagation in dataflows  As stated earlier, in a dataflow graph we need to perform uncertainty propagation at all nodes downstream of uncertain data.",
      "For Monte Carlo simulation-based uncertainty propagation (UP-MC) we can just treat a node as a black box, dynamically generate samples from the input set, and compute the mean and variance for each output using empirically derived distributions (or assume normal distributions in the absence of this information).",
      "The implementation of Differential Analysis (henceforth called UP-DA) is more challenging.",
      "Specifically, when a DAG node produces multiple outputs, we view it as being implemented by multiple sub-functions, each producing one of the outputs\u2026 input covariances can require additional data flow to be added to the DAG for computing output variances and covariances.",
      "If the programmer can provide a partial derivative function, then using this often gives better performance than resorting to numerical differentiation.",
      "Observe that we might make a saving early in the dataflow by introducing uncertainty (e.g. by computing an approximate result), but then we have to pay more later for the resulting uncertainty propagation.",
      "The evaluation explores this trade-off.",
      "UP-MapReduce is an implementation of the above ideas in the in MapReduce.",
      "The UP-MapReduce extension includes three Mapper and three Reducer classes that implement UP-MC, UP-DA for continuous functions, and UP-DA for semi-continuous functions.",
      "The extension also introduce the uncertain type PV (Probabilistic Value) which contains one or more random variables, each described by a mean, a variance-covariance matrix, and possibly an entire empirical distribution.",
      "The UP-DA Continuous Reducer class for example provides an abstract derivative method that a developer can implement to provide a closed-form derivative function.",
      "Uncertainty propagation in practice  We have built a toolbox of common operations (e.g., sum) and modified ten common data processing applications using UP-MapReduce to process uncertain data.",
      "Baselines for the evaluation are established by running a large Monte Carlo experiment over a precise version of each application.",
      "When input errors are small (e.g. below 3%) then UP-MapReduce estimates means with very low error.",
      "The following figure shows the relative errors and execution times for the three variants of UP-MC as compared to the baseline.",
      "Enlarge  For six of the applications UP-MapReduce is highly accurate, but when input errors are significant (e.g. eig, svd) its estimated relative errors can deviate noticeably from baseline values.",
      "The best performance is obtained when using closed-form (user provided) derivatives.",
      "tsocial and latency are both multi-stage approximate workflows.",
      "The following chart shows the execution times and maximum relative errors for sampling rates from 0.1% to 100% (precise).",
      "For tsocial, a sampling rate of 80% or less is required before the overheads of uncertainty propagation are outweighed by the sampling benefits.",
      "Experimentation with ten common data analytic applications revealed that UP-MapReduce is highly accurate in many cases, while its performance overheads are very low\u2014 an average of 6% performance degradation\u2014 when closed-form derivatives are provided.",
      "When numerical differentiation or Monte Carlo simulation must be used, overheads can become much more significant as input size increases.",
      "Fortunately, the impact of these overheads on overall execution time can be reduced by allocating additional computational resources."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3267809.3267833?download=true",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 8984509
  },
  {
    "blog_id": "detecting-emotions-with-cnn-fusion-models-b066944969c8",
    "summary": [
      "Overview  This work proposes models that combine information from different modalities (e.g., images and text) to be able to classify social media content.",
      "Information from different modalities are combined using neural network models through a pooling layer.",
      "In addition, an auxiliary learning task is used to learn a common feature space for all modalities (more on this later).",
      "Motivation  Multimodal approaches become more important as social media networks allow for users to post multimodal posts (e.g., gifs, videos, audio segments, text, etc.).",
      "Analysis of multimodal information allows for better understanding of users (user profiling) and can be used to effectively run ad campaigns on the social network.",
      "In addition, it can be used to better understand other emotion-related behaviors such as mental health disorders, etc.",
      "Example  Consider the examples of multimodal posts in the pictures below.",
      "If we only paid attention to the images (left to right), we would predict emotions such as joy, fear, and contentment.",
      "If we considered both the image and text: the first example remains as joy; the second example is probably mixed emotion (the text convey joy); and the third example is also mixed emotion (the text convey sadness).",
      "These simple examples emphasize on the importance of considering both modalities to deduce the overall emotion conveyed in the social post.",
      "Contribution  The main problem with previous multimodal approaches is the inability to deal with the absence of some important modality.",
      "For instance, let\u2019s assume we can obtain text and video from a piece of content, but we can\u2019t obtain the audio because it is corrupted or unavailable.",
      "In such cases, previous methods did not address this important problem (i.e., missing modality).",
      "The proposed model aims to address this problem and proves its robustness through an emotion classification task.",
      "Besides dealing with the \u201cmissing modality\u201d problem, the authors claim that their approach can also scale to other tasks and modalities.",
      "Challenge Addressed  As previously mentioned, the proposed model can handle situations where there is a missing modality.",
      "In other words, there system supports the following cases: only image or text or both.",
      "Hand-Crafted Features vs. Automatic Feature Learning  I believe this is an important discussion that this paper highlights in the related work.",
      "As it relates to emotion recognition, it is challenging to manually create features as we cannot guarantee that all aspects of emotions (features) that can capture the emotions are covered.",
      "Convolutional neural networks (CNNs) are used in place so as to automatically learn representations that can generalize to the problem of emotion recognition.",
      "I couldn\u2019t help but commenting that even though this argument is strong, hand-crafting features also offer better intuition of what is being learned, something deep learning models may not offer, yet!",
      "However, some good people are tirelessly working on this problem ( Feature Visualization ).",
      "Concepts to Know  Late fusion \u2014 combination of results obtained by different classifiers (trained on different modalities); i.e., fusion is done at the decision level.",
      "Early fusion \u2014 information from different modalities are combined at the feature level, and classification is done on the combined representations.",
      "Multimodal Classification  This work employs an adaptation of early fusion for combining modalities for emotion recognition through CNNs.",
      "Two prominent modalities of social media are used, i.e. text and image.",
      "If both image and text are available for a social post, they are assumed to have semantic relation \u2014 the text describes the image.",
      "Images are represented by vectors, which are obtained after feeding images into a CNN trained on ImageNet .",
      "Texts are represented through pre-trained word embeddings (GloVe).",
      "Model  In the figure above, all types of fusion techniques for combining features to be fed to a classifier are shown.",
      "This work proposes two fusion approaches which enjoy the simplicity of early fusion (a) and the flexibility of late fusion (b).",
      "These approaches are called joint fusion \u00a9 and common space fusion (d).",
      "In the joint fusion model, text and images are fused in the fusion layer, which applies a pooling operation to the text and image vector to obtain a combined feature vector.",
      "The pooling operations require both vectors to be of the same size.",
      "Typically, the image vector has a higher dimension than the text vector, therefore, an extra linear layer is added to map the original image vector to a vector of the same dimension as the text vector.",
      "The joint fusion neural network is trained by minimizing the negative log-likelihood using stochastic gradient descent (SGD).",
      "(See paper for additional details)  The second approach, common feature space fusion, aims to enforce visual and textual vectors of a post to be in the same feature space.",
      "Note that this was motivated by the fact that the joint fusion model considers these signals (visual and textual) as different, i.e., no relationship between them.",
      "An auxiliary task is employed, which enforces similarity between both a text and image vector belonging to a post, ensuring that the rest of text vectors from different classes are different from the image vector.",
      "(See paper for details on how this objective is trained and combined with the main classification task).",
      "Task  An emotion classification task is used to evaluate the proposed multimodal approaches.",
      "Different discrete emotion categories from Plutchik\u2019s wheel of emotions are employed to label two types of datasets.",
      "Datasets  A flickr image dataset was crawled and assigned to Amazon Mechanical Turk workers for annotations; i.e., human workers were asked to annotate the emotion they perceived from the images.",
      "Title and descriptions are also obtained for each image from the flickr website.",
      "In addition, a Reddit dataset was also collected; subreddits related to emotion (happy, creepy, rage, gore) were used to collect data for 4 emotions, joy, fear, anger, and disgust, respectively.",
      "(See paper for more details on collecting and preparing datasets)  Experiments  Unimodal baselines (FastText model for text and InceptionNet model for images), traditional multimodal approaches (early and late fusion), and the proposed multimodal models (joint fusion and common space fusion) are trained on the datasets.",
      "For the embedding layer, GloVe pre-trained word vectors are considered.",
      "From the results in the table above, we can observe that the proposed fusion models (joint fusion and common space fusion) outperform all the other models in both datasets, including the traditional fusion techniques.",
      "The common space fusion model, in particular, performs extremely well even when only one modality for a post existed (see results below).",
      "Analysis  Classification results on several examples are provided for error analysis (see figure below).",
      "We can observe that for the highlighted example, the common space fusion model can detect fear when using both the text and image information.",
      "Somehow, the model can detect that in this particular example, the image descriptor expresses sarcasm, which is obvious from the creepy doll in the closet show in the image.",
      "Kind on interesting and weird at the same time, no pun intended.",
      "(See paper for more interesting analysis and examples)  Conclusion and Future Work  Experimental results show that by combining textual and visual data, fusion models can improve emotion classification accuracy on two types of datasets, with very significant gains in some cases.",
      "The common space fusion model performs well even when only one modality existed because this model contains information from two modalities.",
      "These type of models proof ideal for cases where more modalities may exist in the data.",
      "However, the authors point out that these models may suffer from information loss as one modality may be less informative than the other.",
      "I honestly enjoyed reading this paper.",
      "It is well written and it provides a new idea on how to combine modalities for understanding social media data.",
      "Other applications may include sarcasm detection, which is a more complex task as summarized here .",
      "References  Datasets and code for this work are available here  Ref:  [url]"
    ],
    "author_id": "DAIR-AI",
    "pdf_url": "https://arxiv.org/pdf/1708.02099",
    "author_full_name": "Elvis Saravia",
    "source_website": "https://github.com/dair-ai/nlp_paper_summaries",
    "id": 518237
  },
  {
    "blog_id": "master-of-web-puppets-abusing-web-browsers-for-persistent-and-stealthy-computation",
    "summary": [
      "Master of web puppets: abusing web browsers for persistent and stealthy computation Papadopoulus et al., NDSS\u201919  UPDATE 2019-04-14: An author update has been published for this paper which details that with current browser versions, ServiceWorkers can only stay alive for about a minute after the user navigates away from the site.",
      "This mitigates the main risk detailed in the paper of long running botnet membership.",
      "With thanks to Alex Russell ( @slightylate ) for highlighting this to me.",
      "You\u2019ve probably heard about crypto-currency mining and the like in hijacked browsers.",
      "From a security perspective, a fundamental problem of web applications is that by default their publisher is considered as trusted, and thus allowed to run JavaScript code (even from third parties) on the user side without any restrictions\u2026 On the positive side JavaScript execution so far has been constrained chronologically to the lifetime of the browser window or tab that rendered the compromised or malicious website.",
      "Not any more!",
      "This paper shows how modern browsers with support for Service Workers can be stealthily connected into a botnet, with a connection that persists until the user closes the browser completely: \u201cin contrast to previous approaches for browser hijacking, a key feature of MarioNet is that it remains operational even after the user browses away from the malicious webpage.\u201d  MarioNet building blocks: Service Workers and WebRTC  Service Workers are non-blocking modules that reside in the user\u2019s browser.",
      "Once registered they can run in the background without requiring the user to continue browsing on the originating site.",
      "In addition, service workers have the ability to intercept and handle network requests (for e.g., caching and pre-loading purposes).",
      "No user permission is required to register and maintain a service worker.",
      "When the user browses away from a website, the service worker of that website is typically paused by the browser; it is then restarted and reactivated once the parent domain is visited again.",
      "However, it is possible for the publisher of a website to keep its service worker alive by implementing periodic synchronization.",
      "If the user permits the publishing site to send push notifications, then it is even possible to have the service worker restart when the browser is restarted.",
      "Browser extensions are not permitted to use HTML5 APIs such as the Service Workers API and Push API, and hence cannot interact with or see deployed service workers in any way (e.g., modify their code, monitor their outgoing traffic etc.).",
      "Thus no extension-based mechanism or blocker (save turning off JavaScript completely) can offer protection.",
      "Once browsers have been hijacked, the Web Real-Time Communication (WebRTC) API is also very useful for implementing peer-to-peer communication between browsers.",
      "Key features of MarioNet  MarioNet aims to establish an in-browser botnet that requires only that the user visit a target website (e.g. one owned by the attacker, or one that the attacker has compromised) one time in order to takeover the browser.",
      "The design has the following goals:  Isolation: operation is independent of any browsing session thread or process (so that more heavyweight computation can be done with less chance of detection)  Persistence: operation is completely detached from any ephemeral browsing session, so that the browser remains under the attackers control for periods of time much longer than a website visit  Evasiveness: operations should be performed in a stealthy manner to evade detection and maintain the infection for as long as possible.There are three main components to the system, as shown in the figure below.",
      "The user simply needs to visit one time a page under the attackers control, hosted on a website.",
      "This page installs a service worker (the Servant) when loaded in the browser, and uses background sync registrations to keep the Servant always alive.",
      "As part of its initialization, the Servant establishes a communication channel with its remote command and control server (Puppeteer) and requests the initial set of tasks.",
      "The Puppeteer can send tasks to Servants at any time over the established communications channels.",
      "One easy way to package them is as JavaScript scripts that the servant simply evals.",
      "Because extensions cannot see service workers, the connection between a Servant and the Puppeteer is hidden from all extensions.",
      "The connection is also TLS-encrypted such that it cannot be eavesdropped outside of the browser either.",
      "The only request that reveals the existence of the service worker is the initial GET request at the time of the user\u2019s first website visit, when the service worker gets initially registered.",
      "To further evade detection, the Servant monitors the device\u2019s current status (CPU utilisation, battery, etc.)",
      "and can throttle or pause execution of the malicious workload to minimise risk of detection.",
      "If the user allows the distributor to send push notifications (more likely perhaps when the servant is distributed via a compromised site the user trusts), then MarioNet can send asynchronous notifications and updates to its service workers to re-activate them after browser restarts.",
      "MarioNet use cases  MarioNet can conduct a subset of DDoS attacks by instructing Servants to connect to a specific Internet host.",
      "There is no low-level networking access, but HTTP request methods and the HTTP body can of course be controlled.",
      "If the victim site has enabled WebSockets then this is another attack vector.",
      "UsingXMLHttpRequest.send(), jQuery\u2019s ajax() and WebSocket\u2019s send() methods, we can continually send a flood of messages to a targeted host.",
      "MarioNet can be used for cryptocurrency mining, where it is much more efficient than current Web Worker based approaches since it can continue mining even once the user has left the initial site.",
      "MarioNet can instruct Servants to assist in distributed password cracking.",
      "\u201cA major advantage of MarioNet is that it can be agnostic to the hashing function used, since the function code is transferred from the Puppeteer and executed from the MarioNet nodes through eval().\u201d  A network of MarioNet nodes can form a content distribution network (presumably for content of an unsavoury nature) using WebRTC.",
      "The authors implemented a WebTorrent proof-of-concept for this idea.",
      "In a similar manner, WebRTC can be used to construct obfuscating relay proxies.",
      "Another classic use case would be click fraud, where MarioNet is used to surf targeted websites, stream online videos to increase views, manipulate online polls, and so on.",
      "The Puppeteer would simply need to periodically send a list of online links to visit\u2026  Defenses?",
      "MarioNet works across a wide range of modern desktop and mobile browsers, although in the evaluation Service Worker performance on Safari was found to be very poor (presumably Apple will improve this over time?).",
      "The prime defence against MarioNet would require a mechanism to restrict or disable service workers.",
      "Since many sites now depend on them this has the potential to degrade user experience for legitimate use cases.",
      "Forbidding service workers to use eval and friends would make it more difficult for the Puppeteer.",
      "A same-origin policy for service worker network requests would also help in the case of a compromised distribution site.",
      "Users could also be required to explicitly give permission for a service worker to be installed by a site (currently user consent is only needed for Push notifications).",
      "Network traffic monitors may be able to pick up message exchange patterns, in environments where these are deployed and appropriately configured.",
      "Beyond never accepting push notification requests, and periodically restarting your browser, it seems there\u2019s not much more you can do as an end user to protect against this class of attacks.",
      "Essentially, our work demonstrates that the trust model of web, which considers web publishers as trusted and allows them to execute code on the client-side without any restrictions is \ufb02awed and needs reconsideration.",
      "Furthermore, this work aims to increase the awareness regarding the powerful capabilities that modern browser APIs provide to attackers, and to initiate a serious discussion about implementing restrictions while offering such capabilities that can be easily abused."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://www.ndss-symposium.org/wp-content/uploads/2019/02/ndss2019_01B-2_Papadopoulos_paper.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 71546396
  },
  {
    "blog_id": "cardinality-estimation-done-right-index-based-join-sampling",
    "summary": [
      "Cardinality estimation done right: Index-based join sampling  Cardinality estimation done right: Index-based join sampling Leis et al., CIDR 2017  Let\u2019s finish up our brief look at CIDR 2017 with something closer to the core of database systems research \u2013 query optimisation.",
      "For good background on this topic a great place to start is Selinger\u2019s 1979 classic \u2018 Access path selection in a relational database management system \u2018 (System R).",
      "One of the trickiest areas in query optimisation is determining the best join strategy, and in the almost 40 years since that System R paper, a lot of work has been done on this problem .",
      "If you are joining n tables, the number of possible join orders grows with n!",
      ".",
      "When estimating the potential costs of different join orders, a fundamental input is an estimation of the number of matching tuples in each of the relations to be joined (aka cardinality estimation).",
      "Virtually all industrial-strength systems estimate cardinalities by combining some fixed-size, per-attribute summary statistics (histograms) with strong assumptions (uniformity, independency, inclusion, ad hoc constants).",
      "In other words, most databases try to approximate an arbitrarily large database in a constant amount of space\u2026 For real-world data sets, cardinality estimation errors are large and occur frequently.",
      "These errors lead to slow queries and unpredictable performance.",
      "Instead of using histograms to try and guess, for example, how many tuples a relation might have where  an alternative approach would be to sample a subset of the rows and see how many matches there are in the sampled subset.",
      "Sampling has not traditionally been used for two reasons:  The disk I/O involved in going and fetching sample tuples is too slow, compounded by\u2026  You need to sample a surprisingly large number of tuples in order to ensure that you\u2019re still left with enough tuples to carry on estimating after joining.",
      "Say we\u2019re joining A, B, and C. We want to estimate the cost of joining A and B, and then joining the result with C. We need enough samples remaining after the first join to be able to do a good enough estimation of the join with C. It turns out that with random sampling without replacement, in order to have n expected result tuples after the first join, we need to sample on the order of  tuples.",
      "The more joins we have, the larger the initial samples need to be\u2026.",
      "For in-memory databases, the first consideration largely goes away \u2013 we can sample reasonable numbers of tuples in a small amount of time (though presumably we always still have some kind of space budget we also have to adhere to).",
      "To address the second consideration it would be ideal if the samples we took were somehow more likely to \u2018survive\u2019 the joining process, so that a sufficient number of tuples flow through to the next join\u2026  In this work we propose a novel cardinality estimation technique that produces accurate results but is much cheaper than joining random samples.",
      "The basic building block is an efficient sampling operator that utilizes existing index structures: to get an estimate for  , we obtain a random sample of  and then look up the samples\u2019 join partners in the index for  (we could also start with  using an index on  ).",
      "The resulting sample for  can be used as a starting point for obtaining a sample for  by using an index on the join attribute  and so on.",
      "Suppose we have a sample S as a result of sampling some relation T, and for the next step we want to create a sample of  .",
      "For each tuple in S we use the index to look up how many matching tuples are expected in A. Summing these gives us the total expected number of matching tuples, and the required number of samples is then drawn from this set.",
      "The index-based sampling operator can cheaply compute a sample for a join result, but it is not a full solution by itself.",
      "We also need a join enumeration strategy which can systematically explore the intermediate results of a query using the sampling operator, while also ensuring that the overall sampling time is limited.",
      "If we sampled every possible combination, it would take too long for queries with many joins.",
      "In the Join Order Benchmark (JOB), queries with 7 joins have 84-107 intermediate results, and queries with 13 joins have 1,517-2,032.",
      "A time limit is set on the sampling phase, after which the algorithm falls back to traditional estimation.",
      "The advantage is that one quickly obtains accurate estimates for large intermediate results.",
      "The disadvantage is that many small intermediate results are not sampled and thus have to be estimated using traditional estimation.",
      "It is well known that\u2014due to the independence assumption\u2014traditional estimators tend to underestimate result sizes.",
      "Therefore, when this mix of (accurate) sampling-based estimates and traditional (under-)estimates are injected into a query optimizer, it will often pick a plan based on the traditional estimates (as they appear to be very cheap).",
      "This phenomenon has been called \u201cfleeing from knowledge to ignorance\u201d and\u2014paradoxically\u2014causes additional, accurate information to decrease plan quality.",
      "To address this issue, joins are sampled \u2018bottom-up\u2019 \u2013 i.e., first all 2-way joins are computed, then all 3-way joins, and so on.",
      "\u201cA cost-based query optimizer will thus have precise knowledge for the costs of the early (and often crucial) joins.\u201d  Integrating this approach into an existing DBMS is pretty straightforward since you just need to inject the results into the cardinality estimation component of the query optimizer, and no changes to the cost model or plan space enumeration algorithm are necessary.",
      "Any query optimizer change that increases the performance for the vast majority of queries, will also decrease performance for some queries, which is very undesirable in production systems.",
      "Existing database systems are therefore very conservative with query optimizer changes.",
      "Thus, one could use our approach as an optional tuning feature for queries that are slower than expected.",
      "In other words, if a user is not satisfied with the performance of a particular query, to get better performance she may turn on index-based sampling only for that query.",
      "How well does it work?",
      "Evaluation is based on the Join Order Benchmark, based on the Internet Movie Database.",
      "There are 113 queries with 3 to 16 joins.",
      "As a worst case for planning complexity, indices are created on all primary key and foreign key columns.",
      "To compare with traditional cardinality estimates, the test harness supports injection of cardinality estimates from an outside system \u2013 in this case obtained from PostgreSQL using the EXPLAIN command.",
      "You can see that compared to the PostgreSQL estimates (column 1), index-based sampling with even just 10K samples produces much more accurate estimates.",
      "Also note how estimation accuracy decreases with the number of joins.",
      "The more samples we have of course, the closer we get to the true cardinalities.",
      "Do these more accurate cardinality estimates actually lead to better plans?",
      "In figure 6 below you can see the plan quality (log scale) of the plans produced.",
      "The cost of each query is normalized by the cost of the optimal plan that would have been chosen if the true cardinalities were known.",
      "Using PostgreSQL\u2019s estimates, only around one quarter of the plans are close to the optimum, 42% of the plans are off by a factor of 2 or more, and 12% are off by a factor of 10 or more\u2026  With a budget of 100,000 index lookups, index-based sampling improves performance for many queries \u2013 only 17% are off by a factor of 2 or more, and only 3% by a factor of 10 or more.",
      "As expected, these better plans lead (mostly!)",
      "to faster runtimes:  A small number of plans are actually faster (up to 3x) with inaccurate estimates rather than with the true cardinalities.",
      "This effect is caused by cost model errors rather than inaccurate cardinalities and explains the hesitation of many commercial database systems to change their query optimizers."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://cidrdb.org/cidr2017/papers/p9-leis-cidr17.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 32338938
  },
  {
    "blog_id": "use-of-formal-methods-at-amazon-web-services",
    "summary": [
      "Use of Formal Methods at Amazon Web Services \u2013 Newcombe et al 2014  Leslie Lamport recently gave a talk at the React conference on the specification language TLA.",
      "I wasn\u2019t there to hear the talk, but I was intrigued enough to dig in and find out a little more.",
      "Especially since I have some experience with Z and CSP.",
      "My journey led me to today\u2019s paper choice, a wonderful find and something that I hope spreads further into accepted industry practice.",
      "Something a lot of people believe\u2026  In industry, formal methods have a reputation of requiring a huge amount of training and effort to verify a tiny piece of relatively straightforward code, so the return on investment is only justified in safety- critical domains such as medical systems and avionics.",
      "That isn\u2019t necessarily so:  Our experience with TLA+ has shown that perception to be quite wrong.",
      "So far we have used TLA+ on 10 large complex real-world systems.",
      "In every case TLA+ has added significant value, either finding subtle bugs that we are sure we would not have found by other means, or giving us enough understanding and confidence to make aggressive performance optimizations without sacrificing correctness.",
      "We now have 7 teams using TLA+, with encouragement from senior management and technical leadership.",
      "Engineers from entry level to Principal have been able to learn TLA+ from scratch and get useful results in 2 to 3 weeks, in some cases just in their personal time on weekends and evenings, and without help or training.",
      "The AWS teams have been using formal specification since 2011, and the paper tells the story of how this came to be and how their use has spread.",
      "It\u2019s highly readable so I encourage you to check it out.",
      "I will focus on the rationale and benefits for this short summary.",
      "Building distributed systems is hard  S3 is just one of tens of AWS services that store and process data that our customers have entrusted to us.",
      "To safeguard that data, the core of each service relies on fault-tolerant distributed algorithms for replication, consistency, concurrency control, auto-scaling, load balancing, and other coordination tasks.",
      "When it comes to building real production systems, there\u2019s more to it than just coding up algorithms from the literature:  There are many such algorithms in the literature, but combining them into a cohesive system is a major challenge, as the algorithms must usually be modified in order to interact properly in a real-world system.",
      "In addition, we have found it necessary to invent algorithms of our own.",
      "We work hard to avoid unnecessary complexity, but the essential complexity of the task remains high.",
      "How can you gain confidence that your system is correct?",
      "AWS used multiple verification methods (short of formal methods), but still weren\u2019t finding all the subtle bugs that may lurk.",
      "before launching such a service, we need to reach extremely high confidence that the core of the system is correct.",
      "We have found that the standard verification techniques in industry are necessary but not sufficient.",
      "We use deep design reviews, code reviews, static code analysis, stress testing, fault-injection testing, and many other techniques, but we still find that subtle bugs can hide in complex concurrent fault-tolerant systems.",
      "Some of the most difficult to detect bugs are not in the code, but in the design\u2026  some of the more subtle, dangerous bugs turn out to be errors in design; the code faithfully implements the intended design, but the design fails to correctly handle a particular \u2018rare\u2019 scenario.",
      "Finding a precise design language  In order to find subtle bugs in a system design, it is necessary to have a precise description of that design.",
      "There are at least two major benefits to writing a precise design; the author is forced to think more clearly, which helps eliminate \u2018plausible hand-waving\u2019, and tools can be applied to check for errors in the design, even while it is being written.",
      "AWS needed a precise design language, but also a pragmatic one:  As our designs are unavoidably complex, we needed a highly expressive language, far above the level of code, but with precise semantics.",
      "That expressivity must cover real-world concurrency and fault-tolerance.",
      "And, as we wish to build services quickly, we wanted a language that is simple to learn and apply, avoiding esoteric concepts.",
      "We also very much wanted an existing ecosystem of tools.",
      "They \u2018found what we were looking for in TLA+, a formal specification language.\u2019  Benefits  TLA+ helped AWS find subtle bugs in S3, DynamoDB, EBS, and an internal distributed lock manager, as well as verifying several optimizations.",
      "The effects go deeper than just finding bugs though.",
      "TLA+ has been helping us shift to a better way of designing systems.",
      "The process begins by stating clearly \u2018what needs to go right?\u2019 by defining correctness properties.",
      "Safety properties define what the system is allowed to do, and liveness properties define what the system must eventually do.",
      "The benefits are not just in the initial system design, but over the lifetime of the system.",
      "We have found that writing a formal specification pays several dividends over the lifetime of the system.",
      "All production services at Amazon are under constant development, even those released years ago; we add new features that customers have requested, we re-design components to handle massive increases in scale, and we improve performance by removing bottlenecks.",
      "Many of these changes are complex, and they must be made to the running system with no downtime.",
      "Our first priority is always to avoid causing bugs in a production system, so we often need to answer the question, \u201cis this change safe?\u201d We have found that a major benefit of having a precise, testable model of the core system is that we can rapidly verify that even deep changes are safe, or learn that they are unsafe without doing any harm.",
      "In several cases we have prevented subtle, serious bugs from reaching production.",
      "In other cases we have been able to make innovative performance optimizations \u2013 e.g. removing or narrowing locks, or weakening constraints on message ordering \u2013 which we would not have dared to do without having model checked those changes.",
      "Spreading the word  Success breeds success, but it\u2019s best not to scare people off too early:  \u2026we have found that software engineers more readily grasp the concept and practical value of TLA+ if we dub it: exhaustively testable pseudo-code.",
      "We initially avoid the words \u2018formal\u2019, \u2018verification\u2019, and \u2018proof\u2019, due to the widespread view that formal methods are impractical.",
      "Formal methods seem to be becoming an important part of the AWS processes:  At AWS, formal methods have been a big success.",
      "They have helped us prevent subtle, serious bugs from reaching production, bugs that we would not have found via any other technique.",
      "They have helped us to make aggressive optimizations to complex algorithms without sacrificing quality.",
      "So far, seven teams have used TLA+, and all have found high value in doing so.",
      "At the time of writing, more teams are starting to use TLA+.",
      "We believe that use of TLA+ will accelerate both time-to-market and quality of these projects.",
      "Executive management is now proactively encouraging teams to write TLA+ specs for new features and other significant design changes.",
      "In annual planning, managers are now allocating engineering time to use TLA+.",
      "Summary  It\u2019s great to see the value of formal methods being recognised once more.",
      "This paper is also a good insight into the distributed systems design methodology at AWS.",
      "It adds another dimension to AWS\u2019s challenge to the rest of the industry: keep up if you can!"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://research.microsoft.com/en-us/um/people/lamport/tla/formal-methods-amazon.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 93025332
  },
  {
    "blog_id": "scala-actors-unifying-thread-based-and-event-based-programming",
    "summary": [
      "Scala Actors: Unifying thread-based and event-based programming \u2013 Haller & Odersky 2008  Yesterday we saw a Haskell-based approach to unifying events and threads , today\u2019s paper shows how to apply some of those same ideas on top of the JVM using Scala.",
      "There is an impedance mismatch between message-passing concurrency and virtual machines, such as the JVM.",
      "VMs usually map their threads to heavyweight OS processes.",
      "Without a lightweight process abstraction, users are often forced to write parts of concurrent applications in an event-driven style which obscures control flow, and increases the burden on the programmer.",
      "In this paper we show how thread-based and event-based programming can be unified under a single actor abstraction.",
      "Using advanced abstraction mechanisms of the Scala programming language, we implement our approach on unmodified JVMs.",
      "What is an actor?",
      "An actor is a concurrent process that communicates with other actors by exchanging messages.",
      "Communication is asynchronous; messages are buffered in an actor\u2019s mailbox.",
      "An actor may respond to an asynchronous message by creating new actors, sending messages to known actors (including itself), or changing its behavior.",
      "The behavior specifies how the actor responds to the next message that it receives.",
      "Some of the authors we read earlier in the week would probably put actors in the \u2018event-based\u2019 category.",
      "As Haller and Odersky point out though, they do avoid the inversion of control issue with out and out event-driven systems.",
      "They call it \u2018message-based\u2019 concurrency \u2013 the same term used in the original \u2018dual\u2019 paper we looked at on Monday.",
      "An actor can suspend with a full thread stack (receive) or it can suspend with just a continuation closure (react).",
      "The first form of suspension corresponds to thread-based, the second form to event-based programming.",
      "The new system combines the benefits of both models\u2026  Complexity is reduced for the programmer since:  Accessing an actor\u2019s mailbox is race-free by design  Message-passing with pattern matching is often more convenient than explicit thread-based synchronization  Actors are lightweight, supported fine-grained concurrency models without the need for explicit management of thread pools  Actors interoperate naturally with normal JVM threads (that are treated as actors).",
      "The scheme is implemented in the Scala Actors library.",
      "\u2026 it might seem that Scala is a language specialized for actor concurrency.",
      "In fact, this is not true.",
      "Scala only assumes the basic thread model of the underlying host.",
      "The implementation executes multiple actors on multiple threads:  The basic idea of our implementation is to use a thread pool to execute actors, and to resize the thread pool whenever it is necessary to support general thread operations.",
      "If actors use only operations of the event-based model, the size of the thread pool can be fixed.",
      "This is different if some of the actors use blocking operations such as receive or system I/O.",
      "In the case where every worker thread is occupied by a blocked actor and there are pending tasks, the thread pool has to grow.",
      "Combinators are introduced to enable actor composition.",
      "andThen for example permits sequential composition in a manner reminiscent of the CPS Monad of Li and Zdancewic .",
      "awaitPing andThen sendPong  There are several examples of actors given in the paper.",
      "But before you dive in too deep, it\u2019s worth knowing that the Scala Actors library was deprecated in Scala 2.11 in favour of Akka .",
      "See the Akka documentation on Actors for an overview of the currently supported actor model.",
      "The basic actor notions apply equally in both of course.",
      "Section 6 of the paper gives a short discussion of the benefits of the actor model when building web applications.",
      "Compared to a purely event-based approach, users are relieved from writing their own ad hoc thread pooling code.",
      "Since the internal thread pool can be global to the web application server, the thread pool controller can leverage more information for its decisions.",
      "Finally, accesses to an actor\u2019s mailbox are race-free.",
      "Therefore, resources such as user profiles can be protected by modeling them as (thread-less) actors.",
      "It seems fitting to end with a tribute to Erlang:  Our library was inspired to a large extent by Erlang\u2019s elegant programming model.",
      "Erlang is a dynamically-typed functional programming language designed for programming real-time control systems.",
      "The combination of lightweight isolated processes, asynchronous message passing with pattern matching, and controlled error propagation has been proven to be very effective.",
      "One of our main contributions lies in the integration of Erlang\u2019s programming model into a full-fledged object-oriented and functional language."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://www.sciencedirect.com/science/article/pii/S0304397508006695/pdf?md5=9c06db85926974f6a6f934e60ad7e856&pid=1-s2.0-S0304397508006695-main.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 85123331
  },
  {
    "blog_id": "gans-irl-ebm",
    "summary": [
      "Finn, Christiano, Abbeel, Levine, 2016  In generative modeling, there is a trade-off between maximum-likelihood approaches that produce a moment-matching distribution that try to \u201ccover\u201d all the modes of the unknown data distribution as opposed to approaches that estimate the unknown data distribution by \u201cfilling in\u201d as many modes as possible.",
      "The latter effectively allows one to produce more realistic samples but with lower diversity, while the former leads to a solution with probability mass in parts of the space that have negligible probability under the true distribution.",
      "Hence, the authors make the claim that for imitation learning and generative modeling, having both a \u201cdiscriminator\u201d as well as a \u201cgenerator\u201d encourages mode-seeking behavior.",
      "They show a connection between IRL and GANs by theoretically motivating an optimal IRL discriminator that learns the cost function, and an optimal IRL generator that is able to generate high-quality trajectories.",
      "It is assumed that one can sample from the generator density, i.e., that it is computable and can be held fixed while training the discriminator.",
      "The discriminator is modeled as a Boltzmann distribution with a parameterized energy function.",
      "Since energy-based models are a more general form of the Maximum Entropy IRL problem, the authors also were able to show a direct connection between GANs and EBMs.",
      "No experiments are provided in this paper, so the efficacy of using GANs for IRL remain to be seen.",
      "Guided Cost Learning Demo:  Notes  In Section 2.3.2.",
      "Guided Cost Learning, we have the following importance sampling formulation of the cost function, where the data is modeled as a Boltzmann distribution:  We want our biased distribution $q(\\tau) \\propto | \\exp(-c_{\\theta})(\\tau)) | = \\exp(-c_{\\theta}(\\tau))$.",
      "This is the optimal importance sampling distribution that produces the importance sampling estimate of some function of a random variable $f(X)$ with minimal variance.",
      "It can be shown that $q*(x) \\propto f(x) * p(x)$.",
      "Importance sampling estimates suffer from high variance if the sampling distribution $q$ is biased.",
      "In Guided Cost Learning, to ensure $q$ samples from all trajectories $\\tau$ with high values of $\\exp(-c_{\\theta}(\\tau))$, the demonstration data samples (low cost as result of IRL objective) are mixed with the generated samples from $q$.",
      "Hence, $q$ is replaced with $\\mu = \\frac{1}{2}p + \\frac{1}{2}q$ in the cost function.",
      "In Section 3, the author\u2019s theoretical argument for comparing GANs and IRL begins by assuming that the discriminator can be written as  To see that is similar to the standard sigmoid binary classification loss, recall the identity  Let $\\log Z$ be the bias of the sigmoid and notice that $\\log q(\\tau)$ is subtracted from the input.",
      "The author\u2019s argument continues by showing that this specific form of a GAN optimizes the same thing that MaximumEnt IRL does (pg.",
      "6).",
      "Questions  Being able to compute the generator\u2019s density and evaluate it cheaply enables this method, since you can then realistically learn an unbiased estimate of the partition function.",
      "What happens when the partition function remains biased?",
      "In Guided Cost Learning, what form does $q(\\tau)$ take?",
      "Since it attaches a probability to a trajectory, it should have the same form as the demonstration distribution $p$\u2026i.e., the input is a sequence of ($x_i$, $u_i$) and the output is a probability.",
      "The GAN training procedure minimizes the Jensen-Shannon divergence , which works sort of like the reverse-KL divergence.",
      "However, GANs don\u2019t try to fit as many modes of the data distribution as the model is able to- see Section 3.2.5 of Goodfellow\u2019s 2016 NIPS tutorial.",
      "In fact, this is in part a symptom of the mode collapse problem that GANs have.",
      "So, does training Guided Cost Learning/EBMs with GANs make them susceptible to this problem?",
      "The authors don\u2019t really discuss this, but it may only become apparent in practice.",
      "The motivation for the mixed sampling distribution for the importance sampling formulation seems to be realted to this."
    ],
    "author_id": "pemami",
    "pdf_url": "https://arxiv.org/pdf/1611.03852",
    "author_full_name": "Patrick Emami",
    "source_website": "https://pemami4911.github.io/index.html",
    "id": 61360879
  },
  {
    "blog_id": "tucker-tensor-factorization-for-knowledge-graph-completion",
    "summary": [
      "TuckER is a simple, yet powerful linear model that uses Tucker decomposition for the task of link prediction in knowledge graphs.",
      "Paper  Implementation  Knowledge Graph as a Tensor  Let E be the set of all the entities and R be the set of all the relations in a given knowledge graph (KG).",
      "The KG can be represented as a list of triples of the form (source entity, relation, object entity) or (es, r, eo).",
      "The list of triples can be represented as a third-order tensor (of binary values) where each element corresponds to a triple and each element\u2019s value corresponds to ether that element is present in the KG or not.",
      "The link prediction task can be formulated as - given a set of all triples, learn a scoring function that assigns a score to each triple.",
      "The score indicates whether the triple is actually present in the KG or not.",
      "TuckER Decomposition  Tucker decomposition factorizes a tensor into a set of factor matrices and a smaller core tensor.",
      "In the specific case of three-mode tensors (alternate representation of a KG), the given original tensor X (of shape IxJxK) can be factorized into a core tensor W (of shape PxQxR) and 3 factor matrics - A (of shape IxP), B (of shape JxQ) and C (of shape KxR) such that X is approximately W x1 A x2 B x3 C, where Xn denotes the tensor product along the nth mode.",
      "Generally, P, Q, R are smaller than I, J K (respectively) and W can be seen as a compressed version of X.  TuckER Decomposition for Link Prediction  Two embedding matrics are used for embedding the entities and the relations respectively.",
      "Entity embedding matrix E is shared for both subject and the object ie E = A = B.",
      "The scoring function is gives as W x1 es x2 wr x3 e0 where es, wr and eo are the embedding vectors corresonding to es, er and eo respectively.",
      "Note that both the core tensor and the factor matrices are to be learnt.",
      "Model is trained with the standard negative log-likelihood loss given as (for one triple):  y * log(p) + (1-y) * log(1-p)  To speed up training and increase accuracy, 1-N scoring is used.",
      "A given (es, r) is simultaneously scored for all the entities using the local-closed world assumption (knowledge graph is only locally complete).",
      "Handling asymmetric relations is straightforward by learning a relation embedding alongside a relation-agnostic core tensor which enables knowledge sharing across relations.",
      "Theoretical Analysis  One important consideration would be the expressive power of TuckER models, especially in relation to other models like ComplEx and SimplE.",
      "It can be shown the TuckER is fully expressive ie give any ground truth over E and R, there exists a TuckER model which can perfectly represent the data - using 1-hot entity and relation embedding.",
      "For full expressiveness, dimensionality of entity (relation) is nE (nR) where nE (nR) are the number of entities (relations).",
      "In comparsion, the required dimensionality for ComplEx is nE * nR (for both entity and relations) and for SimplE, it is min(E * nR, number of facts + 1) (for both entity and relations).",
      "Many existing models like RESCAL, DistMult, ComplEx, SimplE etc can be seen as special cases of TuckER.",
      "Experiments  Datasets  FB15k, FB15k-237, WN18, WN18RR  The max number of entities is around 41K and max number of relations is around 1.3K  Implementation  BatchNorm, Dropout and Learning rate decay are used.",
      "Metrics  Mean Reciprocal Rank (MRR) - the average of the inverse of mean rank assigned to the true triple overall ne generated triples.",
      "hits@k (k = 1, 3, 10) - percentage of times the true triple is ranked in the top k of the ne generated triples.",
      "Higher is better for both the metrics.",
      "Results  TuckER outperforms all the baseline models on all but one task.",
      "Dropout is an important factor with higher dropout rates (0, 3, 0.4, 0.5) needed for datasets with fewer training examples per relation (hence more prone to overfitting).",
      "TuckER improves performance more significantly when the number of relations is large.",
      "Even with lower embedding dimensions, TuckER\u2019s performance does not deteriorate as much as other models."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1903.01567",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 47686356
  },
  {
    "blog_id": "yolo9000",
    "summary": [
      "What  They suggest a new version of YOLO, a model to detect bounding boxes in images.",
      "Their new version is more accurate, faster and is trained to recognize up to 9000 classes.",
      "How  Their base model is the previous YOLOv1, which they improve here.",
      "Accuracy improvements  They add batch normalization to the network.",
      "Pretraining usually happens on ImageNet at 224x224, fine tuning for bounding box detection then on another dataset, say Pascal VOC 2012, at higher resolutions, e.g. 448x448 in the case of YOLOv1.",
      "This is problematic, because the pretrained network has to learn to deal with higher resolutions and a new task at the same time.",
      "They instead first pretrain on low resolution ImageNet examples, then on higher resolution ImegeNet examples and only then switch to bounding box detection.",
      "That improves their accuracy by about 4 percentage points mAP.",
      "They switch to anchor boxes, similar to Faster R-CNN.",
      "That's largely the same as in YOLOv1.",
      "Classification is now done per tested anchor box shape, instead of per grid cell.",
      "The regression of x/y-coordinates is now a bit smarter and uses sigmoids to only translate a box within a grid cell.",
      "In Faster R-CNN the anchor box shapes are manually chosen (e.g. small squared boxes, large squared boxes, thin but high boxes, ...).",
      "Here instead they learn these shapes from data.",
      "That is done by applying k-Means to the bounding boxes in a dataset.",
      "They cluster them into k=5 clusters and then use the centroids as anchor box shapes.",
      "Their accuracy this way is the same as with 9 manually chosen anchor boxes.",
      "(Using k=9 further increases their accuracy significantly, but also increases model complexity.",
      "As they want to predict 9000 classes they stay with k=5.)",
      "To better predict small bounding boxes, they add a pass-through connection from a higher resolution layer to the end of the network.",
      "They train their network now at multiple scales.",
      "(As the network is now fully convolutional, they can easily do that.)",
      "Speed improvements  They get rid of their fully connected layers.",
      "Instead the network is now fully convolutional.",
      "They have also removed a handful or so of their convolutional layers.",
      "Capability improvement (weakly supervised learning)  They suggest a method to predict bounding boxes of the 9000 most common classes in ImageNet.",
      "They add a few more abstract classes to that (e.g. dog for all breeds of dogs) and arrive at over 9000 classes (9418 to be precise).",
      "They train on ImageNet and MSCOCO.",
      "ImageNet only contains class labels, no bounding boxes.",
      "MSCOCO only contains general classes (e.g. \"dog\" instead of the specific breed).",
      "They train iteratively on both datasets.",
      "MSCOCO is used for detection and classification, while ImageNet is only used for classification.",
      "For an ImageNet example of class c, they search among the predicted bounding boxes for the one that has highest predicted probability of being c and backpropagate only the classification loss for that box.",
      "In order to compensate the problem of different abstraction levels on the classes (e.g. \"dog\" vs a specific breed), they make use of WordNet.",
      "Based on that data they generate a hierarchy/tree of classes, e.g. one path through that tree could be: object -> animal -> canine -> dog -> hunting dog -> terrier -> yorkshire terrier.",
      "They let the network predict paths in that hierarchy, so that the prediction \"dog\" for a specific dog breed is not completely wrong.",
      "Visualization of the hierarchy:  They predict many small softmaxes for the paths in the hierarchy, one per node:  Results  Accuracy  They reach about 73.4 mAP when training on Pascal VOC 2007 and 2012.",
      "That's slightly behind Faster R-CNN with VGG16 with 75.9 mAP, trained on MSCOCO+2007+2012.",
      "Speed  They reach 91 fps (10ms/image) at image resolution 288x288 and 40 fps (25ms/image) at 544x544.",
      "Weakly supervised learning  They test their 9000-class-detection on ImageNet's detection task, which contains bounding boxes for 200 object classes.",
      "They achieve 19.7 mAP for all classes and 16.0% mAP for the 156 classes which are not part of MSCOCO.",
      "For some classes they get 0 mAP accuracy.",
      "The system performs well for all kinds of animals, but struggles with not-living objects, like sunglasses.",
      "Example images (notice the class labels):"
    ],
    "author_id": "ALEJU",
    "pdf_url": "https://arxiv.org/pdf/1612.08242",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 17675091
  },
  {
    "blog_id": "towards_realistic_predictors",
    "summary": [
      "What  They propose a method for classifiers to estimate how hard/difficult an example is (for the specific classifier).",
      "They call that \"realistic predictors\" (in the sense of \"being realistic with respect to one's own abilities\").",
      "Their method allows them to  focus training on hard examples,  reject too difficult inputs,  based on point (2) guarantee average accuracies by rejecting inputs with certain thresholds of diffculty-estimates.",
      "(This is useful e.g. for some safety critical systems, where no decision might be better than a wrong decision.",
      "It can also be used for combinations of models and humans, where a model would automate easy tasks and a few humans deal with the remaining hard tasks.)",
      "How  Architecture  They have a given classifier F.  They need per example i a hardness estimate s_i denoting how difficult that input is for F.  They predict that value with HP-Net (hardness predictor), a network completely separate from F.  The estimated hardness values s_i influence the training of F for hard negative mining.",
      "The predictions of F are used during the training of HP-Net.",
      "Visualization:  Loss  For F (classifier):  F predicts for multiple classes a probability value (after softmax).",
      "They use a crossentropy loss weighted by the hardness:  Here, s_i is the hardness if the i-th example and p_i^c is the probability predicted by F for the correct class of the i-th example.",
      "This increases the loss for hard examples (high s_i value) and decreases it for easy examples (low s_i).",
      "For HP-Net (hardness predictor):  HP-Net predicts the inverse of the probability that F is going to predict for the correct class.",
      "So if F predicts for the correct class a high probability, HP-Net is trained to predict a low hardness value and vice versa.",
      "They use a binary crossentropy loss between p_i^c (F's prediction for the correct class) and s_i (hardness):  Training Schedule and Refinement  The full training schedules happens in the following steps.",
      "Train F and HP-Net jointly on a training set.",
      "Do that training iteratively (one batch for F, then one batch for HP-Net).",
      "Otherwise the networks would not converge for them.",
      "Use HP-Net to eliminate hard examples from the dataset.",
      "(E.g. the hardest 5%.)",
      "Train a new F on reduced dataset, result is F' (with higher accuracy for these easier examples).",
      "Reuse HP-Net from (1) during this training, keep its weights fixed.",
      "Output pair (F', HP-Net).",
      "Then, at test time examples above a certain hardness threshold are rejected (similar to step 2) and F' is applied to the remaining examples.",
      "Visualization:  Results  They test on MNIST, MIT67 and ImageNet.",
      "They test for both F and HP-Net the networks VGG, ResNet, kerasNet and LeNet5.",
      "(They slightly modify the heads for HP-Net.)",
      "They test with and without weight sharing between F and HP-Net.",
      "They observe that hardness scores predicted by HP-Net start with high values and then shift towards lower values during training.",
      "(As expected.)",
      "They achieve significantly worse accuracies if using weight sharing between F and HP-Net.",
      "This indicates that the two networks solve fairly different tasks.",
      "They achieve the highest accuracies when using the same network architectures for F and HP-Net (e.g. both ResNet).",
      "This indicates that the HP-Net has to operate in a similar way to F in order to predict its predictions.",
      "On ImageNet  They get almost 1 percentage point higher accuracy, even when not rejecting any examples.",
      "This is likely due to the indirect hard negative mining in the loss of F.  Rejecting the 5% most difficult examples results in 0.7 points higher accuracy (on the remaining ones).",
      "At 10% rejection rate they get 1.3 higher accuracy.",
      "One can reject examples based on the hardness score or based on the highest class probability predicted by F. Doing it based on the hardness score performs marginally better (+0.1 points) for high rejection rates and decently better (+0.7 points) for a rejection rate of 5%.",
      "Refining on the reduced dataset (from F to F') gives around 0.1 points higher accuracy.",
      "To reach with VGG the accuracy of ResNet at 2% rejection rate, one has to set VGG's rejection rate to 10%."
    ],
    "author_id": "ALEJU",
    "pdf_url": "http://openaccess.thecvf.com/content_ECCV_2018/papers/Pei_Wang_Towards_Realistic_Predictors_ECCV_2018_paper.pdf",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 15062404
  },
  {
    "blog_id": "1606.02185",
    "summary": [
      "This paper can be thought as proposing a variational autoencoder applied to a form of meta-learning, i.e. where the input is not a single input but a dataset of inputs.",
      "For this, in addition to having to learn an approximate inference network over the latent variable $z_i$ for each input $x_i$ in an input dataset $D$, approximate inference is also learned over a latent variable $c$ that is global to the dataset $D$.",
      "By using Gaussian distributions for $z_i$ and $c$, the reparametrization trick can be used to train the variational autoencoder.",
      "The generative model factorizes as  $p(D=(x_1,\\dots,x_N), (z_1,\\dots,z_N), c) = p(c) \\prod_i p(z_i|c) p(x_i|z_i,c)$  and learning is based on the following variational posterior decomposition:  $q((z_1,\\dots,z_N), c|D=(x_1,\\dots,x_N)) = q(c|D) \\prod_i q(z_i|x_i,c)$.",
      "Moreover, latent variable $z_i$ is decomposed into multiple ($L$) layers $z_i = (z_{i,1}, \\dots, z_{i,L})$.",
      "Each layer in the generative model is directly connected to the input.",
      "The layers are generated from $z_{i,L}$ to $z_{i,1}$, each layer being conditioned on the previous (see Figure 1 *Right* for the graphical model), with the approximate posterior following a similar decomposition.",
      "The architecture for the approximate inference network $q(c|D)$ first maps all inputs $x_i\\in D$ into a vector representation, then performs mean pooling of these representations to obtain a single vector, followed by a few more layers to produce the parameters of the Gaussian distribution over $c$.",
      "Training is performed by stochastic gradient descent, over minibatches of datasets (i.e. multiple sets $D$).",
      "The model has multiple applications, explored in the experiments.",
      "One is of summarizing a dataset $D$ into a smaller subset $S\\in D$.",
      "This is done by initializing $S\\leftarrow D$ and greedily removing elements of $S$, each time minimizing the KL divergence between $q(c|D)$ and $q(c|S)$ (see the experiments on a synthetic Spatial MNIST problem of section 5.3).",
      "Another application is few-shot classification, where very few examples of a number of classes are given, and a new test example $x'$ must be assigned to one of these classes.",
      "Classification is performed by treating the small set of examples of each class $k$ as its own dataset $D_k$.",
      "Then, test example $x$ is classified into class $k$ for which the KL divergence between $q(c|x')$ and $q(c|D_k)$ is smallest.",
      "Positive results are reported when training on OMNIGLOT classes and testing on either the MNIST classes or unseen OMNIGLOT datasets, when compared to a 1-nearest neighbor classifier based on the raw input or on a representation learned by a regular autoencoder.",
      "Finally, another application is that of generating new samples from an input dataset of examples.",
      "The approximate posterior is used to compute $q(c|D)$.",
      "Then, $c$ is assigned to its posterior mean, from which a value for the hidden layers $z$ and finally a sample $x$ can be generated.",
      "It is shown that this procedure produces convincing samples that are visually similar from those in the input set $D$.",
      "**My two cents**  Another really nice example of deep learning applied to a form of meta-learning, i.e. learning a model that is trained to take *new* datasets as input and generalize even if confronted to datasets coming from an unseen data distribution.",
      "I'm particularly impressed by the many tasks explored successfully with the same approach: few-shot classification and generative sampling, as well as a form of summarization (though this last probably isn't really meta-learning).",
      "Overall, the approach is quite elegant and appealing.",
      "The very simple, synthetic experiments of section 5.1 and 5.2 are also interesting.",
      "Section 5.2 presents the notion of a *prior-interpolation layer*, which is well motivated but seems to be used only in that section.",
      "I wonder how important it is, outside of the specific case of section 5.2.",
      "Overall, very excited by this work, which further explores the theme of meta-learning in an interesting way."
    ],
    "author_id": "hlarochelle",
    "pdf_url": "http://arxiv.org/pdf/1606.02185v1",
    "author_full_name": "Hugo Larochelle",
    "source_website": "https://www.shortscience.org/user?name=hlarochelle",
    "id": 87827594
  },
  {
    "blog_id": "helping-developers-help-themselves-automatic-decomposition-of-code-review-changes",
    "summary": [
      "Helping Developers Help Themselves: Automatic Decomposition of Code Review Changes \u2013 Barnett et al. 2015  Earlier this week we saw that pull requests with well organised commits are strongly preferred by integrators .",
      "Unfortunately, developers often make changes that incorporate multiple bug fixes, feature additions, refactorings, etc..",
      "These result in changes that are both large and only loosely related, if at all, leading to difficulty in understanding.",
      "Rounding out this week of papers from ICSE \u201915, Barnett et al. from Microsoft developed a tool called ClusterChanges which decomposes changesets into independent parts.",
      "In a study, developers found the partioning to be a helpful aid during code reviews.",
      "\u2026 we built a prototype graphical tool and used it to investigate changesets submitted for review in Bing and Office at Microsoft.",
      "Our quantitative evaluation shows that over 40% of changes submitted for review at Microsoft can be potentially decomposed into multiple partitions, indicating a high potential for use.",
      "The basic approach to identifying related changes is to take the diff-regions produced by a standard diff tool comparing before and after versions of files, and then group those diff-regions together based on definition-and-use relationships.",
      "We use the def-use relationship as the primary organizing principle for clustering diff-regions.",
      "Programmers often introduce interesting functional changes to code by introducing or modifying definitions along with their uses.",
      "ClusterChanges finds definitions (of types, fields, and methods) that have been changed within a diff-region, and the uses of that definition changed within diff-regions.",
      "Diff-regions f1 and f2 are then grouped into the same partition (RelatedDiffs) if any one of the following conditions is true:  f1 and f2 are both within the same enclosing method, or  there are changes to the definition of some element in f1 and corresponding changes in the use of that element in f2  f1 and f2 both contain a change to the use of some element, and that element is defined within the changeset, but not itself changed  We group diff-regions in the same method together because a) in practice, we observe that changes to the same method are often related, and b) in prior research, we observed that reviewers usually review methods atomically (i.e., they rarely review different diffregions in a method separately).",
      "Given these relations we create a partitioning over the set of diff-regions by computing the reflexive, symmetric and transitive closure of RelatedDiffs.",
      "The result of this process is a set of trivial partitions that are fully enclosed within a single method, or where there is only one diff-region and it is outside of a method, and a set of non-trivial partitions (everything else).",
      "The ClusterChanges tool then displays these partitions graphically:  ClusterChanges was applied to a randomly selected set of 1000 changesets submitted for review in the development of Microsoft Office 2013.",
      "While the most common case are changesets containing just one non-trivial partition, this still makes up only 45%.",
      "Nearly 42% of all changes contain more than one non-trivial partition.",
      "In addition, the proportion of changed methods that end up in non-trivial partitions is 66% on average per review.",
      "To the degree that CLUSTERCHANGES correctly identifies non-trivial partitions, this indicates that i) a large proportion of changesets can be decomposed into multiple independent changes, and ii) our decomposition covers a large fraction of changed methods in a review.",
      "Looking at changesets with lots of partitions, the authors found that many of these could be further consolidated by an enhancement to the tool that also considered:  (a) annotating several methods with common C# attributes such as Serializable or Obsolete, (b) a common refactoring (e.g. addition of a log message or variable renaming) across a large number of methods, and (c) relationships between overridden methods and their implementations.",
      "A user study was conducted in which the developers responsible for the changesets were asked if they agreed with the automated decomposition.",
      "Of the 20 participants, 16 said that our non-trivial partitions were both correct and complete, i.e., the non-trivial partitions were indeed independent, the diff-regions within each partition were related and there were no missing conceptual groups\u2026 most developers agree with our automatic partitioning and believe the decomposition is useful for reviewers to understand their changes better (some even asked for the prototype to use on their own reviews going forward).",
      "With these promising early results, the authors will now be moving on to do further studies with code reviewers."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://research.microsoft.com/pubs/238937/barnett2015hdh.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 187460
  },
  {
    "blog_id": "kondabmv15",
    "summary": [
      "This paper suggests a novel explanation for why dropout training is helpful: because it corresponds to an adaptive data augmentation method.",
      "Indeed, the authors point out that, when sampling a mask of the hidden units in a network (effectively setting the corresponding units to 0), the same effect would have been obtained by feeding as input an example tailored to yield activations of 0 for these units and otherwise the same activation for all other units.",
      "Since this \"ghost\" example will have to be different from the original example, and since each different mask would correspond to a different \"ghost\" example, then effectively mask sampling is similar to data augmentation.",
      "While in practice finding a ghost example that replicates exactly the same dropout hidden activations might not be possible, the authors show that finding an \"approximate\" ghost example that minimizes a distance between the target dropout activation and the deterministic activation of the ghost example works well.",
      "Indeed, they show that training a deep neural net on additional data generated by this procedure yields results that are at least as good as regular dropout on MNIST and CIFAR-10 (actually, the deterministic neural net still uses regular dropout at the input layer, however they do show that the additional ghost examples are necessary to match the neural net trained with dropout at all layers).",
      "Then the authors use that interpretation to justify a variation of dropout where the dropout rate isn't fixed, but itself is randomly sampled in some range for each example.",
      "Indeed, if we think of dropout at a fixed rate as a specific class of ghost data being added, varying the dropout rate corresponds to enriching even more the ghost data pool.",
      "The experiments show that this can help, though not by much.",
      "Finally, the authors propose an explanation of a property of dropout: that it tends to generate hidden representations that are sparser.",
      "Again, the authors rely on their interpretation of dropout as data augmentation.",
      "The explanation goes as follows.",
      "Training on the ghost data distribution might imply that the classification problem has become significantly harder.",
      "Specifically, it is quite possible that the addition of new ghost examples generates new isolated class clusters in input space that the model most now learn to  discriminate.",
      "And they hypothesize that the generation of such additional clusters would encourage sparsity.",
      "To test this hypothesis, the authors synthetically simulate this scenario, by sampling data on a circle, which is clustered in small arcs each assigned to one of 10 possible classes in cycling order.",
      "Decreasing the arc length thus increases the number of arcs, i.e. class clusters.",
      "They show that training deep networks on datasets with increasing number of class clusters does yield representations that are increasingly sparser.",
      "This thus suggests that dropout might indeed be equivalent to modifying the input distribution by adding such isolated class-specific clusters in input space.",
      "One assumption behind this analysis is that the sparsity patterns (i.e. the set of non-zero dimensions) play an important role in classification and incorporate most of the discriminative class information.",
      "This assumption is also confirmed in experiments, where converting the ReLU activation function by a binary activation (that is 1 if the pre-activation is positive and 0 otherwise) after training still yields a network with good performance (though slightly worse).",
      "#### My two cents  This is a really original and thought provoking paper.",
      "One interpretation I make of these results is that the inductive bias corresponding to using a deep neural network with ReLU activations is more valuable than one might have thought, and that the usefulness of deep neural networks goes beyond just being black boxes that can learn data-dependent representations.",
      "Otherwise, it's not clear to me why the ghost data implicitly generated by the architecture would be useful at all.",
      "This also suggests an experiment where such ghost samples would be fed to  another type of classifier, such as an SVM, to test whether the data augmentation is useful in itself and reflects meaningful structure in the data, as opposed to being somehow useful only for neural nets.",
      "I note that the results are mostly specific to architectures based on ReLU activations (not that this is a problem, but one should keep this in mind).",
      "I'd really like to see what the ghost samples look like.",
      "Do they correspond to interpretable images?",
      "The authors also mention that exploring how the samples change with training would be interesting to investigate, and I agree.",
      "Finally, I think there might be a typo in Figure 1.",
      "While the labels of a) and b) states that the arc length is smaller for a) than b), the plot clearly show otherwise."
    ],
    "author_id": "hlarochelle",
    "pdf_url": "http://arxiv.org/pdf/1506.08700",
    "author_full_name": "Hugo Larochelle",
    "source_website": "https://www.shortscience.org/user?name=hlarochelle",
    "id": 58793266
  },
  {
    "blog_id": "intrinsically-motivated-rl",
    "summary": [
      "In the search for more general AI agents, we must eventually abandon the practice of handcrafting reward functions in RL.",
      "As of right now, state-of-the-art RL agents still require a \u201cgood\u201d reward function that helps the agent learn complex behaviors and, by extension, an approximation of the optimal policy.",
      "For example, consider the popular pendulum task.",
      "The goal of the agent is to swing up and balance the pendulum.",
      "If the reward function is simply a +1 for having balanced the pendulum at the end of the episode (achieved the goal), or a -1 if the agent failed to balance the pendulum (failed to achieve the goal), the agent would never learn anything.",
      "With positive rewards arriving so incredibly infrequently, the agent would not have any motivation to explore different swing up behaviors, since every action it tries would seem equally bad.",
      "Hence, a more complex reward function is necessary that encourages the agent to develop intermediate \u201csub-behaviors\u201d which allow it to achieve its goal.",
      "One solution to this problem as presented by the authors is focusing on the acquisition of skills, or options, that provide the agent with the ability to use composition to carry out heirarchical planning-tasks.",
      "The authors claim that agents should have a sophisticated internal motivational system that should not have to be redesigned for different problems.",
      "Options are closed-loop \u201cmini\u201d-policies for taking action over a period of time.",
      "The authors present a learning framework based on semi-Markov Decision Processes , which are used for adding temporal abstractions to RL.",
      "This framework utilizes the saliency of certain events that occur in the environment to generate intrinsic reward signals that stimulate curiosity within the agent.",
      "Eventually, the agent will \u201close interest\u201d in the event, as it loses its novelty (a.k.a.",
      "boredom), but retain knowledge of the interaction.",
      "Extrinsic reward signals are present and are generated by accomplishing goals.",
      "The authors demonstrated their framework on a small world experiment, where an agent in a grid-world was able to interact with a few objects.",
      "Some of the options it could learn involved turning a light switch on and off, kicking a ball, or making a bell ring.",
      "Notes  The policies of many options are updated simultaneously during an agent\u2019s interaction with the environment.",
      "If an option could have produced the current action in the current state, its policy can be updated as well.",
      "In general, options have to be provided by the system designer.",
      "The state that could lead to the execution of an option, the option\u2019s terminating condition, and the reward function that evaluates the option\u2019s performance is required.",
      "It is desirable to automate the discovery of new options.",
      "This paper is a good starting point for looking into the area of RL that deals with the reward-function problem.",
      "The concept of developing a motivational system that is shared across tasks is one that has not really been explored/is not prevalent today.",
      "Going back to my example of the pendulum task- an RL agent would require an internal motivational system that understood physics in order to explore the environment effectively and receive a reward from the salient event of balancing the pendulum.",
      "If the agent could understand from prior experiences what balancing means (i.e., the agent utilizes a learned model of physics to generate some type of understanding of the physical properties of an object in a balanced state), then the agent could motivate itself to select specific sequences of actions that bring the pendulum closer to a balanced state.",
      "Therefore, the system designer could use a much simpler and less informative reward signal without having to do almost any hand-crafting.",
      "An interesting experiment to test this would be to train an RL agent on a number of different tasks that involve balancing, and then to use transfer learning (sharing network weights?)",
      "to have the agent solve the pendulum task.",
      "Or, to use a form of (differentiable) memory to store an approximately optimal set of sequences of actions related to balancing objects on a number of tasks.",
      "The idea is to capture the learned \u201cskills\u201d and transfer them to new tasks.",
      "The agent could use these experiences to accelerate learning on a novel balancing task.",
      "If the memory is associative, you could also store multiple physical skills beyond just balancing.",
      "From a practical standpoint, the prior experience would need to influence the Q-values of a given state and action through some sort of \u201cbonus\u201d."
    ],
    "author_id": "pemami",
    "pdf_url": "http://www.cs.cornell.edu/~helou/IMRL.pdf",
    "author_full_name": "Patrick Emami",
    "source_website": "https://pemami4911.github.io/index.html",
    "id": 39353178
  },
  {
    "blog_id": "conditional-similarity-networks",
    "summary": [
      "Problem Statement  A common way of measuring image similarity is to embed them into feature spaces where distance acts as a proxy for similarity.",
      "But this feature space can capture one (or a weighted combination) of the many possible notions of similarity.",
      "What if contracting notions of similarity could be captured at the same time - in terms of semantically distinct subspaces.",
      "The paper proposes a new architecture called as Conditional Similarity Networks (CSNs) which learns a disentangled embedding such that the features, for different notions of similarity, are encoded into separate dimensions.",
      "It jointly learns masks (or feature extractors) that select and reweights relevant dimensions to induce a subspace that encodes a specific notion of similarity.",
      "Conditional Similarity Networks  Given an image, x, learn a non-linear feature embedding f(x) such that for any 2 images x1 and x2, the euclidean distance between f(x1) and f(x2) reflects their similarity.",
      "Conditional Similarity Triplets  Given a triplet of images (x1, x2, x3) and a condition c (the notion of similarity), an oracle (say crowd) is used to determmine if x1 is more similar to x2 or x3 as per the given criteria c.  In general, for images i, j, l, the triplet t is ordered {i, j, l | c} if i is more similar to j than l.  Learning From Triplets  Define a loss function LT() to model the similarity structure over the triplets.",
      "LT(i, j, l) = max{0, D(i, j) - D(i, l) + h} where D is the euclidean distance function and h is the similarity scalar margin to prevent trivial solutions.",
      "To model conditional similarities, masks m are defined as m = \u03c3(\u03b2) where \u03c3 is the RELU unit and \u03b2 is a set of parameters to be learnt.",
      "mc denotes the selection of the c-th mask column from feature vector.",
      "It thus acts as an element-wise gating function which selects the relevant dimensions of the embedding to attend to a particular similarity concept.",
      "The euclidean function D now computes the masked distance (f(i, c)mc) between the two given images.",
      "Two regularising terms are also added - L2 norm for D and L1 norm for m.  Experiments  Datasets  Fonts dataset by Bernhardsson  3.1 million 64 by 64-pixel grey scale images.",
      "Zappos50k shoe dataset  Contains 50,000 images of individual richly annotated shoes.",
      "Characteristics of interest:  Type of the shoes (i.e., shoes, boots, sandals or slippers)  Suggested gender of the shoes (i.e., for women, men, girls or boys)  Height of the shoes\u2019 heels (0 to 5 inches)  Closing mechanism of the shoes (buckle, pull on, slip on, hook and loop or laced up)  Models  Initial model for the experiments is a ConvNet pre-trained on ImageNet  Standard Triplet Network  Learn from all available triplets jointly as if they have the same notion of similarity.",
      "Set of Task Specific Triplet Networks  Train n separate triplet networks such that each is trained on a single notion of similarity.",
      "Needs far more parameters and compute.",
      "Conditional Similarity Networks - fixed disjoint masks  In this version, only the convolutional filters and the embedding is learnt and masks are predefined to be disjoint.",
      "Aims to learn a fully disjoint embedding.",
      "Conditional Similarity Networks - learned masks  Learns all the components - conv filters, embedding and the masks.",
      "Refer paper for details on hyperparameters.",
      "Results  Visual exploration of the learned subspaces (t-sne visualisation) show that network successfully disentangles different features in the embedded vector space.",
      "The learned masks are very sparse and share dimensions.",
      "This shows that CSNs may learn to only use the required number of dimensions thereby doing away with the need of picking the right size of embedding.",
      "Order of performance:  CSNs with learned masks > CSNs with fixed masks > Task-specific networks > standard triplet network.",
      "Though CSNs with learned masks require more training data.",
      "CSNs also outperform Standard Triplet Network when used as off the shelf features for (brand) classification task and is very close to the performance of ResNet trained on ImageNet.",
      "This shows that while CSN retained most of the information in the original network, the training mechanism of Standard Triplet Network hurts the underlying conv features and their generalising capability"
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1603.07810.pdf",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 31303819
  },
  {
    "blog_id": "dialoguernn-emotion-classification-in-conversation-1e389d035aff",
    "summary": [
      "Welcome to another paper review!",
      "DialogueRNN is a method that aims to perform emotion classification of utterances in the context of a conversation.",
      "Many applications can benefit from such type of analysis such as understanding the emotional context and interchange in debates and social media threads.",
      "Previous methods do not pay attention to individuals\u2019 emotional states.",
      "The proposed emotion detection model considers individual speakers by focusing on three different aspects: the speaker, the context of preceding utterances, and the emotion from preceding utterances.",
      "The idea is that these three aspects are important to accurately predict the emotion of the utterance.",
      "Model  Utterances for a party (i.e., individual) are represented through textual features obtained from a convolutional neural network.",
      "As utterances come in a multimodal setting, audio and visual features are also extracted using 3D-CNN and openSMILE, respectively.",
      "The network is trained at the utterance level with the target emotion labels.",
      "The proposed model (called DialogueRNN), illustrated in the figure above, determines the final emotion of the utterance through the following factors:  Party state \u2014 models the parties\u2019 emotion dynamics through the conversations.",
      "The basic idea behind the party state is to ensure that the model is aware of the speaker of each utterance in the conversation.",
      "Global state \u2014 models the context of an utterance in the dialogue, given by jointly encoding preceding utterances and the party state.",
      "Note that attention mechanism is applied to the global state to provide improved context representation.",
      "This state basically serves as the speaker-specific utterance representation.",
      "Emotion representation \u2014 inferred through party state and preceding speaker\u2019s states as context (global state).",
      "This representation is used to perform the final emotion classification via a softmax layer.",
      "Each component of the architecture is modeled by a gated recurrent unit (GRU).",
      "It\u2019s important to note that during training, the speaker state is updated using the current utterance along with its context, which is nothing less than the preceding global states applied an attention mechanism.",
      "The role of the attention mechanism is that it assigns higher attention scores to the utterances that are emotionally relevant to the current utterance.",
      "Overall, the speaker update encodes \u2014 via the Party GRU (shown in blue) \u2014 the information on the current utterance along with its context from the Global GRU (shown in green).",
      "All this information is important for performing the final emotion classification, which is performed by the emotion GRU (shown in maroon).",
      "Note that the current emotion classification also relies on the previous emotion-relevant information as well.",
      "Variants  Several variants of the DialogueRNN model are proposed and compared in this study:  DialogueRNN_l \u2014 considers an extra listener state (defined at the end of this post) while a speaker utters.",
      "BiDialogueRNN \u2014 a bidirectional RNN architecture is used instead  DialogueRNN+Att \u2014 attention is applied over all surrounding emotion representations  BiDialogueRNN+Att \u2014 similar to the previous model but considers a bidirectional RNN instead  Other baselines are also proposed which you can refer to in the paper.",
      "Results  Two datasets are used for all experiments: IEMOCAP and AVEC .",
      "Both datasets contain interactions between multiple parties.",
      "From the table below, we can observe that DialogueRNN (highlighted in green) outperforms all baselines and the state-of-the-art model (CMN) on both datasets.",
      "Note that these results are only using the text modality.",
      "We can also observe in the table above that the listener component (model highlighted in orange) doesn\u2019t improve the model\u2019s performance.",
      "In general, the other variants were found to perform well, especially the BiDialogueRNN+Att, which in general produced the better results.",
      "As shown in the table below, the proposed model, DialogueRNN, also significantly outperforms other models in the multimodal setting (using a fusion of modalities).",
      "As a case study, we can observe from the attention figure below that DialogueRNN correctly anticipates the emotion of frustration (labeled Turn 44) using the preceding context (41 and 42).",
      "For the CMN model, this was found not to be the case.",
      "An important ablation study was conducted to observe the importance of Emotion GRU and Party State components.",
      "We can see from the table below that the absence of part state decreases performance.",
      "In fact, it can be observed that the party state seems to be more important than Emotion GRU.",
      "Listener update changes the state of the listener based on the current speaker utterance.",
      "Visual cues are used to represent this information.",
      "However, authors found via the experiments that this update has no effect in a conversation as a silent party has no influence in a conversation.",
      "Reference  DialogueRNN: An Attentive RNN for Emotion Detection in Conversations \u2014 [ Paper ] | [ Code ]"
    ],
    "author_id": "DAIR-AI",
    "pdf_url": "https://arxiv.org/pdf/1811.00405",
    "author_full_name": "Elvis Saravia",
    "source_website": "https://github.com/dair-ai/nlp_paper_summaries",
    "id": 11418476
  },
  {
    "blog_id": "r-fcn",
    "summary": [
      "What  They present a variation of Faster R-CNN, i.e. a model that predicts bounding boxes in images and classifies them.",
      "In contrast to Faster R-CNN, their model is fully convolutional.",
      "In contrast to Faster R-CNN, the computation per bounding box candidate (region proposal) is very low.",
      "How  The basic architecture is the same as in Faster R-CNN:  A base network transforms an image to a feature map.",
      "Here they use ResNet-101 to do that.",
      "A region proposal network (RPN) uses the feature map to locate bounding box candidates (\"region proposals\") in the image.",
      "A classifier uses the feature map and the bounding box candidates and classifies each one of them into C+1 classes, where C is the number of object classes to spot (e.g. \"person\", \"chair\", \"bottle\", ...) and 1 is added for the background.",
      "During that process, small subregions of the feature maps (those that match the bounding box candidates) must be extracted and converted to fixed-sizes matrices.",
      "The method to do that is called \"Region of Interest Pooling\" (RoI-Pooling) and is based on max pooling.",
      "It is mostly the same as in Faster R-CNN.",
      "Visualization of the basic architecture:  Position-sensitive classification  Fully convolutional bounding box detectors tend to not work well.",
      "The authors argue, that the problems come from the translation-invariance of convolutions, which is a desirable property in the case of classification but not when precise localization of objects is required.",
      "They tackle that problem by generating multiple heatmaps per object class, each one being slightly shifted (\"position-sensitive score maps\").",
      "More precisely:  The classifier generates per object class c a total of k*k heatmaps.",
      "In the simplest form k is equal to 1.",
      "Then only one heatmap is generated, which signals whether a pixel is part of an object of class c.  They use k=3*3.",
      "The first of those heatmaps signals, whether a pixel is part of the top left corner of a bounding box of class c. The second heatmap signals, whether a pixel is part of the top center of a bounding box of class c (and so on).",
      "The RoI-Pooling is applied to these heatmaps.",
      "For k=3*3, each bounding box candidate is converted to 3*3 values.",
      "The first one resembles the top left corner of the bounding box candidate.",
      "Its value is generated by taking the average of the values in that area in the first heatmap.",
      "Once the 3*3 values are generated, the final score of class c for that bounding box candidate is computed by averaging the values.",
      "That process is repeated for all classes and a softmax is used to determine the final class.",
      "The graphic below shows examples for that:  The above described RoI-Pooling uses only averages and hence is almost (computationally) free.",
      "They make use of that during the training by sampling many candidates and only backpropagating on those with high losses (online hard example mining, OHEM).",
      "\u00c0 trous trick  In order to increase accuracy for small bounding boxes they use the \u00e0 trous trick.",
      "That means that they use a pretrained base network (here ResNet-101), then remove a pooling layer and set the \u00e0 trous rate (aka dilation) of all convolutions after the removed pooling layer to 2.",
      "The \u00e1 trous rate describes the distance of sampling locations of a convolution.",
      "Usually that is 1 (sampled locations are right next to each other).",
      "If it is set to 2, there is one value \"skipped\" between each pair of neighbouring sampling location.",
      "By doing that, the convolutions still behave as if the pooling layer existed (and therefore their weights can be reused).",
      "At the same time, they work at an increased resolution, making them more capable of classifying small objects.",
      "(Runtime increases though.)",
      "Training of R-FCN happens similarly to Faster R-CNN.",
      "Results  Similar accuracy as the most accurate Faster R-CNN configurations at a lower runtime of roughly 170ms per image.",
      "Switching to ResNet-50 decreases accuracy by about 2 percentage points mAP (at faster runtime).",
      "Switching to ResNet-152 seems to provide no measureable benefit.",
      "OHEM improves mAP by roughly 2 percentage points.",
      "\u00c0 trous trick improves mAP by roughly 2 percentage points.",
      "Training on k=1 (one heatmap per class) results in a failure, i.e. a model that fails to predict bounding boxes.",
      "k=7 is slightly more accurate than k=3."
    ],
    "author_id": "ALEJU",
    "pdf_url": "https://arxiv.org/pdf/1605.06409",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 57983162
  },
  {
    "blog_id": "weston16",
    "summary": [
      "This paper investigates different paradigms for learning how to answer natural language queries through various forms of feedback.",
      "Most interestingly, it investigates whether a model can learn to answer correctly questions when the feedback is presented purely in the form of a sentence (e.g. \"Yes, that's right\", \"Yes, that's correct\", \"No, that's incorrect\", etc.).",
      "This later form of feedback is particularly hard to leverage, since the model has to somehow learn that the word \"Yes\" is a sign of a positive feedback, but not the word \"No\".",
      "Normally, we'd trained a model to directly predict the correct answer to questions based on feedback provided by an expert that always answers correctly.",
      "\"Imitating\" this expert just corresponds to regular supervised learning.",
      "The paper however explores other variations on this learning scenario.",
      "Specifically, they consider 3 dimensions of variations.",
      "The first dimension of variation is who is providing the answers.",
      "Instead of an expert (who is always right), the paper considers the case where the model is instead observing a different, \"imperfect\" expert whose answers come from a fixed policy that answers correctly only a fraction of the time (the paper looked at 0.5, 0.1 and 0.01).",
      "Note that the paper refers to these answers as coming from \"the learner\" (which should be the model), but since the policy is fixed and actually doesn't depend on the model, I think one can also think of it as coming from another agent, which I'll refer to as the imperfect expert (I think this is also known as \"off policy learning\" in the RL world).",
      "The second dimension of variation on the learning scenario that is explored is in the nature of the \"supervision type\" (i.e. nature of the labels).",
      "There are 10 of them (see Figure 1 for a nice illustration).",
      "In addition to the real expert's answers only (Type 1), the paper considers other types that instead involve the imperfect expert and fall in one of the two categories below:  1.",
      "Explicit positive / negative rewards based on whether the imperfect expert's answer is correct.",
      "2.",
      "Various forms of natural language responses to the imperfect expert's answers, which vary from worded positive/negative feedback, to hints, to mentions of the supporting fact for the correct answer.",
      "Also, mixtures of the above are considered.",
      "Finally, the third dimension of variation is how the model learns from the observed data.",
      "In addition to the regular supervised learning approach of imitating the observed answers (whether it's from the real expert or the imperfect expert), two other distinct approaches are considered, each inspired by the two categories of feedback mentioned above:  1.",
      "Reward-based imitation: this simply corresponds to ignoring answers from the imperfect expert for which the reward is not positive (as for when the answers come from the regular expert, they are always used I believe).",
      "2.",
      "Forward prediction: this consists in predicting the natural language feedback to the answer of the imperfect expert.",
      "This is essentially treated as a classification problem over possible feedback (with negative sampling, since there are many possible feedback responses), that leverages a soft-attention architecture over the answers the expert could have given, which is also informed by the actual answer that was given (see Equation 2).",
      "Also, a mixture of both of these learning approaches is considered.",
      "The paper thoroughly explores experimentally all these dimensions, on two question-answering datasets (single supporting fact bAbI dataset and MovieQA).",
      "The neural net model architectures used are all based on memory networks.",
      "Without much surprise, imitating the true expert performs best.",
      "But quite surprisingly, forward prediction leveraging only natural language feedback to an imperfect expert often performs competitively compared to reward-based imitation.",
      "#### My two cents This is a very thought provoking paper!",
      "I very much like the idea of exploring how a model could learn a task based on instructions in natural language.",
      "This makes me think of this work  [ref]  on using zero-shot learning to learn a model that can produce a visual classifier based on a description of what must be recognized.",
      "Another component that is interesting here is studying how a model can learn without knowing a priori whether a feedback is positive or negative.",
      "This sort of makes me think of [this work]( [url]"
    ],
    "author_id": "hlarochelle",
    "pdf_url": "http://arxiv.org/pdf/1604.06045",
    "author_full_name": "Hugo Larochelle",
    "source_website": "https://www.shortscience.org/user?name=hlarochelle",
    "id": 18521109
  },
  {
    "blog_id": "learning-independent-causal-mechanisms",
    "summary": [
      "The paper presents a very interesting approach for learning independent (inverse) data transformation from a set of transformed data points in an unsupervised manner.",
      "Formulation  We start with a given data distribution P (say the MNIST dataset) where each x \u03b5 Rd.",
      "Consider N transformations M1, \u2026, MN (functions that map input x to transformed input x\u2019).",
      "Note that N need not be known before hand.",
      "These transformations can be thought of as independent (from other transformations) causal mechanisms.",
      "Applying these transformation would give N new distributions Q1, \u2026, QN.",
      "These individual distributions are combined to form a single transformed distribution Q which contains the union of samples from the individual distributions.",
      "At training time, two datasets are created.",
      "One dataset corresponds to untransformed objects (sampled from P), referred to as DP.",
      "The other dataset corresponds to samples from the transformed distribution Q and is referred to as DQ.",
      "Note that all the samples in DP and DQ are sampled independently and no supervising information is needed.",
      "A series of N\u2019 parametric models, called as experts, are initialized and would be trained to learn the different mechanisms.",
      "For simplicity, assume that N = N\u2019.",
      "If N > N\u2019, some experts would learn more than one transformation or certain transformations would not be learnt.",
      "If N < N\u2019, some experts would not learn anything or some experts would learn the same distribution.",
      "All of these cases can be diagnosed and corrected by changing the number of experts.",
      "The experts are trained with the goal of maximizing an objective parameter c: Rd to R. c takes high values on the support of P and low values outside.",
      "During training, an example xQ (from DQ) is fed to all the experts at the same time.",
      "Each expert produces a value cj = c(Ej(xQ))  The winning expert is the one whose output is the max among all the outputs.",
      "Its parameters are updated to maximise its output while the other experts are not updated.",
      "This forces the best performing model to become even better and hence specialize.",
      "The objective c comes from adversarial training where a discriminator network discriminates between the untransformed input and the output of the experts.",
      "Each expert can be thought of as a GAN that conditions on the input xQ (and not on a noise vector).",
      "The output of the different experts is fed to the discriminator which provides both a selection mechanism and the gradients for training the experts.",
      "Experiments  Experiments are performed on the MNIST dataset using the transformations like translation along 4 directions and along 4 diagonals, contrast shift and inversion.",
      "The discriminator is further trained against the output of all the losing experts thereby furthering strengthing the winning expert.",
      "Approximate Identity Initialization  The experts are initialized randomly and then pretrained to approximate the identity function by training with identical input-output pairs.",
      "This ensures that the experts start from a similar level.",
      "In practice, it seems necessary for the success of the proposed approach.",
      "Observations  During the initial phase, there is a heavy competition between the experts and eventually different winners emerge for different transformations.",
      "The approximate quality of reconstructed output was also evaluated using a downstream task.",
      "3 type of inputs were created:  Untransformed images  Transformed images  Transformed images a being processed by experts.",
      "These inputs are fed to a pretrained MNISTN classifier.",
      "The classifier performs poorly on the transformed images while the performance for images processed by experts quickly catches up with the performance on untransformed images.",
      "The experts Ei generalize on the data points from a different dataset as well.",
      "To test the generalisation capabilities of the expert, a sample of data from the omniglot dataset is transformed and fed to experts (which are trained only on MNIST).",
      "Each expert consistently applies the same transformation even though the inputs are outside the training domain.",
      "This suggests that the experts have generalized to different transformations irrespective of the underlying dataset.",
      "Comments  The experiments are quite limited in terms of complexity of dataset and complexity of transformation but it provides evidence for a promising connection between deep learning and causality.",
      "Appendix mentions that in case there are too many experts, for most of the tasks, only one model specialises and the extra experts do not specialize at all.",
      "This is interesting as there is no explicit regularisation penalty which prevents the emergence of multiple experts per task."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1712.00961",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 8784223
  },
  {
    "blog_id": "detect_to_track_and_track_to_detect",
    "summary": [
      "What  They suggest a variation of Faster R-CNN architectures that also tracks objects in videos (additionally to detecting them, i.e. additionally to bounding box detection).",
      "The model does detection and tracking in one forward pass.",
      "The \"tracking\" here is rather primitive: It only signals where an object frame t will likely be in frame t+x.",
      "That information can then be used to compute chains of bounding boxes over time.",
      "How  Architecture  They base their model on R-FCN, which is similar to Faster R-CNN.",
      "They have two images/frames (t and t+x) as inputs.",
      "Same as in Faster R-CNN:  They apply a base network (e.g. ResNet) to both of them.",
      "They use an RPN to locate RoIs in the feature maps.",
      "They use RoI-Pooling to extract and pool each RoI's features.",
      "They apply classification (object class) and regression (object location/dimensions) to each RoI.",
      "They compute correlations between the feature maps of both frames.",
      "They then extract and pool each RoI from the feature and correlation maps of from t.  They then apply a tracking branch, which predicts the new position (x, y) and dimensions (height, width) of the RoI in frame t+x.",
      "That prediction can be used to find a matching bounding box in frame t+x, which again can be used to track objects over time.",
      "During training, they use a smooth L1 loss for the tracking branch.",
      "Visualization of the architecture:  Visualization of the tracking process:  Correlation  Correlations are estimated between the feature maps of frame t and t+x.",
      "In the simplest form, correlation is measured by extracting some point (i,j) from the feature maps t and t+x (resulting in two vectors) and then computing the scalar product (resulting in a scalar).",
      "They use a more complex correlation, which compares point (i,j) in frame t not only to (i,j) in t+x, but to (i+d,j+q), where d and q come from an interval.",
      "I.e. they compare to a neighbourhood in frame t+x.",
      "They use d=8 (same for q), resulting in 64 correlation values per (i,j).",
      "Furthermore, they compute correlations for multiple scales (i.e. early and late in the base network).",
      "(With striding so that the correlation maps end up having the same sizes.)",
      "Tubelets  The previous architecture only regresses the new location of a bounding box in frame t+x.",
      "From that information they derive tubelets, tracks of bounding boxes over multiple frames (i.e. objects tracked over time).",
      "Core of the method to do that are scores between pairs of bounding boxes (between frame t and t+x).",
      "The scores are computed per class, i.e. no score between two bounding boxes of different classes.",
      "Each score has three components:  (a) Probability of the bounding box in frame t having the specified class,  (b) Probability of the bounding box in frame t+x having the specified class,  (c) A flag whether the bounding box in frame t+x matches the expected future position of the bounding box in frame t. The future position is estimated using the new tracking branch.",
      "The flag has a value of 1 if the IoU between future bounding box and expectation is >0.5.",
      "Once all scores are computed, the Viterbi algorithm can be used to compute optimal paths over the frames, leading to tracked objects, which are the tubelets.",
      "After generating a tubelet, they increase the object detection scores of the respective class of all bounding boxes in the tubelet.",
      "(Because future frames are now giving support that a bounding box really does have a specific object class.)",
      "Results  They train on ImageNet VID dataset (after first training on standard ImageNet).",
      "Training with the tracking branch improves raw object detection by about 1.6 points mAP.",
      "Predicting on two frames (e.g. t and t+1) with tracking improves object detection scores for some classes significantly (e.g.",
      "+9 points for rabbits).",
      "Adding a significant gap between the frames (t and t+10) is still enough to improve object detection scores (a bit less though).",
      "Results overview:  Their tracking branch barely adds runtime to the model.",
      "Only the tubelet generation adds significant time.",
      "They modify it (somehow, not described) to a non-causal(?)",
      "version, which runs faster.",
      "Additional runtime per frame with tracking is then 14ms."
    ],
    "author_id": "ALEJU",
    "pdf_url": "https://arxiv.org/pdf/1710.03958",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 38779502
  },
  {
    "blog_id": "cuckoo-search-via-levy-flights",
    "summary": [
      "Cuckoo Search via L\u00e9vy Flights \u2013 Yang et al. 2010  Another nature inspired optimisation algorithm today \u2013 and this time it\u2019s the turn of the cuckoos coupled with the flight pattern of fruit flies (which follow a L\u00e9vy flight) A L\u00e9vy flight is a random walk in which the step-lengths follow a heavy-tailed probability distribution.",
      "A recent study by Reynolds and Frye shows that fruit flies or Drosophila melanogaster, explore their landscape using a series of straight flight paths punctuated by a sudden 90o turn, leading to a L\u00e9vy-flight-style intermittent scale free search pattern.",
      "Studies on human behaviour such as the Ju/\u2019hoansi hunter-gatherer foraging patterns also show the typical feature of L\u00e9vy flights.",
      "Even light can be related to L\u00e9vy flights.",
      "Subsequently, such behaviour has been applied to optimization and optimal search, and preliminary results show its promising capability.",
      "Cuckoos lay their eggs in the nests of other birds (host birds).",
      "Some host birds can engage direct conflict with the intruding cuckoos.",
      "If a host bird discovers the eggs are not their own, they will either throw these alien eggs away or simply abandon its nest and build a new nest elsewhere.",
      "Some cuckoo species such as the New World brood-parasitic Tapera have evolved in such a way that female parasitic cuckoos are often very specialized in the mimicry in colour and pattern of the eggs of a few chosen host species.",
      "This reduces the probability of their eggs being abandoned and thus increases their reproductivity.",
      "In addition, the timing of egg-laying of some species is also amazing.",
      "Parasitic cuckoos often choose a nest where the host bird just laid its own eggs.",
      "In general, the cuckoo eggs hatch slightly earlier than their host eggs.",
      "Once the first cuckoo chick is hatched, the first instinct action it will take is to evict the host eggs by blindly propelling the eggs out of the nest, which increases the cuckoo chick\u2019s share of food provided by its host bird.",
      "Studies also show that a cuckoo chick can also mimic the call of host chicks to gain access to more feeding opportunity.",
      "Each egg in a nest will represent a solution.",
      "A cuckoo egg represents a candidate new solution, and the aim is to use the new and potentially better solutions to replace not-so-good solutions in the nests.",
      "To keep things simple we assume that each nest has only one egg.",
      "When generating a new solution for a cuckoo i, a  L\u00e9vy flight is performed:  xit+1 = xit + \u03b1 \u2295L\u00e9vy(\u03bb)  \u2026where \u03b1 > 0 is the step size which should be related to the scales of the problem of interest.",
      "In most cases, we can use \u03b1 = 1.",
      "The above equation is essentially the stochastic equation for random walk.",
      "In general, a random walk is a Markov chain whose next status/location only depends on the current location (the first term in the above equation) and the transition probability (the second term).",
      "The product\u2295means entrywise multiplications.",
      "This entrywise product is similar to those used in PSO, but here the random walk via L\u00e9vy  flight is more efficient in exploring the search space as its step length is much longer in the long run.",
      "We start by generating an initial population of host nests, and then each iteration proceeds as follows:  Get a cuckoo randomly by L\u00e9vy flights, evaluate its fitness function  Choose a nest randomly, and if the new cuckoo performs better than the egg in the nest, replace the egg in the nest with the new cuckoo  Choose some fraction pa of the worst nests: abandon those nests and build new ones.",
      "Cuckoo search compares favourably against genetic algorithms and PSO (as did bats and fireflies \u2013 I wish we could get some comparison of these against each other\u2026 ).",
      "Simulations and comparison show that CS is superior to these existing algorithms for multimodal objective functions.",
      "This is partly due to the fact that there are fewer parameters to be fine-tuned in CS than in PSO and genetic algorithms.",
      "In fact, apart from the population size n, there is essentially one parameter pa.",
      "Furthermore, our simulations also indicate that the convergence rate is insensitive to the parameter pa.",
      "This also means that we do not have to fine tune these parameters for a specific problem.",
      "Subsequently, CS is more generic and robust for many optimization problems, comparing with other meta-heuristic algorithms.",
      "It seems to me that the fruit fly L\u00e9vy flight behaviour is more significant in this solution than the cuckoo part.",
      "So we could easily have called this the Fruit Fly Algorithm too."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://www.cs.tufts.edu/comp/150GA/homeworks/hw3/_reading7%20Cuckoo%20search.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 70335565
  },
  {
    "blog_id": "pengllw15",
    "summary": [
      "This paper presents a neural network architecture that can take as input a question and a sequence of facts expressed in natural language (i.e. a sequence of words) and produce its output the answer to that question.",
      "The main components of the architecture are as follows:  * The question (q) and the facts (f_1, ... , f_K) are each individually transformed into a fixed size vector using the same GRU RNN (with the last hidden layer serving as the vector representation).",
      "* These vectors are each passed through \"reasoning layers\", where each layer transforms the question q and the facts f_k into a new vector representation.",
      "This is done by feeding each question fact pair (q,f_k) to a neural network that outputs a new representation for the fact f_k (which replaces its old representation in the layer), as well as a new representation for the question.",
      "All K new question representations are then pooled to obtain a single question representation that replace the old one in the layer.",
      "* The last reasoning layer is either fed to a softmax layer for binary questions, or to a scoring layer for questions with multiple and varying candidate answers.",
      "This so-called Neural Reasoner can be trained by backpropagation, in an end-to-end, supervised way.",
      "The authors also suggest the use of auxiliary tasks, to improve results.",
      "The first (\"original\") adds an autoencoder reconstuction cost, that reproduces the question and facts from its first layer encoding.",
      "The second (\"abstract\") instead reconstructs a more abstract version of the sentences (e.g. \"The triangle is above the pink rectangle.\"",
      "becomes \"x is above y\").",
      "Importantly, while the Neural Reasoner framework is presented in this paper as covering many different variants, the version that is experimentally tested is one where the fact representations f_k are actually left unchanged throughout the reasoning layers, with only the question representation being changed.",
      "The paper presents experiments on two synthetic reasoning tasks and report performances that compare favorably with previously published alternatives (based on the general Memory Network architecture).",
      "The experiments also show that the auxiliary tasks can substantially improve the performance of the model   #### My two cents  The proposed Neural Reasoner framework is actually very close to work published on arXiv at about the same time on End-to-End Memory Networks  [ref] .",
      "In fact, the version tested in the paper, with unchanged fact representations throughout layers, is extremely close to End-to-End Memory Networks.",
      "That said, there are also lots of differences.",
      "For instance, this paper proposes the use of multilayer networks within each Reasoning Layer, to produce updated question representations.",
      "In fact, experiments suggest that using several layers can be very beneficial for the path finding task.",
      "The sentence representation at the first layer is also different, being based on a non-linear RNN instead of being based on linear operations on embeddings as in Memory Networks.",
      "The most interesting aspect of this paper to me is probably the demonstration that the use of an auxiliary task such as \"original\", which is unsupervised, can substantially improve the performance, again for the path finding task.",
      "That is, to me, probably the most exciting direction of future research that this paper highlights as promising.",
      "I also liked how the model is presented.",
      "It didn't take me much time to understand the model, and I actually found it easier to absorb than the Memory Network model, despite both being very similar.",
      "I think this model is indeed a bit simpler than Memory Networks, which is a good thing.",
      "It also suggests a different approach to the problem, one where the facts representations are also updated during forward propagation, not just the question's representation (which is the version initially described in the paper...",
      "I hope experiments on that variant are eventually presented).",
      "It's unfortunate that the authors only performed experiments on 2 of the 20 synthetic question-answering tasks.",
      "I hope a future version of this work can report results on the full benchmark and directly compare with End-to-End Memory Networks.",
      "I was also unable to find out which of the question representation pooling mechanism (section 3.2.2) was used in the experiments.",
      "Perhaps the authors forgot to state it?",
      "Overall, a pretty interesting paper that open different doors towards reasoning with neural networks."
    ],
    "author_id": "hlarochelle",
    "pdf_url": "http://arxiv.org/pdf/1508.05508",
    "author_full_name": "Hugo Larochelle",
    "source_website": "https://www.shortscience.org/user?name=hlarochelle",
    "id": 73414525
  },
  {
    "blog_id": "deep_clustering_for_unsupervised_learning_of_visual_features",
    "summary": [
      "What  They describe a strategy to train CNNs in unsupervised fashion.",
      "The method is based on iterating k-means clustering in feature space, followed by training the CNN to predict the cluster labels.",
      "The method achieves sizeable improvements over the state of the art in unsupervised learning on ImageNet.",
      "How  Method  They start with a CNN (not pretrained)  They then iterate the following steps:  They apply the CNN to their dataset, converting each image to a feature vector.",
      "(At the start, this will be mostly noise.)",
      "They apply k-means to these feature vectors.",
      "They use the resulting clustering as (pseudo-)labels for the images.",
      "They train the CNN for some batches to predict these (pseudo-)labels.",
      "They start again at (1).",
      "Visualization:  Avoiding trivial solutions  Empty Clusters  While k-means is iterating, some clusters may become empty.",
      "If that happens, they move the centroid of such a cluster (e.g. call it \"cluster A\" here) to another cluster (e.g.",
      "\"cluster B\"), where B must have associated feature vectors.",
      "After moving, they perturb the centroid of cluster A's location by a small amount.",
      "They then split the feature vectors of cluster B between A and B.",
      "Trivial Parameterization  If one cluster becomes dominating, i.e. if most of the feature vectors are associated to it, the CNN can start to learn trivial parameterizations that only focus on differentiating between that one cluster and everything else, because.",
      "They avoid this by sampling images based on a uniform distribution over the (pseudo-)labels.",
      "Other stuff  Their test their methods mainly with AlexNet.",
      "Also a bit with VGG.",
      "(Both modified to use BN.)",
      "They update the clusters once per epoch on ImageNet.",
      "They train on ImageNet for 500 epochs.",
      "They apply a Sobel-based filter before their models (i.e. they do not get colors as inputs).",
      "Results  ImageNet  They measure the normalized mutual information between cluster assignments and ImageNet labels.",
      "This mutual information increases over time, indicating that the cluster assignments matches more and more the labels (which were not used during training).",
      "They measure the normalized mutual information of cluster assignments between each pair of two consecutive epochs.",
      "This mutual information increases over time, indicating that the clustering becomes more stable.",
      "They measure the achieved mAP on Pascal VOC 2007 (I guess they use the model as pretrained weights?)",
      "based on the number of clusters in k-means.",
      "They achieve best results with 10,000 clusters.",
      "Accuracy per layer  After training they place a linear classifier on top of their model and train it on the labels.",
      "They do repeat this for each layer to get the accuracy per layer.",
      "They significantly outperform all competing methods (by several percentage points, depending on layer).",
      "At conv5 (max conv layer in AlexNet) they are beaten by supervised learning by ~12 points (50.5 vs 38.2).",
      "The difference is lower for conv3 (44.2 vs 41.0).",
      "They also compare models pretrained on ImageNet, with the linear classifier fine-tuned on the Places dataset.",
      "In this case they can keep up more with the model pretrained in supervised fashion (38.7 vs 34.7 at conv5, 39.4 vs 39.8 at conv4).",
      "Stats:  YFCC100M  They train on ImageNet and then search on YFCC100M for images leading to high activations in filters.",
      "Images with high activations and generated \"ideal\" images for that filter:  Images with high activations for filters in layer conv5:  They note that some filters in conv5 seem to just re-learn patterns that are already covered by filters in lower layers.",
      "Pascal VOC  They pretrain on ImageNet and then finetune on Pascal VOC for classification, object detection and segmentation.",
      "They are still behind a model pretrained with labels.",
      "Depending on the task between ~1 (detection), ~3 (segmentation) or ~6 (classification) points.",
      "They outperform other unsupervised methods by several points.",
      "They outperform a randomly initialized (i.e. not pretrained) model significantly.",
      "(This is even more the case if only the last fully connected layers are finetuned, not the convolutional layers.)",
      "They train with VGG16 instead of AlexNet and can improve their accuracy by an amount comparable to when the same model switch is done for the supervised training."
    ],
    "author_id": "ALEJU",
    "pdf_url": "http://openaccess.thecvf.com/content_ECCV_2018/papers/Mathilde_Caron_Deep_Clustering_for_ECCV_2018_paper.pdf",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 45538669
  },
  {
    "blog_id": "memory-based-parameter-adaption",
    "summary": [
      "Standard Deep Learning networks are not suitable for continual learning setting as the change in the data distribution leads to catastrophic forgetting.",
      "The paper proposes Memory-based Parameter Adaptation (MbPA), a technique that augments a standard neural network with an episodic memory (containing examples from the previous tasks).",
      "This episodic memory allows for rapid acquisition of new knowledge (corresponding to the current task) while preserving performance on the previous tasks.",
      "Architecture  MbPA consists of 3 components:  Embedding Network f  Memory M  Output network g  f and g are parametric components while M is a non-parametric component.",
      "M is a dynamically sized dictionary where the key represents the output of the embedding network and the value represents the desired output for a given input (input to the model).",
      "When a new training tuple (xj, yj) is fed as input to the model, a key-value pair (hj, vj) is added to the memory.",
      "hj = f(xj)  The memory has a fixed size and acts as a circular buffer.",
      "When it gets filled up, earlier examples are dropped.",
      "When accessing the memory using a key hkey, the k-nearest neighbours (in terms of distance from the given key) are retrieved.",
      "Training Phase  During the training phase, the memory is only used to store the input examples and does not interfere with the training procedure.",
      "Testing Phase  During testing, the memory is used to adapt the parameters of the output network g while the embedding network f remains the same.",
      "Given the input x, obtain the embedding corresponding to x and using that as the key, retrieve the k-nearest neighbours from the memory.",
      "Each retrived neighbour is a tuple of the form (hk, vk, wk) where wk is propotional to the closeness between the input query and the key corresponding to the retrived example.",
      "The collection of all the retrieved examples are referred to as the context C.  The parameters of the output network g are adapted from \u03b8 to \u03b8x where \u03b8x = \u03b8 + \u03b4M(x, \u03b8)  \u03b4M(x, \u03b8) is referred to as the contextual update of parameters of the output network.",
      "Interpretation of MbPA  MbPA can be interpreted as decreasing the weighted average of negative log likelihood over the retrieved neighbours in the context C.  The expression corresponding to  \u03b4M(x, \u03b8) can be obtained by performing gradient descent to minimise the max a posterior over the context C.  The a posterior expression can be written as a sum of two terms - one corresponding to a weighted likelihood of data in the context C and the other corresponding to a regularisation term to prevent overfitting the data.",
      "This idea can be thought of as a generalisation of attention.",
      "Attention can be viewed as fitting a constant function over the neighbourhood of memories while MbPA fits a more general function which is parameterised by the output network of the given model.",
      "Refer appendix E in the paper for further details.",
      "Experiments  MbPA aims to solve the fundamental problem of enabling the model to deal with changes in data distribution.",
      "In that sense, it is evaluated on a wide range of settings: continual learning, incremental learning, unbalanced datasets and change in data distribution at test time.",
      "Continual Learning:  In this setting, the model encounters a sequence of tasks and cannot revisit a previous task.",
      "Permuted MNIST dataset was used.",
      "The key takeaway is that once a task is catastrophically forgotten, only a few gradient updates on a carefully selected data, are sufficient to recover the performance.",
      "Incremental Learning:  In this setting, the model is trained on a subset of classes and then introduced to novel, unseen classes.",
      "The model is tested to see if it can incorporate the new knowledge while retaining the knowledge about the previous classes.",
      "Imagenet dataset with Resnet V1 model is used.",
      "It is first pretrained on 500 classes and then fine-tuned to see how quickly could it adapt to new classes.",
      "Unbalanced Dataset:  This setting is similar to the incremental learning setting with the key difference that once the model has been trained on a part of the dataset and is to be finetuned to acquire new knowledge, the dataset used for finetuning is much smaller than the initial dataset thus creating the effect of unbalanced datasets.",
      "Language Modelling:  MbPA is used to adapt to the shift in the word distribution that is common to language modelling tasks.",
      "PTB and WikiText datasets were used.",
      "MbPA exhibits strong performance on all these tasks showing that the memory-based parameter adaption technique is effective across a range of tasks in supervised learning."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1802.10542",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 79471009
  },
  {
    "blog_id": "a-few-useful-things-to-know-about-machine-learning",
    "summary": [
      "A few useful things to know about machine learning \u2013 Domingos 2012  Developing successful machine learning applications requires a substantial amount of \u2018black art\u2019 that is hard to find in textbooks  This paper looks at twelve key lessons including pitfalls to avoid, important issues to focus on, and answers to common questions.",
      "The paper was published in 2012, and since then the excellent \u2018 Data Science for Business \u2018 book by Provost and Fawcett has been released by O\u2019Reilly.",
      "Now much of the wisdom from this paper can indeed be found in a textbook!",
      "If you enjoy this paper, I highly recommend the book as well.",
      "According to the paper, the key to not getting lost in the huge space of learning algorithms is to understand that they are composed of three elements: a representation model (e.g. k-nearest neighbour, naive bayes, decision trees); an evaluation function (scoring function) to tell good from bad; and an optimization technique to search for the highest scoring classifier.",
      "Most textbooks are organized by representation, and it\u2019s easy to overlook the fact that the other components are equally important.",
      "You want your machine learning to work well (generalize) outside of the examples in the training set you gave it.",
      "The most common mistake among machine learning beginners is to test on the training data and have the illusion of success\u2026 in the early days of machine learning, the need to keep training and test data separate was not widely appreciated.",
      "(Of course, if you started your journey as a \u2018machine learning beginner\u2019 by taking Andrew Ng\u2019s online course you won\u2019t be falling into this trap!).",
      "Data alone is not enough, you also need to supply some domain knowledge to help guide the process.",
      "Machine learning is not magic; it can\u2019t get something from nothing.",
      "What it does is get more from less.",
      "Programming, like all engineering, is a lot of work: we have to build everything from scratch.",
      "Learning is more like farming, which lets nature do most of the work.",
      "Farmers combine seeds with nutrients to grow crops.",
      "Learners combine knowledge with data to grow programs.",
      "Overfitting is a well-known problem in machine learning, and cross-validation can help to combat this.",
      "Nevertheless, you should be skeptical of claims that a particular technique \u2018solves\u2019 the overfitting problem.",
      "It\u2019s easy to avoid overfitting (variance), by falling into the opposite problem of underfitting (bias).",
      "Simultaneously avoiding both requires learning a perfect classifier, and short of knowing it in advance there is no single technique that will always do best.",
      "The \u2018curse of dimensionality\u2019 refers to the fact that many algorithms that work in low dimensions become intractable when the input is high-dimensional.",
      "\u2026the similarity-based reasoning that machine learning algorithms depend on (explicitly or implicitly) breaks down in high dimensions  Fortunately many problem domains are non-uniform giving an effectively lower dimension, or algorithms for explicitly reducing the dimensionality can be used.",
      "The most important factor in the success of a machine learning project is the features used.",
      "If the raw data is not in a form that is amenable to learning, you may be able to construct features from it that are:  First-timers are often surprised by how little time in a machine learning project is spent actually doing machine learning.",
      "But it makes sense if you consider how time consuming it is to gather data, integrate it, clean it and pre-process it, and how much trial and error can go into feature design.",
      "What\u2019s better?",
      "Smart algorithms or lots of data:  As a rule of thumb, a dumb algorithm with lots and lots of data beats a clever one with modest amounts of it.",
      "This brings up the problem of scalability:  In most of computer science, the two main limited resources are time and memory.",
      "In machine learning there is a third one: training data.",
      "Which one is the bottleneck has changed from decade to decade.",
      "Why have just one learner though, when you can have many, ensembles of learners do best.",
      "\u2026researchers noticed that if instead of selecting the best variation found, we combine many variations, the results are better \u2013 often much better \u2013 and at little extra effort for the user  The winner of the \u2018 Netflix prize \u2018 was a stacked ensemble of over 100 learners.",
      "There\u2019s plenty more folk wisdom in the paper, so do check it out if this has piqued your interest."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 11006232
  },
  {
    "blog_id": "modular-meta-learning",
    "summary": [
      "The paper proposes an approach for learning neural networks (modules) that can be combined in different ways to solve different tasks (combinatorial generalization).",
      "The proposed model is called as BOUNCEGRAD.",
      "Link to the code  Setup  Focuses on supervised learning.",
      "Task distribution p(T).",
      "Each task is a joint distribution pT(x, y) over (x, y) data pairs.",
      "Given data from m meta-training tasks, and a meta-test task, find a hypothesis h which performs well on the unseen data drawn from the meta-test task.",
      "Structured Hypothesis  Given a compositional scheme C, a set of modules F1, \u2026, Fk (represented as a whole by F) and the set of their respective parameters \u03b81, \u2026, \u03b8k (represented as a whole by \u03b8), (C, F, \u03b8) represents the set of possible functional input-output mappings.",
      "These mappings form the hypothesis space.",
      "A structured hypothesis model is specified by what modules to use and their parametric forms (but not the values).",
      "Examples of compositional schemes  Choosing a single module for the task at hand.",
      "Fixed compositional structure but different modules selected every time.",
      "Weight ensemble (maybe using attention mechanism)  General function composition tree  Phases  Offline Meta Learning Phase:  Take training and validation dataset for the first k tasks and generate a parameterization for each module \u03b81, \u2026, \u03b8k.",
      "The hypothesis (or composition) to use comes from the online meta-test learning phase.",
      "In this stage, find the best \u03b8 given a structure.",
      "Online Meta-test Learning Phase  Given a hypothesis space and \u03b8, the output is a compositional form (or hypothesis) that specifies how to compose the models.",
      "In this stage, find the best structure, given a hypothesis space and \u03b8.",
      "Learning Algorithm  During Meta-test learning phase, simulated annealing is used to find the optimal structure, with temperature T decreased over time.",
      "During meta-learning phrase, the actual objective function is replaced by a surrogate, smooth objective function (during the search step) to avoid local minima.",
      "Once a structure has been picked, any gradient descent based approach can be used to optimize the modules.",
      "Basically the state of optimization process comprises of the parameters and the temperature.",
      "Together, they are used to induce a distribution over the structures.",
      "Given a structure, \u03b8 is optimized and T is annealed over time.",
      "The learning procedure can be improved upon by performing parameter tuning during the online (meta-test learning) phase as well.",
      "the resulting approach is referred to as MOMA - MOdular MAml.",
      "Experiments  Approaches  Pooled - Single network using combined data of all the tasks.",
      "MAML - Single network using MAML  BOUNCEGRAD - Modular Network without MAML adaptation in online learning.",
      "MOMA - BOUNCEGRAD with MAML adaptation in online learning.",
      "Domains  Simple Functional Relationships  Sine-function prediction problem  In general, MOMA outperforms other models.",
      "With a small amount of online training data, BOUNCEGRAD outperforms other models as it has a better structural prior.",
      "Predicting next frame of a kinematic skeleton (motion capture data)  11 different objects (with different shapes) on 4 surfaces with different friction properties.",
      "2 meta-learning scenarios are considered.",
      "In the first case, the object-surface combination in the test case was present in some meta-training tasks and in the other case, it was not present.",
      "For previously seen combinations, MOMA performs the best followed by BOUNCEGRAD and MAML.",
      "For unseen combinations, all the 3 are equally good.",
      "Compositional scheme is the attention mechanism.",
      "An interesting result is that the modules seem to specialize (and activate more often) based on the shape of the object.",
      "Predicting next frame of a kinematic selection (using motion capture data)  Composition Structure - generating kinematics subtrees for each body part (2 legs, 2 arms, 2 torsi).",
      "Again 2 setups are used - one where all activities in the training and the meta-test task are shared while the other setup where the activities are not shared.",
      "For known activities MOMA and BOUNCEGRAD perform the best while for unknown activities, MOMS performs the best.",
      "Notes  While the approach is interesting, maybe a more suitable set of tasks (from the point of composition) would be more convincing.",
      "It would be useful to see the computational tradeoff between MAML, BOUNCEGRAD, and MOMA."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1806.10166",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 56196
  },
  {
    "blog_id": "task-oriented-query-reformulation-with-reinforcement-learning",
    "summary": [
      "The paper introduces a query reformulation system that rewrites a query to maximise the number of \u201crelevant\u201d documents that are extracted from a given black box search engine.",
      "A Reinforcement Learning (RL) agent selects the terms that are to be added to the reformulated query and the rewards are decided on the basis of document recall.",
      "Implementation  Key Aspect  The underlying problem is as follows: when the end user makes a query to a search engine, the engine often relies on word matching techniques to perform retrieval.",
      "This means relevant documents could be missed if there is no exactly matching words between the query and the document.",
      "This problem can be handled at two levels: First, the search engine itself takes care of query semantics.",
      "Alternatively, we assume the search engine to be dumb and instead have a system in place that can improve the original queries (automatic query reformulation).",
      "The paper takes the latter approach and expands the original query by adding terms from the set of retrieved documents (pseudo relevance feedback).",
      "Datasets  TREC - Complex Answer Retrieval (TREC-CAR)  Jeopardy Q&A dataset  Microsoft Academic (MSA) dataset - created by the authors using papers crawled from Microsoft Academic API  Framework  Query Reformulation task is modeled as an RL problem where:  Environment is the search engine.",
      "Actions are whether a word is to be added to the query or not and if yes, then what word is added.",
      "Reward is the retrieval accuracy.",
      "The input to the system is a query q0 consisting of a sequence of words w1, \u2026, wn and a candidate term ti with some context words.",
      "Candidate terms are all the terms that appear in the original query and the documents retrieved using the query.",
      "The words are mapped to vectors and then a fixed size representation is obtained for the sequence using CNN\u2019s or RNNs.",
      "Similarly, a representation is obtained for the candidate words by feeding them and their context words to the CNN or RNNs.",
      "Finally, a sigmoidal score is computed for all the candidate words.",
      "An RNN sequentially applies this model to emit query words till an end token is emitted.",
      "Vocabulary is used only from the extracted documents and not the entire vocabulary set, to keep the inference fast.",
      "Training  The model is trained using REINFORCE algorithm which minimizes the Ca = (R \u2212 R~) * sum(log(P(t|q))) where R~ is the baseline.",
      "Value network minimises Cb = &\\alpha(||R-R~||2)  Ca and Cb are minimised using SGD.",
      "An entropy regulation term is added to prevent the probability distribution from reaching the peak.",
      "Experiments  Baseline Methods  Raw - Original query is fed to the search engine without any modification.",
      "Pseudo-Relevance Feedback (PRF-TFIDF) - The query is expanded using the top-N TF-IDF terms.",
      "PRF-Relevance Model (PRF-RM) - Probability of adding token t to the query q0 is given by P(t|q0) = (1 \u2212 \u03bb)P\u2032(t|q0) + \u03bb sum (P(d)P(t|d)P(q0|d))  Proposed Methods  Supervised Learning  Assumes that the query words contribute indepently to the query retrival performace.",
      "(Too strong an assumption).",
      "A term is marked as relevant if (R(new_query) - R(old_query))/R(old_query) > 0.005  Reinforcement Learning  RL-RNN/CNN - RL Framework + RNN/CNN to encode the input features.",
      "RL-RNN-SEQ - Add a sequential generator.",
      "Metrics  Recall@K  Precision@K  Mean Average Precision@K  Reward - The paper uses Recall@K as a reward when training the RL-based models with the argument that the \u201cmetric has shown to be effective in improving the other metrics as well\u201d, without any justification though.",
      "SL-Oracle - classifier that perfectly selects terms that will increase performance based on the supervised learning approach.",
      "RL-Oracle - Produces a conservative upper-bound for the performance of the RL Agent.",
      "It splits the test data into N subsets and trains an RL agent for each subset.",
      "Then, the reward is averaged over all the N subsets.",
      "Observations  Reformulation based methods > original query  RL methods > Supervised methods > unsupervised methods  RL-RNN-SEQ performs slightly worse than RL-RNN but is much faster (as it produces shorter queries).",
      "RL-based model benefits from more candidate terms while the classical PRF method quickly saturates.",
      "Comments  Interestingly, for each raw query, they carried out the reformulation step just once and not multiple times.",
      "The number of times a query is reformulated could also have become a part of the RL framework."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1704.04572",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 61558513
  },
  {
    "blog_id": "challenging-common-assumptions-in-the-unsupervised-learning-of-disentangled-representations",
    "summary": [
      "Challenging common assumptions in the unsupervised learning of disentangled representations Locatello et al., ICML\u201919  Today\u2019s paper choice won a best paper award at ICML\u201919.",
      "The \u2018common assumptions\u2019 that the paper challenges seem to be: \u201cunsupervised learning of disentangled representations is possible, and useful!\u201d  The key idea behind the unsupervised learning of disentangled representations is that real-world data is generated by a few explanatory factors of variation which can be recovered by unsupervised learning algorithms.",
      "In this paper, we provide a sober look at recent progress in the field and challenge some common assumptions.",
      "What exactly is a \u2018disentangled representation\u2019 and why might we want one?",
      "Put the \u2018disentangled\u2019 part to one side for a moment, and let\u2019s start out by revisiting what we mean by a representation.",
      "Given a real-world observation  (e.g. of an image or video), a representation  is a transformation of  (typically to a lower dimensional space in order to be useful) that somehow preserves the salient information in the  so that we can still use  to extract useful information about the input (e.g.",
      "for building classifiers).",
      "As a trivial example, suppose we had real world observations consisting of 1000 points sampled from a straight line, a good lower-valued representation would be a (gradient, intercept) tuple.",
      "Of course real-world examples are much more complex than this!",
      "A disentangled representation is a representation with a compact and interpretable structure, which captures the essence of the input independent of the task the representation is ultimately going to be used for.",
      "That\u2019s quite tricky \u2013 even in my contrived straight line example what looked to be a great representation would be useless if the task turned out to be calculating the area of the the rectangle enclosed by the points in the observation.",
      "While there is no single formalized notion of disentanglement (yet) which is widely accepted, the key intuition is that a disentangled representation should separate the distinct information factors of variation in the data.",
      "A change in a single underlying factor of variation should lead to a change in a single factor in the learned representation.",
      "The state of the art for representation learning centres around Variational Autoencoders, using one deep neural network to learn a representation, and another one to attempt to reconstruct the original input from that representation.",
      "The representation  is usually taken as the mean of the approximate posterior distribution of the first (encoding) network.",
      "In theory, disentanglement is impossible  We theoretically prove that (perhaps unsurprisingly) the unsupervised learning of disentangled representations is fundamentally impossible without inductive biases both on the considered learning approaches and the data sets.",
      "The full proof is giving in appendix A (missing from my copy of the pdf), but it boils down to this:  My layman\u2019s interpretation is this: given all the possible ways we could   decompose the input into factors, whatever representation we ultimately choose there is some other representation in which a change to a single dimension in the first impacts all the dimensions of the second (they are entangled).",
      "There\u2019s no way for an unsupervised method to distinguish between these two equivalent generative models, and thus the resulting learned representation must be entangled with at least one of them.",
      "After observing  , we can construct many generative models which have the same marginal distribution of  .",
      "Any one of these models could be the true causal generative model for the data, and the right model cannot be identified given only the distribution of  .",
      "For a wonderful demonstration of this in lower dimensions, see \u2018 Same stats, different graphs \u2019.",
      "In practice, might we be able to learn disentangled representations anyway?",
      "While Theorem 1 shows that unsupervised disentanglement learning is fundamentally impossible for arbitrary generative models, this does not necessarily mean that it is an impossible endeavour in practice.",
      "After all, real world generative models may have a certain structure that could be exploited through suitably chosen inductive biases.",
      "But, the authors argue, you should make explicit the inductive biases you are selecting.",
      "To investigate all this the authors take six recent unsupervised disentanglement methods, train them over seven different data sets, and evaluate them using six different disentanglement measures.",
      "The result is  a corpus of more than 10,000 trained models.",
      "The library used to train and evaluate these models, disentanglement_lib has been made available at  [url]"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://proceedings.mlr.press/v97/locatello19a/locatello19a.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 84030160
  },
  {
    "blog_id": "unsupervised-learning-by-predicting-noise",
    "summary": [
      "Convolutional Neural Networks are extremely good feature extractors in the sense that features extracted for one task (say image classification) can be easily transferred to another task (say image segmentation).",
      "Existing unsupervised approaches do not aim to learn discriminative features and supervised approaches for discriminative features do not scale well.",
      "The paper presents an approach to learn features in an unsupervised setting by using a set of target representations called as Noise As Target (NAT) which acts as a kind of proxy supervising signal.",
      "Approach  Unsupervised Setting  Given a collection of image X (x1, x2, \u2026, xn), we want to learn a parameterized mapping f such that f(xi) gives the features of image xi.",
      "We would jointly learn the target vectors yi (more on it later).",
      "Loss Function  Squared L2 norm is used as the distance measure while making sure that final activations are unit normalized.",
      "Fixed Target Representation  In the setting of the problem where we are learning both the features and the target representation, a trivial solution would be the one where all the input images map to the same target and are assigned the same representation.",
      "No discriminative features are learned in this case.",
      "To avoid such situations, a set of k predefined target representations are chosen and each image is mapped to one of these k representations (based on the features).",
      "There is an assumption that k > n so that each image is assigned a different target.",
      "One simple choice of target representation is the standard one-hot vector which implies that all the class (and by extension, the associated images) are orthogonal and equidistant from each other.",
      "But this is not a reasonable approximation as not all the image pairs are equally similar or dissimilar.",
      "Instead, the target vectors are uniformly sampled from a d-dimensional unit sphere, where d is the dimensionality of the feature representation.",
      "That is, the idea is to map the features to the manifold of the d-dimensional L2 sphere by using the K predefined representations as for the discrete approximation of the manifold.",
      "Since each data point (image) is mapped to a new point on the manifold, the algorithm is suited for online training as well.",
      "Optimisation  For the training, the number of target K is reduced to the number of images n and an assignment matrix P is learned which ensures that the mapping between the image to target is 1-to-1.",
      "The resulting optimisation equation can be solved using the Hungarian Algorithm but at a high-cost O(n^3).",
      "An optimisation is to take a batch of b images and update the square matrix PB for dimension bXb (made of the images and their corresponding targets).",
      "This reduces the overall complexity of O(nb^2).",
      "Other optimisation techniques, that are common to supervised learning, like batch norm used in this setting as well.",
      "Implementation Detail  Used AlexNet with NATs to train the unsupervised model.",
      "An MLP is trained on these features to learn the classifier.",
      "Standard preprocessing techniques like random cropping/flipping are used.",
      "Experimental Details  Dataset  ImageNet for training the AlexNet architecture with the proposed approach.",
      "Pascal VOC 2007 for transfer learning experiments.",
      "Baselines  Unsupervised approaches like autoencoder, GAN, BiGAN  Self-supervised  SOTA models using hand-made features SIFT with Fisher Vector.",
      "Observation  Using squared loss instead of softmax does not deteriorate the performance too much.",
      "The authors compare the effect of using discrete vs continuous target representations for transfer learning.",
      "For the discrete representation, elements of the canonical basis of a k-dimensional space (k=1000, 10000, 100000) are used.",
      "Experiments demonstrate that d-dimensional continuous vectors perform much better than the discrete vectors.",
      "While training the unsupervised network, its features were extracted after every 20 iterations to evaluate the performance on transfer learning task.",
      "The test accuracy increases up to around 100 iterations then saturate.",
      "Comparing the visualization of the first convolutional layer filters (for AlexNet with and without supervision) shows that while unsupervised filters are less sharp, they maintain the edge and orientation information.",
      "The proposed unsupervised method outperforms all the unsupervised baselines and is competitive with respect to the supervised baseline.",
      "But it is still far behind the model using handcrafted features.",
      "For transfer learning, on Pascal VOC, the proposed approach beats the supervised baseline and works at par with the supervised approach.",
      "Notes  The paper proposed a simple unsupervised framework for learning discriminative features without having to rely on proxy tasks like image generation and without having to make an assumption about the input domain.",
      "The key aspect of the proposed approach is that each image is assigned to a unique point in the d-dimensional manifold which means 2 images could be very close to each other on the manifold while being quite distinct in reality.",
      "It is interesting to see that such a simple strategy is able to give such good results."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1704.05310",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 75937139
  },
  {
    "blog_id": "darwinian-data-structure-selection",
    "summary": [
      "Darwinian data structure selection Basios et al., FSE\u201918  GraphIt may have caught your attention for the success of its approach, but I suspect for many readers it\u2019s not something you\u2019ll be immediately applying.",
      "Darwinian Data Structures (DDSs) on the other hand looks to be of immediate interest to many Java and C++ projects (and generalises beyond those languages).",
      "What I would have called an ADT (e.g., a List), the authors call Darwinian Data Structures.",
      "The \u2018Darwinian\u2019 part comes from the fact that ADTs have multiple concrete implementations, and Artemis, \u201ca multi-objective, cloud-based search-based optimisation framework\u201d finds the best implementation class (e.g. ArrayList, LinkedList) for your specific use case.",
      "It does this using the NSGA-II genetic algorithm-based optimiser in the current implementation.",
      "In brief, Artemis finds the places in your code where you are using an ADT, and explores the possible concrete instantiation space for those ADTs using your test suite as a guide to performance.",
      "Then it outputs the transformed source.",
      "You might be wondering whether e.g. LinkedList vs ArrayList makes that big a difference in most real world projects:  Artemis achieves substantial performance improvements for every project in 5 Java projects from DaCapo benchmark, 8 popular projects, and 30 uniformly sampled projects from GitHub.",
      "For execution time, CPU usage, and memory consumption, Artemis finds at least one solution that improves all measures for 86% (37/43) of the projects.",
      "The median improvement across the best solutions is 4.8%, 10.1%, and 5.1% for runtime, memory, and CPU usage.",
      "For example, consider this code from google-http-java-client, which currently uses ArrayList :  Switching to LinkedList and comparing performance over the same test set for 30 runs, we get a median 46% reduction in execution time.",
      "We are interested not just in searching the space of Darwinian data structures, but also tuning them via their constructor parameters.",
      "For example, choosing an appropriate initial capacity size for an ArrayList.",
      "End-to-end Artemis works like this:  There\u2019s a one-off up-front exercise to analyse the collections library / libraries of interest and build a dictionary that describes the search space.",
      "Then given the source code and test suite of a project Artemis explores the AST to find uses of DDSs, outputting a templated version of the source code with replacement points for each usage.",
      "A search algorithm is then used to find the best choice in each location, with the test suite being used to judge performance.",
      "Finding candidate program points for DDS substitution  Given an AST, it\u2019s easy to find declarations using the abstract data type (e.g. List), but in the code bases under study the authors also found many cases where programmers had over-specified, using a concrete type for variable and parameter type declarations, e.g.",
      "Artemis will apply further transformations to replace these with the abstract type instead, thus permitting DDS exploration.",
      "Many programs make extensive use of collection types, resulting in a very large overall search space.",
      "Artemis profiles the input program while running the test suite to identify the highest value points in the program to explore and thus prunes the search space.",
      "Profiling is done using the JConsole profiler.",
      "Searching for the best parameters  The overall search space for a given DDS consists of all the possible concrete implementation types, together with the parameter spaces for their respective constructor arguments.",
      "For each generation, NSGA-II applies tournament selection, followed by a uniform crossover and a uniform mutation operation.",
      "In our experiments, we designed fitness functions to capture execution time, memory consumption, and CPU usage.",
      "After fitness evaluation, Artemis applies standard non-dominated selection to form the next generation.",
      "Artemis repeats this process until the solutions in a generation converge.",
      "At this point, Artemis returns all non-dominated solutions in the final population.",
      "In the evaluation, the initial population size is set to 30, with a limit of 900 function evaluations.",
      "To assess fitness Artemis relies on running the test suite.",
      "Therefore the results will only apply to production use cases to the extent that your test suite mirrors production usage.",
      "Even though performance test suites are a more appropriate and logical choice for evaluating the non-functional properties of the program, most real world programs in GitHub do not provide a performance suite.",
      "For this reason, we use the regression test suites to evaluate the non-functional properties of the GitHub projects of this study whenever a performance test suite is not available.",
      "Test suite execution time is measured using the Maven Surefire plugin, with profiling done by JConsole.",
      "Each generated solution is run for at least 30 simulations, with start-up/ JVM warm-up runs not counting towards this total.",
      "(See \u2018[Virtual machine warmup blows hot and cold](  [url]"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://arxiv.org/pdf/1706.03232",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 84548548
  },
  {
    "blog_id": "higher-order-organization-of-complex-networks",
    "summary": [
      "The paper presents a generalized framework for graph clustering (clusters of network motifs) on the basis of higher-order connectivity patterns.",
      "Approach  Given a motif M , the framework aims to find a cluster of the set of nodes S such that nodes of S participate in many instances of M and avoid cutting instances of M (that is only a subset of nodes in instances of M appears in S).",
      "Mathematically, the aim is to minimise the motif conductance metric given as cutM(S, S\u2019) / min[volM(S), volM(S\u2019)] where S\u2019 is complement of S, cutM(S, S\u2019) = number of instances of M which have atleast one node from both S and S\u2019 and volM(S) = Number of nodes in instances of M that belong only to S.  Solving the above equation is computationally infeasible and an approximate solution is proposed using eigenvalues and matrices.",
      "The approximate solution is easy to implement, efficient and guaranteed to find clusters that are at most a quadratic factor away from the optimal.",
      "Algorithm  Given the network and motif M, form a motif adjacency matrix WM where WM(i, j) is the number of instances of M that contains i and j.  Compute spectral ordering of the nodes from normalized motif laplacian matrix.",
      "Compute prefix set of spectral ordering with small motif conductance.",
      "Scalability  Worst case O(m1.5), based on experiments O(m1.2) where m is the number of edges.",
      "Advantages  Applicable to directed, undirected and weighted graphs (allows for negative edge weights as well).",
      "In case the motif is not known beforehand, the framework can be used to compute significant motifs.",
      "The proposed framework unifies the two fundamental tools of network science (motif analysis and network partitioning) along with some worst-case guarantees for the approximations employed and can be extended to identify higher order modular organization of networks."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1612.08447.pdf",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 78860785
  },
  {
    "blog_id": "pre-training-graph-neural-networks-with-kernels",
    "summary": [
      "The paper proposes a pretraining technique that can be used with the GNN architecture for learning graph representation as induced by powerful graph kernels.",
      "Paper  Idea  Graph Kernel methods can learn powerful representations of the input graphs but the learned representation is implicit as the kernel function actually computes the dot product between the representations.",
      "GNNs are flexible and powerful in terms of the representations they can learn but they can easily overfit if a large amount of training data is not available as is commonly the case of graphs.",
      "Kernel methods can be used to learn an unsupervised graph representation that can be finetuned using the GNN architectures for the supervised tasks.",
      "Architecture  Given a dataset of graphs g1, g2, \u2026, gn, use a relevant kernel function to compute k(gi, gj) for all pairs of graphs.",
      "A siamese network is used to encode the pair of graphs into representations f(gi) and f(gj) such that dot(f(gi), f(gj)) equals k(gi, gj).",
      "The function f is trained to learn the compressed representation of kernel\u2019s feature space.",
      "Experiments  Datasets  Biological node-labeled graphs representing chemical compounds - MUTAG, PTC, NCI1  Baselines  DGCNN  Graphlet Kernel (GK)  Random Walk Kernel  Propogation Kernel  Weisfeiler-Lehman subtree kernel (WL)  Results  Pretraining uses the WL kernel  Pretrained model performs better than the baselines for 2 datasets but lags behind WL method (which was used for pretraining) for the NCI1 dataset.",
      "Notes  The idea is straightforward and intuitive.",
      "In general, this kind of pretraining should help the downstream model.",
      "It would be interesting to try it on more datasets/kernels/GNNs so that more conclusive results can be obtained."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1812.00420",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 83248182
  },
  {
    "blog_id": "cyclical-learning-rates-for-training-neural-networks",
    "summary": [
      "Conventional wisdom says that when training neural networks, learning rate should monotonically decrease.",
      "This insight forms the basis of the different type of adaptive learning rates.",
      "Counter to this expected behaviour, the paper demonstrates that using a cyclical learning rate (CLR), varying between a minimum and a maximum value, helps to train the neural network faster without requiring fine-tuning of learning rate.",
      "The paper also provides a simple approach to estimate the lower and upper bound for CLR.",
      "Link to the implementation  Intution  Difficulty in minimizing the loss arises from saddle points and not from local minima.",
      "[Ref]  Increasing the learning rate allows for rapid traversal of saddle points.",
      "Alternatively, the optimal learning rate is expected to be between bounds of CLR and thus the learning rate would always be close to the optimal learning rate.",
      "Parameter Estimation  Cycle Length = Number of iterations till learning rate returns to the initial value = 2 * step_size  step_size should be set to 2-10 times the number of iterations in an epoch.",
      "Estimating the CLR boundary values:  Run the model for several epochs while increasing the learning rate between the allowed low and high values.",
      "Plot accuracy vs learning rate and note the learning rate values when the accuracy starts to fall.",
      "This gives a good candidate value for upper and lower bound.",
      "Alternatively, the lower bound could be set to be 1/3 or 3/4 of the upper bound.",
      "But it is difficult to judge if the model has run for the sufficient number of epochs in the first place.",
      "Notes  The idea in itself is very simple and straight-forward to add to any existing model which makes it very appealing.",
      "The author has experimented with various architectures and datasets (from vision domain) and has reported faster training results."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1506.01186",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 79512250
  },
  {
    "blog_id": "theisob15",
    "summary": [
      "This paper presents a variety of issues related to the evaluation of image generative models.",
      "Specifically, they provide evidence that evaluations of generative models based on the popular Parzen windows estimator or based on a visual fidelity (qualitative) measure both present serious flaws.",
      "The Parzen windows approach to generative modeling evaluation works by taking a finite set of samples generated from a given model and then using those as the centroids of a Parzen windows Gaussian mixture.",
      "The constructed Parzen windows mixture is then used to compute a log-likelihood score on a set of test examples.",
      "Some of the key observations made in this paper are: 1.",
      "A simple, k-means based approach can obtain better Parzen windows performance than using the original training samples for a given dataset, even though these are samples from the true distribution!",
      "2.",
      "Even for the fairly low dimensional space of 6x6 image patches, a Parzen windows estimator would require an extremely large number of samples to come close to the true log-likelihood performance of a model.",
      "3.",
      "Visual fidelity is a bad predictor of true log-likelihood performance, as it is possible to Obtain great visual fidelity and arbitrarily low log-likelihood, with a Parzen windows model made of Gaussians with very small variance.",
      "Obtain bad visual fidelity and high log-likelihood by taking a model with high log-likelihood and mixing it with a white noise model and putting as much as 99% of the mixing probability on the white noise model (i.e. which would produce bad samples 99% of the time).",
      "4.",
      "Measuring overfitting of a model by taking samples from the model and making sure their training set nearest neighbors are different is ineffective, since it is actually trivial to generate samples that are each visually almost identical to a training example, but that yet each have large euclidean distance with their corresponding (visually similar) training example."
    ],
    "author_id": "hlarochelle",
    "pdf_url": "http://arxiv.org/pdf/1511.01844",
    "author_full_name": "Hugo Larochelle",
    "source_website": "https://www.shortscience.org/user?name=hlarochelle",
    "id": 85933068
  },
  {
    "blog_id": "multiple-model-based-reinforcement-learning",
    "summary": [
      "The paper presents some general ideas and mechanisms for multiple model-based RL.",
      "Even though the task and model architecture may not be very relevant now, I find the general idea and the mechanisms to be quite useful.",
      "As such, I am focusing only on high-level ideas and not the implementation details themselves.",
      "The main idea behind Multiple Model-based RL (MMRL) is to decompose complex tasks into multiple domains in space and time so that the environment dynamics within each domain is predictable.",
      "MMRL proposes an RL architecture composes of multiple modules, each with its own state prediction model and RL controller.",
      "The prediction error from each of the state prediction model defines the \u201cresponsibility signal\u201d for each module.",
      "This responsibility signal is used to:  Weigh the state prediction output ie the predicted state is the weighted sum of individual state predictions (weighted by the responsibility signal).",
      "Weigh the parameter update of the environment models as well as the RL controllers.",
      "Weighing the action output - ie predicted action is a weighted sum of individual actions.",
      "The framework is amenable for incorporating prior knowledge about which module should be selected.",
      "In the modular decomposition of a task, the modules should not change too frequently and some kind of spatial and temporal continuity is also desired.",
      "Temporal continuity can be accounted for by using the previous responsibility signal as input during the current timestep.",
      "Spatial continuity can b ensured by considering a spatial prior like the Gaussian spatial prior.",
      "Though model-free methods could be used for learning the RL controllers, model-based methods could be more relevant given that the modules are learning state-prediction models as well.",
      "Exploration can be ensured by using a stochastic version of greedy action selection.",
      "One failure mode for such modular architectures is when a single module tries to perform well across all the tasks.",
      "The modules themselves should be relatively simplistic (eg linear models) which can learn quickly and generalize well.",
      "Non-stationary hunting task in a grid world and non-linear, non-stationary control task of swinging up a pendulum provides the proof of concept for the proposed methods."
    ],
    "author_id": "shugan",
    "pdf_url": "https://www.mitpressjournals.org/doi/pdf/10.1162/089976602753712972",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 88969640
  },
  {
    "blog_id": "cooijmansblc16",
    "summary": [
      "This paper describes how to apply the idea of batch normalization (BN) successfully to recurrent neural networks, specifically to LSTM networks.",
      "The technique involves the 3 following ideas:  **1) Careful initialization of the BN scaling parameter.",
      "** While standard practice is to initialize it to 1 (to have unit variance), they show that this situation creates problems with the gradient flow through time, which vanishes quickly.",
      "A value around 0.1 (used in the experiments) preserves gradient flow much better.",
      "**2) Separate BN for the \"hiddens to hiddens pre-activation and for the \"inputs to hiddens\" pre-activation.",
      "** In other words, 2 separate BN operators are applied on each contributions to the pre-activation, before summing and passing through the tanh and sigmoid non-linearities.",
      "**3) Use of largest time-step BN statistics for longer test-time sequences.",
      "** Indeed, one issue with applying BN to RNNs is that if the input sequences have varying length, and if one uses per-time-step mean/variance statistics in the BN transformation (which is the natural thing to do), it hasn't been clear how do deal with the last time steps of longer sequences seen at test time, for which BN has no statistics from the training set.",
      "The paper shows evidence that the pre-activation statistics tend to gradually converge to stationary values over time steps, which supports the idea of simply using the training set's last time step statistics.",
      "Among these ideas, I believe the most impactful idea is 1).",
      "The papers mentions towards the end that improper initialization of the BN scaling parameter probably explains previous failed attempts to apply BN to recurrent networks.",
      "Experiments on 4 datasets confirms the method's success.",
      "**My two cents**  This is an excellent development for LSTMs.",
      "BN has had an important impact on our success in training deep neural networks, and this approach might very well have a similar impact on the success of LSTMs in practice."
    ],
    "author_id": "hlarochelle",
    "pdf_url": "http://arxiv.org/pdf/1603.09025",
    "author_full_name": "Hugo Larochelle",
    "source_website": "https://www.shortscience.org/user?name=hlarochelle",
    "id": 687757
  },
  {
    "blog_id": "hardnet",
    "summary": [
      "What:  HardNet model which improves state-of-the-art in wide baseline stereo, patch matching, verification and image retrieval.",
      "They introduced a new triplet-like loss function with built-in hard-negative mining.",
      "How:  HardNet Triplet loss is a regular Triplet-Loss, i.e. MAX(0, alpha + distances_to_positives - distances_to_negatives), where:  alpha (sometimes called \"margin\") is a hyper-parameter  distance_to_positives are distances (here, L2 is used)  distance_to_negative are distances to the hardest negatives for each anchor in a batch.",
      "As input HardNet operates with N * 2 images (N anchor/query images and N corresponding to them positives)  Mining algorithm: 1.",
      "Compute distance matrix D between N anchors and N positives.",
      "2. distances_to_positives = trace of distance matrix (diagonal elements) 3.",
      "For each row minimal non-diagonal element is taken as a distance to the hardest negatives (closest to anchor).",
      "From these chosen values distances_to_negatives are obtained.",
      "All this can be rewritten as:  Loss = MAX(0, alpha + Trace(D) + row_wise_min(D + I * inf)), where I is the identity matrix.",
      "Architecture:  Notes:  The described mining procedure highly relies on a fact that all N anchors would should to N different classes.",
      "And from my personal point of view requires minor modification to handle such corner case.",
      "The given loss/mining procedure is fast, but in contrast to other mining strategies doesn't provide hardest positive (furthest from anchor).",
      "Results:  Wide baseline stereo example:  The bigger batch size, the better:  PhotoTour Patch Verification Results:  Oxford 5k, Paris 6k Patch Verification Results:"
    ],
    "author_id": "ALEJU",
    "pdf_url": "https://arxiv.org/pdf/1705.10872",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 69119323
  },
  {
    "blog_id": "incentivizing-exploraton-in-rl",
    "summary": [
      "\u201cOptimism in the face of uncertainty\u201d, the mantra of the Upper-Confidence Bound 1 algorithm, becomes impractical to follow when the action space is continuous.",
      "Hence, most approaches default to using epsilon-greedy exploration.",
      "This paper proposes a scalable and efficient method for assigning exploration bonuses in large RL problems with complex observations.",
      "A model of the task dynamics is learned to assess the novelty of a new state.",
      "As the ability to model the dynamics of a particular state-action pair improves, the \u201cunderstanding\u201d of the state is thus better and hence its novelty is lower.",
      "This circumvents the need to explicitly maintain visitation frequencies for states and state-action pairs in a table.",
      "When a state-action pair is not understood well enough to make accurate predictions, it is assumed that more knowledge is needed and hence a higher \u201cnovelty\u201d value is assigned to that reward signal.",
      "Evidence  This approach was evaluated on 14 games in the Arcade Learning Environment (ALE)  The reinforcement learning algorithm that was employed was DQN, and performance was evaluated against DQN with epsilon-greedy exploration, Boltzman exploration, and Thompson Sampling  Not clear that this approach outperforms other state-of-the-art methods consistently  Strengths  The paper references methods that were attempted but ultimately failed, such as learning a dynamics model that would predict raw frames (next states) for the Atari simulation  Weaknesses  Need to see this method tested on other environments and scenarios  Interesting related works  Thompson Sampling  Boltzman exploration  Notes  PAC-MDP algorithms such as MBIE-EB and Bayesian algorithms such as Bayesian Exploration Bonuses manage the exploration versus exploitation tradeoff by assigning bonuses to novel states.",
      "(What are thooose).",
      "These sound similar to the UCB1 exploration strategy  An autoencoder was used to obtain the function sigma that encodes the state prediction model.",
      "The choice of autoencoder was for dimensionality reduction of the state space  \u201cThe hidden layers are reduced in dimension until maximal compression occurs with 128 units\u201d  An MLP with 2 layers was used to predict model dynamics.",
      "The sixth layer of the auto-encoder produces the state with reduced dimensionality"
    ],
    "author_id": "pemami",
    "pdf_url": "http://arxiv.org/pdf/1507.00814",
    "author_full_name": "Patrick Emami",
    "source_website": "https://pemami4911.github.io/index.html",
    "id": 12279648
  },
  {
    "blog_id": "stn",
    "summary": [
      "What:  They introduced a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network.",
      "How:  Spatial Transformer allows the spatial manipulation of the data (any feature map or particularly input image).",
      "This differentiable module can be inserted into any CNN, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself.",
      "The action of the spatial transformer is conditioned on individual data samples, with the appropriate behavior learned during training for the task in question.",
      "No additional supervision or modification of the optimization process is required.",
      "Spatial manipulation consists of cropping, translation, rotation, scale, and skew.",
      "STN structure:  Localization net: predicts parameters of the transform theta.",
      "For 2d case, it's 2 x 3 matrix.",
      "For 3d case, it's 3 x 4 matrix.",
      "Grid generator: Uses predictions of Localization net to create a sampling grid, which is a set of points where the input map should be sampled to produce the transformed output.",
      "Sampler: Produces the output map sampled from the input feature map at the predicted grid points.",
      "Notes:  Localization net can predict several transformations(thetas) for subsequent transformation applied to the input image(feature map).",
      "The final regression layer should be initialized to regress the identity transform (zero weights, identity transform bias).",
      "Grid generator and Transforms:  The transformation can have any parameterized form, provided that it is differentiable with respect to the parameters  The most popular is just a 2d affine transform:  or particularly an attention mechanism:  The source/target transformation and sampling is equivalent to the standard texture mapping and coordinates used in graphics.",
      "Sampler:  The key why STN works.",
      "They introduced a (sub-)differentiable sampling mechanism that allows loss gradients to flow back not only to the \"input\" feature map, but also to the sampling grid coordinates, and therefore back to the transformation parameters \u03b8 and Localisation Net.",
      "Results:  Street View House Numbers multi-digit recognition:  Distored MNIST:  CUB-200-2011 birds dataset:  MNIST addition:"
    ],
    "author_id": "ALEJU",
    "pdf_url": "https://arxiv.org/pdf/1506.02025",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 63511892
  },
  {
    "blog_id": "graves16",
    "summary": [
      "This paper proposes a neural architecture that allows to backpropagate gradients though a procedure that can go through a variable and adaptive number of iterations.",
      "These \"iterations\" for instance could be the number of times computations are passed through the same recurrent layer (connected to the same input) before producing an output, which is the case considered in this paper.",
      "This is essentially achieved by pooling the recurrent states and respective outputs computed by each iteration.",
      "The pooling mechanism is essentially the same as that used in the really cool Neural Stack architecture of Edward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman and Phil Blunsom  [ref] .",
      "It relies on the introduction of halting units, which are sigmoidal units computed at each iteration and which gives a soft weight on whether the computation should stop at the current iteration.",
      "Crucially, the paper introduces a new ponder cost $P(x)$, which is a regularization cost that penalizes what is meant to be a smooth upper bound on the number of iterations $N(t)$ (more on that below).",
      "The paper presents experiment on RNNs applied on sequences where, at each time step t (not to be confused with what I'm calling computation iterations, which are indexed by n) in the sequence the RNN can produce a variable number $N(t)$ of intermediate states and outputs.",
      "These are the states and outputs that are pooled, to produce a single recurrent state and output for the time step t. During each of the $N(t)$ iterations at time step t, the intermediate states are connected to the same time-step-t input.",
      "After the $N(t)$ iterations, the RNN pools the $N(t)$ intermediate states and outputs, and then moves to the next time step $t+1$.",
      "To mark the transitions between time steps, an extra binary input is appended, which is 1 only for the first intermediate computation iteration.",
      "Results are presented on a variety of synthetic problems and a character prediction problem."
    ],
    "author_id": "hlarochelle",
    "pdf_url": "http://arxiv.org/pdf/1603.08983",
    "author_full_name": "Hugo Larochelle",
    "source_website": "https://www.shortscience.org/user?name=hlarochelle",
    "id": 56843191
  },
  {
    "blog_id": "c7066ce7de051d769908b8fab11990",
    "summary": [
      "The paper explores the domain of conditional image generation by adopting and improving PixelCNN architecture.",
      "Based on PixelRNN and PixelCNN  Models image pixel by pixel by decomposing the joint image distribution as a product of conditionals.",
      "PixelRNN uses two-dimensional LSTM while PixelCNN uses convolutional networks.",
      "PixelRNN gives better results but PixelCNN is faster to train.",
      "Gated PixelCNN  PixelRNN outperforms PixelCNN due to the larger receptive field and because they contain multiplicative units, LSTM gates, which allow modelling more complex interactions.",
      "To account for these, deeper models and gated activation units (equation 2 in the paper ) can be used respectively.",
      "Masked convolutions can lead to blind spots in the receptive fields.",
      "These can be removed by combining 2 convolutional network stacks:  Horizontal stack - conditions on the current row.",
      "Vertical stack - conditions on all rows above the current row.",
      "Every layer in the horizontal stack takes as input the output of the previous layer as well as that of the vertical stack.",
      "Residual connections are used in the horizontal stack and not in the vertical stack (as they did not seem to improve results in the initial settings).",
      "Conditional PixelCNN  Model conditional distribution of image, given the high-level description of the image, represented using the latent vector h (equation 4 in the paper )  This conditioning does not depend on the location of the pixel in the image.",
      "To consider the location as well, map h to spatial representation s = m(h) (equation 5 in the the paper )  PixelCNN Auto-Encoders  Start with a traditional auto-encoder architecture and replace the deconvolutional decoder with PixelCNN and train the network end-to-end.",
      "Experiments  For unconditional modelling, Gated PixelCNN either outperforms PixelRNN or performs almost as good and takes much less time to train.",
      "In the case of conditioning on ImageNet classes, the log likelihood measure did not improve a lot but the visual quality of the generated sampled was significantly improved.",
      "Paper also included sample images generated by conditioning on human portraits and by training a PixelCNN auto-encoder on ImageNet patches."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1606.05328",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 96217072
  },
  {
    "blog_id": "extrapolating-beyond-suboptimal-demonstrations-via-inverse-reinforcement-learning-from-observations",
    "summary": [
      "The paper proposes a new inverse RL (IRL) algorithm, called as Trajectory-ranked Reward EXtrapolation (T-REX) that learns a reward function from a collection of ranked trajectories.",
      "Standard IRL approaches aim to learn a reward function that \u201cjustifies\u201d the demonstration policy and hence those approaches cannot outperform the demonstration policy.",
      "In contrast, T-REX aims to learn a reward function that \u201cexplains\u201d the ranking over demonstrations and can learn a policy that outperforms the demonstration policy.",
      "Approach  The input is a sequence of trajectories T1, \u2026 Tm which are ranked in the order of preference.",
      "That is, given any pair of trajectories, we know which of the two trajectories is better.",
      "The setup is to learn from observations where the learning agent does not have access to the true reward function or the action taken by the demonstration policy.",
      "Reward Inference  A parameterized reward function r\u03b8 is trained with the ranking information using a binary classification loss function which aims to predict which of the two given trajectory would be ranked higher.",
      "Given a trajectory, the reward function predicts the reward for each state.",
      "The sum of rewards (corresponding to the two trajectories) is used used to predict the preferred trajectory.",
      "T-REX uses partial trajectories instead of full trajectories as a data augmentation strategy.",
      "Policy Optimization  Once a reward function has been learned, standard RL approaches can be used to train a new policy.",
      "Results  Environments: Mujoco (Half Cheetah, Ant, Hooper), Atari  Demonstrations generated using PPO (checkpointed at different stages of training).",
      "Ensemble of networks used to learn the reward functions.",
      "The proposed approach outperforms the baselines Behaviour Cloning from Observations and Generative Adversarial Imitation Learning .",
      "In terms of reward extrapolation, T-REX can predict the reward for trajectories which are better than the demonstration trajectories.",
      "Some ablation studies considered the effect of adding noise (random swapping the preference between trajectories) and found that the model is somewhat robust to noise up to an extent."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1904.06387",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 47257042
  },
  {
    "blog_id": "pixel-recursive-super-res",
    "summary": [
      "This paper proposes a novel neural architecture for solving the super resolution task for large factors of magnification.",
      "The problem is challenging, because the input images can be as low-res as 8 x 8; hence, there is a wide variety of high-res images that could correspond to this image.",
      "The author\u2019s neural architecture consists of an autoregressive PixelCNN network augmented with a conditioning Convolutional Neural Network; the autoregressive aspect is to allow the network to output pixels sequentially and thereby capture the conditional dependency between a pixel and its neighbors.",
      "This is in contrast to prior work that assume conditional independence between pixels, and use a MSE per-pixel loss for supervision.",
      "Questions for discussion  Not too familiar with this line of research; 32 x 32 images is still fairly \u201clow-res\u201d.",
      "Is this state-of-the-art?",
      "The authors highlighted how the perceptual quality did not always correspond with negative log likelihood.",
      "Why might this be?",
      "NLL is equivalent to MLE, which is equivalent to minimizing KL divergence\u2026  General commentary  Dictionary-inspired methods that search a bank of pre-learned filters on images and selecting appropriate patches by an efficient hashing mechanism has comparable performance  PixelCNN is a stochastic model that provides an explicit model for $ \\log p( y_i | x, y_{< i}) $.",
      "However, the auto-regressive distribution largely ignores the conditioning on the low-resoultion image without explicitly separating the network into two components, one being a \u201cconditioning\u201d network.",
      "Had to add an extra term to the loss, the cross-entropy between the conditioning network\u2019s predictions via a softmax over the K possible values that the i'th output pixel can take and the ground truth labels"
    ],
    "author_id": "pemami",
    "pdf_url": "https://arxiv.org/pdf/1702.00783",
    "author_full_name": "Patrick Emami",
    "source_website": "https://pemami4911.github.io/index.html",
    "id": 37796162
  },
  {
    "blog_id": "chengs15",
    "summary": [
      "This paper presents an approach to initialize a neural network from the parameters of a smaller and previously trained neural network.",
      "This is effectively done by increasing the size (in width and/or depth) of the previously trained neural network, in such of a way that the function represented by the network doesn't change (i.e. the output of the larger neural network is still the same).",
      "The motivation here is that initializing larger neural networks in this way allows to accelerate their training, since at initialization the neural network will already be quite good.",
      "In a nutshell, neural networks are made wider by adding several copies (selected randomly) of the same hidden units to the hidden layer, for each hidden layer.",
      "To ensure that the neural network output remains the same, each incoming connection weight must also be divided by the number of replicas that unit is connected to in the previous layer.",
      "If not training using dropout, it is also recommended to add some noise to this initialization, in order to break its initial symmetry (though this will actually break the property that the network's output is the same).",
      "As for making a deeper network, layers are added by initializing them to be the identity function.",
      "For ReLU units, this is achieved using an identity matrix as the connection weight matrix.",
      "For units based on sigmoid or tanh activations, unfortunately it isn't possible to add such identity layers.",
      "In their experiments on ImageNet, the authors show that this initialization allows them to train larger networks faster than if trained from random initialization.",
      "More importantly, they were able to outperform their previous validation set ImageNet accuracy by initializing a very large network from their best Inception network."
    ],
    "author_id": "hlarochelle",
    "pdf_url": "http://arxiv.org/pdf/1511.05641",
    "author_full_name": "Hugo Larochelle",
    "source_website": "https://www.shortscience.org/user?name=hlarochelle",
    "id": 17980043
  },
  {
    "blog_id": "1901.10912",
    "summary": [
      "How can we learn causal relationships that explain data?",
      "We can learn from non-stationary distributions.",
      "If we experiment with different factorizations of relationships between variables we can observe which ones provide better sample complexity when adapting to distributional shift and therefore are likely to be causal.",
      "If we consider the variables A and B we can factor them in two ways:  $P(A,B) = P(A)P(B|A)$ representing a causal graph like $A\\rightarrow B$  $P(A,B) = P(A|B)P(B)$ representing a causal graph like $A \\leftarrow B$  The idea is if we train a model with one of these structures; when adapting to a new shifted distribution of data it will take longer to adapt if the model does not have the correct inductive bias.",
      "For example let's say that the true relationship is $A$=Raining causes $B$=Open Umbrella (and not vice-versa).",
      "Changing the marginal probability of Raining (say because the weather changed) does not change the mechanism that relates $A$ and $B$ (captured by $P(B|A)$), but will have an impact on the marginal $P(B)$.",
      "So after this distributional shift the function that modeled $P(B|A)$ will not need to change because the relationship is the same.",
      "Only the function that modeled $P(A)$ will need to change.",
      "Under the incorrect factorization $P(B)P(A|B)$, adaptation to the change will be slow because both $P(B)$ and $P(A|B)$ need to be modified to account for the change in $P(A)$ (due to Bayes rule).",
      "Here a difference in sample complexity can be observed when modeling the joint of the shifted distribution.",
      "$B\\rightarrow A$ takes longer to adapt:  [url]"
    ],
    "author_id": "joecohen",
    "pdf_url": "http://arxiv.org/pdf/1901.10912v2",
    "author_full_name": "Joseph Cohen",
    "source_website": "https://www.shortscience.org/user?name=joecohen",
    "id": 11790475
  },
  {
    "blog_id": "learning-to-compute-word-embeddings-on-the-fly",
    "summary": [
      "Word based language models suffer from the problem of rare or Out of Vocabulary (OOV) words.",
      "Learning representations for OOV words directly on the end task often results in poor representation.",
      "The alternative is to replace all the rare words with a single, unique representation (loss of information) or use character level models to obtain word representations (they tend to miss on the semantic relationship).",
      "The paper proposes to learn a network that can predict the representations of words using auxiliary data (referred to as definitions) such as dictionary definitions, Wikipedia infoboxes, the spelling of the word etc.",
      "The auxiliary data encoders are trained jointly with the end task to ensure that word representations align with the requirements of the end task.",
      "Approach  Given a rare word w, let d(w) = <x1, x2\u2026> denote its defination where xi are words.",
      "d(w) is fed to a defination reader network f (LSTM) and its last state is used as the defination embedding ed(w)  In case w has multiple definitions, the embeddings are combined using mean pooling.",
      "The approach can be extended to in-vocabulary words as well by using the definition embedding of such words to update their original embeddings.",
      "Experiments  Auxiliary data sources  Word definitions from WordNet  Spelling of words  The proposed approach was tested on following tasks:  Extractive Question Answering over SQuAD  Base model from Xiong et al. 2016  Entailment Prediction over SNLI corpus  Base models from Bowman et al.",
      "2015 and Chen et al. 2016  One Billion Words Language Modelling  For all the tasks, models using both spelling and dictionary (SD) outperformed the model using just one.",
      "While SD does not outperform the Glove model (with full vocabulary), it does bridge the performance gap significantly.",
      "Future Work  Multi-token words like \u201cSan Francisco\u201d are not accounted for now.",
      "The model does not handle the rare words which appear in the definition and just replaces them by the  token.",
      "Making the model recursive would be a useful addition."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1506.03134",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 39357436
  },
  {
    "blog_id": "hindsight-experience-replay",
    "summary": [
      "Hindsight Experience Replay(HER) is a sample efficient technique to learn from sparse rewards.",
      "Idea  Assume a footballer misses the goal narrowly.",
      "Even though the player does not get any \u201creward\u201d(in terms of goal), the player realizes that had the goal post been shifted a bit, it would have resulted in a goal(reward).",
      "The same intuition is applied for the RL agent - let us say that the true goal state was g while the agent ends up in the state s.  While the action sequence is not useful for reaching the goal state g, it is indeed useful for reaching state s. Hence the trajectory could be replayed with the goal as s(and not g).",
      "Technical Details  Multi-goal policy trained using Universal Value Function Approximation (UVFA).",
      "Every episode starts by sampling a start state and a goal state.",
      "Each goal has a different reward function.",
      "Policy uses both the current state and the current goal state and leads to a state transition sequence s1, s2,\u2026, sn.",
      "Each of these transitions si -> si+1 are stored in a buffer with both the original goal and a subset of the other goals.",
      "For the goal selection, following strategies are tried:  Future - goal state is the state k steps after observing the state transition.",
      "Final - goal state is the final state of the current episode.",
      "Episode - k random states are selected from the current episode.",
      "Randon - k states are selected randomly.",
      "Any off-policy algorithm can be used.",
      "Specifically, DDPG is used.",
      "Experiments  Robotic arm simulated using MuJoCo for push, slide and pick and place tasks.",
      "DDPG with and without HER evaluated on the 3 tasks.",
      "DDPG with the HER variant significantly outperforms the baseline in all the cases."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1707.01495",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 7570292
  },
  {
    "blog_id": "mcmc-rev",
    "summary": [
      "Persi Diaconis, 2009  The Fundamental Theorem of Markov Chains  From any starting state $x$, the $n^{th}$ step of a run of the MC has chance close to $\\pi (y)$ of being at $y$ if $n$ is large.",
      "The MC must be connected, i.e., in the limit, the kernel $K$/proposal distribution/Markov transition matrix has no zero-probability transitions.",
      "Metropolis Algorithm  Based on \u201cproposal\u201d and \u201cacceptance\u201d  The acceptance ratio is to ensure that the fraction of time spent in each state is proportional to $\\pi(x)$ for $x \\in \\chi$  In this algorithm, the normalization constants of the stationary distributions cancel out!",
      "In Equation 2.3, if the acceptance ratio is $< 1$, you are multiplying the probabilities $J(x,y)$ and $A(x,y)$ together.",
      "This generates the success probability $J(x,y)A(x,y)$ for transitioning x -> y.",
      "You want to accept transitions that move to states that are reversible (and hence move you closer to the true stationary distribution), and stay away from states that are not.",
      "The algorithm hence allows the Markov Chain to stay in the same place with some probability if the acceptance ratio is low.",
      "This algorithm produces a reversible Markov chain:  Since $\\pi K = \\pi$ (the stationary distribution is unchanged by the operation of the kernel $K$), $\\pi$ is a left eigenvector of $K$ with eigenvalue 1.",
      "The basic result on convergence to the stationary distribution can be found by taking the spectral decomposition of $K$.",
      "Gibbs sampler  Example  Here\u2019s a neat example of the Metropolis Hastings algorithm for sampling from a boltzmann distribution .",
      "Remember- low-energy states have high boltzmann probability!"
    ],
    "author_id": "pemami",
    "pdf_url": "https://www.ams.org/journals/bull/2009-46-02/S0273-0979-08-01238-X/S0273-0979-08-01238-X.pdf",
    "author_full_name": "Patrick Emami",
    "source_website": "https://pemami4911.github.io/index.html",
    "id": 71425952
  },
  {
    "blog_id": "8da7f77418aa22751ffed115779126",
    "summary": [
      "The paper presents gradient computation based techniques to visualise image classification models.",
      "Experimental Setup  Single deep convNet trained on ILSVRC-2013 dataset (1.2M training images and 1000 classes).",
      "Weight layer configuration is: conv64-conv256-conv256-conv256-conv256-full4096-full4096-full1000.",
      "Class Model Visualisation  Given a learnt ConvNet and a class (of interest), start with the zero image and perform optimisation by back propagating with respect to the input image (keeping the ConvNet weights constant).",
      "Add the mean image (for training set) to the resulting image.",
      "The paper used unnormalised class scores so that optimisation focuses on increasing the score of target class and not decreasing the score of other classes.",
      "Image-Specific Class Saliency Visualisation  Given an image, class of interest, and trained ConvNet, rank the pixels of the input image based on their influence on class scores.",
      "Derivative of the class score with respect to image gives an estimate of the importance of different pixels for the class.",
      "The magnitude of derivative also indicated how much each pixel needs to be changed to improve the class score.",
      "Class Saliency Extraction  Find the derivative of the class score with respect with respect to the input image.",
      "This would result in one single saliency map per colour channel.",
      "To obtain a single saliency map, take the maximum magnitude of derivative across all colour channels.",
      "Weakly Supervised Object Localisation  The saliency map for an image provides a rough encoding of the location of the object of the class of interest.",
      "Given an image and its saliency map, an object segmentation map can be computed using GraphCut colour segmentation.",
      "Color continuity cues are needed as saliency maps might capture only the most dominant part of the object in the image.",
      "This weakly supervised approach achieves 46.4% top-5 error on the test set of ILSVRC-2013.",
      "Relation to Deconvolutional Networks  DeconvNet-based reconstruction of the n-th layer input is similar to computing the gradient of the visualised neuron activity f with respect to the input layer.",
      "One difference is in the way RELU neurons are treated:  In DeconvNet, the sign indicator (for the derivative of RELU) is computed on output reconstruction while in this paper, the sign indicator is computed on the layer input."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1312.6034",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 22127536
  },
  {
    "blog_id": "accurate_image_super-resolution",
    "summary": [
      "What  They describe a model that upscales low resolution images to their high resolution equivalents (\"Single Image Super Resolution\").",
      "Their model uses a deeper architecture than previous models and has a residual component.",
      "How  Their model is a fully convolutional neural network.",
      "Input of the model: The image to upscale, already upscaled to the desired size (but still blurry).",
      "Output of the model: The upscaled image (without the blurriness).",
      "They use 20 layers of padded 3x3 convolutions with size 64xHxW with ReLU activations.",
      "(No pooling.)",
      "They have a residual component, i.e. the model only learns and outputs the change that has to be applied/added to the blurry input image (instead of outputting the full image).",
      "That change is applied to the blurry input image before using the loss function on it.",
      "(Note that this is a bit different from the currently used \"residual learning\".)",
      "They use a MSE between the \"correct\" upscaling and the generated upscaled image (input image + residual).",
      "They use SGD starting with a learning rate of 0.1 and decay it 3 times by a factor of 10.",
      "They use weight decay of 0.0001.",
      "During training they use a special gradient clipping adapted to the learning rate.",
      "Usually gradient clipping restricts the gradient values to [-t, t] (t is a hyperparameter).",
      "Their gradient clipping restricts the values to [-t/lr, t/lr] (where lr is the learning rate).",
      "They argue that their special gradient clipping allows the use of significantly higher learning rates.",
      "They train their model on multiple scales, e.g. 2x, 3x, 4x upscaling.",
      "(Not really clear how.",
      "They probably feed their upscaled image again into the network or something like that?)",
      "Results  Higher accuracy upscaling than all previous methods.",
      "Can handle well upscaling factors above 2x.",
      "Residual network learns significantly faster than non-residual network.",
      "Architecture of the model.",
      "Super-resolution quality of their model (top, bottom is a competing model)."
    ],
    "author_id": "ALEJU",
    "pdf_url": "http://arxiv.org/pdf/1511.04587",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 93060618
  },
  {
    "blog_id": "good-enough-compositional-data-augmentation",
    "summary": [
      "The paper introduces a simple data augmentation protocol that provides a good compositional inductive bias for sequential models.",
      "Synthetic examples are created by taking real sequences and replacing the fragments in sequences which appear in similar environments.",
      "This operation is referred to as GECA (Good Enough Compositional Augmentation).",
      "The underlying idea is that if two fragments of training examples occur in some environment, then any environment where the first fragment appears is also a valid environment for the second fragment.",
      "Approach  Discover substitutable fragments (ie pairs of fragments that co-occur with a common fragment) and use them to generate new sequences by swapping fragments.",
      "The current work uses very simple criteria to decide if fragments are substitutable - fragments should occur in at least one lexical environment that is exactly the same.",
      "A lexical environment is the k-word window around each span of the fragment.",
      "Though the idea can be motivated by work in generative syntax and distributional semantics, it would not hold like a physical law when applied to the real data.",
      "The authors view this tradeoff as a balance between the shortage of training data vs relative frequency of mistake in the proposed data augmentation approach.",
      "Results  The approach is evaluated on the SCAN dataset when the model is trained on the short sequence of English commands.",
      "Though the dataset augmentation helps the baseline models, it is not surprising given the nature of the SCAN dataset.",
      "More challenging tasks (for evaluating the proposed approach) are semantic parsing (where the query is represented in the form of \u03bb calculus or SQL and low resource language modeling.",
      "While the improvement (in terms of metrics) is sometimes limited, the gains are consistent across different datasets.",
      "Given that the proposed approach is relatively simple and straightforward, it appears to be quite promising."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1904.09545",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 98781755
  },
  {
    "blog_id": "wide_residual_networks",
    "summary": [
      "What  The authors start with a standard ResNet architecture (i.e. residual network has suggested in \"Identity Mappings in Deep Residual Networks\").",
      "Their residual block:  Several residual blocks of 16 filters per conv-layer, followed by 32 and then 64 filters per conv-layer.",
      "They empirically try to answer the following questions:  How many residual blocks are optimal?",
      "(Depth)  How many filters should be used per convolutional layer?",
      "(Width)  How many convolutional layers should be used per residual block?",
      "Does Dropout between the convolutional layers help?",
      "Results  Layers per block and kernel sizes:  Using 2 convolutional layers per residual block seems to perform best:  Using 3x3 kernel sizes for both layers seems to perform best.",
      "However, using 3 layers with kernel sizes 3x3, 1x1, 3x3 and then using less residual blocks performs nearly as good and decreases the required time per batch.",
      "Width and depth:  Increasing the width considerably improves the test error.",
      "They achieve the best results (on CIFAR-10) when decreasing the depth to 28 convolutional layers, with each having 10 times their normal width (i.e. 16*10 filters, 32*10 and 64*10):  They argue that their results show no evidence that would support the common theory that thin and deep networks somehow regularized better than wide and shallow(er) networks.",
      "Dropout:  They use dropout with p=0.3 (CIFAR) and p=0.4 (SVHN).",
      "On CIFAR-10 dropout doesn't seem to consistently improve test error.",
      "On CIFAR-100 and SVHN dropout seems to lead to improvements that are either small (wide and shallower net, i.e. depth=28, width multiplier=10) or significant (ResNet-50).",
      "They also observed oscillations in error (both train and test) during the training.",
      "Adding dropout decreased these oscillations.",
      "Computational efficiency:  Applying few big convolutions is much more efficient on GPUs than applying many small ones sequentially.",
      "Their network with the best test error is 1.6 times faster than ResNet-1001, despite having about 3 times more parameters."
    ],
    "author_id": "ALEJU",
    "pdf_url": "http://arxiv.org/pdf/1605.07146v1",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 77737945
  },
  {
    "blog_id": "1605.06465",
    "summary": [
      "This paper presents Swapout, a simple dropout method applied to Residual Networks (ResNets).",
      "In a ResNet, a layer $Y$ is computed from the previous layer $X$ as  $Y = X + F(X)$  where $F(X)$ is essentially the composition of a few convolutional layers.",
      "Swapout simply applies dropout separately on both terms of a layer's equation:  $Y = \\Theta_1 \\odot X + \\Theta_2 \\odot F(X)$  where $\\Theta_1$ and $\\Theta_2$ are independent dropout masks for each term.",
      "The paper shows that this form of dropout is at least as good or superior as other forms of dropout, including the recently proposed [stochastic depth dropout][1].",
      "Much like in the stochastic depth paper, better performance is achieved by linearly increasing the dropout rate (from 0 to 0.5) from the first hidden layer to the last.",
      "In addition to this observation, I also note the following empirical observations:  1.",
      "At test time, averaging the output layers of multiple dropout mask samples (referenced to as stochastic inference) is better than replacing the masks by their expectation (deterministic inference), the latter being the usual standard.",
      "2.",
      "Comparable performance is achieved by making the ResNet wider (e.g. 4 times) and with fewer layers (e.g.",
      "32) than the orignal ResNet work with thin but very deep (more than 1000 layers) ResNets.",
      "This would confirm a similar observation from [this paper][2].",
      "Overall, these are useful observations to be aware of for anyone wanting to use ResNets in practice.",
      "[1]:  [url]"
    ],
    "author_id": "hlarochelle",
    "pdf_url": "http://arxiv.org/pdf/1605.06465v1",
    "author_full_name": "Hugo Larochelle",
    "source_website": "https://www.shortscience.org/user?name=hlarochelle",
    "id": 18304114
  },
  {
    "blog_id": "towards-a-natural-benchmark-for-continual-learning",
    "summary": [
      "Continual Learning paradigm focuses on learning from a non-stationary stream of data with additional desiderata - transferring knowledge from previously seen task to unseen tasks and being resilient to catastrophic forgetting - all with a fixed memory and computational budget.",
      "This is in contrast to the IID (independent and identically distributed) assumption in statistical learning.",
      "One common example of the non-iid data is setups involving sequential decision making - eg Reinforcement learning.",
      "Paper  Benchmark  Many existing benchmarks use MNIST as the underlying dataset (eg Permuted MNIST, Split MNIST, etc).",
      "These benchmarks lack complexity and make it hard to observe positive and negative backward transfer.",
      "Most works focus only on the catastrophic forgetting challenge and ignore the other issues (like computation and memory footprint, the capacity of the network, etc).",
      "The paper proposes a new benchmark based on Starcraft II video game to understand the different approaches for lifelong learning.",
      "The sequence of tasks is designed to be a curriculum - the learning agent stats with learning simple skills and later move to more complex tasks.",
      "These complex tasks require remembering and composing skills learned in the earlier levels.",
      "To evaluate for catastrophic forgetting, the tasks are designed such that not all the skills are needed for solving each task.",
      "Hence the learning agent needs to remember skills even though they are not needed at the current level.",
      "Each level comes with a fixed computational budget of episodes and each episode has a fixed time limit.",
      "Once the budget is consumed the agent has to proceed to the next level.",
      "Hence agents with better sample efficiency would benefit.",
      "The benchmark supports both RL and supervised learning version.",
      "In the supervised version, expert agents (pretrained on each level) are also provided.",
      "Baselines are provided for distillation (using experts): sequential training (fine tuning), Dropout and SER.",
      "None of the baseline methods achieve positive or negative backward transfer.",
      "When modeled as a pure RL task, the benchmark is extremely difficult to solve.",
      "The paper suggests using a metric to record the amount of learning/data required to recover performance on the previous task."
    ],
    "author_id": "shugan",
    "pdf_url": "https://www.mitpressjournals.org/doi/pdf/10.1162/089976602753712972",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 24000799
  },
  {
    "blog_id": "get-to-the-point-summarization-with-pointer-generator-networks",
    "summary": [
      "Sequence-to-Sequence models have made abstract summarization viable but they still suffer from issues like out of vocabulary words and repetitive sentences.",
      "The paper proposes to overcome these limitations by using a hybrid Pointer-Generator network (to copy words from the source text) and a coverage vector that keeps track of content that has already been summarized so as to discourage repetition.",
      "Code  Model  Pointer Generator Network  It is a hybrid model between the Sequence-to-Sequence network and Pointer Network such that when generating a word, the model decides whether the word would be generated using the softmax vocabulary (Sequence-to-Sequence) or using the source vocabulary (Pointer Network).",
      "Since the model can choose a word from the source vocabulary, the issue of out of vocabulary words is handled.",
      "Coverage Mechanism  The model maintains a coverage vector which is the sum of attention distributions over all previous decoder timesteps.",
      "This coverage vector is fed as an input to the attention mechanism.",
      "A coverage loss is added to prevent the model from repeatedly attending to the same word.",
      "The idea is to capture how much coverage different words have already received from the attention mechanism.",
      "Observation  Model when evaluated on CNN/Daily Mail summarization task, outperforms the state-of-the-art by at least 2 ROUGE points though it still does not outperform the lead-3 baseline.",
      "Lead-3 baseline uses first 3 sentences as the summary of the article which should be a strong baseline given that the dataset is actually about news articles.",
      "The model is initially trained without coverage and then finetuned with the coverage loss.",
      "During training, the model first learns how to copy words and then how to generate words (pgen starts from 0.3 and converges to 0.53).",
      "During testing, the model strongly prefers copying over generating (pgen = 0.17).",
      "Further, whenever the model is at beginning of sentences or at the join between switched-together fragments, it prefers to generate a word instead of copying one from the source language.",
      "The overall model is very simple, neat and interpretable and also performs well in practice."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1704.04368",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 5180208
  },
  {
    "blog_id": "oordkk16",
    "summary": [
      "This paper explores the use of convolutional (PixelCNN) and recurrent units (PixelRNN) for modeling the distribution of images, in the framework of autoregression distribution estimation.",
      "In this framework, the input distribution $p(x)$ is factorized into a product of conditionals $\\Pi p(x_i | x_i-1)$.",
      "Previous work has shown that very good models can be obtained by using a neural network parametrization of the conditionals (e.g. see our work on NADE  [ref] ).",
      "Moreover, unlike other approaches based on latent stochastic units that are directed or undirected, the autoregressive approach is able to compute log-probabilities tractably.",
      "So in this paper, by considering the specific case of x being an image, they exploit the topology of pixels and investigate appropriate architectures for this.",
      "Among the paper's contributions are:  1.",
      "They propose Diagonal BiLSTM units for the PixelRNN, which are efficient (thanks to the use of convolutions) while making it possible to, in effect, condition a pixel's distribution on all the pixels above it (see Figure 2 for an illustration).",
      "2.",
      "They demonstrate that the use of residual connections (a form of skip connections, from hidden layer i-1 to layer $i+1$) are very effective at learning very deep distribution estimators (they go as deep as 12 layers).",
      "3.",
      "They show that it is possible to successfully model the distribution over the pixel intensities (effectively an integer between 0 and 255) using a softmax of 256 units.",
      "4.",
      "They propose a multi-scale extension of their model, that they apply to larger 64x64 images.",
      "The experiments show that the PixelRNN model based on Diagonal BiLSTM units achieves state-of-the-art performance on the binarized MNIST benchmark, in terms of log-likelihood.",
      "They also report excellent log-likelihood on the CIFAR-10 dataset, comparing to previous work based on real-valued density models.",
      "Finally, they show that their model is able to generate high quality image samples."
    ],
    "author_id": "hlarochelle",
    "pdf_url": "http://arxiv.org/pdf/1601.06759",
    "author_full_name": "Hugo Larochelle",
    "source_website": "https://www.shortscience.org/user?name=hlarochelle",
    "id": 435175
  },
  {
    "blog_id": "character-based_neural_machine_translation",
    "summary": [
      "What  Most neural machine translation models currently operate on word vectors or one hot vectors of words.",
      "They instead generate the vector of each word on a character-level.",
      "Thereby, the model can spot character-similarities between words and treat them in a similar way.",
      "They do that only for the source language, not for the target language.",
      "How  They treat each word of the source text on its own.",
      "To each word they then apply the model from Character-aware neural language models , i.e. they do per word:  Embed each character into a 620-dimensional space.",
      "Stack these vectors next to each other, resulting in a 2d-tensor in which each column is one of the vectors (i.e. shape 620xN for N characters).",
      "Apply convolutions of size 620xW to that tensor, where a few different values are used for W (i.e. some convolutions cover few characters, some cover many characters).",
      "Apply a tanh after these convolutions.",
      "Apply a max-over-time to the results of the convolutions, i.e. for each convolution use only the maximum value.",
      "Reshape to 1d-vector.",
      "Apply two highway-layers.",
      "They get 1024-dimensional vectors (one per word).",
      "Visualization of their steps:  Afterwards they apply the model from Neural Machine Translation by Jointly Learning to Align and Translate to these vectors, yielding a translation to a target language.",
      "Whenever that translation yields an unknown target-language-word (\"UNK\"), they replace it with the respective (untranslated) word from the source text.",
      "Results  They the German-English WMT dataset.",
      "BLEU improvemements (compared to neural translation without character-level words):  German-English improves by about 1.5 points.",
      "English-German improves by about 3 points.",
      "Reduction in the number of unknown target-language-words (same baseline again):  German-English goes down from about 1500 to about 1250.",
      "English-German goes down from about 3150 to about 2650.",
      "Translation examples (Phrase = phrase-based/non-neural translation, NN = non-character-based neural translation, CHAR = theirs):"
    ],
    "author_id": "ALEJU",
    "pdf_url": "http://arxiv.org/pdf/1603.00810v3",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 76514111
  },
  {
    "blog_id": "dc31e3c7999ad4a1edf4f289deaa88",
    "summary": [
      "Recurrent Neural Networks (RNNs) are very powerful at modelling sequences but they are not good at learning long-term dependencies.",
      "The paper discusses the reasons behind this difficulty and some suggestions to mitigate it.",
      ".",
      "Optimization Difficulty  RNNs form a deterministic state variable ht as function of input observation and previous state.",
      "Learnable parameters to decide what will be remembered about the past sequence.",
      "Using local optimisation techniques like Stochastic Gradient Descent (SGD) are unlikely to find optimal values of tunable parameters  When computations performed by RNN are unfolded through time, a deep Neural Network with shared weights is realised.",
      "The cost function of this deep network depends on the output of hidden layers.",
      "Gradient descent updates could \"explode\" (become very large) or \"vanish\" (become very small).",
      "Training Recurrent Networks  Clip Gradient - when the norm of the gradient vector (g) is above a threshold, update is done in direction of threshold.g/||g||.",
      "This normalisation implements a simple form of second-order normalisation (the second-order derivate will also be large in regions of exploding gradient).",
      "Use a leaky integration state-to-state map: ht, i = \u03b1iht-1, i + (1-\u03b1i)Fi(ht-1, xt)  Different values of \u03b1 allow a different amount of the previous state to \"leak\" through the unfolded layers to further in time.",
      "This simply expands the time-scale of vanishing gradients and not totally remove them.",
      "Use output probability models like Restricted Boltzmann Machine or NADE to capture higher order dependencies between variables in case of multivariate prediction.",
      "By using rectifier non-linearities, the gradient on hidden units becomes sparse and these sparse gradients help the hidden units to specialise.",
      "The basic idea is that if the gradient is concentrated in fewer paths (in the unfolded computational graph) the vanishing gradient effect would be limited.",
      "A simplified Nesterov Momentum rule is proposed to allow storing past velocities for a longer time while actually using these velocities more conservatively.",
      "The new formulation is also easier to implement.",
      "Results  SGD with these optimisations outperforms a vanilla SGD."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1606.03126",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 21919380
  },
  {
    "blog_id": "fengzkxm16",
    "summary": [
      "This paper presents the theoretical notion of ensemble robustness and how it might provide an explanation for the success of deep learning algorithms.",
      "This work is an extension of some of the author's previous work (see Definition 2), demonstrating a theoretical relationship between a notion of robustness to adversarial examples and generalization performance.",
      "One initial observation made in this work is that this previous notion of robustness cannot explain the good performance of deep neural networks, since they have been shown to in fact not be robust to adversarial examples.",
      "So in this paper, the authors propose to study a notion of ensemble robustness (see Definition 3), and show that it can also be linked to generalization performance (see Theorem 1 and Corollary 1).",
      "The \"ensemble\" part comes from taking into account the stochasticity of the learning algorithm, i.e. the fact that the models they produce can vary from one run to another, even if applied on the same training set.",
      "The stochasticity here can come from the use of dropout, of SGD with random ordering of the training examples or from the random parameter initialization.",
      "Other theoretical results are also presented, such as one relating the variance of the robustness to generalization performance and another specific to the use of dropout.",
      "Finally, the paper also proposes a semi-supervised learning algorithm inspired from their definition of ensemble robustness, in which a model is trained to classify the perturbed (adversarial) version of an example in the same class as the original (non perturbed) example.",
      "On MNIST, they achieve excellent results, matching the performance of the state-of-the-art Ladder Networks."
    ],
    "author_id": "hlarochelle",
    "pdf_url": "http://arxiv.org/pdf/1602.02389",
    "author_full_name": "Hugo Larochelle",
    "source_website": "https://www.shortscience.org/user?name=hlarochelle",
    "id": 7904666
  },
  {
    "blog_id": "balanrmw15",
    "summary": [
      "This paper combines two ideas.",
      "The first is stochastic gradient Langevin dynamics (SGLD), which is an efficient Bayesian learning method for larger datasets, allowing to efficiently sample from the posterior over the parameters of a model (e.g. a deep neural network).",
      "In short, SGLD is stochastic (minibatch) gradient descent, but where Gaussian noise is added to the gradients before each update.",
      "Each update thus results in a sample from the SGLD sampler.",
      "To make a prediction for a new data point, a number of previous parameter values are combined into an ensemble, which effectively corresponds to Monte Carlo estimate of the posterior predictive distribution of the model.",
      "The second idea is distillation or dark knowledge, which in short is the idea of training a smaller model (student) in replicating the behavior and performance of a much larger model (teacher), by essentially training the student to match the outputs of the teacher.",
      "The observation made in this paper is that the step of creating an ensemble of several models (e.g. deep networks) can be expensive, especially if many samples are used and/or if each model is large.",
      "Thus, they propose to approximate the output of that ensemble by training a single network to predict to output of ensemble.",
      "Ultimately, this is done by having the student predict the output of a teacher corresponding to the model with the last parameter value sampled by SGLD.",
      "Interestingly, this process can be operated in an online fashion, where one alternates between sampling from SGLD (i.e. performing a noisy SGD step on the teacher model) and performing a distillation update (i.e.",
      "updating the student model, given the current teacher model).",
      "The end result is a student model, whose outputs should be calibrated to the bayesian predictive distribution."
    ],
    "author_id": "hlarochelle",
    "pdf_url": "http://papers.nips.cc/paper/5965-bayesian-dark-knowledge.pdf",
    "author_full_name": "Hugo Larochelle",
    "source_website": "https://www.shortscience.org/user?name=hlarochelle",
    "id": 3517250
  },
  {
    "blog_id": "gulsm15",
    "summary": [
      "This paper presents a method for training feed-forward neural networks with stochastic hidden units (e.g. sigmoid belief networks), to optimize the expectation (over the stochastic units) of some arbitrary loss function.",
      "While the proposed method is applicable to any type of stochastic units, it is most interesting for the case of discrete stochastic units, since the reparametrization trick of variational autoencoders cannot be applied to backprop through the sampling step.",
      "In short, the method builds on the likelihood ratio method (of which REINFORCE is a special case) and proposes a baseline (also known as control variate) which, according to the authors, is such that an unbiased gradient is obtained.",
      "Specifically, the baseline corresponds to the first-order Taylor expansion of the loss function around some deterministic value of the hidden units (x\u0304) that doesn't depend on the stochastic hidden units (noted x in the paper).",
      "For a likelihood ratio method to be unbiased, it is required that the expectation of the baseline (times the gradient of the model's log distribution) with respect to the model's distribution be tractable.",
      "For the proposed baseline, it can be shown that computing this expectation requires the gradient of the mean (\u03bc) of each stochastic unit in the network with respect to each parameter.",
      "The key idea behind the proposed method is that 1) an estimate of this expectation can be obtained simply using mean-field and 2)  since mean-field is estimated by a feedforward deterministic pass over the network, it is thus possible to compute the gradients of \u03bc by backpropagation through the mean-field pass (hence the name of the method, MuProp).",
      "Experiments show that this method converges much faster than previously proposed unbiased methods and often performs better.",
      "Experiments also show that the method obtains competitive performance compared to biased methods (such as the \"straight through\" method)."
    ],
    "author_id": "hlarochelle",
    "pdf_url": "http://arxiv.org/pdf/1511.05176",
    "author_full_name": "Hugo Larochelle",
    "source_website": "https://www.shortscience.org/user?name=hlarochelle",
    "id": 70691684
  },
  {
    "blog_id": "lfd-in-the-wild",
    "summary": [
      "The motivation behind this work is to develop an automated process for learning the behaviors of road users from large amounts of unlabeled video data.",
      "A generative model (trained policy) of road user behavior could be used within a larger traffic scene understanding pipeline.",
      "In this paper, they propose Horizon GAIL, an imitation-learning algorithm based on GAIL, that stabilizes learning from demonstration (LfD) over long horizons.",
      "Expert policy demonstrations are provided by a slightly improved Deep SORT tracker, and they use PPO as the \u201cstudent\u201d RL algorithm.",
      "The Unity game engine is used to build an RL env that mimcs the scene from the real-world environment to rollout their PPO Horizon-GAIL agent.",
      "Their experiments are on 850 minutes of traffic camera data of a large roundabout.",
      "By using a curriculum where the episode horizon is extended by 1 timestep each training epoch, they demonstrated how Horizon-GAIL can match the expert policy\u2019s state/action distribution much more closely than GAIL, PS-GAIL, and behavior cloning while also improving on training stability.",
      "Observations  The ability to auto-generate the Unity env from Google Maps would be crucial to scaling this technique up.",
      "maps2sim?",
      "They provided an empirical comparison of DeepSORT with ViBe\u2019s vision tracker, and showed that running the Kalman Filter in 3D space improved Deep SORT\u2019s performance in multiple multi-object tracking metrics by a few percentage points  Each road user is modeled independently, i.e., the policy does not account for other agents in the environment explicitly.",
      "It looks like the policy used for learning vehicle and pedestrian behavior is the same, although because of the Mask R-CNN detector, they are able to differentiate between the two classes.",
      "In scenarios where the behaviors exhibited by the road users can be highly unpredictable and diverse (a busy traffic intersection with heavy pedestrian presence), perhaps a hierarchical policy could be useful that conditions on the inferred object class.",
      "Interesting future work might include incorporating multi-agent modeling in the RL framework for more complex traffic scenarios."
    ],
    "author_id": "pemami",
    "pdf_url": "https://arxiv.org/pdf/1811.03516v1",
    "author_full_name": "Patrick Emami",
    "source_website": "https://pemami4911.github.io/index.html",
    "id": 94015576
  },
  {
    "blog_id": "simple-baseline-for-visual-question-answering",
    "summary": [
      "Problem Statement  VQA Task: Given an image and a free-form, open-ended, natural language question (about the image), produce the answer for the image.",
      "The paper attempts to fine tune the simple baseline method of Bag-of-Words + Image features (iBOWIMG) to make it competitive against more sophisticated LSTM models.",
      "Model  VQA modelled as a classification task where the system learns to choose among one of the top k most prominent answers.",
      "Text Features - Convert input question to a one-hot vector and then transform to word vectors using a word embedding.",
      "Image Features - Last layer activations from GoogLeNet.",
      "Text features are concatenated with image features and fed into a softmax.",
      "Different learning rates and weight clipping for word embedding layer and softmax layer with the learning rate for embedding layer much higher than that of softmax layer.",
      "Results  iBOWIMG model reports an accuracy of 55.89% for Open-ended questions and 61.97% for Multiple-Choice questions which is comparable to the performance of other, more sophisticated models.",
      "Interpretation of the model  Since the model is very simple, it is possible to interpret the model to know what exactly is the model learning.",
      "This is the greatest strength of the paper even though the model is very simple and naive.",
      "The model attempts to memorise the correlation between the answer class and the informative words (in the question) and image features.",
      "Question words generally can influence the answer given the bias in images occurring in COCO dataset.",
      "Given the simple linear transformation being used, it is possible to quantify the importance of each single words (in the question) to the answer.",
      "The paper uses the Class Activation Mapping (CAM) approach (which uses the linear relation between softmax and final image feature map) to highlight the informative image regions relevant to the predicted answer.",
      "While the results reported by the paper are not themselves so significant, the described approach provides a way to interpret the strengths and weakness of different VQA datasets."
    ],
    "author_id": "shugan",
    "pdf_url": "http://arxiv.org/pdf/1512.02167.pdf",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 12711637
  },
  {
    "blog_id": "pointer-networks",
    "summary": [
      "The paper introduces a novel architecture that generates an output sequence such that the elements of the output sequence are discrete tokens corresponding to positions in the input sequence.",
      "Such a problem can not be solved using Seq2Seq or Neural Turing Machines as the size of the output softmax is variable (as it depends on the size of the input sequence).",
      "Architecture  Traditional attention-base sequence-to-sequence models compute an attention vector for each step of the output decoder and use that to blend the individual context vectors of the input into a single, consolidated attention vector.",
      "This attention vector is used to compute a fixed size softmax.",
      "In Pointer Nets, the normalized attention vector (over all the tokens in the input sequence) is normalized and treated as the softmax output over the input tokens.",
      "So Pointer Net is a very simple modification of the attention model.",
      "Application  Any problem where the size of the output depends on the size of the input because of which fixed length softmax is ruled out.",
      "eg combinatorial problems such as planar convex hull where the size of the output would depend on the size of the input.",
      "Evaluation  The paper considers the following 3 problems:  Convex Hull  Delaunay triangulations  Travelling Salesman Problem (TSP)  Since some of the problems are NP hard, the paper considers approximate solutions whereever the exact solutions are not feasible to compute.",
      "The authors used the exact same architecture and model parameters of all the instances of the 3 problems to show the generality of the model.",
      "The proosed Pointer Nets outperforms LSTMs and LSTMs with attention and can generalise quite well for much larger sequences.",
      "Interestingly, the order in which the inputs are fed to the system affects its performance.",
      "The authors discussed this apsect in their subsequent paper titled Order Matters: Sequence To Sequence for Sets"
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1506.03134",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 35704901
  },
  {
    "blog_id": "intention-aware",
    "summary": [
      "The proposed contribution is a framework for assessing risk by estimating the intentions of drivers and detecting conflicts between them.",
      "Traffic rules are explicitly represented in the model in order to reason about what the drivers are expected to do.",
      "Bayesian programming is used to generate the risk probabilities.",
      "Particle filtering is used to approximately solve the inference problem of finding the risk based on the probability that a driver does not intend to stop at an intersection when he is expected to.",
      "Evidence  The algorithm was tested on a T-shaped intersection with two passenger vehicles equipped with Vehicle-to-Vehicle communication modems that shared their pose and speed information at a rate of 10 Hz.",
      "The test vehicles were not equipped with autonomous emergency braking functions, instead an auditory and visual warning were triggered whenever the algorithm detected a dangerous situation.",
      "Experimentation involved a priority vehicle and obstacle vehicle.",
      "Evaluation for the performance of the risk assessment algorithms was based on:  The rate of false alarms  The rate of missed detections  The collision prediction horizon  Out of the 90 dangerous trials, 60 were performed with the warning system running on the priority vehicle, and 30 on the obstacle vehicle.",
      "In the 20 non-dangerous trials, there were no false alarms.",
      "For every one of the 90 dangerous tests, the system was able to issue a warning early enough for the driver to avoid collision by breaking.",
      "Strengths  No need for lengthy training  The proposed algorithm is generic and could be implemented for various driving scenarios  No trajectory rollouts  Weaknesses  Speed of the vehicles during experimentatin was not reported  No collisions during experimentation since only real vehicles were used  Evaluation of the robustness of the algorithm is left in question due to minimal variablility in experimentation scenarios  Notes  The risk of a situation is computed based on the probability that intention and expectation do not match, given measurements of the state  a Markov State Space Model is used (this appears to be very similar to the dynamic Bayes net) to propagate the system variables for the bayesian computation"
    ],
    "author_id": "pemami",
    "pdf_url": "https://hal.inria.fr/hal-00875356/document",
    "author_full_name": "Patrick Emami",
    "source_website": "https://pemami4911.github.io/index.html",
    "id": 32560739
  },
  {
    "blog_id": "learning-to-navigate-in-complex-envs",
    "summary": [
      "A primary goal of this work is to incorporate the learning of complex navigation into the RL problem.",
      "Auxiliary tasks are used to augment the loss to provide denser training signals.",
      "The first auxiliary task is reconstruction of a low-dimensional depth map.",
      "The second task is self-supervised; the agent is trained to predict if the current location has been previously visited within a local trajectory.",
      "\u201cThe agent is trained by applying a weighted sum of the gradients coming from A3C, the gradients from depth prediction, and ther gradients from the loop closure\u201d  \u201cIn particular if the prediction loss shares representation with the policy, it could help build useful features for RL much faster, bootstrapping learning\u201d\u00a0\u00bb This is interesting, and a bit confusing.",
      "It seems like this is generalizing from the observation that including depth prediction in the loss was superior to directly using it as an input.",
      "I\u2019m not sure if this is always true, since it seems hard to quantify/evaluate this.",
      "It\u2019s pretty cool that they\u2019re incorporating aspects of SLAM- penalizing loop closures for efficient exploration.",
      "The authors test a couple different variations on a 3D maze navigation task.",
      "They use dynamic mazes and multiple goals to increase the difficulty.",
      "The A3C-variant with both auxiliary tasks performed the best on the most difficult tasks.",
      "The auxiliary tasks were shown to improve data efficiency.",
      "Takeaways  Engineering auxiliary tasks into the loss is an interesting direction.",
      "This won\u2019t scale on its own for more complex tasks beyond navigation, but can potentially be used in conjunction with transfer learning.",
      "How does this compare with SOTA SLAM-and-RRT motion-planners?",
      "I would like to see this implemented on some robots!",
      "Cool, but still relies on RL methods based on data-inefficient and high-variance algorithms (A3C)"
    ],
    "author_id": "pemami",
    "pdf_url": "https://arxiv.org/pdf/1611.03673v2.pdf",
    "author_full_name": "Patrick Emami",
    "source_website": "https://pemami4911.github.io/index.html",
    "id": 92451519
  },
  {
    "blog_id": "resnet_in_resnet",
    "summary": [
      "What  They describe an architecture that merges classical convolutional networks and residual networks.",
      "The architecture can (theoretically) learn anything that a classical convolutional network or a residual network can learn, as it contains both of them.",
      "The architecture can (theoretically) learn how many convolutional layers it should use per residual block (up to the amount of convolutional layers in the whole network).",
      "How  Just like residual networks, they have \"blocks\".",
      "Each block contains convolutional layers.",
      "Each block contains residual units and non-residual units.",
      "They have two \"streams\" of data in their network (just matrices generated by each block):  Residual stream: The residual blocks write to this stream (i.e. it's their output).",
      "Transient stream: The non-residual blocks write to this stream.",
      "Residual and non-residual layers receive both streams as input, but only write to their stream as output.",
      "Their architecture visualized:  Because of this architecture, their model can learn the number of layers per residual block (though BN and ReLU might cause problems here?",
      "):  The easiest way to implement this should be along the lines of the following (some of the visualized convolutions can be merged):  Input of size CxHxW (both streams, each C/2 planes)  Concat  Residual block: Apply C/2 convolutions to the C input planes, with shortcut addition afterwards.",
      "Transient block: Apply C/2 convolutions to the C input planes.",
      "Apply BN  Apply ReLU  Output of size CxHxW.",
      "The whole operation can also be implemented with just a single convolutional layer, but then one has to make sure that some weights stay at zero.",
      "Results  They test on CIFAR-10 and CIFAR-100.",
      "They search for optimal hyperparameters (learning rate, optimizer, L2 penalty, initialization method, type of shortcut connection in residual blocks) using a grid search.",
      "Their model improves upon a wide ResNet and an equivalent non-residual CNN by a good margin (CIFAR-10: 0.5-1%, CIFAR-100: 1-2%)."
    ],
    "author_id": "ALEJU",
    "pdf_url": "http://arxiv.org/pdf/1603.08029",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 81262575
  },
  {
    "blog_id": "distributed-representations-of-words-and-phrases",
    "summary": [
      "Mikolov, et al., 2013  Skip-gram model  Objective is to find word representations that are useful for predicting the surrounding words in a sentence or a document.",
      "Given a sequence of words $w_1, w_2, \u2026, w_T$, the Skip-gram model aims to max the average log probability  where c is the size of the training context.",
      "Larger c results in more training examples and thus can lead to a higher accuracy at the expense of increased training time.",
      "The probability $p(w_O | w_I )$ is represented with a softmax.",
      "Heirarchical Softmax  Instead of evaluated W output nodes of a neural network to get the probability distribution, where W is the size of the target dictionary, only need to evaluate about $\\log_2 (W)$ nodes.",
      "The idea is to represent the output layer as a binary tree with W leaves and, for each node, explicitly represents the relative probabilities of its child nodes.",
      "Then the probability $p(w_O | w_I )$ can be defined by the product of probabilities of a path down the tree from the root.",
      "The root here is the first word in the sequence.",
      "The individual probabilities are outputs of a sigmoid, scaled by +1 or -1 if the current word w\u2019s probability matches that of its child.",
      "Negative Sampling  A simplified form of something called Noice Constrastive Estimation (NCE).",
      "NCE aims to learn a model that is able to differentiate data from noise by means of logistic regression.",
      "The negative sampling objective simplifies this because for the Skip-gram model, only the high-quality vector representation is needed.",
      "The task becomes to distinguish the target word from draws from a noise distribution using logistic regression over k negative samples for each data sample.",
      "Conclusion  The authors used a few other tricks, like sub-sampling frequent words such as \u201cin\u201d, \u201cthe\u201d, \u201ca\u201d.",
      "Also, they used unigrams and bigrams to identify phrases during training.",
      "This approach can be applied to massive monolingual corpuses to quickly learn high-quality vector representations of words."
    ],
    "author_id": "pemami",
    "pdf_url": "https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf",
    "author_full_name": "Patrick Emami",
    "source_website": "https://pemami4911.github.io/index.html",
    "id": 4647896
  },
  {
    "blog_id": "stylistic-transfer-in-natural-language-generation-systems-using-recurrent-neural-networks",
    "summary": [
      "This workshop paper explores the problem of style transfer in natural language generation (NLG).",
      "One possible manifestation would be rewriting technical articles in an easy-to-understate manner.",
      "Challenges  Identifying relevant stylistic cues and using them to control text generation in NLG systems.",
      "Absence of a large amount of training data.",
      "Pitch  Using Recurrent Neural Networks (RNNs) to disentangle the style from semantic content.",
      "Autoencoder model with two components - one for learning style and another for learning content.",
      "This allows for \u201cstyle\u201d component to be replaced while keeping the \u201ccontent\u201d component same, resulting in a style transfer.",
      "One way to think about this is - the encoder generates a 100-dimensional vector.",
      "In this, the first 50 entries, correspond to the \u201cstyle\u201d component and remaining to the \u201ccontent\u201d component.",
      "The proposal is that the loss function should be modified to include a cross-covariance term for ensuring disentanglement.",
      "I think one way of doing this is to have two loss functions:  The first loss function ensures that the input sentence is decoded properly into the target sentence.",
      "This loss is computed for each sentence.",
      "The second loss ensures that the first 50 entries across all the encoded represenations are are correlated.",
      "This loss operates at the batch level.",
      "The total loss is the weighted sum of these 2 losses.",
      "Possible Datasets  Complete works of Shakespeare  Wikpedia Kaggle dataset  Oxford Text Archive  Twitter data  Possible Metrics  Soundness - is the generated text entailed with the input sentence.",
      "Coherence - free of grammatical errors, proper word usage etc.",
      "Effectiveness - how effective was the style transfer  Since some of the metrics are subjective, human evaluators also need to be employed."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1802.04687",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 90011197
  },
  {
    "blog_id": "lingghkswb16",
    "summary": [
      "This paper presents a conditional generative model of text, where text can be generated either one character at a time or by copying some full chunks of character taken directly from the input into the output.",
      "At each step of the generation, the model can decide which of these two modes of generation to use, mixing them as needed to generate a correct output.",
      "They refer to this structure for generation as Latent Predictor Networks  [ref] .",
      "The character-level generation part of the model is based on a simple output softmax over characters, while the generation-by-copy component is based on a Pointer Network architecture.",
      "Critically, the authors highlight that it is possible to marginalize over the use of either types of components by dynamic programming as used in semi-Markov models  [ref] .",
      "One motivating application is machine translation, where the input might contain some named entities that should just be directly copied at the output.",
      "However, the authors experiment on a different problem, that of generating code that would implement the action of a card in the trading card games Magic the Gathering and Hearthstone.",
      "In this application, copying is useful to do things such as copy the name of the card or its numerically-valued effects.",
      "In addition to the Latent Predictor Network structure, the proposed model for this application includes a slightly adapted form of soft-attention as well as character-aware word embeddings as in  [ref]  Also, the authors experiment with a compression procedure on the target programs, that can help in reducing the size of the output space.",
      "Experiments show that the proposed neural network approach outperforms a variety of strong baselines (including systems based on machine translation or information retrieval)."
    ],
    "author_id": "hlarochelle",
    "pdf_url": "http://arxiv.org/pdf/1603.06744",
    "author_full_name": "Hugo Larochelle",
    "source_website": "https://www.shortscience.org/user?name=hlarochelle",
    "id": 21532550
  },
  {
    "blog_id": "learned-optimizers-that-scale-and-generalize",
    "summary": [
      "The paper introduces a learned gradient descent optimizer that has low memory and computational overhead and that generalizes well to new tasks.",
      "Key Advantage  Uses a hierarchial RNN architecture augmented by features like adapted input an output scaling, momentum etc.",
      "A meta-learning set of small diverse optimization tasks, with diverse loss landscapes is developed.",
      "The learnt optimizer generalizes to much more complex tasks and setups.",
      "Architecture  A hierarchical RNN is designed to act as a learned optimizer.",
      "This RNN is the meta-learner and its parameters are shared across different tasks.",
      "The learned optimizer takes as input the gradient (and related metadata) for each parameter and outputs the update to the parameters.",
      "At the lowest level of hierarchical, a small \u201cparameter RNN\u201d ingests the gradient (and related metadata).",
      "One level up, an intermediate \u201cTensor RNN\u201d incorporates information from a subset of Parameter RNNS (eg one Tensor RNN per layer of feedforward network).",
      "At the highest level is the glocal RNN which receives input from all the Tensor RNNs and can keep track of weight updates across the task.",
      "the input of each RNN is averaged and fed as input to the subsequent RNN and the output of each RNN is fed as bias to the previous RNN.",
      "In practice, the hidden states are fixed at 10, 30 and 20 respectively.",
      "Features inspired from existing optimizers  Attention and Nesterov\u2019s momentum  Attention mechanism is incorporated by attending to new regions of the loss surface (which are an offset from previous parameter location).",
      "To incorporate momentum on multiple timescales, the exponential moving average of the gradient at several timescales is also provided as input.",
      "The average gradients are rescaled (as in RMSProp and Adam)  Relative log gradient magnitudes are also provided as input so that the optimizer can access how the gradient magnitude changes with time."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1703.04813",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 30905451
  },
  {
    "blog_id": "neural-probabilistic-language",
    "summary": [
      "Bengio, et al., 2003  associate with each word in the vocabulary a distributed word feature vector (real valued vector in $\\mathbb{R}^n$)  express the joint probability function of word sequences in terms of the feature vectors of these words in the sequence  learn simultaneously the word feature vectors and the parameters of that probability function  For discrete random variables, learning a joint probability distribution is hard because a small change in one of the variables could cause a large change in the value of the function to be estimated.",
      "Instead, transforming the discrete random variables into a vector space in $\\mathbb{R}^n$ allows the use of neural nets or GMMs which are smooth approximators.",
      "Additionally, the notion of a \u201cnearby\u201d word within the continuous vector space representation is now defined more clearly.",
      "Words are mapped into a matrix $C$ of size ($|V| \\times m$) for a vocabulary size $|V|$ and embedding dim $m$.",
      "The feature vectors (columns of $C$) are learned simultanesouly with the parameters of the neural network.",
      "The input to the time-lagged neural network (RNN) is the concatenated vector of word representations.",
      "The objective is to maximize the log-likehood of a given sequence of out-of-sample words.",
      "This essentially is the encoder in Neural Machine Translation.",
      "Things of interest  The curse of dimensionality in modeling joint probability of sequences of words in a language is a major stumbling block  One can reduce the difficulty by using the fact that temporally closer words in the word sequence are statistically more dependent $\\rightarrow$ n-gram models  It was noted by the authors that n-gram models and the neural models made different \u201cerrors\u201d, so an averaging model of the two showed improvements overall  It is suggested by the authors as well to train multiple smaller networks on partitions of the training data to speed things up  Learning word embeddings is very parallelizable- specifically, the computation of the output layer of the neural model was found to take up roughly 99.7% of the computation (since you\u2019re computing a likelihood of a sequence out of the entire vocabulary)"
    ],
    "author_id": "pemami",
    "pdf_url": "http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf",
    "author_full_name": "Patrick Emami",
    "source_website": "https://pemami4911.github.io/index.html",
    "id": 57934330
  },
  {
    "blog_id": "fernandog16",
    "summary": [
      "This paper describes how rank pooling, a very recent approach for pooling representations organized in a sequence $\\\\{{\\bf v}_t\\\\}_{t=1}^T$, can be used in an end-to-end trained neural network architecture.",
      "Rank pooling is an alternative to average and max pooling for sequences, but with the distinctive advantage of maintaining some order information from the sequence.",
      "Rank pooling first solves a regularized (linear) support vector regression (SVR) problem where the inputs are the vector representations ${\\bf v}_t$ in the sequence and the target is the corresponding index $t$ of that representation in the sequence (see Equation 5).",
      "The output of rank pooling is then simply the linear regression parameters $\\bf{u}$ learned for that sequence.",
      "Because of the way ${\\bf u}$ is trained, we can see that ${\\bf u}$ will capture order information, as successful training would imply that ${\\bf u}^\\top {\\bf v}_t <\u00a0{\\bf u}^\\top {\\bf v}_{t'} $ if $t < t'$.",
      "See [this paper]( [url]"
    ],
    "author_id": "hlarochelle",
    "pdf_url": "http://proceedings.mlr.press/v48/fernando16.pdf",
    "author_full_name": "Hugo Larochelle",
    "source_website": "https://www.shortscience.org/user?name=hlarochelle",
    "id": 41467472
  },
  {
    "blog_id": "instance_normalization_the_missing_ingredient_for_fast_stylization",
    "summary": [
      "What  Style transfer between images works - in its original form - by iteratively making changes to a content image, so that its style matches more and more the style of a chosen style image.",
      "That iterative process is very slow.",
      "Alternatively, one can train a single feed-forward generator network to apply a style in one forward pass.",
      "The network is trained on a dataset of input images and their stylized versions (stylized versions can be generated using the iterative approach).",
      "So far, these generator networks were much faster than the iterative approach, but their quality was lower.",
      "They describe a simple change to these generator networks to increase the image quality (up to the same level as the iterative approach).",
      "How  In the generator networks, they simply replace all batch normalization layers with instance normalization layers.",
      "Batch normalization normalizes using the information from the whole batch, while instance normalization normalizes each feature map on its own.",
      "Equations  Let H = Height, W = Width, T = Batch size  Batch Normalization:  Instance Normalization  They apply instance normalization at test time too (identically).",
      "Results  Same image quality as iterative approach (at a fraction of the runtime).",
      "One content image with two different styles using their approach:"
    ],
    "author_id": "ALEJU",
    "pdf_url": "https://arxiv.org/pdf/1607.08022",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 29981772
  },
  {
    "blog_id": "madmax-surviving-out-of-gas-conditions-in-ethereum-smart-contracts",
    "summary": [
      "MadMax: surviving out-of-gas conditions in ethereum smart contracts Grech et al., OOPSLA\u201918  We\u2019re transitioning to look at a selection of papers from the recent OOPSLA conference this week.",
      "MadMax won a distinguished paper award, and makes a nice bridge from the CCS blockchain papers we were looking at last week.",
      "Analysis and verification of smart contracts is a high-value task, possibly more so than in any other programming setting.",
      "The combination of monetary value and public availability makes the early detection of vulnerabilities a task of paramount importance.",
      "(Detection may occur after contract deployment.",
      "Despite the code immutability, which prevents bug fixes, discovering a vulnerability before an attacker may exploit it could enable a trusted third party to move vulnerable funds to safety).",
      "MadMax is in the same vein as Securify , performing EVM bytecode analysis using Datalog (also with Souffl\u00e9 ) to infer security issues in contracts.",
      "In this instance, MadMax focuses on detecting vulnerabilities caused by out-of-gas conditions.",
      "The paper touches on some nice reusable building blocks (e.g. Vandal ).",
      "I could easily see Vandal + Souffl\u00e9 becoming a standard foundation for powerful EVM-based smart contract analysis.",
      "MadMax is available on GitHub at  [url]"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://www.nevillegrech.com/madmax-oopsla18.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 57613420
  },
  {
    "blog_id": "nfq",
    "summary": [
      "The key insight here is that neural networks react globally to local weight changes, which results in unwanted behavior.",
      "For a neural network that is representing the Q-value function, the influence of a weight change for a new datapoint must be constrained by presenting previous knowledge in the form or prior experiences.",
      "The proposed algorithm is a special case of experience replay.",
      "In principle, classical Q-learning can be directly implemented in a neural network.",
      "An MSE can be used to calculate a loss between the expected Q-value and the generated Q-value.",
      "Vanilla application of this transformation requires tens of thousands of training examples due to the problem stated above.",
      "NFQ attempts to address this by doing off-line learning considering an entire set of transition experiences.",
      "RPROP can be used for updating the weights of the neural network, which is an advanced supervised learning technique.",
      "Strengths  NFQ is very flexible.",
      "One variant is to incrementally add new experiences to D, the set of transition experiences.",
      "This is useful for cases where a reasonable set of experiences can not be collected by controlling the system with purely random actions.",
      "It seems like you would want to do a combination of both this, and pre-training on sample paths.",
      "Methods  For the mountain car setup, the authors used an MLP with 3 input neurons (2 state and 1 action), 2 layers of 5 hidden neurons each and 1 output neuron, all with sigmoidal activation functions.",
      "Interesting related works  RPROP and Batch Learning, both my Riedmiller"
    ],
    "author_id": "pemami",
    "pdf_url": "http://ml.informatik.uni-freiburg.de/_media/publications/rieecml05.pdf",
    "author_full_name": "Patrick Emami",
    "source_website": "https://pemami4911.github.io/index.html",
    "id": 11481096
  },
  {
    "blog_id": "galg15",
    "summary": [
      "This paper presents an interpretation of dropout training as performing approximate Bayesian learning in a deep Gaussian process (DGP) model.",
      "This connection suggests a very simple way of obtaining, for networks trained with dropout, estimates of the model's output uncertainty.",
      "This estimate is based and computed from an ensemble of networks each obtained by sampling a new dropout mask.",
      "#### My two cents  This is a really nice and thought provoking contribution to our understanding of dropout.",
      "Unfortunately, the paper in fact doesn't provide a lot of comparisons with either other ways of estimating the predictive uncertainty of deep networks, or to other approximate inference schemes in deep GPs (actually, see update below).",
      "The qualitative examples provided however do suggest that the uncertainty estimate isn't terrible.",
      "Irrespective of the quality of the uncertainty estimate suggested here, I find the observation itself really valuable.",
      "Perhaps future research will then shed light on how useful that method is compared to other approaches, including Bayesian dark knowledge  [ref] .",
      "`Update: On September 27th`, the authors uploaded to arXiv a new version that now includes comparisons with 2 alternative Bayesian learning methods for deep networks, specifically the stochastic variational inference approach of Graves and probabilistic back-propagation of Hernandez-Lobato and Adams.",
      "Dropout actually does very well against these baselines and, across datasets, is almost always amongst the best performing method!"
    ],
    "author_id": "hlarochelle",
    "pdf_url": "http://arxiv.org/pdf/1506.02142",
    "author_full_name": "Hugo Larochelle",
    "source_website": "https://www.shortscience.org/user?name=hlarochelle",
    "id": 8136900
  },
  {
    "blog_id": "resnets",
    "summary": [
      "Residual Neural Networks (ResNets) is the current state-of-the-art Convolutional Neural Network architecture.",
      "The key difference between ResNets and other popular architectures is the use of \u201cshortcut-connections\u201d.",
      "Basically, it was determined that \u201cvery deep\u201d neural networks, or DNNs with a large number of stacked layers,  were exhibiting a degradation in accuracy not caused by overfitting.",
      "Residual Layers in a DNN attempt to learn a \u201cresidual\u201d mapping, which is the desired hidden mapping minus the input to the residual layer.",
      "The input to the layer is then \u201cadded\u201d back in later; hence the name \u201cshortcut-connection\u201d.",
      "One of the motivations for doing this is that it is easy for the network to learn to send the residual to 0 (and hence learn an identity mapping), which is useful if an identity mapping is optimal for the situation.",
      "Learning an  identity mapping is very difficult for a stack of nonlinear layers.",
      "These residual layers do not add any extra parameters.",
      "Evidence  The authors tested ResNets on ImageNet and CIFAR-10, and won first place on the ILSVRC 2015 classification task.",
      "Strengths  The idea is simple and effective.",
      "The material is presented clearly with lots of data to back it up.",
      "Interesting related works  Batch Norm, Highway Networks"
    ],
    "author_id": "pemami",
    "pdf_url": "https://arxiv.org/pdf/1512.03385v1.pdf",
    "author_full_name": "Patrick Emami",
    "source_website": "https://pemami4911.github.io/index.html",
    "id": 49519055
  },
  {
    "blog_id": "harp-hierarchical-representation-learning-for-networks",
    "summary": [
      "HARP is an architecture to learn low-dimensional node embeddings by compressing the input graph into smaller graphs.",
      ".",
      "Given a graph G = (V, E), compute a series of successively smaller (coarse) graphs G0, \u2026, GL.",
      "Learn the node representations in GL and successively refine the embeddings for larger graphs in the series.",
      "The architecture is independent of the algorithms used to embed the nodes or to refine the node representations.",
      "Graph coarsening technique that preserves global structure  Collapse edges and stars to preserve first and second order proximity.",
      "Edge collapsing - select the subset of E such that no two edges are incident on the same vertex and merge their nodes into a single node and merge their edges as well.",
      "Star collapsing - given star structure, collapse the pairs of neighboring nodes (of the central node).",
      "In practice, first apply star collapsing, followed by edge collapsing.",
      "Extending node representation from coarse graph to finer graph  Lets say node1 and node2 were merged into node12 during coarsening.",
      "First copy the representation of node12 into node1, node2.",
      "Additionally, if hierarchical softmax was used, extend the B-tree such that node12 is replaced by 2 child nodes node1 and node2.",
      "Time complexity for HARP + DeepWalk is O(number of walks * |V|) while for HARP + LINE is O(number of iterations * |E|).",
      "The asymptotic complexity remains the same as the HARP-less version for the two cases.",
      "Multilabel classification task shows that HAR improves all the node embedding technique with gains up to 14%."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1706.07845",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 2774914
  },
  {
    "blog_id": "1409.7495",
    "summary": [
      "The goal of this method is to create a feature representation $f$ of an input $x$ that is domain invariant over some domain $d$.",
      "The feature vector $f$ is obtained from $x$ using an encoder network (e.g. $f = G_f(x)$).",
      "The reason this is an issue is that the input $x$ is correlated with $d$ and this can confuse the model to extract features that capture differences in domains instead of differences in classes.",
      "Here I will recast the problem differently from in the paper:  **Problem:** Given a conditional probability $p(x|d=0)$ that may be different from $p(x|d=1)$:  $$p(x|d=0) \\stackrel{?",
      "}{\\ne} p(x|d=1)$$  we would like it to be the case that these distributions are equal.",
      "$$p(G_f(x) |d=0) = p(G_f(x)|d=1)$$  aka:  $$p(f|d=0) = p(f|d=1)$$  Of course this is an issue if some class label $y$ is correlated with $d$ meaning that we may hurt the performance of a classifier that now may not be able to predict $y$ as well as before.",
      "[url]"
    ],
    "author_id": "joecohen",
    "pdf_url": "http://arxiv.org/pdf/1409.7495v2",
    "author_full_name": "Joseph Cohen",
    "source_website": "https://www.shortscience.org/user?name=joecohen",
    "id": 93218559
  },
  {
    "blog_id": "swish-a-self-gated-activation-function",
    "summary": [
      "The paper presents a new activation function called Swish with formulation f(x) = x.sigmod(x) and its parameterised version called Swish-\u03b2 where f(x, \u03b2) = 2x.sigmoid(\u03b2.x) and \u03b2 is a training parameter.",
      "The paper shows that Swish is consistently able to outperform RELU and other activations functions over a variety of datasets (CIFAR, ImageNet, WMT2014) though by small margins only in some cases.",
      "Properties of Swish  Smooth, non-monotonic function.",
      "Swish-\u03b2 can be thought of as a smooth function that interpolates between a linear function and RELU.",
      "Uses self-gating mechanism (that is, it uses its own value to gate itself).",
      "Gating generally uses multiple scalar inputs but since self-gating uses a single scalar input, it can be used to replace activation functions which are generally pointwise.",
      "Being unbounded on the x>0 side, it avoids saturation when training is slow due to near 0 gradients.",
      "Being bounded below induces a kind of regularization effect as large, negative inputs are forgotten.",
      "Since the Swish function is smooth, the output landscape and the loss landscape are also smooth.",
      "A smooth landscape should be more traversable and less sensitive to initialization and learning rates.",
      "Criticism  Swish is much more complicated than ReLU (when weighted against the small improvements that are provided) so it might not end up with as strong an adoption as ReLU."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1710.05941",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 85820029
  },
  {
    "blog_id": "nan_for_video_face_recognition",
    "summary": [
      "What  They suggest a method to get cumulative/aggregated embedding from a sequence of embeddings (i.e get a single face embedding vector from a video).",
      "How  Use attention mechanism to weight embeddings in a sequence.",
      "They suggest two options:  Single attention block \u2013 Universal face feature quality measurement.",
      "where f_k = embedding for k-th image in a sequence, a_k = obtained weight corresponded to k-th embedding  Trainable parameter is: q (shape = embedding size x 1)  Cascaded two attention blocks \u2013 Content-aware aggregation.",
      "This q^1 replaces the q in above formula that computes coefficients a_k.",
      "Trainable parameters are: W (shape = embeddings size x embeddings size), b (shape = embedding size x 1).",
      "Face embedder (could be any CNN) and \"Attention blocks\" can be trained together in end-to-end manner or separately one-by-one.",
      "Training procedure:  For verification problem they used siamese structure with contrastive loss.",
      "For identification problem they used softmax and cross-entropy as loss function.",
      "No recurrent blocks, but still input size independent.",
      "Coefficients a (from the first attention block) strongly correlates with face quality and it's usefulness for recognition.",
      "Results  Shows better results than combining a single embedding by taking mean, median, l2/cos closest, etc.",
      "Shows state-of-the-art performance on YouTubeFaces and IJB-A datasets."
    ],
    "author_id": "ALEJU",
    "pdf_url": "https://arxiv.org/pdf/1603.05474",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 42851012
  },
  {
    "blog_id": "progressive-growing-gans",
    "summary": [
      "The basic idea is to introduce a curriculum into the GAN training procedure.",
      "One starts by training the generator to produce 4 x 4 images, progressively adding layers to increase the resolution.",
      "In the paper, they generated high-quality 1024 x 1024 samples from CelebA, LSUN, and CIFAR-10.",
      "This is a nice applied paper where the core idea is quite simple and explained clearly.",
      "They describe all of the challenges hidden under the surface of training large-scale GANs and tell the reader how they tackled them.",
      "Lots of good deep learning voodoo in this paper.",
      "They found that the progressive scheme helps the GAN converege to much better optimum (image quality is amazing) and reduces total training time by about a factor of 2.",
      "They mainly use the WGAN-GP loss.",
      "Recall that the WGAN loss is  The main change made in WGAN-GP is the addition of a gradient penalty term to take care of the 1-Lipschitz constraint.",
      "Previous, hard weight clipping within some [-c, c] was used.",
      "The new loss looks like  , and $\\lambda$ is set to 10.",
      "Definition: Inception score is an evaluation metric for GANs where generated samples are fed into an Inception model trained on ImageNet.",
      "Images with meaningful objects are supposed to have low label entropy, but the entropy across images should be high (high variation)."
    ],
    "author_id": "pemami",
    "pdf_url": "http://research.nvidia.com/sites/default/files/pubs/2017-10_Progressive-Growing-of/karras2018iclr-paper.pdf",
    "author_full_name": "Patrick Emami",
    "source_website": "https://pemami4911.github.io/index.html",
    "id": 38867178
  },
  {
    "blog_id": "firmament-fast-centralized-cluster-scheduling-at-scale",
    "summary": [
      "Firmament: Fast, centralized cluster scheduling at scale Gog et al. OSDI\u2019 16  Updated link to point to official usenix hosted version  As this paper demonstrates very well, cluster scheduling is a tricky thing to get right at scale.",
      "It sounds so simple on the surface: \u201chere are some new jobs/tasks \u2013 where should I run them?\u201d Of course the slightly more nuanced question, and where the troubles begin, is \u201cwhere should I run them in order to optimize for this objective, given these constraints\u2026?\u201d Typically you have a trade-off between distributed schedulers that can operate at scale and make fast decisions, and a centralized scheduler that can make higher quality decisions (e.g, improve utilisation, load balance, or whatever else your policy dictates) but struggles to make those decisions quickly enough as workload scales.",
      "What we have here is Firmament, a new centralised scheduler that combines high-quality placements on a par with advanced centralized schedulers, and the speed and scale of a distributed scheduler.",
      "It comes from a strong team of researchers across Cambridge, MIT, and Google, and is available in open source at  [url]"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://www.usenix.org/system/files/conference/osdi16/osdi16-gog.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 40774761
  },
  {
    "blog_id": "an-empirical-analysis-of-anonymity-in-zcash",
    "summary": [
      "An empirical analysis of anonymity in Zcash Kappos et al., USENIX Security\u201918  As we\u2019ve seen before, in practice Bitcoin offers little in the way of anonymity .",
      "Zcash on the other hand was carefully designed with privacy in mind.",
      "It offers strong theoretical guarantees concerning privacy.",
      "So in theory users of Zcash can remain anonymous.",
      "In practice though it depends on the way those users interact with Zcash.",
      "Today\u2019s paper choice, \u2018An empirical analysis of anonymity in Zcash\u2019 studies how identifiable transaction participants are in practice based on the 2,242,847 transactions in the blockchain at the time of the study.",
      "We conclude that while it is possible to use Zcash in a private way, it is also possible to shrink its anonymity set considerably by developing simple heuristics based on identifiable patterns of usage.",
      "The analysis also provides some interesting insights into who is using Zcash and for what as well.",
      "Founders and miners combined account for around 66% of the value drawn from the shielded pool.",
      "The code for the analysis is available online at  [url]"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://www.usenix.org/system/files/conference/usenixsecurity18/sec18-kappos.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 36679338
  },
  {
    "blog_id": "infogan",
    "summary": [
      "Chen, et al., 2016  InfoGAN is an extension to Generative Adversarial Networks that learns disentangled representations of the latent variables within the generator network.",
      "The authors employ the variational information maximization framework to optimize a lower bound on a mutual information criterion.",
      "This MI is between a small subset of the latent variables and the output of the generator.",
      "The authors argue that this encourages these latent variables to become \u201cdisentangled\u201d.",
      "In turn, this allows the GANs to learn, in a completely unsupervised manner, interesting data representations such as stylistic factors on the MNIST dataset.",
      "The change to the traditional GAN architecture is minimal, since this is simply a regularized MI term added to the minimax function.",
      "Questions  Why don\u2019t they use the reverse KL in Eq.",
      "4?",
      "This is what is normally used in Variational Bayes (why?).",
      "The reverse KL, KL(Q || P), is minimized when Q places no probability mass where P has no probability mass..  What justifies moving $f(x,y)$ into the third integral in Eq.",
      "7?",
      "What other potential applications of GANs are there besides generating realistic samples from $p_{data}$.",
      "How can it be used for general density estimation?",
      "(This seems like a big question)  Are the features learned by the conv nets any different between InfoGAN and GAN?",
      "(What is the potential importance of this?)",
      "Does InfoGAN produce sharper/more realistic images than GANs?",
      "Seems like the answer would be no, but hard to quantify this"
    ],
    "author_id": "pemami",
    "pdf_url": "https://arxiv.org/pdf/1606.03657",
    "author_full_name": "Patrick Emami",
    "source_website": "https://pemami4911.github.io/index.html",
    "id": 67744436
  },
  {
    "blog_id": "726334de3014defeeb701099a3b4b3",
    "summary": [
      "Conditional version of Generative Adversarial Nets (GAN) where both generator and discriminator are conditioned on some data y (class label or data from some other modality).",
      "Architecture  Feed y into both the generator and discriminator as additional input layers such that y and input are combined in a joint hidden representation.",
      "Experiment  Unimodal Setting  Conditioning MNIST images on class labels.",
      "z (random noise) and y mapped to hidden layers with ReLu with layer sizes of 200 and 1000 respectively and are combined to obtain ReLu layer of dimensionality 1200.",
      "Discriminator maps x (input) and y to maxout layers and the joint maxout layer is fed to sigmoid layer.",
      "Results do not outperform the state-of-the-art results but do provide a proof-of-the-concept.",
      "Multimodal Setting  Map images (from Flickr) to labels (or user tags) to obtain the one-to-many mapping.",
      "Extract image and text features using convolutional and language model.",
      "Generative Model  Map noise and convolutional features to a single 200 dimensional representation.",
      "Discriminator Model  Combine the representation of word vectors (corresponding to tags) and images.",
      "Future Work  While the results are not so good, they do show the potential of Conditional GANs, especially in the multimodal setting."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1411.1784",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 69537377
  },
  {
    "blog_id": "6835964df0e49fdef0459c8b334b94",
    "summary": [
      "The paper presents a domain agnostic approach for conversational modelling based on Sequence to Sequence Learning Framework .",
      "Model  Neural Conversational Model (NCM)  A Recurrent Neural Network (RNN) reads the input sentence, one token at a time, and predicts the output sequence, one    token at a time.",
      "Learns by backpropagation.",
      "The model maximises the cross entropy of correct sequence given its context.",
      "Greedy inference approach where predicted output token is used as input to predict the next output token.",
      "Dataset  IT HelpDesk dataset of conversations about computer related issues.",
      "OpenSubtitles dataset containing movie conversations.",
      "Results  The paper has reported some samples of conversations generated by the interaction between human actor and the NCM.",
      "NCM reports lower perplexity as compared to n-grams model.",
      "NCM outperforms CleverBot in a subjective test involving human evaluators to grade the two systems.",
      "Strengths  Domain-agnostic.",
      "End-To-End training without handcrafted rules.",
      "Underlying architecture (Sequence To Sequence Framework) can be leveraged for machine translation, question answering etc.",
      "Weakness  The responses are simple, short and at times inconsistent.",
      "The objective function of Sequence To Sequence Framework is not designed to capture the objective of conversational models."
    ],
    "author_id": "shugan",
    "pdf_url": "http://arxiv.org/pdf/1506.05869",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 29962756
  },
  {
    "blog_id": "chen2016scaleaware",
    "summary": [
      "They represent an image as a tree where leafs are pixels and nodes represent clusters of those pixels.",
      "They train by regressing for some possible segmented region $r$ on the following function for every segmentation example and ground truth: $$S(r)=\\frac{\\\\#(g) - \\\\#(r)}{\\max(\\\\#(r), \\\\#(g)))}$$  Here $\\\\#(g)$ is the number of pixels in the ground truth and $\\\\#(r)$ is the number of pixels in the example segmentation.",
      "What is not explained here is what other information is used because it cannot simple be pixel counts.",
      "This function is used to rank the nodes in every path from the root to the leafs in Figure (a).",
      "The idea for the segmentation is that there is some set of nodes such that you can draw a line shown in Figure (b) which is equivalent to selecting a segmentation.",
      "The paper goes on to compute this using a dynamic programming solution based on the fact that the same pixel segmentations will be considered multiple times.",
      "!",
      "[]( [url]"
    ],
    "author_id": "joecohen",
    "pdf_url": "http://varcity.eu/paper/cvpr2016_chen_alignment.pdf",
    "author_full_name": "Joseph Cohen",
    "source_website": "https://www.shortscience.org/user?name=joecohen",
    "id": 47798365
  },
  {
    "blog_id": "deep-hessian-free",
    "summary": [
      "James Martens, 2010  This paper introduces a fairly complex optimization algorithm for deep nets that uses approximate 2nd-order gradient information  In Hessian-Free optimization, you can directly approximate a Hessian-vector product $Hv$ with the method of finite-differences; this only costs 1 more gradient evaluation  linear conjugate gradient algorithm allows one to solve for the optimal search direction in $O(N)$ iterations ($N$ is the number of parameters) with only matrix-vector products  Newton\u2019s method is scale invariant, e.g., for a new parameterization $\\hat{\\theta} = A \\theta$ for some invertible matrix $A$, the optimal search direction is now $\\hat{p} = A p$ where $p$ is the original optimal search direction.",
      "Gradient descent is not (need proof!)",
      "- so many bad things about GD, but it\u2019s so easy to implement..",
      "Considerations when applying this technique  Need to use an adaptive damping parameters $\\lambda$ beause the relative scale of $B = H(\\theta)$ is changing and $H(\\theta)$ must remain positive semidefinite.",
      "Recommended heuristic is given in Section 4.1  Gauss-Newton matrix $G$ can produce better search directions than $H$, see this blog post for a summary  Compute gradient on entire dataset, but use minibatches to compute Hessian-vector products.",
      "SGD requires 10\u2019s of thousands of iterations versus ~200 for HF"
    ],
    "author_id": "pemami",
    "pdf_url": "http://www.cs.toronto.edu/~jmartens/docs/Deep_HessianFree.pdf",
    "author_full_name": "Patrick Emami",
    "source_website": "https://pemami4911.github.io/index.html",
    "id": 4206973
  },
  {
    "blog_id": "18238e6aefd7b1e8c922cda9e10488",
    "summary": [
      "The paper introduces GuessWhat - a two-player guessing game where the goal is to locate an object in a rich image scene.",
      "The game is used to produce a large scale dataset of visual question-answer pairs on the image.",
      "The paper also describes three tasks based on the game and provides a neural architecture based baselines for each task.",
      "GuessWhat?!",
      "Game  One player, called as the oracle, is randomly assigned an object in the given image.",
      "The second player, called as the questioner, tries to locate the object, given just the image.",
      "The questioner can ask a series of questions about the object and the oracle can reply in \"yes\" or \"no\" or \"not applicable\".",
      "Once the questioner is confident of having identified the image, the oracle presents a list of objects to the questioner to choose from.",
      "A small penalty is added, every time a question is asked, so as to encourage informative questions only.",
      "Dataset  A filtered subset of images from MSCOCO is used as the image set.",
      "Two separate tasks create on Amazon Mechanical Turk (AMT) - for the role of oracle and questioner.",
      "Data was post processed -- both manually and using AMT -- to account for things like spelling mistakes and validation.",
      "Final dataset comprises of 150K thousand human game iterations with 800K question-answer pairs on 60K images.",
      "Dataset is available at  [url]"
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1611.08481",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 49439664
  },
  {
    "blog_id": "eslamihwtkh16",
    "summary": [
      "This paper presents an unsupervised generative model, based on the variational autoencoder framework, but where the encoder is a recurrent neural network that sequentially infers the identity, pose and number of objects in some input scene (2D image or 3D scene).",
      "In short, this is done by extending the DRAW model to incorporate discrete latent variables that determine whether an additional object is present or not.",
      "Since the reparametrization trick cannot be used for discrete variables, the authors estimate the gradient through the sampling operation using a likelihood ratio estimator.",
      "Another innovation over DRAW is the application to 3D scenes, in which the decoder is a graphics renderer.",
      "Since it is not possible to backpropagate through the renderer, gradients are estimated using finite-difference estimates (which require going through the renderer several times).",
      "Experiments are presented where the evaluation is focused on the ability of the model to detect and count the number of objects in the image or scene.",
      "**My two cents**  This is a nice, natural extension of DRAW.",
      "I'm particularly impressed by the results for the 3D scene setting.",
      "Despite the fact that setup is obviously synthetic and simplistic, I really surprised that estimating the decoder gradients using finite-differences worked at all.",
      "It's also interesting to see that the proposed model does surprisingly well compared to a CNN supervised approach that directly predicts the objects identity and pose.",
      "Quite cool!",
      "To see the model in action, see [this cute video][1].",
      "[1]:  [url]"
    ],
    "author_id": "hlarochelle",
    "pdf_url": "http://arxiv.org/pdf/1603.08575",
    "author_full_name": "Hugo Larochelle",
    "source_website": "https://www.shortscience.org/user?name=hlarochelle",
    "id": 22528856
  },
  {
    "blog_id": "stadiela15",
    "summary": [
      "The main idea in this paper is to use the agent's ability to predict observations at the next step as a measure of how much exploration of that action should be encouraged.",
      "This prediction is based on a deep architecture, specifically a deep autoencoder representation of observations, and accuracy of prediction is measured at the level of that learned, deep representation.",
      "Exploration is encourage by increasing the reward whenever the models prediction of the representation at the next time step is bad.",
      "#### My two cents  I'm not sure how novel this idea is in RL, but at the very least it's interesting that it was explored the way it was here, with deep learning.",
      "As a non-expert in RL, I certainly enjoyed reading the paper.",
      "Also, this implements nicely an idea that just seems like common sense, as an exploration strategy for an agent: actions that merit exploration are those that yield results that are unexpected to you.",
      "It will be interesting to see if this general approach will be able to exploit upcoming progress in the development of better generative deep learning models, an area that is currently very active."
    ],
    "author_id": "hlarochelle",
    "pdf_url": "http://arxiv.org/pdf/1507.00814",
    "author_full_name": "Hugo Larochelle",
    "source_website": "https://www.shortscience.org/user?name=hlarochelle",
    "id": 81465314
  },
  {
    "blog_id": "texture_synthesis_through_cnns_and_spectrum_constraints",
    "summary": [
      "What  The well known method of Artistic Style Transfer can be used to generate new texture images (from an existing example) by skipping the content loss and only using the style loss.",
      "The method however can have problems with large scale structures and quasi-periodic patterns.",
      "They add a new loss based on the spectrum of the images (synthesized image and style image), which decreases these problems and handles especially periodic patterns well.",
      "How  Everything is handled in the same way as in the Artistic Style Transfer paper (without content loss).",
      "On top of that they add their spectrum loss:  The loss is based on a squared distance, i.e. 1/2 d(I_s, I_t)^2.",
      "I_s is the last synthesized image.",
      "I_t is the texture example.",
      "d(I_s, I_t) then does the following:  It assumes that I_t is an example for a space of target images.",
      "Within that set it finds the image I_p which is most similar to I_s.",
      "That is done using a projection via Fourier Transformations.",
      "(See formula 5 in the paper.)",
      "The returned distance is then I_s - I_p.",
      "Results  Equal quality for textures without quasi-periodic structures.",
      "Significantly better quality for textures with quasi-periodic structures.",
      "Overview over their method, i.e. generated textures using style and/or spectrum-based loss."
    ],
    "author_id": "ALEJU",
    "pdf_url": "http://arxiv.org/pdf/1605.01141v3",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 78190587
  },
  {
    "blog_id": "end-of-term-5",
    "summary": [
      "We\u2019ve reached the end of term again on The Morning Paper, and I\u2019ll be taking a two week break.",
      "The Morning Paper will resume on Tuesday 7th May (since Monday 6th is a public holiday in the UK).",
      "My end of term tradition is to highlight a few of the papers from the term that I especially enjoyed, but this time around I want to let one work stand alone:  Making reliable distributed systems in the presence of software errors , Joe Armstrong, December 2003.",
      "You might also enjoy \u201c The Mess We\u2019re In ,\u201d and Joe\u2019s seven deadly sins of programming:  Code even you cannot understand a week after you wrote it \u2013 no comments  Code with no specifications  Code that is shipped as soon as it runs and before it is beautiful  Code with added features  Code that is very very fast very very very obscure and incorrect  Code that is not beautiful  Code that you wrote without understanding the problem  We\u2019re in an even bigger mess without you Joe.",
      "Thank you for everything.",
      "RIP."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://erlang.org/download/armstrong_thesis_2003.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 73424952
  },
  {
    "blog_id": "r-net-machine-reading-comprehension-with-self-matching-networks",
    "summary": [
      "R-NET is an end-to-end trained neural network model for machine comprehension.",
      "It starts by matching the question and the given passage (using gated attention based RNN) to obtain question-aware passage representation.",
      "Next, it uses a self-matching attention mechanism to refine the passage representation by matching the passage against itself.",
      "Lastly, it uses pointer networks to determine the position of the answer in the passage.",
      "Datasets  SQuAD  MS-MARCO  Architecture  Question / Passage Encoder  Concatenate the word level and character level embeddings for each word and feed into a bidirectional GRU to obtain question and passage representation.",
      "Gated Attention based RNN  Given question and passage representation, sentence pair representation is generated via soft-alignment of the words in the question and in the passage.",
      "The newly added gate captures the relation between the question and the current passage word as only some parts of the passage are relevant for answering the given question.",
      "Self Matching Attention  The passage representation obtained so far would not capture most of the context.",
      "So the current representation is matched against itself so as to collect evidence from the entire passage and encode the evidence relevant to the current passage word and question.",
      "Output Layer  Use pointer network (initialized using attention pooling over answer representation) to predict the position of the answer.",
      "Loss function is the sum of negative log probabilities of start and end positions.",
      "Results  R-NET is ranked second on SQuAD Leaderboard as of 7th August, 2017 and achieves best-published results on MS-MARCO dataset.",
      "Using ideas like sentence ranking, using syntax information performing multihop inference and augmenting question dataset (using seqToseq network) do not help in improving the performance."
    ],
    "author_id": "shugan",
    "pdf_url": "https://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/r-net.pdf",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 77534612
  },
  {
    "blog_id": "nalisnickr15",
    "summary": [
      "This paper introduces a version of the skipgram word embeddings learning algorithm that can also learn the size (nb.",
      "of dimensions) of these embeddings.",
      "The method, coined infinite skipgram (iSG), is inspired from my work with Marc-Alexandre C\u00f4t\u00e9 on the infinite RBM, in which we describe a mathematical trick for learning the size of a latent representation.",
      "This is done by introducing an additional latent variable $z$ representing the number of dimensions effectively involved in the energy function.",
      "Moreover, a term penalizing increasing values for $z$ is also incorporated, such that the infinite sum over $z$ is converging.",
      "In this paper, the authors extend the probabilistic model behind skipgram with such a variable $z$, now corresponding to the number of dimensions involved in the dot product between word embeddings.",
      "They also propose a few approximations required to allow for an efficient training algorithm.",
      "Mainly they optimize an upper bound on the regular skipgram objective (see Section 3.2) and they approximate the computation of the conditional over $z$ for a given word $w$, which requires summing over all possible context words $c$, by summing only over the words observed in the immediate current context of $w$ (thus this sum will very across training example of the same word $w$).",
      "Experiments show that the iSG better learns to exploit different dimensions to model different senses of words, better than the original skipgram model.",
      "Quantitatively, the iSG seems to provide better probabilities to context words."
    ],
    "author_id": "hlarochelle",
    "pdf_url": "http://arxiv.org/pdf/1511.05392",
    "author_full_name": "Hugo Larochelle",
    "source_website": "https://www.shortscience.org/user?name=hlarochelle",
    "id": 70481957
  },
  {
    "blog_id": "an-experimental-security-analysis-of-an-industrial-robot-controller",
    "summary": [
      "An experimental security analysis of an industrial robot controller Quarta et al., IEEE Security and Privacy 2017  This is an industrial robot:  The International Federation of Robotics forecasts that, by 2018, approximately 1.3 million industrial robot units will be employed in factories globally, and the international market value for \u201crobotized\u201d systems is approximately 32 billion USD.",
      "In all of their forms, robots are complex automation devices that heavily interact with the physical world\u2026  Most of these control systems were born in an era when they were assumed to be isolated from the network, but are now gaining new interconnections.",
      "And hey, guess what:  Unfortunately, even a simple Shodan query shows that sometimes industrial robots are exposed on the Internet without being properly secured.",
      "In this paper, the authors undertake a systematic analysis of the attack surface and potential impacts of cyber attacks against industrial robots.",
      "Their findings are sadly not surprising, yet at the same time some of the things you\u2019re about to read may leave you open-mouthed in disbelief.",
      "It\u2019s a perfect case study in how not to do things!",
      "Welcome to Industry 4.0 and the world of connected robots  Industrial robots are \u201cconnected\u201d primarily for programming and maintenance purposes \u2013 a use case specified by ISO standards.",
      "For instance, in a large car production plant developed by KUKA Robotics, all the 259 robots are connected to central control and monitoring systems.",
      "The industrial robot ecosystem is blooming, there are \u201cRobot Web Service\u201d HTTP REST APIs, a Robot App Store (  [url]"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://robosec.org/downloads/paper-robosec-sp-2017.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 5681713
  },
  {
    "blog_id": "network-motifs-simple-building-blocks-of-complex-networks",
    "summary": [
      "The paper presents the concept of \u201cnetwork motifs\u201d to understand the structural design of a network or a graph.",
      "Idea  A network motif is defined as \u201ca pattern of inter-connections occurring in complex networks in numbers that are significantly higher than those in randomized networks\u201d.",
      "In the practical setting, given an input network, we first create randomized networks which have same single node characteristics (like a number of incoming and outgoing edges) as the input network.",
      "The patterns that occur at a much higher frequency in the input graph (than the randomized graphs) are reported as motifs.",
      "More specifically, the patterns for which the probability of appearing in a randomized network an equal or more number of times than in the real network is lower than a cutoff value (say 0.01).",
      "Motivation  Real-life networks exhibit properties like \u201csmall world\u201d property ( the majority of nodes are within a distance of fewer than 7 hops from each other) and \u201cscale-free\u201d property (fraction of nodes having k edges decays as a power-law).",
      "Motifs are one such structural property that is exhibited by networks in biochemistry, neurobiology, ecology, and engineering.",
      "Further, motifs shared by graphs of different domains are different which hints at the usefulness of motifs as a fundamental structural property of the graph and relates to the process of evolution of the graph."
    ],
    "author_id": "shugan",
    "pdf_url": "https://science.sciencemag.org/content/298/5594/824/tab-pdf",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 99440206
  },
  {
    "blog_id": "e6d2364c278c97b1b2f4ec53255c56",
    "summary": [
      "The paper demonstrates how simple CNNs, built on top of word embeddings, can be used for sentence classification tasks.",
      "Implementation  Architecture  Pad input sentences so that they are of the same length.",
      "Map words in the padded sentence using word embeddings (which may be either initialized as zero vectors or initialized as word2vec embeddings) to obtain a matrix corresponding to the sentence.",
      "Apply convolution layer with multiple filter widths and feature maps.",
      "Apply max-over-time pooling operation over the feature map.",
      "Concatenate the pooling results from different layers and feed to a fully-connected layer with softmax activation.",
      "Softmax outputs probabilistic distribution over the labels.",
      "Use dropout for regularisation.",
      "Hyperparameters  RELU activation for convolution layers  Filter window of 3, 4, 5 with 100 feature maps each.",
      "Dropout - 0.5  Gradient clipping at 3  Batch size - 50  Adadelta update rule.",
      "Variants  CNN-rand  Randomly initialized word vectors.",
      "CNN-static  Uses pre-trained vectors from word2vec and does not update the word vectors.",
      "CNN-non-static  Same as CNN-static but updates word vectors during training.",
      "CNN-multichannel  Uses two set of word vectors (channels).",
      "One set is updated and other is not updated.",
      "Datasets  Sentiment analysis datasets for Movie Reviews, Customer Reviews etc.",
      "Classification data for questions.",
      "Maximum number of classes for any dataset - 6  Strengths  Good results on benchmarks despite being a simple architecture.",
      "Word vectors obtained by non-static channel have more meaningful representation.",
      "Weakness  Small data with few labels.",
      "Results are not very detailed or exhaustive."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1408.5882",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 93142771
  },
  {
    "blog_id": "interaction-networks-for-learning-about-objects-relations-physics",
    "summary": [
      "Battaglia, et al., 2016  This ambitious paper proposes a deep learning framework for modeling the physical interactions between objects in an environment.",
      "The authors present Interaction Networks (IN), which explicitly separate the processes of learning object dynamics and relations between objects.",
      "IN is designed to work on graphs where objects are nodes and edges are relations between objects.",
      "This is to make it scalable to different environments.",
      "The architecture is simple; two MLPs learn representations over object dynamics and relations between objects, respectively.",
      "The input to the system is the state decomposed into the objects and their physical relations (gravitational attraciton, collisions, springs), as well as external effects (gravity).",
      "Object states can be further decomposed into position and velocity.",
      "In general, the output is the velocity of the objects at the subsequent time step.",
      "The system is evaluated on interesting physical reasoning tasks, such as n-bodies interacting, bouncing balls colliding, and string/mass systems.",
      "IN showed significantly lower MSE when predicting future states of objects in the scenes compared to simple baselines.",
      "A custom physics engine was used to generate trajectories for training data.",
      "All training objectives and test measures used MSE between the model\u2019s predictions and the ground truth target."
    ],
    "author_id": "pemami",
    "pdf_url": "https://arxiv.org/pdf/1612.00222",
    "author_full_name": "Patrick Emami",
    "source_website": "https://pemami4911.github.io/index.html",
    "id": 62056200
  },
  {
    "blog_id": "1811.12889",
    "summary": [
      "The paper discusses neural module network trees (NMN-trees).",
      "Here modules are composed in a tree structure to answer a question/task and modules are trained in different configurations to ensure they learn more core concepts and can generalize.",
      "Longer summary:  How to perform systematic generalization?",
      "First we need to ask how good current models are at understanding language.",
      "Adversarial examples show how fragile these models can be.",
      "This leads us to conclude that systematic generalization is an issue that requires specific attention.",
      "Maybe we should rethink the modeling assumptions being made.",
      "We can think that samples can come from different data domains but are generated by some set of shared rules.",
      "If we correctly learned these rules then domain shift in the test data would not hurt model performance.",
      "Currently we can construct an experiment to introduce systematic bias in the data which causes the performance to suffer.",
      "From this experiment we can start to determine what the issue is.",
      "A recent new idea is to force a model to have more independent units is neural module network trees (NMN-trees).",
      "Here modules are composed in a tree structure to answer a question/task and modules are trained in different configurations to ensure they learn more core concepts and can generalize."
    ],
    "author_id": "joecohen",
    "pdf_url": "http://arxiv.org/pdf/1811.12889v3",
    "author_full_name": "Joseph Cohen",
    "source_website": "https://www.shortscience.org/user?name=joecohen",
    "id": 6799048
  },
  {
    "blog_id": "lle",
    "summary": [
      "Roweis, Saul, 2000  ISOMAP and MDS require estimates of pairwise distances between data points.",
      "LLE gets around this by \u201cthinking\u201d globally but fitting locally.",
      "Essentially, each data point should hypothetically be representable by a locally linear patch.",
      "Therefore, LLE seeks $W$ such that  is minimized.",
      "Hence, a data point should be reconstructed by its neighbors; the problem is solved via least squares.",
      "Note that the weights are invariant to affine transformations and translations.",
      "Assuming that $W$ should be preserved in a lower dimensional representation of the data, LLE seeks to solve  The optimal coordinates $Y$ can be found by solving a sparse $n \\times n$ eigenvalue problem.",
      "Because of the simple construction and use of simple linear algebra, LLE has better theoretical properties than other algorithms like autoencoders  It also has less hyperparameters  Doesn\u2019t need to be rerun when new dimensions are added to the embedding space (old ones do not change)  Does LLE work on spheres?",
      "It seems like it would run into the same problem if the sphere didn\u2019t have a hole taken out of it"
    ],
    "author_id": "pemami",
    "pdf_url": "http://www.robots.ox.ac.uk/~az/lectures/ml/lle.pdf",
    "author_full_name": "Patrick Emami",
    "source_website": "https://pemami4911.github.io/index.html",
    "id": 12175403
  },
  {
    "blog_id": "2746f15889f7f4a798bf7f9ec4b7d8",
    "summary": [
      "Introduces fastText, a simple and highly efficient approach for text classification.",
      "At par with deep learning models in terms of accuracy though an order of magnitude faster in performance.",
      "Architecture  Built on top of linear models with a rank constraint and a fast loss approximation.",
      "Start with word representations that are averaged into text representation and feed them to a linear classifier.",
      "Think of text representation as a hidden state that can be shared among features and classes.",
      "Softmax layer to obtain a probability distribution over pre-defined classes.",
      "High computational complexity O(kh), k is the number of classes and h is dimension of text representation.",
      "Hierarchial Softmax  Based on Huffman Coding Tree  Used to reduce complexity to O(hlog(k))  Top T results (from the tree) can be computed efficiently O(logT) using a binary heap.",
      "N-gram Features  Instead of explicitly using word order, uses a bag of n-grams to maintain efficiency without losing on accuracy.",
      "Uses hashing trick to maintain fast and memory efficient mapping of the n-grams.",
      "Experiments  Sentiment Analysis  fastText benefits by using bigrams.",
      "Outperforms char-CNN and char-CRNN and performs a bit worse than VDCNN .",
      "Order of magnitudes faster in terms of training time.",
      "Note: fastText does not use pre-trained word embeddings.",
      "Tag Prediction  fastText with bigrams outperforms Tagspace .",
      "fastText performs upto 600 times faster at test time."
    ],
    "author_id": "shugan",
    "pdf_url": "http://arxiv.org/pdf/1607.01759v3",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 99668388
  },
  {
    "blog_id": "two-too-simple-adaptations-of-word2vec-for-syntax-problems",
    "summary": [
      "The paper proposes two variants of Word2Vec model so that it may account for syntactic properties of words and perform better on syntactic tasks like POS tagging and dependency parsing.",
      "In the original Skip-Gram setting, the model predicts the 2c words in the context window (c is the size of the context window).",
      "But it uses the same set of parameters whether predicting the word next to the centre word or the word farthest away, thus losing all information about the word order.",
      "Similarly, the CBOW (Continuous Bas Of Words) model just adds the embedding of all the surrounding words thereby losing the word order information.",
      "The paper proposes to use a set of 2c matrices each for a different word in the context window for both Skip-Gram and CBOW models.",
      "This simple trick allows for accounting of syntactic properties in the word vectors and improves the performance of dependency parsing task and POS tagging.",
      "The downside of using this is that now the model has far more parameters than before which increases the training time and needs a large enough corpus to avoid sparse representation."
    ],
    "author_id": "shugan",
    "pdf_url": "http://www.cs.cmu.edu/~lingwang/papers/naacl2015.pdf",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 25809281
  },
  {
    "blog_id": "split-level-io-scheduling",
    "summary": [
      "Split-Level IO Scheduling \u2013 Yang et al. 2015  The central idea in today\u2019s paper is pretty simple: block-level I/O schedulers (the most common kind) lack the higher level information necessary to perform write-reordering and accurate accounting, whereas system-call  level schedulers have the appropriate context but lack the low-level knowledge needed to build efficient schedulers \u2013 so why not create a scheduling framework that can intercept at both the block and system call levels to get the best of both worlds?",
      "This is what the authors call \u2018split-level IO scheduling.\u2019  By implementing a judiciously selected set of handlers at key junctures within the storage stack (namely, at the system-call, page-cache, and block layers), a developer can implement a scheduling discipline with full control over behavior and with no loss in high- or low-level information.",
      "Split schedulers can determine which processes issued I/O (via graph tags that track causality across levels) and accurately estimate I/O costs.",
      "Furthermore, memory noti\ufb01cations make schedulers aware of write work as soon as possible (not tens of seconds later when writeback occurs).",
      "Finally, split schedulers can prevent \ufb01le systems from imposing orderings that are contrary to scheduling goals.",
      "On top of this split-level framework, the authors build a fair queuing scheduler that reduces priority misallocations by 28x compared to the Linux CFQ scheduler; a deadline-based scheduler that reduces tail latencies by 4x; and a resource limiting scheduler that improves isolation by 6x for some workloads.",
      "This latter scheduler can improve isolation for virtual machines and for HDFS, and the deadline based scheduler  provides a solution to fsync-induced freezes in database systems.",
      "[A] block-level framework fails to support correct cause mapping (due to write delegation such as journaling and delayed allocation) or control over reordering (due to \ufb01le-system ordering requirements).",
      "[A] system-call framework solves these two problems, but fails to provide enough information to schedulers for accurate cost estimation because it lacks low-level knowledge.",
      "These problems are general to many \ufb01le systems; even if journals are not used, similar issues arise from the ordering constraints imposed by other mechanisms such as copy-on-write techniques  or soft up- dates.",
      "Our split framework meets all the needs in Table 1 (cause mapping, cost-estimation, and reordering) by incorporating ideas from the other two frameworks and exposing additional memory-related hooks.",
      "The ideas apply generally, but the implementation in the paper is integrated with the ext4 and XFS filesystems in Linux.",
      "Three split-level schedulers are built on top: Actually-Fair Queuing (AFQ) \u2013 950 loc; Split-Deadline \u2013 750 loc; and Split-Token \u2013 950 loc.",
      "Actually Fair Queuing  AFQ allocates I/O fairly among processes according to their priorties.",
      "It has performance similar to Linux\u2019s CFQ, while avoiding the priority inversions that can happen with CFQ.",
      "AFQ employs a two-level scheduling strategy.",
      "Reads are handled at the block level and writes (and calls that cause writes, such as fsync) are handled at the system-call level.",
      "This design allows reads to hit the cache while protecting writes from journal entanglement.",
      "Beneath the journal, low-priority blocks may be prerequisites for high-priority fsync calls, so writes at the block level are dispatched immediately.",
      "AFQ chooses I/O requests to dequeue at the block and system-call levels using the stride algorithm .",
      "Whenever a block request is dispatched to disk, AFQ charges the responsible processes for the disk I/O.",
      "The I/O cost is based on a simple seek model.",
      "Split-Deadline  [Linux\u2019s] Block-Deadline scheduler does poorly when trying to limit tail latencies, due to its inability to reorder block I/Os in the presence of \ufb01lesystem ordering requirements.",
      "Split-level scheduling, with system-call scheduling capabilities and memory-state knowledge, is better suited to this task.",
      "We implement the Split-Deadline scheduler by modifying the Linux deadline scheduler (Block-Deadline)\u2026  To show the benefits for real databases, SQLite and PostgreSQL are measured with both Split-Deadline and Block-Deadline.",
      "\u2026when running on top of Block-Deadline, 4% of transactions fail to meet their latency target, and over 1% take longer than 500ms.",
      "After further inspection, we found that the latency spikes happen at the end of each checkpoint period, when the system begins to \ufb02ush a large amount of dirty data to disk using fsync.",
      "Such data \ufb02ushing interferes with foreground I/Os, causes long transaction latency and low system throughput.",
      "The database community has long experienced this \u201cfsync freeze\u201d problem, and has no great solution for it.",
      "We show next that Split-Deadline provides a simple solution to this problem\u2026.",
      "it effectively eliminates tail latency: 99.99% of the transactions are completed within 15 ms.  Split-Token  [In Split-Token] throttled processes are given tokens at a set rate.",
      "I/O costs tokens, I/O is blocked if there are no tokens, and the number of tokens that may be held is capped.",
      "Split-Token throttles a process\u2019s system-call writes and block-level reads if and only if the number of tokens is negative.",
      "System-call reads are never throttled (to utilize the cache).",
      "Block writes are never throttled (to avoid entanglement).",
      "Our implementation uses memory-level and block-level hooks for accounting.",
      "The scheduler promptly charges tokens as soon as buffers are dirtied, and then revises when the writes are later \ufb02ushed to the block level (\u00a73.2), charging more tokens (or refunding them) based on ampli\ufb01cation and sequentiality.",
      "Tokens represent bytes, so accounting normalizes the cost of an I/O pattern to the equivalent amount of sequential I/O (e.g., 1 MB of random I/O may be counted as 10 MB).",
      "Under evaluation with QEMU, split-token is much better than the SCS scheduler at eliminating noisy I/O neighbour problems.",
      "The authors also evaluate split-token in an HDFS context:  To show that local split scheduling is a useful foundation to provide isolation in a distributed environment, we integrate HDFS with Split-Token to provide isolation to HDFS clients.",
      "We modify the client-to-worker protocol so workers know which account should be billed for disk I/O generated by the handling of a particular RPC call.",
      "Account information is propagated down to Split-Token and across to other workers (for pipelined writes)\u2026 We conclude that local scheduling can be used to meet distributed isolation goals; however, throttled applications may get worse-than-expected performance if the system is not well balanced.",
      "In conclusion\u2026  While our experiments indicate that simple layering must be abandoned, we need not sacri\ufb01ce modularity.",
      "In our split framework, the scheduler operates across all layers, but is still abstracted behind a collection of handlers.",
      "This approach is relatively clean, and enables pluggable scheduling.",
      "Supporting a new scheduling goal simply involves writing a new scheduler plug-in, not re-engineering the entire storage system.",
      "Our hope is that split-level scheduling will inspire future vertical integration in storage stacks.",
      "Our source code is available at  [url]"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://sigops.org/sosp/sosp15/current/2015-Monterey/printable/168-yang.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 67696398
  },
  {
    "blog_id": "the-quic-transport-protocol-design-and-internet-scale-deployment",
    "summary": [
      "The QUIC transport protocol: design and Internet-scale deployment Langley et al., SIGCOMM\u201917  QUIC is a transport protocol designed from the ground up by Google improve the performance of HTTPS traffic.",
      "The chances are you\u2019ve already used it \u2013 QUIC is deployed in Chrome, in the YouTube mobile app, and in the Google Search app on Android.",
      "It\u2019s also used at Google\u2019s front-end servers, handling billions of requests a day.",
      "All told, about 7% of global Internet traffic is now flowing over QUIC.",
      "There are three interesting aspects to the paper, that we can essentially divide into the Why, the What, and the How.",
      "How QUIC was built and evolved over time is an integral part of the story.",
      "Why build a new protocol  Two forces combined to create the pressures that eventually led to QUIC.",
      "On the one hand, the use of the web as a platform for applications, and the latency sensitivity of those applications (and their key metrics), create a strong pressure to reduce web latency.",
      "On the other hand, the rapid shift towards securing web traffic adds delays.",
      "TLS adds two round trips to connection establishment.",
      "Both of these forces must contend with the speed of light that sets a lower bound on round-trip times.",
      "Meanwhile, HTTP/2 allows multiplexing of object streams, but is still fundamentally limited by TCP\u2019s bytestream abstraction that forces all application frames to pay a latency tax when wait for retransmission of lost TCP segments from any stream.",
      "Furthermore, rolling out changes to a typical TCP based stack takes a long time as it is embedded in the OS kernel (including on end user controlled devices).",
      "And if you could roll out a change, it\u2019s likely that it will break due to entrenched middleboxes:  Middleboxes have accidentally become key control points in the Internet\u2019s architecture: firewalls tend to block anything unfamiliar for security reasons and Network Address Translators (NATs) rewrite the transport header, making both incapable of allowing traffic from new transports without adding explicit support for them.",
      "What is QUIC?",
      "An overview  QUIC was designed to be easily deployable and secure, and to reduce handshake and head-of-line blocking delays.",
      "Security and deployability are both helped by one of the key QUIC decisions \u2013 to base on top of UDP.",
      "QUIC encrypts transport headers and builds transport functions atop UDP, avoiding dependence on vendors and network operators and moving control of transport deployment to the applications that directly benefit from them.",
      "As well as being more secure, encrypting everything also stops those pesky middleboxes from ossifying the protocol by baking in dependencies on specific formats etc..  Establishing a connection  QUIC combines the cryptographic and transport handshake into one round trip when setting up a secure transport connection.",
      "If a connection is successfully established the client can cache information about the origin it connected to.",
      "When reconnecting to the same origin this cached information can be used to establish an encrypted connection with no additional round trips.",
      "Data can be sent immediately after sending the client hello, without even needing to wait for the the server\u2019s reply (0-RTT).",
      "If the client\u2019s cached information becomes out of date, then this 0-RTT handshake falls back into the 1-RTT variant.",
      "Stream multiplexing  To avoid head-of-line blocking due to TCP\u2019s sequential delivery, QUIC supports multiple streams within a connection, ensuring that a lost UDP packet only impacts those streams whose data was carried in the packet.",
      "Streams are lightweight enough that a new stream can reasonably be used for each of a series of small messages.",
      "Loss recovery  In TCP, retransmitted segments carry the same sequence numbers as the original packet, which complicates ACKs.",
      "In QUIC, every packet carries a new sequence number, including those carrying retransmitted data.",
      "Stream offsets in stream frames are used for delivery ordering, separating the two functions [ordering and acknowledgement] that TCP conflates.",
      "The packet number represents an explicit time-ordering, which enables simpler and more accurate loss detection than in TCP.",
      "The separation also allows more accurate network RTT estimation helping delay-sensing congestion controllers such as BBR and PCC.",
      "Authentication and encryption  With the exception of the early handshake packets and reset packets, QUIC packets are fully authenticated and mostly encrypted, as shown below:  QUIC\u2019s cryptography provides two levels of secrecy: initial client data is encrypted using initial keys, and subsequent client data and all server data are encrypted using forward-secure keys.",
      "All of the information sent in the clear is also included in the derivation of the final connection keys \u2013 so any in network tampering will be detected and cause the connection to fail.",
      "Flow control  QUIC use stream-level flow control, in conjunction with an overall connection flow control mechanism.",
      "As in HTTP/2, the flow control mechanism is credit based:  A QUIC receiver advertises the absolute byte offset within each stream up to which the receiver is willing to receive data.",
      "As data is sent, received, and delivered on a particular stream, the receiver periodically sends window update frames that increase the advertised offset limit for that stream, allowing the peer to send more data on that stream.",
      "Congestion control  Congestion control in QUIC is pluggable.",
      "The initial implementation uses Cubic .",
      "How Google built and evolved QUIC  Our development of the QUIC protocol relies heavily on continual Internet-scale experimentation to examine the value of various features and to tune parameters\u2026 We drove QUIC experimentation by implementing it in Chrome, which has a strong experimentation and analysis framework that allows new features to be A/B tested and evaluated before full launch.",
      "QUIC was also able to directly link experiments into the analytics of the application services using QUIC connections.",
      "This enabled the team to quantify the end-to-end effect on user and application centric performance metrics.",
      "QUIC support in the YouTube and the Android Google Search apps was also able to take advantage of their respective experimentation frameworks.",
      "Through small but repeatable improvements and rapid iteration, the QUIC project has been able to establish and sustain an appreciable and steady trajectory of cumulative performance gains.",
      "Is QUIC actually quick?",
      "The table below shows the reductions in search and video latency achieved when using the QUIC protocol.",
      "The improvement mostly comes from reducing handshake latency.",
      "Users of QUIC also saw reduced playback interruptions due to rebuffering.",
      "This is due to QUIC\u2019s reduced loss-recovery latency.",
      "And here\u2019s how QUIC stands up against the TCP/TLS alternative:  The downside is that QUIC is currently twice as expensive as TCP/TLS in terms of CPU."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3098822.3098842?download=true",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 81001972
  },
  {
    "blog_id": "blockchain-provenance",
    "summary": [
      "Fine-grained, secure and efficient data provenance on blockchain systems Ruan et al., VLDB\u201919  We haven\u2019t covered a blockchain paper on The Morning Paper  for a while, and today\u2019s choice won the best paper award at VLDB\u201919.",
      "The goal here is to enable smart contracts to be written in which the contract logic depends on the history, or provenance of its inputs.",
      "For example, a contract that sends a reward amount of tokens to a user based on that user\u2019s average balance per day over some period.",
      "That\u2019s hard to do in today\u2019s blockchain systems for two reasons:  Provenance can only be determined by querying and replaying all on-chain transactions, which is inefficient and an offline activity.",
      "As a consequence the computation of provenance and issuing a subsequent transaction are decoupled and hence there are no serializability guarantees.",
      "\u201cIn blockchains with native currencies, serializabiliy violations can be exploited for Transaction-Ordering attacks that cause substantial financial loss to the users.\u201d  In other words, smart contracts cannot access historical blockchain states in a tamper-evident manner.",
      "In designing a blockchain-friendly provenance mechanism, three aspects unique to the blockchain environment differentiate the problem from that of traditional database provenance:  There are no higher-order data operators (e.g. join) whose semantics capture provenance in the form of input-output dependencies.",
      "Instead we have to work directly in terms of smart contract read and write sets.",
      "Blockchains assume an adversarial environment, so the captured provenance must be made tamper-evident  Provenance queries must be fast to avoid imposing a large cost on miners during verification (see the Verifier\u2019s Dilemma ).",
      "Let\u2019s look at each of these in turn.",
      "Capturing provenance  In LineageChain, every contract method can be made provenance-friendly via a helper method (prov_helper).",
      "LineageChain invokes prov_helper immediately after every successful contract execution.",
      "If the smart chain developer doesn\u2019t provide an implementation, then the default behaviour is to make all identifiers in the read set of the transaction be dependencies of every write in the write set.",
      "My biggest question here is what guarantees we can have on provenance when the developer does provide their own implementation.",
      "Since we\u2019re assuming an adversarial environment, what stops the prov_helper method from making up whatever it likes?",
      "The paper is silent on this question.",
      "Presumably we can always check that a write dependency is not declared on a read we didn\u2019t make (because we know the read set), and likewise we can check that we\u2019re not claiming any writes we didn\u2019t make.",
      "But outside of that, the whole point of the helper method seems to be to give the developer control over the provenance.",
      "For example, suppose the read set is  and the write set is  .",
      "In the true calculation the value of D depends on all of A, B, and C. But in the helper method the developer could return a provenance that says D only depends on B.",
      "Or on nothing at all.",
      "What does this mean for the trust we can place in the provenance chain??",
      "All the fancy provenance tracking mechanisms that follow mean nothing if the provenance inputs themselves can\u2019t be trusted.",
      "Maybe subsetting the provenance history like this is always safe?",
      "I\u2019d like to see that argument made in the paper if so, rather than being left to reason this out for myself.",
      "Inspecting the public code of all smart contracts to make sure developers aren\u2019t cheating doesn\u2019t seem very satisfactory.",
      "Anyway, putting that question aside, once we\u2019ve got provenance data, smart contracts can access it via three additional smart contract APIs that are exposed: Hist for finding the value of a stateID at the start of a given block, Backward for tracing provenance backwards in the chain, and Forward for tracing provenance forwards in the chain.",
      "Here\u2019s an example that marks an address as blacklisted if one of its last 5 transactions is with a blacklisted address.",
      "Securely storing provenance information  To securely store provenance information, LineageChain turns the original Merkle tree backing a blockchain into a Merkle DAG.",
      "In the above figure,  is a unique identifier of a smart contract account whose state is to be tracked,  is a version number (block number), and  is the state of the contract at version  .",
      "You can see in the figure above that the Merkle tree has additional branches feeding into the root,  one for each tracked smart contract, with the smart contract state hashes aggregating along transaction dependency edges.",
      "Our new Merkle DAG can be easily integrated to existing blockchain index structures\u2026.",
      "Since the [state entry hash] is protected by the Merkle index for tamper evidence, so is the state history.",
      "In other words, we add integrity protection for provenance without any extra cost to the index structure.",
      "Forward dependencies for a version  are added when the state is next updated to version  .",
      "At this point we know it is safe to do so as no other forward dependencies can now emerge from the old version.",
      "Making provenance queries fast  At this point we have the structures and APIs needed to make version queries, but now we need to make them fast.",
      "This is especially important since version queries will have to be executed by miners during verification, and we don\u2019t want an adversary to deliberately submit expensive version queries (e.g. starting from a very early block id).",
      "The solution here is to build a skip-list based index on top of the Merkle DAG.",
      "Compared to a normal skip list we have two additional properties: (i) we know that the blockchain is append-only, and (ii) the index structure is uniquely determined by the values of the appended items.",
      "This gives us a Deterministic Append-Only Skip List (DASL).",
      "DASL queries are executed in just the same way as for a regular skip list,  appends are made using the algorithm below:  The node structure for the DASL nodes is stored in the Merkle DAG state entries.",
      "For permissioned blockchains we\u2019re all set.",
      "For permissionless blockchains we also need to figure out who pays for the extra storage overheads\u2026  As DASL consumes resources, its costs must be explicitly accounted for in permissionless blockchains.",
      "More specifically, during deployment, the contract owner specifies which states require DASL support.",
      "Alternatively, DASL support can be automatically inferred from the contract\u2019s source code.",
      "The deployment fee should reflect this extra storage cost for DASL\u2026  Implementation and evaluation  The implementation is part of the Hyperledger++ project, and the evaluation uses Blockbench .",
      "Full details can be found in sections 6 and 7 of the paper, but the short version is this:  We implemented LineageChain on top of Hyperledger and benchmarked it against several baselines.",
      "The results show the benefits of LineageChain in supporting rich, provenance-dependent applications.",
      "They demonstrate that provenance queries are efficient, and the system incurs small storage overhead."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://www.vldb.org/pvldb/vol12/p975-ruan.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 7305163
  },
  {
    "blog_id": "the-design-philosophy-of-the-darpa-internet-protocols",
    "summary": [
      "The Design Philosophy of the DARPA Internet Protocols \u2013 Clark 1988  While there have been papers and specifications that describe how the (internet) protocols work, it is sometimes difficult to deduce from these why the protocol is as it is.",
      "For example, the Internet protocol is based on a connectionless or datagram mode of service.",
      "The motivation for this has been greatly misunderstood.",
      "This paper attempts to capture some of the early reasoning which shaped the Internet protocols.",
      "Understanding the underlying principles behind something can turn what might on the surface seem to be simply a collection of facts into a chain of causes and consequences that makes it much easier to see how those parts fit together.",
      "Clark provides us with some of those insights for the design of the Internet Protocols, working from the goals towards the implementation consequences.",
      "The top level goal for the DARPA Internet Architecture was to develop an effective technique for multiplexed utilization of existing interconnected networks.",
      "This implied integrating networks spanning different administrative boundaries of control.",
      "And since the intial networks to be connected used packet switching, packet switching was adopted as a fundamental component of the internet architecture.",
      "From the ARPANET project the notion of store-and-forward packet switching for interconnects was well understood.",
      "From these assumptions comes the fundamental structure of the Internet: a packet switched communications facility in which a number of distinguishable networks are connected together using packet communications processors called gateways which implement a store and forward packet forwarding algorithm.",
      "The top level goal says \u2018what\u2019 is to be achieved, but says very little about the desired characteristics of a system that accomplishes it.",
      "There were seven second-level goals, which are presented below in priority order.",
      "Internet communication must continue despite loss of networks or gateways.",
      "The Internet must support multiple types of communications service.",
      "The Internet architecture must accommodate a variety of networks.",
      "The Internet architecture must permit distributed management of its resources.",
      "The Internet architecture must be cost effective.",
      "The Internet architecture must permit host attachment with a low level of effort.",
      "The resources used in the internet architecture must be accountable.",
      "These goals are in order of importance, and an entirely different network architecture would result if the order were changed.",
      "For example, since this network was designed to operate in a military context, which implied the possibility of a hostile environment, survivability was put as a first goal, and accountability as a last goal.",
      "It turns out that the top three goals on the list had the most influence on the resulting design.",
      "See the full paper (link at the top) for reflections on the remaining four.",
      "Surviving in the face of failure  If two entities are communicating over the Internet, and some failure causes the Internet to be temporarily disrupted and reconfigured to reconstitute the service, then the entities communicating should be able to continue without having to reestablish or reset the high level state of their conversation.",
      "The only error the communicating parties should ever see is the case of total partition.",
      "If the application(s) on either end of the connection are not required to resolve any other failures, then the state necessary for recovery must be held in the lower layers \u2013 but where?",
      "One option is to put it in the intermediate nodes in the network, and of course to protect it from loss it must be replicated.",
      "I think the knee-jerk reaction of many system designers today might be to distribute the state in some such manner, maybe using a gossip-protocol.",
      "But the original designers of the internet had a insight which enabled a much simpler solution, and they called it \u2018fate sharing.\u2019  The alternative, which this architecture chose, is to take this information and gather it at the endpoint of the net, at the entity which is utilizing the service of the network.",
      "I call this approach to reliability \u201cfate-sharing.\u201d The fate-sharing model suggests that it is acceptable to lose the state information associated with an entity if, at the same time, the entity itself is lost.",
      "Specifically, information about transport level synchronization is stored in the host which is attached to the net and using its communication service.",
      "Two consequences of this are that the intermediate nodes must not store any (essential) state \u2013 leading to a datagram (stateless packet switching) based design, and that the host becomes an important trusted part of the overall solution.",
      "Handling multiple types of communications service  Debugging protocols and VOIP were the first two use cases that suggested something more than just TCP might be needed.",
      "You most want the debugger to work precisely when things are going wrong \u2013 so a model that says it first requires a fully reliable transport is not a good one!",
      "It\u2019s much better to make do with whatever you can get.",
      "When it comes to VOIP, regular delivery of packets (even if it means losing some) is more important than reliability for a good user experience.",
      "A surprising observation about the control of variation in delay is that the most serious source of delay in networks is the mechanism to provide reliable delivery!",
      "It was thus decided\u2026 to split TCP and IP into two layers.",
      "TCP provided one particular type of service, the reliable sequenced data stream, while IP attempted to provide a basic building block out of which a variety of types of service could be built\u2026 The User Datagram Protocol (UDP) was created to provide a application-level interface to the basic datagram service of Internet.",
      "Accommodating a variety of networks.",
      "The easiest way to accommodate a wide variety of networks, is to make the requirements for integrating a network as simple as possible.",
      "This boils down to:  being able to transport a packet or datagram of reasonable size (e.g. 100 bytes),  reasonable but not reliable delivery, and some form of addressing.",
      "There are a number of services which are explicitly not assumed from the network.",
      "These include reliable or sequenced delivery, network level broadcast or multicast, priority ranking of transmitted packet, support for multiple types of service, and internal knowledge of failures, speeds, or delays.",
      "On datagrams  There is a mistaken assumption often associated with datagrams, which is that the motivation for datagrams is the support of a higher level service which is essentially equivalent to the datagram.",
      "In other words, it has sometimes been suggested that the datagram is provided because the transport service which the application requires is a datagram service.",
      "In fact, this is seldom the case.",
      "The importance of datagrams instead stems from:  Eliminating the need for connection state in intermediate nodes  Providing a building block on top of which a variety of services can be built  Representing the minimum network server assumption, enabling a wide variety of networks to be easily incorporated."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://ccr.sigcomm.org/archive/1995/jan95/ccr-9501-clark.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 21050319
  },
  {
    "blog_id": "texture-networks-feed-forward-synthesis-of-textures-and-stylized-images",
    "summary": [
      "Texture Networks: Feed-forward synthesis of textures and stylized images Ulyanov et al., arXiv, March 2016  During the summer break I mostly stayed away from news feeds and twitter, which induces terrible FOMO (Fear Of Missing Out) to start with.",
      "What great research was published / discussed that I missed?",
      "Was there a major industry announcement I\u2019m completely ignorant of?",
      "One thing I\u2019m glad I didn\u2019t miss was the Prisma app that produces quite beautiful stylized versions of photos from your smartphone.",
      "It\u2019s a great example of deep technology behind a simple interface, and also of the rapid packaging and exploitation of research results \u2013 today\u2019s choice is the paper describing the technology breakthrough that makes Prisma possible, and it was released to arXiv in March 2016.",
      "The source code and models described in the paper can also be found on GitHub.",
      "Gatys et al. recently (2015) showed that deep networks can generate beautiful textures and stylized images from a single texture example.",
      "If you want to style a lot of images though (to provide styling-as-a-service for example), you\u2019ll find that their technique is slow and uses a lot of memory.",
      "To generate images of equivalent quality, an implementation of Gatys et al. required about 10 seconds and 1.1GB of memory, whereas the approach described by Ulyanov et al.",
      "in this paper requires about 20ms and only 170MB of memory.",
      "Significantly faster and cheaper therefore, and although the algorithm doesn\u2019t quite match the results of Gatys et al. for all images, it\u2019s still very good.",
      "Just in case you haven\u2019t seen it, here are some examples.",
      "First, generating textures in the style of sample image:  And combining a style image with a content image:  If you download the app, you can play with examples using your own photos.",
      "One of the possibilities I\u2019m personally excited about is the opportunities the image creation speed opens up for applying the technique to movies.",
      "I like the images, but when I saw the movies created by Ruder et al. using an extension of the Gatys technique I was really blown away [ paper , explanation and video ].",
      "Update: I just learned about the Artisto app that does this for you!",
      "High-level approach  In general, one may look at the process of generating an image x as the problem of drawing a sample from a certain distribution p(x).",
      "In texture synthesis, the distribution is induced by an example texture instance x0 such that we can write x ~ p(x|x0).",
      "In style transfer, the distributed is induced by an image x0 representative of the visual style (e.g. an impressionist painting) and a second image x1 representative of the visual content (e.g.",
      "a boat), such that x ~ p(x|x0,x1).",
      "Gatys et al. cast this as an optimisation problem looking to minimise the difference between certain image statistics of the generated image, and the statistics of the example image(s).",
      "They use an iterative optimisation procedure with back propagation to gradually change the values of the pixels in the generated image until the desired statistics are achieved.",
      "In contrast, in the texture networks approach a feed-forward generation network produces the image, which requires only a single evaluation of the network and does not incur in the cost of backpropagation.",
      "A separate generator network is trained for each texture or style and, once trained, it can synthesize an arbitrary number of images of arbitrary size in an efficient feed-forward manner.",
      "The loss function used in training the generator network is derived from Gatys et al. and compares image statistics extracted from a fixed pre-trained descriptor CNN.",
      "This is used to measure the mismatch between the prototype texture and the generated image.",
      "The texture loss function compares feature activations across all spatial locations.",
      "A similar content loss function compares feature activations at corresponding spatial locations, and therefore preserves spatial information.",
      "Analogously to Gatys et al. we use the texture loss alone when training a generator network for texture synthesis, and we use a weighted combination of the texture loss and the content loss when training a generator network for stylization.",
      "Textures  A texture generator network is trained to transform a noise vector sampled from a certain distribution into texture samples that match, according to the texture loss function, a certain prototype texture x0, a three colour channel tensor.",
      "We experimented with several architectures for the generator network g\u2026 we found that multi-scale architectures result in images with small texture loss and better perceptual quality while using fewer parameters and training faster.",
      "[\u2026] Each random noise tensor is first processed by a sequence of convolutional and non-linear activation layers, then upsampled by a factor of two, and finally concatenated as additional feature channels to the partially processed tensor from the scale below.",
      "(Click on image for larger view).",
      "Each convolutional block contains three convolutional layers containing respectively 3\u00d73, 3\u00d73, and 1\u00d71 filters applied using circular convolution to remove boundary effects.",
      "Each convolutional layer is followed by a ReLU activation layer.",
      "When learning using stochastic gradient descent each iteration draws a mini-batch of noise vectors, performs forward evaluation of the generator network to obtain the corresponding images, and computes the loss vs x0.",
      "\u2026 After that, the gradient of the texture loss with respect to the generator network parameters \u03b8 is computed using backpropagation, and the gradient is used to update the parameters.",
      "Styling  For stylized image generator networks the network is modified to take as input in addition to the noise vector z , the image y to which the noise should be applied.",
      "The generator network is then trained to output an image x that is close in content to y and in texture/style to a reference texture x0.",
      "The architecture is the same as that used for texture synthesis, _with the important difference that the noise tensors at the K scales are concatenated (as additional feature channels) with downsampled versions of the input image y.",
      "The learning objective is to minimize the combination of the content and texture loss.",
      "In practice, we found that learning is surprisingly resilient to overfitting and that it suffices to approximate the distribution on natural images with a very small pool of images (e.g 16).",
      "Broader applicability  The success of this approach highlights the suitability of feed-forward networks for complex data generation and for solving complex tasks in general.",
      "The key to this success is the use of complex loss functions that involve different feed-forward architectures serving as \u201cexperts\u201d assessing the performance of the feed-forward generator."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://arxiv.org/pdf/1603.03417.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 50897768
  },
  {
    "blog_id": "software-engineering-for-machine-learning",
    "summary": [
      "Software engineering for machine learning: a case study Amershi et al., ICSE\u201919  Previously on The Morning Paper we\u2019ve looked at the spread of machine learning through Facebook and Google and some of the lessons learned together with processes and tools to address the challenges arising.",
      "Today it\u2019s the turn of Microsoft.",
      "More specifically, we\u2019ll be looking at the results of an internal study with over 500 participants designed to figure out how product development and software engineering is changing at Microsoft with the rise of AI and ML.",
      "\u2026 integration of machine learning components is happening all over the company, not just on teams historically known for it.",
      "A list of application areas includes search, advertising, machine translation, predicting customer purchases, voice recognition, image recognition, identifying customer leads, providing design advice for presentations and word processing documents, creating unique drawing features, healthcare, improving gameplay, sales forecasting, decision optimisation, incident reporting, bug analysis, fraud detection, and security monitoring.",
      "As you might imagine, these are underpinned by a wide variety of different ML models.",
      "The teams doing the work are also varied in their make-up, some containing data scientists with many years of experience, and others just starting out.",
      "In a manner that\u2019s very reminiscent of the online experimentation evolution model at Microsoft we looked at previously, data science moves from a bolt-on specialized skill to a deeply integrated capability over time:  Some software teams employ polymath data scientists, who \u201cdo it all,\u201d but as data science needs to scale up, their roles specialize into domain experts who deeply understand the business problems, modelers who develop predictive models, and platform builders who create the cloud-based infrastructure.",
      "To help spread these skills through the company a variety of tactics are used: a twice-yearly internal conference on machine learning and data science dedicates at least one day to the basics of technologies, algorithms, and best practices; internal talks are given year round on engineering details behind projects, and cutting-edge advances from academic conferences; several teams host weekly open forums on ML and deep learning; and there are mailing lists and online forums with thousands of participants.",
      "A survey informed by conversations with 14 experienced ML leaders within Microsoft was sent to 4,195 members of those internal mailing lists, garnering 551 replies.",
      "Respondents were well spread across data and applied science (42%), software engineering (32%), program management (17%), research (7%) and other (1%).",
      "21% of respondents were managers and the rest were individual contributors.",
      "A general process  The generic machine learning process looks like this:  ( Enlarge )  That diagram is hopefully pretty self-explanatory so I won\u2019t spell out all of the individual stages.",
      "For simplicity the view in Figure 1 is linear, however, machine learning workflows are highly non-linear and contain several feedback loops.",
      "For example, if engineers notice that there is a large distribution shift between the training data and the data in the real world, they might want to go back and collect more representative data and rerun the workflow\u2026 This workflow can become even more complex if the system is integrative, containing multiple ML components which interact together in complex and unexpected ways.",
      "Learnings and emerging best practices  Having a seamless development experience covering (possibly) all the different stages in the process outlined above is important to automation.",
      "But getting there is far from easy!",
      "It is important to develop a \u201crock solid data pipeline, capable of continuously loading and massaging data, enabling engineers to try out many permutations of AI algorithms with different hyper-parameters without hassle.\u201d  IDEs with visual tools are useful when starting out with machine learning, but teams tend to grow out of them with experience.",
      "The success of ML-centric projects depends heavily on data availability, quality, and management.",
      "In addition to availability, our respondents focus most heavily on supporting the following data attributes: \u201caccessibility, accuracy, authoritativeness, freshness, latency, structuredness, ontological typing, connectedness, and semantic joinability.\u201d  Microsoft teams found a need to blend traditional data management tools with their ML frameworks and pipelines.",
      "Data sources are continuously changing and rigorous data versioning and sharing techniques are required.",
      "Models have a provenance tag explaining which data it has been trained on and which version of the model was used.",
      "Datasets are tagged with information about where they came from and the version of the code used to extract it.",
      "ML-centric software also sees frequent revisions initiated by model changes, parameter tuning, and data updates, the combination of which can have a significant impact on system performance.",
      "To address this, rigorous rollout processes are required.",
      "\u2026 [teams] developed systematic processes by adopting combo-flighting techniques (i.e., flighting a combination of changes and updates), including multiple metrics in their experiment score cards, and performing human-driven evaluation for more sensitive data categories.",
      "Model building should be integrated with the rest of the software development process, including common code repositories and tightly coupled sprints and stand-ups.",
      "The support a team requires changes according to their level of experience with ML, but regardless of experience levels, data availability, collection, cleaning, and management remains the number one concern!",
      "( Enlarge )  The big three  We identified three aspects of the AI domain that make it fundamentally different than prior application domains.",
      "Their impact will require significant research efforts to address in the future.",
      "Discovering, managing, and versioning the data needed for machine learning applications is much more complex and difficult than other types of software engineering.",
      "\u201cWhile there are very well-designed technologies to version code, the same is not true for data\u2026\u201c  Model customisation and model reuse require very different skills than those typically found in software teams (\u201cyou can\u2019t simply change parameters with a text editor\u201d !!).",
      "AI components are more difficult to handle as distinct modules than traditional software components \u2014 models may be \u201centangled\u201d in complex ways and experience non-monotonic error behaviour.",
      "While the first two points are hopefully pretty self-explanatory, the third warrants a little more unpacking.",
      "Maintaining strict module boundaries between machine learned models is difficult for two reasons.",
      "First, models are not easily extensible.",
      "For example, one cannot (yet) take an NLP model of English and add a separate NLP model for ordering pizza and expect them to work properly together\u2026 Second, models interact in non-obvious ways.",
      "In large scale systems with more than a single model, each model\u2019s results will affect one another\u2019s training and tuning processes.",
      "Under these conditions, even with separated code, one model\u2019s effectiveness can change as a result of changes in another model.",
      "This phenomenon is sometimes known as component entanglement and can lead to non-monotonic error propagation: improvements in one part of the system may actually decreases the overall system quality."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://dl.acm.org/doi/pdf/10.1109/ICSE-SEIP.2019.00042?download=true",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 3913778
  },
  {
    "blog_id": "dynamic-word-embeddings-for-evolving-semantic-discovery",
    "summary": [
      "Dynamic word embeddings for evolving semantic discovery Yao et al., WSDM\u201918  One of the most popular posts on this blog is my introduction to word embeddings with word2vec (\u2018 The amazing power of word vectors \u2019).",
      "In today\u2019s paper choice Yao et al. introduce a lovely extension that enables you to track how the meaning of words changes over time.",
      "It would be a superb tool for analysing brands for example.",
      "Human language is an evolving construct, with word semantic associations changing over time.",
      "For example, apple which was traditionally only associated with fruits, is now also associated with a technology company.",
      "Similarly the association of names of famous personalities (e.g., trump) changes with a change in their roles.",
      "For this reason, understanding and tracking word evolution is useful for time-aware knowledge extraction tasks\u2026.",
      "Let\u2019s go straight to some examples of what we\u2019re talking about here:  ( Enlarge )  Consider the trajectory of \u2018apple\u2019: in 1994 it\u2019s most closely associated with fruits, and by 2000 changing dietary associations can be seen, and apple is associated with the less healthy \u2018cake,\u2019 \u2018tart,\u2019 and \u2018cream.\u2019 From 2005 through 2016 though, the word is strongly associated with Apple the company, and moreover you can see the changing associations with Apple over time, from \u2018iTunes\u2019 to Google, Microsoft, Samsung et al..",
      "Likewise \u2018amazon\u2019 moves from a river to the company Amazon, and \u2018Obama\u2019 moves from his pre-presidential roles to president, as does \u2018Trump.\u2019  These embeddings are learned from articles in The New York Times between 1990 and 2016.",
      "The results are really interesting (we\u2019ll see more fun things you can do with them shortly), but you might be wondering why this is hard to do.",
      "Why not simply divide up the articles in the corpus (e.g., by year), learn word embeddings for each partition (which we know how to do), and then compare them?",
      "What makes this complicated is that when you learn an embedding for a word in one time window (e.g., \u2018bank\u2019), there\u2019s no guarantee that the embedding will match that in another time window, even if there is no semantic change in the meaning of the word across the two.",
      "So the meaning of \u2018bank\u2019 in 1990 and 1995 could be substantially the same, and yet the learned embeddings might not be.",
      "This is known as the alignment problem.",
      "A key practical issue of learning different word embeddings for different time periods is alignment.",
      "Specifically, most cost functions for training are invariant to rotations, as a byproduct, the learned embeddings across time may not be placed in the same latent space.",
      "Prior approaches to solving this problem first use independent learning as per our straw man, and then post process the embeddings in an alignment phase to try and match them up.",
      "But Yao et al. have found a way to learn temporal embeddings in all time slices concurrently, doing away with the need for a separate alignment phase.",
      "The experimental results suggests that this yields better outcomes that the prior two-step methods, and the approach is also robust against data sparsity (it will tolerate time slices where some words are rarely present or even missing).",
      "Temporal word embeddings  Recall that underpinning word embeddings is the idea of a co-occurrence matrix (see \u2018 GloVe: Global vectors for word representation \u2019) capturing the pointwise mutual information (PMI) between any two words in the vocabulary.",
      "Given a corpus  , we can compute a PMI matrix using windows of size L (around the word in question), where the entry at (w,c) for words w and_c_ is given by:  Where  counts the number of times that words w and c co-occur within a window of size L in corpus  and  and  count the number of occurences of w and c in the corpus respectively.",
      "The learned embedding vectors for w and c,  and  , are such that  .",
      "Adding a temporal dimension to this, for each time slice t the positive PMI matrix  is defined as :  And the temporal word embeddings  must satisfy  .",
      "This still doesn\u2019t solve the alignment problem though.",
      "To encourage alignment, the authors cast finding temporal word embeddings as the solution to the following joint optimisation problem:  where  and  are configurable parameters greater than zero.",
      "The penalty term  enforces low-rank data fidelity as has been widely adopted in previous work  The smoothing term  encourages the word embeddings to align.",
      "The parameter  controls how fast the embeddings can change over time.",
      "This is decomposed to solve the objective function across time for each  , using block coordinate descent (BCD) which minimises with respect to a single block ($U(t)$) at a time.",
      "In theory BCD lacks convergence guarantees, but in practice it seems to work well.",
      "You could always swap BCD for e.g. SGD if you wanted to (but it would make slower progress).",
      "Finding equivalent terms over time  Let\u2019s return to the fun things you can do with the resulting embeddings.",
      "Here\u2019s an example of finding conceptually equivalent items or people over time.",
      "For example, the closest equivalent to the \u2019iPhone\u2019 as of 2012 was \u2018pc\u2019 in 2003, and by 2013-16 it was \u2018smartphone.\u2019  Likewise back in 1990-94 the equivalents of twitter were \u2018broadcast\u2019, \u2018cnn\u2019, \u2018radio\u2019 etc..",
      "In the \u2018mp3\u2019 column you can clearly see associations with the dominant form of music consumption in the given time periods.",
      "We can do a similar thing asking who played a certain political role in a given year:  Even more impressive, is this search for equivalence in sport by looking for the ATP No.",
      "1 ranked male tennis player in a given year.",
      "Here we\u2019re asking, who played the same role that Nadal did in the year 2010?",
      "Tracking popularity over time  The learned word vector norms across times grow with word frequency, and can be viewed as a time series for detecting trending concepts with more robustness than word frequency.",
      "Generally, comparing to frequencies which are more sporadic and noisy, we note that the norm of our embeddings encourages smoothness and normalization while being indicative of the periods when the corresponding words were making news rounds.",
      "Here\u2019s a comparison of norms (top chart) and word frequency (bottom chart) for the names of US presidents over time.",
      "The last word  Our proposed method simultaneously learns the embeddings and aligns them across time, and has several benefits: higher interpretability for embeddings, better quality with less data, and more reliable alignment for across-time querying."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://arxiv.org/pdf/1703.00607",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 69467563
  },
  {
    "blog_id": "deep-learning-scaling-is-predictable-empirically",
    "summary": [
      "Deep learning scaling is predictable, empirically Hestness et al., arXiv, Dec.2017  With thanks to Nathan Benaich for highlighting this paper in his excellent summary of the AI world in 1Q18  This is a really wonderful study with far-reaching implications that could even impact company strategies in some cases.",
      "It starts with a simple question: \u201chow can we improve the state of the art in deep learning?\u201d We have three main lines of attack:  We can search for improved model architectures.",
      "We can scale computation.",
      "We can create larger training data sets.",
      "As DL application domains grow, we would like a deeper understanding of the relationships between training set size, computational scale, and model accuracy improvements to advance the state-of-the-art.",
      "Finding better model architectures often depends on \u2018unreliable epiphany,\u2019 and as the results show, has limited impact compared to increasing the amount of data available.",
      "We\u2019ve known this for some time of course, including from the 2009 Google paper, \u2018The unreasonable effectiveness of data .\u2019  The results from today\u2019s paper help us to quantify the data advantage across a range of deep learning applications.",
      "The key to understanding is captured in the following equation:  Which says that the generalisation error  as a function of the amount of training data  , follows a power-law with exponent  .",
      "Empirically,  usually seems to be in the range -0.07 and -0.35.",
      "With error of course, lower is better, hence the negative exponents.",
      "We normally plot power-laws on log-log graphs, where they result in straight lines with the gradient indicating the exponent.",
      "Making better models can move the y-intercept down (until we reach the irreducible error level), but doesn\u2019t seem to impact the power law coefficient.",
      "On the other hand, more data puts us on an power law path of improvement.",
      "The three learning zones  The learning curves for real applications can be broken down into three regions:  The small data region is where models struggle to learn from insufficient data and models can only perform as well as \u2018best\u2019 or \u2018random\u2019 guessing.",
      "The middle region is the power-law region, where the power-law exponent defines the steepness of the curve (slope on a log-log scale).",
      "The exponent is an indicator of the difficulty for models to represent the data generating function.",
      "\u201cResults in this paper indicate that the power-law exponent is unlikely to be easily predicted with prior theory and probably dependent on aspects of the problem domain or data distribution.\u201d  The irreducible error region is the non-zero lower-bound error past which models will be unable to improve.",
      "With sufficiently large training sets, models saturate in this region.",
      "On model size  We expect the number of model parameters to fit a data set should follow  where  is the required model size to fin a training set of size  , and  .",
      "Best-fit models grow sublinearly in training shard size.",
      "Higher values of  indicate models that make less effective use of extra parameters on larger data sets.",
      "Despite model size scaling differences though, \u201cfor a given model architecture, we can accurately predict the model size that will best fit increasingly larger data sets.\u201d  Implications of the power-law  Predictable learning curves and model size scaling indicate some significant implications on how DL could proceed.",
      "For machine learning practitioners and researchers, predictable scaling can aid model and optimization debugging and iteration time, and offer a way to estimate the most impactful next steps to improve model accuracy.",
      "Operationally, predictable curves can guid decision making about whether or how to grow data sets or computation.",
      "One interesting consequence is that model exploration can be done on smaller data sets (and hence faster / cheaper).",
      "The data set needs to be large enough to show accuracy in the power-law region of the curve.",
      "Then the most promising models can be scaled to larger data sets to ensure proportional accuracy gains.",
      "This works because growing training sets and models is likely to result in the same relative gains across models.",
      "When building a company we look for product-market fit before scaling the business.",
      "In deep learning it seems the analogy is to look for model-problem fit before scaling, a search then scale strategy:  If you need to make a business decision about the return on investment, or likely accuracy improvement, from investing in the collection of more data, the power-law can help you predict returns:  Conversely, if generalisation error within the power-law region drifts from the power-law predictions, it\u2019s a clue that increasing model size might help, or a more extensive hyperparameter search (i.e., more compute), might help:  \u2026predictable learning and model size curves may offer a way to project the compute requirements to reach a particular accuracy level.",
      "Can you beat the power law?",
      "Model architecture improvements (e.g., increasing model depth) seem only to shift learning curves down, but not improve the power-law exponent.",
      "We have yet to find factors that affect the power-law exponent.",
      "To beat the power-law as we increase data set size, models would need to learn more concepts with successively less data.",
      "In other words, models must successively extract more marginal information from each additional training sample.",
      "If we can find ways to improve the power-law exponent though, then the potential accuracy improvements in some problem domains are \u2018immense.\u2019  The empirical data  The empirical data to back all this up was collected by testing various training data sizes (in powers of two) with state-of-the-art deep learning models in a number of different domains.",
      "Here are the neural machine translation learning curves.",
      "On the right you can see results for the best-fit model at each training set size.",
      "As training set sizes grow, the empirical error tends away from the power-law trend, and a more exhaustive hyperparameter search would be needed to bring it back in line.",
      "The next domain is word language models.",
      "A variety of model architectures are used, and although they differ appreciably, they all show the same learning curve profile as characterised by the power-law exponent.",
      "And here are the results for character language models:  With image classification, we see the \u2018small data region\u2019 appear on the plots when there is insufficient data to learn a good classifier:  Finally, here\u2019s speech recognition:  We empirically validate that DL model accuracy improves as a power-law as we grow training sets for state-of-the-art (SOTA) model architectures in four machine learning domains: machine translation, language modeling, image processing, and speech recognition.",
      "These power-law learning curves exist across all tested domains, model architectures, optimizers, and loss functions."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://arxiv.org/pdf/1712.00409",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 80421460
  },
  {
    "blog_id": "generative_adversarial_networks",
    "summary": [
      "What  GANs are based on adversarial training.",
      "Adversarial training is a basic technique to train generative models (so here primarily models that create new images).",
      "In an adversarial training one model (G, Generator) generates things (e.g. images).",
      "Another model (D, discriminator) sees real things (e.g. real images) as well as fake things (e.g.",
      "images from G) and has to learn how to differentiate the two.",
      "Neural Networks are models that can be trained in an adversarial way (and are the only models discussed here).",
      "How  G is a simple neural net (e.g. just one fully connected hidden layer).",
      "It takes a vector as input (e.g. 100 dimensions) and produces an image as output.",
      "D is a simple neural net (e.g. just one fully connected hidden layer).",
      "It takes an image as input and produces a quality rating as output (0-1, so sigmoid).",
      "You need a training set of things to be generated, e.g. images of human faces.",
      "Let the batch size be B.  G is trained the following way:  Create B vectors of 100 random values each, e.g. sampled uniformly from [-1, +1].",
      "(Number of values per components depends on the chosen input size of G.)  Feed forward the vectors through G to create new images.",
      "Feed forward the images through D to create ratings.",
      "Use a cross entropy loss on these ratings.",
      "All of these (fake) images should be viewed as label=0 by D. If D gives them label=1, the error will be low (G did a good job).",
      "Perform a backward pass of the errors through D (without training D).",
      "That generates gradients/errors per image and pixel.",
      "Perform a backward pass of these errors through G to train G.  D is trained the following way:  Create B/2 images using G (again, B/2 random vectors, feed forward through G).",
      "Chose B/2 images from the training set.",
      "Real images get label=1.",
      "Merge the fake and real images to one batch.",
      "Fake images get label=0.",
      "Feed forward the batch through D.  Measure the error using cross entropy.",
      "Perform a backward pass with the error through D.  Train G for one batch, then D for one (or more) batches.",
      "Sometimes D can be too slow to catch up with D, then you need more iterations of D per batch of G.  Results  Good looking images MNIST-numbers and human faces.",
      "(Grayscale, rather homogeneous datasets.)",
      "Not so good looking images of CIFAR-10.",
      "(Color, rather heterogeneous datasets.)",
      "Faces generated by MLP GANs.",
      "(Rightmost column shows examples from the training set.)",
      "Rough chapter-wise notes  Introduction  Discriminative models performed well so far, generative models not so much.",
      "Their suggested new architecture involves a generator and a discriminator.",
      "The generator learns to create content (e.g. images), the discriminator learns to differentiate between real content and generated content.",
      "Analogy: Generator produces counterfeit art, discriminator's job is to judge whether a piece of art is a counterfeit.",
      "This principle could be used with many techniques, but they use neural nets (MLPs) for both the generator as well as the discriminator.",
      "Adversarial Nets  They have a Generator G (simple neural net)  G takes a random vector as input (e.g. vector of 100 random values between -1 and +1).",
      "G creates an image as output.",
      "They have a Discriminator D (simple neural net)  D takes an image as input (can be real or generated by G).",
      "D creates a rating as output (quality, i.e. a value between 0 and 1, where 0 means \"probably fake\").",
      "Outputs from G are fed into D. The result can then be backpropagated through D and then G. G is trained to maximize log(D(image)), so to create a high value of D(image).",
      "D is trained to produce only 1s for images from G.  Both are trained simultaneously, i.e. one batch for G, then one batch for D, then one batch for G...  D can also be trained multiple times in a row.",
      "That allows it to catch up with G.  Theoretical Results  Let  pd(x): Probability that image x appears in the training set.",
      "pg(x): Probability that image x appears in the images generated by G.  If G is now fixed then the best possible D classifies according to: D(x) = pd(x) / (pd(x) + pg(x))  It is proofable that there is only one global optimum for GANs, which is reached when G perfectly replicates the training set probability distribution.",
      "(Assuming unlimited capacity of the models and unlimited training time.)",
      "It is proofable that G and D will converge to the global optimum, so long as D gets enough steps per training iteration to model the distribution generated by G. (Again, assuming unlimited capacity/time.)",
      "Note that these things are proofed for the general principle for GANs.",
      "Implementing GANs with neural nets can then introduce problems typical for neural nets (e.g. getting stuck in saddle points).",
      "Experiments  They tested on MNIST, Toronto Face Database (TFD) and CIFAR-10.",
      "They used MLPs for G and D.  G contained ReLUs and Sigmoids.",
      "D contained Maxouts.",
      "D had Dropout, G didn't.",
      "They use a Parzen Window Estimate aka KDE (sigma obtained via cross validation) to estimate the quality of their images.",
      "They note that KDE is not really a great technique for such high dimensional spaces, but its the only one known.",
      "Results on MNIST and TDF are great.",
      "(Note: both grayscale)  CIFAR-10 seems to match more the texture but not really the structure.",
      "Noise is noticeable in CIFAR-10 (a bit in TFD too).",
      "Comes from MLPs (no convolutions).",
      "Their KDE score for MNIST and TFD is competitive or better than other approaches.",
      "Advantages and Disadvantages  Advantages  No Markov Chains, only backprob  Inference-free training  Wide variety of functions can be incorporated into the model (?)",
      "Generator never sees any real example.",
      "It only gets gradients.",
      "(Prevents overfitting?)",
      "Can represent a wide variety of distributions, including sharp ones (Markov chains only work with blurry images).",
      "Disadvantages  No explicit representation of the distribution modeled by G (?)",
      "D and G must be well synchronized during training  If G is trained to much (i.e. D can't catch up), it can collapse many components of the random input vectors to the same output (\"Helvetica\")"
    ],
    "author_id": "ALEJU",
    "pdf_url": "http://arxiv.org/pdf/1406.2661",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 19173630
  },
  {
    "blog_id": "european-union-regulations-on-algorithmic-decision-making-and-a-right-to-explanation",
    "summary": [
      "European Union regulations on algorithmic decision-making and a \u201cright to explanation\u201d Goodman & Flaxman, 2016  In just over a year, the General Data Protection Regulation (GDPR) becomes law in European member states.",
      "This paper focuses on just one particular aspect of the new law, article 22, as it relates to profiling, non-discrimination, and the right to an explanation.",
      "Article 22: Automated individual decision-making, including profiling, potentially prohibits a wide swath of algorithms currently in use in, e.g., recommendation systems, credit and insurance risk assessments, computational advertising, and social networks.",
      "This raises important issues that are of particular concern to the machine learning community.",
      "In its current form, the GDPR\u2019s requirements could require a complete overhaul of standard and widely used algorithmic techniques.",
      "Profiling has a very inclusive definition, being anything \u201caimed at analysing or predicting aspects concerning that natural person\u2019s performance at work, economic situation, health, personal preferences, interests, reliability, behaviour, location, or movements.\u201d Underlying this is the right to non-discrimination.",
      "Non-discrimination  The use of algorithmic profiling for the allocation of resources is, in a certain sense, inherently discriminatory: profiling takes place when data subjects are grouped in categories according to various variables, and decisions are made on the basis of subjects falling within so-defined groups.",
      "It is thus not surprising that concerns over discrimination have begun to take root in discussion over the ethics of big data.",
      "Personal data is any information relating to an identified or identifiable natural person.",
      "Sensitive personal data includes \u201cpersonal data revealing racial or ethnic origin, political opinions, religious or philosophical beliefs, or trade-union membership, and the processing of genetic data, biometric data for the purpose of uniquely identifying a natural person, data concerning health or data concerning a natural person\u2019s sex life or sexual orientation\u2026\u201d Profiling using personal data is permitted when explicit consent is obtained, it is deemed necessary for the contract between a subject and a data controller, and suitable measures are in place to safeguard the data subject\u2019s rights and freedoms.",
      "However, such profiling is not permitted if it involves sensitive data.",
      "Goodman and Flaxman discuss two possible interpretations of the prohibition on the use of sensitive data in profiling.",
      "The minimal interpretation says that it refers only to cases where an algorithm makes explicit direct use of sensitive data.",
      "However, it is widely acknowledged that simply removing certain variables from a model does not ensure predictions that are, in effect, uncorrelated to those variables.",
      "For example, if a certain geographic region has a high number of low income or minority residents, an algorithm that employs geographic data to determine loan eligibility is likely to produce results that are, in effect, informed by race and income.",
      "(On this point, I recently read and enjoyed \u201c Weapons of Math Destruction \u201d by Cathy O\u2019Neil \u2013 with thanks to Daniel Bryant for the recommendation).",
      "A second maximal interpretation is therefore possible in which decisions based on sensitive data extend to those using variables correlated with sensitive data.",
      "The difficulty here is that correlations can be very difficult to detect.",
      "The link between geography and income may be obvious, but less obvious correlations \u2013 say between IP address and race \u2013 are likely to exist within large enough datasets and could lead to discriminatory effects\u2026 With sufficiently large data sets, the task of exhaustively identifying and excluding data features correlated with \u201csensitive categories\u201d a priori may be impossible.",
      "Companies may also be reluctant to exclude certain covariates \u2013 web-browsing patterns are a very good predictor for various recommendation systems, but they are also correlated with sensitive categories.",
      "In another example of \u2018bias in, bias out\u2019, Goodman and Flaxman provide a thought-provoking example whereby purging variables from the dataset still leaves open a door for unintentional discrimination.",
      "They call this uncertainty bias, and it arises under two conditions:  One group is underrepresented in the sample, so there is more uncertainty associated with predictions about that group.",
      "The algorithm is risk averse, so it will ceteris paribus prefer to make decisions about which it is more confident (i.e., those with smaller confidence intervals.",
      "Here\u2019s a concrete example showing how biased decisions can emerge under such conditions:  A classifier that genuinely had \u2018white\u2019 and \u2018non-white\u2019 categories would definitely fall under all interpretations of sensitive data.",
      "However in practice a classifier will most likely use complicated combinations of multiple categories (occupation, location, consumption patterns, etc.",
      "), and any rare combinations will have very few observations.",
      "The complexity and multifaceted nature of algorithmic discrimination suggests that appropriate solutions will require an understanding of how it arises in practice.",
      "This highlights the need for human-intelligible explanations of algorithmic decision making.",
      "The right to an explanation  When profiling takes place, a data subject has the right to \u201cmeaningful information about the logic involved.\u201d In \u201c How the machine thinks: understanding opacity in machine learning algorithms \u201d Burrell outlines three barriers to transparency:  Intentional concealment on the part of corporations or other institutions, where decision making procedures are kept from public scrutiny  Gaps in technical literacy which mean that, for most people, simply having access to underlying code is insufficient  A \u201cmismatch between the mathematical optimization in high-dimensionality characteristic of machine learning and the demands of human-scale reasoning and styles of interpretation.\u201d  The first barrier is addressed by the requirement for information to be made available to the data subject.",
      "For the second barrier, the GDPR requires that communication with data subjects is in a \u201cconcise, intelligible, and easily accessible form\u201d (emphasis mine).",
      "The third barrier is mostly a function of algorithmic selection and design (though see e.g. \u201c Why should I trust you?",
      "Explaining the predictions of any classifier \u201d  for a system that attempts to explain classifier results ex post facto).",
      "Putting aside any barriers arising from technical fluency, and also ignoring the importance of training the model, it stands to reason that an algorithm can only be explained if the trained model can be articulated and understood by a human.",
      "It is reasonable to suppose that any adequate explanation would, at a minimum, provide an account of how input features relate to predictions, allowing one to answer questions such as: Is the model more or less likely to recommend a loan if the applicant is a minority Which features play the largest role in prediction?",
      "A description of your network architecture and the values of all of the parameters is unlikely to cut it.",
      "The last word  Above all else, the GDPR is a vital acknowledgement that, when algorithms are deployed in society, few if any decisions are purely \u201ctechnical\u201d.",
      "Rather, the ethical design of algorithms requires coordination between technical and philosophical resources of the highest caliber.",
      "A start has been made, but there is far to go."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://arxiv.org/pdf/1606.08813v3.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 31194274
  },
  {
    "blog_id": "musketeer-part-i-whats-the-best-data-processing-system",
    "summary": [
      "Musketeer: all for one, one for all in data processing systems \u2013 Gog et al. 2015  For between 40-80% of the jobs submitted to MapReduce systems, you\u2019d be better off just running them on a single machine\u2026  It was Eurosys 2015 last week, and a great new crop of papers were presented.",
      "Gog et al. from the Cambridge Systems at Scale ( CamSaS ) initiative published today\u2019s choice, \u2018Musketeer\u2019.",
      "In fact, it\u2019s going to be tomorrow\u2019s choice as well since there\u2019s more good material here than I can do justice to in one write-up.",
      "Today I want to focus on the motivation section from the Musketeer paper, which sheds a lot of light on the question \u201cwhat\u2019s the best big data processing system?\u201d Tomorrow we\u2019ll look at Musketeer itself.",
      "What\u2019s the best big data processing system?",
      "\u201cFor what?\u201d should probably be your first question here.",
      "No-one system is universally best.",
      "The Musketeers conducted a very interesting study that looked into this:  We evaluated a range of contemporary data processing systems \u2013 Hadoop, Spark, Naiad, PowerGraph, Metis and GraphChi \u2013 under controlled and comparable conditions.",
      "We found that (i) their performance varies widely depending on the high-level workflow; (ii) no single system always out-performs all others; and (iii) almost every system performs best under some circumstances  It\u2019s common sense that each system must have a design point, and therefore you should expect it to work best with workloads close to that design point.",
      "But it\u2019s easy to lose sight of this in religious wars over the \u2018best\u2019 data processing system \u2013 which can often take place without any context.",
      "Let\u2019s assume you do have a particular workload in mind, so that we can ask the much better question: \u201cwhat\u2019s the best data processing system for this workload?\u201d.",
      "Even then\u2026  Choosing the \u201cright\u201d parallel data processing system is difficult.",
      "It requires significant expert knowledge about the programming paradigm, design goals and implementation of the many available systems.",
      "Tomorrow we\u2019ll see how Musketeer can help make this choice for you, and even retarget your workflow to the back-end for which it is best suited.",
      "Even if you don\u2019t use Musketeer though, the analysis from section 2 of the paper is of interest.",
      "Gog et al. examined makespan \u2013 the entire time to execute a workflow including not only the computation itself, but also any data loading, pre-processing, and output materialization.",
      "From this, they determine four key factors that influence system performance:  The size of the input data.",
      "Single machine frameworks outperform distributed ones for smaller inputs.",
      "The structure of the data.",
      "Skew and selectivity impact I/O performance and work distribution.",
      "Engineering decisions made during the constructing of the data processing system itself.",
      "For example, how efficiently it can load data.",
      "The computation type, since specialized systems operate more efficiently.",
      "In all systems they studied, the ultimate source and sink of data is files in HDFS.",
      "Do you really need that fancy distributed framework?",
      "You might recall the story from last year of awk and grep on the command-line of a single machine outperforming a Hadoop cluster by a factor of 235 .",
      "Gog et al. studied the effect on input size on framework performance.",
      "Take a look at their figure 2a, below:  For small inputs (\u22640.5GB), the Metis single-machine MapReduce system performs best.",
      "This matters, as small inputs are common in practice: 40\u2013 80% of Cloudera customers\u2019 MapReduce jobs and 70% of jobs in a Facebook trace have \u2264 1GB of input.",
      "This last point bears repeating, and if I can generalize slightly: for between 40-80% of the jobs submitted to MapReduce systems, you\u2019d be better off just running them on a single machine.",
      "Likewise a join workflow producing 1.9GB of data runs best on a single machine.",
      "A larger join producing 29GB works best on Hadoop.",
      "See figure 2b below.",
      "Do you really need that new shiny thing?",
      "Well, maybe!",
      "But likewise there is no universal guarantee that e.g. Spark is better than Hadoop MR.",
      "It depends on what you\u2019re trying to do\u2026  Once the data size grows, Hive, Spark and Hadoop all surpass the single-machine Metis, not least since they can stream data from and to HDFS in parallel.",
      "However, since there is no data re-use in this workflow, Spark performs worse than Hadoop: it loads all data into a distributed in-memory RDD before performing the projection.",
      "What are you optimising for?",
      "For workflows involving iterative computations on graphs, it won\u2019t surprise you to learn that specialized graph processing systems do well.",
      "It is evident that graph-oriented paradigms have significant advantages for this computation: a GraphLINQ implementation running on Naiad outperforms all other systems.",
      "PowerGraph also performs very well, since its vertex-centric sharding reduces the communication overhead that dominates PageRank\u2026 However, the fastest system is not always the most efficient.",
      "Look at figure 3a below.",
      "With smaller graphs the 100 node clusters may be the fastest, but you\u2019re getting nowhere near a 100x speed-up for all that investment (we\u2019re on the RHS of the Universal Scalability Law curve).",
      "If you prepared to wait just a little longer for results, you can get your answer with dramatically less compute power (also compare e.g. PowerGraph on 16 nodes with GraphChi on one).",
      "Yes, but what\u2019s the best data processing system?",
      "Our experiments show that the \u201cbest\u201d system for a given workflow varies considerably.",
      "The right choice \u2013 i.e., the fastest or most efficient system \u2013 depends on the workflow, the input data size and the scale of parallelism available.",
      "If you think a little carefully about what you\u2019re trying to achieve \u2013 when you really need fully precise results vs. good approximations ; when you really need to run on a distributed framework vs. a single machine; when you really need results quickly vs. waiting a little bit longer but being much more efficient \u2013 you can significantly improve the overall effectiveness of your data platform.",
      "Tomorrow we\u2019ll see how Musketeer can help to make all this more practical and manageable by enabling workflows to be written once and mapped to many systems \u2013 even combining systems within a workflow.",
      "Postscript  The authors did something very neat with this paper \u2013 in the pdf version, every figure is actually a link to a webpage describing the experiments and data sets behind it.",
      "Really great idea, thanks!"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://www.cl.cam.ac.uk/research/srg/netos/camsas/pubs/eurosys15-musketeer.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 11773091
  },
  {
    "blog_id": "a-design-methodology-for-reliable-software-systems",
    "summary": [
      "A design methodology for reliable software systems Liskov 1972  We\u2019ve come to the end of Liskov\u2019s list .",
      "The final paper is by Barbara Liskov herself, on the question of how best to go about designing software systems so that we can have some confidence they will work.",
      "The unfortunate fact is that the standard approach to building systems, involving extensive debugging, has not proved successful in producing reliable software, and there is no reason to suppose it ever will.",
      "So we\u2019re going to need some testing, and for high levels of confidence we\u2019ll need good coverage via:  a complete but minimal set of test cases, and  a system in which the set of relevant test cases is small, such that it is possible to generate every case  It is the system design which determines how many test cases there are and how easily they can be identified, the problems can be solved most effectively during the design process.\u201d  And with that short introduction, the rest of the paper focuses on the questions of \u2018What is a good system design?\u2019 and \u2018What process will help to ensure we produce one?\u2019  A good system design is one where complexity is tamed by dividing it into modules (called \u2018partitions\u2019 in the paper, because the term module had already become very overloaded).",
      "As we\u2019ve looked at previously , just dividing a system into modules isn\u2019t enough though \u2013 it matters very much how you make those divisions.",
      "In fact,  \u2026the division of a system into modules may introduce additional complexity\u2026 if modularity is viewed only as an aid to management, then any ad hoc modularization of the system is acceptable.",
      "However, the success of modularity depends directly on how well the modules are chosen.",
      "A good modularity is based on levels of abstraction, and uses structural programming within modules.",
      "Level of abstraction were first defined by Dijktsra.",
      "They provide a conceptual framework for achieving a clear and logical design for the system.",
      "The entire system is conceived as a hierarchy of levels, the lowest levels being those closest to the machine.",
      "There are two important rules given for levels of abstraction:  Each level has resources which it owns exclusively and which other levels are not permitted to access.",
      "Lower levels are not aware of the existence of higher levels and therefore may not refer to them in any way.",
      "With good modularity, the system is broken into a hierarchy of partitions (modules), with each partition representing one level of abstraction and consisting of one or more functions which share common resources.",
      "The connections between partitions are limited as follows:  Control connections are limited by the rules about the hierarchy of levels of abstraction  Connections in data passed between partitions are limited to the explicit arguments passed from the functions of one partition to the (external) functions of another partition.",
      "Implicit interaction on common data may only occur among functions within a partition.",
      "The combined activity of the functions in a partition support its abstraction and nothing more.",
      "The definition of connections in the above follows Parnas : \u201cThe connections between modules are the assumptions which the modules make about each other.\u201d  We know what good modularity looks like when we see it now.",
      "But how do you arrive at good modularity in the first place?",
      "The traditional technique for modularization is to analyze the execution-time flow of the system and organize the system structure around each major sequential task.",
      "This technique leads to a structure which has very simple connections in control, but the connections in data tend to be complex.",
      "(See Parnas again).",
      "Select modules to support abstractions or concepts which you find helpful in thinking about the system\u2026.",
      "Abstraction is a very valuable aid to ordering complexity.",
      "Abstractions are introduced in order to make what the system is doing clearer and more understandable; an abstraction is a conceptual simplification because it expresses what is being done without specifying how it is done.",
      "What kinds of abstractions should we be on the lookout for?",
      "Abstractions of resources \u2013 modules that map the characteristics of an abstract resource into the real underlying resource or resources  Abstractions that hide data storage representations  Abstractions that limit information:  According to the third requirement for good modularizatio, the functions comprising a partition support only one abstraction and nothing more.",
      "Sometimes it is difficult to see that this restriction is being violated, or to recognize that the possibility for identification of another abstraction exists.",
      "One technique for simplification is to limit the amount of information which the functions in the partition need to know (or even have access to).",
      "One way to limit information is to introduce modules at a lower level, on which the higher-level module depends, which hide that knowledge.",
      "Abstractions that generalize a function or group of functions.",
      "\u201cSeparating such groups is a common technique in system implementation and is also useful for error avoidance, minimization of work, and standardization.\u201d  Abstractions that encapsulate areas likely to change  The design process proceeds iteratively as follows.",
      "First determine an initial set of abstractions which represent the eventual system behaviour in a very general way.",
      "Then establish the data and flow of control connections among the partitions.",
      "The second phase occurs concurrently with the first; as abstractions are proposed, their utility and practicality are immediately investigated\u2026 A partition has been adequately investigated when its connections with the rest of the system are known and when the designers are confident that they understand exactly what its effect on the system will be.",
      "Varying depths of analysis will be necessary to achieve this confidence.",
      "When do you start programming modules?",
      "There is a tendency to think of this as the era of the strict waterfall, but that\u2019s not what Liskov proposes:  It is not clear exactly how early structured programming of the system should begin\u2026 The best rule is probably to keep trying to write structured programs; failure will indicate that the system abstractions are not yet sufficiently understood and perhaps this exercise will shed some light on where more effort is needed or where other abstractions are required.",
      "Finally, a design can be considered \u2018finished\u2019 when the following criteria are met:  All major abstractions have been identified and partitions defined for them; the system resources have been distributed among the partitions and their positions in the hierararchy established.",
      "The system exists as a structured program\u2026 this consists of several components, but no component is likely to be completely defined.",
      "Rather each component is likely to use the names of lower-level components which are not yet defined.",
      "Sufficient information is available so that a skeleton of a user\u2019s guide to the system could be written.",
      "(This was an era of much simpler user interfaces remember)."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://valbonne-consulting.com/papers/classic/Liskov_72-Design_Methodology_for_Reliable_Software_Systems.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 74642936
  },
  {
    "blog_id": "early-detection-of-configuration-errors-to-reduce-failure-damage",
    "summary": [
      "Early detection of configuration errors to reduce failure damage Xu et al, OSDI \u201916  Here\u2019s one of those wonderful papers that you can read in the morning, and be taking advantage of the results the same afternoon!",
      "Remember the \u2018 Simple testing can prevent most critical failures \u2018 paper from OSDI\u201914 that we looked at last month?",
      "In that paper we learned that trivial mistakes in error handling, which are easy to test for, accounted for a vast majority of catastrophic production incidents.",
      "Well, as soon as you\u2019ve got your error / exception handlers sorted out, you might want to read today\u2019s paper to discover another class of easy-to-test for bugs that are also disproportionately responsible for nasty production failures.",
      "Facebook\u2019s \u2018 Holistic configuration management \u2018 paper stresses the importance of version control and testing for configuration, as configuration errors are a major source of site errors.",
      "Xu et al. study configuration parameters in the wild, focusing especially on those associated with reliability, availability, and serviceability (RAS) features.",
      "What they find is that very often configuration values are not tested as part of system initialization.",
      "The program runs along happily until reaching a point (say for example, it needs to failover) where it needs to read some configuration for the first time, and then it blows up \u2013 typically when you most need it.",
      "So here\u2019s the short takeaway test all of your configuration settings as part of system initialization and fail-fast if there\u2019s a problem.",
      "Do that, and you\u2019ll cut out another big source of major production errors.",
      "The paper itself is in two parts: the first part (where I\u2019ll focus most of my attention in this short write-up) is an analysis of these latent configuration errors in real code bases; the second part introduces a tool called PCheck, which if your program is written in C or Java can even find latent configuration usage and automatically write tests for you!",
      "Latent configuration errors can result in severe failures, as they are often associated with configurations used to control critical situations such as fail-over, error handling, backup, load balancing, mirroring, etc\u2026 Their detection or exposure is often too late to limit the failure damage.",
      "In a study of real-world configuration issues in the the products of COMP-A, \u201cmajor storage company in the US,\u201d with footnote \u201cwe are required to keep the company and its products anonymous,\u201d it turns out that 75% of all high severity\u00a0 configuration-related errors are caused by latent configuration errors.",
      "It may well be that the authors are required to keep COMP-A anonymous, but I couldn\u2019t help noticing the author affiliations printed in big type on the front page.",
      "A more than fair chance that company is NetApp I would say!",
      "The authors also studied a number of real-world open-source systems (see table below), and inspected usage of all of their RAS-related configuration parameters.",
      "They looked at how many of those parameters were explicity checked vs simply being used when first required, yielding the results below:  Many of the studied RAS parameters do not have any special code for checking the correctness of their settings.",
      "Instead, the correctness is verified (implicitly) when the parameters\u2019 values are actually used in operations such as a file open call.",
      "Here\u2019s an example of a real-world latent configuration error in MapReduce:  And here are some other bugs found during the study, in the most recent versions of the software under inspection in (a) HDFS:  and (b), Apache httpd:  So we know that many configuration parameters aren\u2019t checked before usage.",
      "It\u2019s also the case that many of these parameters aren\u2019t used during system startup (and so are not verified even implicitily):  Many (12-38.6%) of the studied RAS configuration parameters are not used at all during the system\u2019s initialization phase.",
      "Put these two finding together, and what you have is a collection of ticking time bombs!",
      "Remember that since these are configuration settings they may be changed on deployment \u2013 i.e. these are not bugs that unit testing can catch.",
      "4.7-38.6% of the studied RAS parameters do not have any early checks and and thereby subject to latent configuration errors which can cause severe impact on the system\u2019s dependability.",
      "Here\u2019s the summary of how many of these ticking time bombs can exist in the systems studied:  The threats are prevalent: Latent configuration errors can reside in 10+% of the RAS parameters in five out of six systems.",
      "As all theses latent configuration erros are discovered in the latest versions, any of them could appear in a real deployment and would impair the system\u2019s dependability in a latent fashion.",
      "The authors wrote a tool called PCheck which uses static code analysis to find instructions that load configuration parameters into program variables, looks for all instructions that use the parameter value, figures out the execution context of those instructions and composes checkers that can be run at initialization to verify the configurarion is well-formed in the target environment.",
      "I feel bad skipping over all of the details of the authors hard work here, but I\u2019m going to refer you to the paper for full details if you\u2019re interested.",
      "With the PCheck tests in place, the authors harvested 830 configuration files for the studied systems (from mailing lists and technical forums) and validated them.",
      "With the checks in place, 70+% of latent configuration errors were detected.",
      "PCheck reports 282 true configuration errors (from 830 configs!)",
      "and three false alarms.",
      "Many (37.5-87.8%) of the reported configuration erros can only be detected by considering the system\u2019s native execution environment.",
      "These configuration settings are valid in terms of format and syntax (in fact, they are likely to be correct in the original hosts).",
      "However, they are erroneous when used on the current system because the values violate environment constraints such as undefined environment variables, non-existing file-paths, unreachable IP addresses etc..  (Which leaves me a little unsure as to what the authors count as a \u2018true configuration error\u2019  \u2013 a configuration file which may have been perfectly valid on the system from which it was cut-and-pasted onto a mailing list may of course give problems in another context, but this doesn\u2019t imply it was truly a configuration error on the original system).",
      "Nevertheless, I think the overall message of this paper is clear: Ladies and Gentlemen, please eagerly check all of your configuration variables on system startup.",
      "This paper advocates early detection of configuration errors to minimize failure davage, especially in cloud and data-center systems.",
      "Despite all the efforts of validation, review, and testing, configuration errros (even those obvious errors) still cause many high-impact incidents of today\u2019s Internet and cloud systems."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://www.usenix.org/system/files/conference/osdi16/osdi16-xu.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 96308075
  },
  {
    "blog_id": "symmetry-reduction-enables-model-checking-of-more-complex-emerging-behaviours-of-swarm-navigation-algorithms",
    "summary": [
      "Symmetry Reduction Enables Model Checking of More Complex Emerging Behaviours of Swarm Navigation Algorithms \u2013 Antu\u00f1ya et al. 2015  Don\u2019t let the title put you off \u2013 this paper is all about robot swarms!",
      "Previously we looked at some nature-inspired optimisation algorithms, including Particle Swarm Optimisation which draws inspiration from the behaviour of flocks of birds.",
      "Today\u2019s paper comes from the field of robotics, and it\u2019s all about emulating some of nature\u2019s behaviours by creating swarms of robots.",
      "Each individual robot follows simple rules, but taken together those simple rules, when applied in a swarm of robots, produce interesting emergent behaviours.",
      "It would be nice to know the robots we let loose will always behave as we wish\u2026  There are some fun parallels to distributed systems too.",
      "Robotic swarms consist of a set of robots with simple individual behaviour rules, working together in cooperation to achieve a more complex or emergent final behaviour.",
      "Appealing characteristics of swarms are the low cost incurred in producing the robots, which have a simple hardware design, scalability, and fault tolerance.",
      "Examples of their application to real-life tasks include nanorobotics, disaster rescue missions, and mining or agricultural foraging tasks.",
      "Just as with concurrent and distributed systems, we have to worry about safety and liveness in this context.",
      "And as we\u2019re releasing real machines out into the real world, we might want to verify those properties hold.",
      "The emergent behaviours of a swarm of robots need to be verified, with respect to safety and liveness requirements, and validated to determine whether it is fit for purpose in the target environment.",
      "Safety requirements are the allowed behaviours of the system, and liveness requirements specify the dynamic behaviours expected to happen during the execution of the system.",
      "Also in common with formal methods in distributed systems, state space explosion puts practical bounds on the complexity of systems that can be addressed.",
      "Letting the robots move around in continuous space (in  modelling terms \u2013 they obviously do IRL) leads to an infinite state space.",
      "Instead we can chop space up into cells in a grid\u2026  The discretization of the continuous space into cells of fixed size \u2014i.e., a grid\u2014 is a solution that has been applied in swarms, to enable model checking.",
      "Even with the discretization of the environment into a \u201csmall\u201d grid (e.g., 4\u00d74 cells), the state-space explosion problem can occur due to the presence of other variables, which results in too many possible configurations of the robots in the grid.",
      "Symmetry reduction techniques have been used to reduce state spaces in model checking.",
      "The authors  show a way of encoding a swarm environment that eliminates symmetrically equivalent states from the state space \u2013 thus enabling formal verification to be applied to larger scale problems.",
      "We implemented a relative encoding of a swarm environment model that eliminates symmetrically equivalent states from the state space.",
      "The swarm is assumed to be homogeneous; i.e., all the robots are considered identical in capabilities and rank.",
      "In the relative encoding, one robot is set as the \u201creference\u201d, with a fixed location and direction of motion.",
      "The other robots\u2019 locations and directions are defined based on this reference.",
      "In an absolute encoding, if all the robots in a grid are simultaneously rotated in the same direction and shifted horizontally or vertically by the same distance, the robot\u2019s new configurations change in location and direction.",
      "But with the relative encoding the locations and directions remain the same.",
      "This is the key to the state-space reduction.",
      "In a grid of size m x m, and for robots that can travel in d different directions, the state space is reduced by a factor of dm2:  If a model with r robots in a m \u00d7 m size grid (locations), with d possible directions, p other robots\u2019 variables of domain sizes vi,  i = 1, \u2026, p, and q global variables of domain sizes sj ,  j = 1, \u2026, q, is globally encoded, the size of the state space to be explored is (d \u00d7 m2 \u00d7 v1 \u00d7 v2 \u00d7 \u2026 \u00d7 vp)r \u00d7 (s1 \u00d7 s2 \u00d7 \u2026 \u00d7 sq).",
      "In a relative encoding, the reference robot will have fixed location and direction, and the resulting state space will be of size (v1 \u00d7 v2 \u00d7 \u2026 \u00d7 vp) x (d \u00d7 m2 \u00d7 v1 \u00d7 v2 \u00d7 \u2026 \u00d7 vp)r-1 \u00d7 (s1 \u00d7 s2 \u00d7 \u2026 \u00d7 sq).",
      "This corresponds to a reduction of the state space of d\u00d7m2.",
      "In practice this bound changes according to the variables used in the relative encoding of an algorithm.",
      "This also means that finding a counterexample in the relative model is equivalent to a class of counterexamples in the global model.",
      "This approach is validated on the Alpha navigation algorithm, which rather disappointingly is known not to produce the desired behaviour!",
      "The Alpha algorithm has been used as a case study to demonstrate how to verify emergent behaviours in swarms through model checking tools.",
      "In the Alpha algorithm, the robots in the swarm navigate the environment trying to maintain connectivity, defined as a wireless range.",
      "This is achieved by the  following rules: (a) the default movement of a robot is forward, maintaining its current direction.",
      "(b) When a robot loses connection with another robot, if the remaining number of connected robots is smaller than a value \u03b1, the robot makes a 180 degree turn.",
      "(c) Every time a robot regains connectivity with another, it performs a random turn.",
      "The desired behaviour for a swarm is this case is that all robots shall eventually be connected.",
      "In prior work, a formal expression of the algorithm in Linear Temporal Logic showed this is false.",
      "A global model and a relative model are both encoded for the NuSMV model checker.",
      "Both were able to show that the desired condition does not hold, but the relative model gives a much smaller state space (by about 2 orders of magnitude):  Thus, verification through model checking can be performed over larger grid sizes and higher numbers of robots, and also for more detailed abstractions that model navigation algorithms using more variables.",
      "Although the state space reduction is more significant in terms of the grid size, and not as expressive if considering realistic swarm sizes, analysing small robot groups can help to understand larger swarms within the lower limit bounds of swarm size, which demands more from the navigation algorithm.",
      "Here\u2019s an example of a counter-example found using the global encoding:  And this a more compact example found using the relative encoding:"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://lib-arxiv-008.serverfarm.cornell.edu/pdf/1505.05695v2.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 75779006
  },
  {
    "blog_id": "unifying-count-based-exploration-and-intrinsic-motivation",
    "summary": [
      "This paper presents a novel RL exploration bonus based on an adaptation of count-based exploration for high-dimensional spaces.",
      "The main contribution is the derivation of the relationships between prediction gain (PG), a quantity called the pseudo-count, and the well-known information gain from the intrinsic RL literature.",
      "The overall presentation is clear and precise, especially when it comes to defining notation and explaining the core concepts.",
      "The authors use results from prior work on applying count-based exploration in complex domains, so reading this paper without having that background is a bit challenging (see the bibliography for recent papers by Bellemare and Veness on this topic).",
      "These results will help make previous count-based exploration methods, such as those devised around the idea of optimism in the face of uncertainty, viable for high-dimensional problems.",
      "Quick Definitions  This paper presents the following definitions and notation, along with theoretical results about their properties, in a rich manner.",
      "I will simply present definitions necessary for understanding the main contribution here; please see the paper for further details.",
      "$ \\mathcal{X} := $ finite or countable alphabet, with sub-sequences of length $n$ denoted by $x_{1:n} \\in \\mathcal{X}^n $  A sequential density model over $\\mathcal{X}$ is a mapping from finite sub-sequences to probalitiy distributions over $\\mathcal{X}$.",
      "Denote these distributions by $\\rho_{n}(x) := \\rho(x ; x_{1:n})$  The recoding probability of a symbol $x$ is denoted by $\\rho_{n}^{\\prime} := \\rho(x ; x_{1:n}x)$.",
      "That is, the probability assigned to $x$ by our sequential density model after observing a new occurence of $x$  $ \\hat{N}(x) := $ the pseudo-count function  $ \\hat{n} := $ the pseudo-count total  Main Result  The following equations summarize the main results presented in this paper.",
      "The authors relate the pseudo-count function and the pseudo-count total as follows.",
      "From (1), we can write:  By choosing an appropriate sequential density model, such as a graphical model.",
      "See the appendix of the paper for the description of the CTS model used by the authors in the experimentation; this model treats a 42 x 42 processed image as a factorizable observation, computing the probability of the image $x$ as the product of the probabilities of each (i, j) pixel.",
      "These pixel probabilities are computed via the neighbors of that pixel.",
      "See section 5.2 as well for a discussion on using directed graphical models as sequential density models.",
      "The prediction gain is defined as  For the information gain (IG) defined as the change in posterior w.r.t.",
      "the KL-divergence within a mixture model $\\xi$ defined over a class $\\mathcal{M}$ of sequential density models, we have:  The reward bonus proposed in this paper is as follows:  with $\\beta$ = 0.05 selected from a short parameter sweep and the small added constant for numerical stability.",
      "The authors compared different forms of this bonus, using $\\hat{N}(x)^{-1}$ and $PG_{n}(x)$.",
      "Notes  They implemented their exploration as a reward bonus for Double-DQN and A3C , and were able to show significant improvements on many Atari tasks.",
      "Most notably, they achieved the highest score to date on Montezuma\u2019s Revenge.",
      "I am very intrigued by this result; without using some form of memory such as an LSTM to \u201cremember\u201d long-term behaviors, their agent was able to explore efficiently enough to learn how to achieve the hierarchical sub-goals required to visit most of the rooms and achieve high scores.",
      "One of the main benefits of prediction gain and pseudo-counts is that the agent is able to recognize and adjust its behaviors efficiently to salient events, which clearly plays a major role in solving games like Montezuma\u2019s Revenge.",
      "A convolutional neural net used to represent the value function for an Atari game must have a very large learning capacity, which is normally under-utilized when it comes to \u201chard\u201d games due to the inefficiency of $\\epsilon$-greedy exploration.",
      "For example, Montezuma\u2019s Revenge has 23 total rooms the agent is able to visit; the authors showed that the agent tends to only visit about 2 of these rooms without the reward bonus.",
      "After 100 million frames of training on Montezuma\u2019s Revenge with the suggested bonus, the agent had visited about 15 of these rooms!",
      "The CNN that uses a reward bonus must be learning representations from within all or most of the rooms, and encoding them in its hidden layers.",
      "Even though Montezuma\u2019s Revenge is partially observable, the agent is able to \u201cremember\u201d how to do the sub-tasks just by eventually stumbling upon the sparse rewards and cleverly updating its Q-values by means of the bonus.",
      "I would like to see what representations were encoded by the hidden layers of the network after training.",
      "Perhaps using a recurrent network and/or memory with attention + the reward bonus would help improve the learning speed, so that the sequential nature of the sub-tasks within the game can be encoded more readily.",
      "It would be nice to see this approach compared with VIME .",
      "VIME computes the amount of information gained about the dynamics model due to the agent taking an action and seeing a certain following state.",
      "The authors show that the results should be similar, as maximizing the information gain also maximizes a lower bound on the inverse of the pseudo count.",
      "The authors mention that other sequential density models with more sophistication could be used, as opposed to the simple CTS model.",
      "There clearly is a trade-off, however, such that more complex sequential density models would increase the time and space complexity significantly.",
      "For example, Oh, et al., 2015 designed a recurrent convolutional neural network architecture to predict future frames from a video sequence of an Atari game.",
      "They estimated the visitation frequency of a predicted frame by an empirical distribution over the contents of the replay memory; the count was computed using a gaussian kernel that provided a distance metric between frames.",
      "This was used as an exploration bonus.",
      "The authors presented the appropriate metrics and figures to convince the reader of the effectiveness of their solution.",
      "They tested it widely on many (~60) Atari games to prove its widespread impact.",
      "(On a related note, it makes the difficulty for researchers that do not have access to the computational resources that DeepMind does to carry out such extensive experimentation all the more apparent!",
      ")."
    ],
    "author_id": "pemami",
    "pdf_url": "https://arxiv.org/pdf/1606.01868",
    "author_full_name": "Patrick Emami",
    "source_website": "https://pemami4911.github.io/index.html",
    "id": 11779349
  },
  {
    "blog_id": "machine-learning-systems-are-stuck-in-a-rut",
    "summary": [
      "Machine learning systems are stuck in a rut Barham & Isard, HotOS\u201919  In this paper we argue that systems for numerical computing are stuck in a local basin of performance and programmability.",
      "Systems researchers are doing an excellent job improving the performance of 5-year old benchmarks, but gradually making it harder to explore innovative machine learning research ideas.",
      "The thrust of the argument is that there\u2019s a chain of inter-linked assumptions / dependencies from the hardware all the way to the programming model, and any time you step outside of the mainstream it\u2019s sufficiently hard to get acceptable performance that researchers are discouraged from doing so.",
      "Take a simple example: it would be really nice if we could have named dimensions instead of always having to work with indices.",
      "Named dimensions improve readability by making it easier to determine how dimensions in the code correspond to the semantic dimensions described in, .e.g., a research paper.",
      "We believe their impact could be even greater in improving code modularity, as named dimensions would enable a language to move away from fixing an order on the dimensions of a given tensor, which in turn would make function lifting more convenient\u2026  For the readability point I feel I ought to mention you could always declare a \u2018constant\u2019 , e.g FEATURE_NAME = 1, and use that to make your code more readable (constant is in quotes there because Python doesn\u2019t really have constants, but it still has variable names!).",
      "But that won\u2019t solve the ordering issue of course.",
      "It would be interesting to explore unordered sets of named dimensions in the programming model and see what benefits that could bring, however,\u2026  It is hard to experiment with front-end features like named dimensions, because it is painful to match them to back ends that expect calls to monolithic kernels with fixed layout.",
      "On the other hand, there is little incentive to build high quality back ends that support other features, because all the front ends currently work in terms of monolithic operators.",
      "Named dimensions are an easy to understand example, but the challenges go much deeper than that, and were keenly felt by the authors during research on Capsule networks .",
      "Challenges compiling non-standard kernels  Convolutional Capsule primitives can be implemented reasonably efficiently on CPU but problems arise on accelerators (e.g. GPU and TPU).",
      "Performance on accelerators matters because almost all current machine learning research, and most training of production models, uses them.",
      "Scheduling instructions for good performance in accelerators is a complex business.",
      "It\u2019s \u201cvery challenging\u201d just for standard convolutions, and convolutional capsules add several dimensions of complexity.",
      "Because it\u2019s so tricky, high-performance back-ends for accelerators tend to spend a lot of effort optimising a small set of computational kernels.",
      "New primitives that don\u2019t fit into these existing kernels can be compiled into custom kernels using e.g. Tensor Comprehensions or PlaidML , but the current state-of-the-art only really supports small code fragments and frequently doesn\u2019t get close to peak performance (e.g.",
      "a factor of 8x slower after a one hour search, for a conventional 2D convolution the authors used as an experiment).",
      "Our interpretation of these results is that current frameworks excel at workloads where it makes sense to manually tune the small set of computations used by a particular model or family of models.",
      "Unfortunately, frameworks become poorly suited to research, because there is a performance cliff when experimenting with computations that haven\u2019t previously been identified as important.",
      "That said, after around 17 minutes Tensor Comprehensions does find a solution that outperforms a hand-tuned CUDA solution.",
      "Which doesn\u2019t seem so bad, until you remember this is just one kernel out of what may be a large overall computation.",
      "The easiest and best performing solution for convolutional Capsules in both TensorFlow and PyTorch turns out to be to target high-level operations already supported by those frameworks.",
      "This comes at a cost though; copying, rearranging, and materialising to memory two orders of magnitude more data than is strictly necessary.",
      "Challenges optimising whole programs  It might be hard to performance tune a single non-standard kernel, but full programs must typically evaluate a large graph of kernels.",
      "In order to make use of pre-optimised kernels, it\u2019s necessary to use one of a small number of parameter layouts that have been chosen ahead of time to be optimal in isolation.",
      "In practice there are so few choices of layout available that frameworks like XLA and TVM do not attempt a global layout assignment, and instead choose fixed layouts for expensive operators like convolution, then propagate those layouts locally through the operator graph inserting transposes where necessary.",
      "Similar considerations make it hard to experiment with different choices for quantised and  low-precision types.",
      "Whole program optimisations such as common sub-expression elimination are attractive for machine learning, but hard to exploit to the fullest extent with the current state-of-the-art: \u201cit seems likely that it will be necessary to architect machine learning frameworks with automatic optimizers in mind before it will be possible to make the best use of whole-program optimization.\u201c  Challenges evolving programming languages  Recall that back ends are structured around calls to large monolithic kernels.",
      "In this section we argue that this back-end design approach is slowing progress in the maintainability, debuggability, and expressiveness of programming models.",
      "Worse, the resulting brake on innovation in languages is in itself reducing the incentive for back-end developers to improve on the current situation.",
      "We saw one such example at the top of this piece: support for named dimensions.",
      "Another consequence is the choice of kernels or \u2018operators\u2019 as the dominant abstraction, with user programs in Python calling into operators written in terms of specific back-end languages and libraries.",
      "This tends to fix both the set of operators and also their interfaces.",
      "Breaking out of the rut  Our main concern is that the inflexibility of languages and back ends is a real brake on innovative research, that risks slowing progress in this very active field\u2026  How might we break out of the rut?",
      "Embrace language design including automatic differentiation, using purely named dimensions and kernels expressed within the language syntax  Support a back-end IR defining a graph of layout-agnostic general purpose loop nests  Use transformation passes over the IR to lower it to a concrete common sub-expression elimination strategy, with layouts for each materialised intermediate  Compilation passes that generate accelerator code given the lowered IR, with adequate code produced quickly and close to peak performance achievable after searching  At a high level, this reminds me a little of the approach taken by Musketeer ."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3317550.3321441?download=true",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 13040160
  },
  {
    "blog_id": "fawn-a-fast-array-of-wimpy-nodes",
    "summary": [
      "FAWN: A Fast Array of Wimpy Nodes \u2013 Andersen et al. 2009  A few days ago we looked at FaRM (Fast Remote Memory), which used RDMA to match network speed with the speed of CPUs and got some very impressive results in terms of queries & transactions per second.",
      "But maybe there\u2019s another way of addressing the imbalance \u2013 use less powerful CPUs!",
      "Which perhaps sounds a little odd at first, but starts to make sense when you consider a less-often quoted metric: queries / transactions per Joule.",
      "High performance DRAM-based clusters, storing terabytes or petabytes of data, are both expensive and consume a surprising amount of power\u2014two 2 GB DIMMs consume as much energy as a 1 TB disk.",
      "The power draw of these clusters is becoming an increasing fraction of their cost\u2014up to 50% of the three-year total cost of owning a computer.",
      "The density of the datacenters that house them is in turn limited by their ability to supply and cool 10\u201320 kW of power per rack and up to 10\u201320 MW per datacenter.",
      "Future datacenters may require as much as 200 MW, and datacenters are being constructed today with dedicated electrical substations to feed them.",
      "So queries per Joule (qpj) is a significant determinant of the overall cost of your solution.",
      "These challenges necessitate the question: Can we build a cost-effective cluster for data-intensive workloads that uses less than a tenth of the power required by a conventional architecture, but that still meets the same capacity, availability, throughput, and latency requirements?",
      "Andersen et al. set out to do just this with FAWN by building a fast array of \u201cwimpy\u201d (low-power AMD) nodes with SSDs, and a key value store on top designed to work well on this hardware platform.",
      "A  21-node FAWN cluster built with 500MHz CPUs (that were old even in 2009 when this paper was written) achieved 364 qpj, two orders of magnitude better than traditional disk-based clusters.",
      "Each node could support up to 1300 256 byte queries per second, enough to exploit nearly all the raw capacity of its attached SSDs.",
      "Total power consumption under 5W per node, compared to an Intel processor consuming about 83-90W under load.",
      "The workload under study is a key-value store.",
      "For other big data workloads, it turns out that the network and disk may not be the bottleneck, it could still be the CPU !",
      "This suggests yet another way of addressing the imbalance \u2013 do lots of expensive serialization and deserialization!",
      "That wouldn\u2019t be my top recommendation ;).",
      "In their analysis, the authors consider dataset size and desired query rate, and provide recommendations as to the most cost-effective hardware+software platform to meet the targets.",
      "The result is shown in Figure 16 in the paper (reproduced below) \u2013 which is striking for showing that there is only a narrow band in which traditional approaches actually make sense for the small random access workloads (e.g. K-V store) studied!",
      "Let\u2019s take a look at FAWN\u2019s hardware choices and rationale, and then dive into a few details of the K-V store built on top\u2026  FAWN Hardware  FAWN couples low-power, efficient embedded CPUs with flash storage to provide efficient, fast, and cost-effective access to large, random-access data.",
      "Flash is significantly faster than disk, much cheaper than the equivalent amount of DRAM, and consumes less power than either.",
      "CPU power consumption grows super-linearly with speed, therefore you can get a better price-performance point by moving back down the curve:  A FAWN cluster\u2019s slower CPUs dedicate more transistors to basic operations.",
      "These CPUs execute significantly more instructions per Joule than their faster counterparts: multi-GHz superscalar quad-core processors can execute approximately 100 million instructions per Joule, assuming all cores are active and avoid stalls or mispredictions.",
      "Lower-frequency in-order CPUs, in contrast, can provide over 1 billion instructions per Joule\u2014an order of magnitude more efficient while still running at 1/3rd the frequency.",
      "Flash devices support fast random reads and efficient I/O,  but with slower random writes.",
      "Flash devices consume less than one Watt even under heavy load, whereas mechanical disks can consume over 10 W at load.",
      "Flash is over two orders of magnitude more efficient than mechanical disks in terms of queries/Joule.",
      "The evaluation hardware consisted of single-core 500 MHz AMD Geode LX processors, with 256 MB DDR SDRAM operating at 400 MHz, and 100 Mbit/s Ethernet.",
      "Each node contained one 4 GB Sandisk Extreme IV CompactFlash device.",
      "FAWN K-V Store  The FAWN data store is a log-structured key-value store.",
      "FAWN-DS is designed specifically to perform well on flash storage and to operate within the constrained DRAM available on wimpy nodes: all writes to the datastore are sequential, and reads require a single random access.",
      "To provide this property, FAWN-DS maintains an in-DRAM hash table (Hash Index) that maps keys to an offset in the append-only Data Log on flash\u2026.",
      "The key design choice in FAWN-KV is the use of a log- structured per-node datastore called FAWN-DS that provides high performance reads and writes using flash memory.",
      "This append-only data log provides the basis for replication and strong consistency using chain replication between nodes.",
      "FAWN backends divide up the key space using consistent hashing, with each physical node responsible for multiple key ranges.",
      "Chain replication is used between nodes.",
      "Individual nodes use an in-memory hash index to map keys to values stored in the data log.",
      "To save space only a fragment of the key (the index bits) is kept in the index.",
      "This allows for a small probability that the key retrieved is not actually the one being sought (collision).",
      "In this situation hash-chaining is used continue searching the hash table.",
      "With the 15-bit key fragment, only 1 in 32,768 retrievals from the flash will be incorrect and require fetching an additional record.",
      "A smaller tier (about 1:80) of front-end nodes sits in front of the data storing back ends.",
      "Each front-end node manages the VID membership list and queries for a large contiguous chunk of the key space (in other words, the circular key space is divided into pie-wedges, each owned by a front-end).",
      "A front-end receiving queries for keys outside of its range forwards the queries to the appropriate front-end node.",
      "This design either requires clients to be roughly aware of the front-end mapping, or doubles the traffic that front-ends must handle, but it permits front ends to cache values without a cache consistency protocol.",
      "The front-end and back-end nodes implement a two-level caching hierarchy.",
      "Front-end nodes maintain a small high-speed query cache that reduces latency and helps managed hot spots.",
      "Back-ends implicitly cache recently accessed data in their file system buffer cache.",
      "About 1300 queries per second can be served from flash, but 85,000 queries per second from the buffer cache."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://www.sigops.org/sosp/sosp09/papers/andersen-sosp09.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 30995172
  },
  {
    "blog_id": "scootr-scaling-r-dataframes-on-dataflow-systems",
    "summary": [
      "ScootR: scaling R dataframes on dataflow systems Kunft et al., SoCC\u201918  The language of big data is Java ( / Scala).",
      "The languages of data science are Python and R. So what do you do when you want to run your data science analysis over large amounts of data?",
      "\u2026programming languages with rich support for data manipulation and statistics, such as R and Python, have become increasingly popular\u2026 [but]\u2026 they are typically designed for single machine and in-memory usage\u2026.",
      "In contrast, parallel dataflow systems, such as Apache Flink and Apache Spark, are able to handle large amounts of data.",
      "However, data scientists are often unfamiliar with the systems\u2019 native language and programming abstraction, which is crucial to achieve good performance.",
      "A tempting solution is to embed Python / R support within the dataflow engine.",
      "There are two basic approaches to this today:  Keep the guest language components in a separate process and use IPC (inter-process communication) to exchange input and output data between the dataflow engine and the guest language process.",
      "This approach can support the full power of the guest language, but pays a heavy price in IPC and serialisation costs.",
      "Use source-to-source (STS) translation to translate guest language code into the dataflows native API.",
      "The translated code can achieve near native performance, but comprehensive source-to-source translation is difficult and so tends to be restricted to a subset of the guest language functions and libraries.",
      "SparkR is of interest here because it supports both STS translation and IPC.",
      "It uses STS where possible, and falls back to IPC outside of this.",
      "Executing a simple user-defined function via IPC is about 100x slower than native execution after STS translation:  Clearly what we want is the performance of native execution (STS), but with the flexibility to use the full scope of the guest language and libraries.",
      "In theory we could just invest in building better and better STS translators (e.g., R to Java in this case), but this entails a huge effort and results in a hard-to-maintain point solution.",
      "When faced with an M:N style integration problem, it often pays to look for a common intermediate representation (IR).",
      "If only there was some IR which both Java and R (and Python, Ruby, JavaScript,\u2026 ) could compile into, then we could build the inter-operation at the IR level.",
      "The JVM has byte code, LLVM has bitcode, and a little bit closer to home, Weld has an IR based on linear types.",
      "In this research, Graal and Truffle provide the common ground (see One VM to rule them all ).",
      "Graal, Truffle, and FastR  Truffle is a language implementation framework that supports development of high performance language runtimes through self-optimising AST interpreters.",
      "The ASTs collect profiling information at runtime and specialise their structure accordingly.",
      "Languages built on top of Truffle can efficiently exchange data and access functions.",
      "Graal is a dynamic compiler that produces highly-optimised machine code as soon as a Truffle AST reaches a stable state.",
      "De-optimisations and speculation failures are handled automatically by falling back to the AST.",
      "The GraalVM is a multi-language execution runtime capable of running multiple languages in the same virtual machine instance, with full interoperability between all its supported languages.",
      "GraalVM can execute Java applications on top of the HotSpot Java VM, and can execute other Truffle-based language runtimes such as JavaScript, Ruby, Python, and LLVM.",
      "One of the default languages of the GraalVM is fastR, a high-performance GNU-R compatible R language runtime implemented using Truffle and relying on the Graal dynamic compiler.",
      "fastR supports the C API from GNU-R, and so can support many of the R packages that depend on underlying C implementations.",
      "Some R packages rely on GNU-R internals which make fastR integration harder, but fastR is continually being enhanced to support these too.",
      "Introducing ScootR  ScootR builds on the capabilities of the GraalVM to expose Flink\u2019s internal data structures to the fastR engine.",
      "First ScootR creates an execution plan based on the R source code, and then this plan can be deployed and executed on a Flink cluster.",
      "R user-defined functions (UDFs) are executed in parallel by each worker node, and automatically optimised by the Graal JIT compiler.",
      "Here\u2019s an example R application making use of the ScootR dataframe API:  R dataframes are mapped to Flink TupleN dataset types, and invocations to ScootR\u2019s API are mapped to Flink operators via new Truffle AST nodes (_RBuiltinNode_s).",
      "For R functions that involve user-defined code, ScootR needs to infer the input and output types.",
      "Input types are specified by the user when reading files (e.g., line 7 in the listing above).",
      "For output types ScootR just instantiates a temporary tuple, invokes the function, and inspects the output.",
      "For efficient access of Java data types within R and vice-versa, ScootR makes use of Truffle\u2019s language interoperability features.",
      "R functions are also rewritten to create and return Flink tuples directly in the R function.",
      "Here\u2019s an example of the execution plan generated for the sample R program above.",
      "Only the apply function includes user-defined code, all other functions are replaced with the corresponding Flink operators during the plan generation phase.",
      "Evaluation  The evaluation is based on two datasets:  The Airline On-Time Performance Dataset with arrival data for US flights, converted into CSV format.",
      "The resulting file size is 9.5GB.",
      "The Reddit Comments Dataset (four consecutive months\u2019 worth, at about 14GB per month in CSV format).",
      "ScootR is compared against native GNU-R, fastR, and SparkR, as well as against natively coded pipelines in Spark and Flink.",
      "Here are the single node and cluster comparisons for an ETL pipeline on the airline data as follows:  Key findings from the evaluation are as follows:  For non-UDF functions, both ScootR and SparkR provide reliable mapping of R functions to native API calls with overhead below 1.2x.",
      "With UDFs, ScootRs performance is competitive with SparkR\u2019s STS approach when SparkR is able to use STS, and an order of magnitude faster when SparkR has to fallback to IPC.",
      "Total overheads in operator pipelines (vs fully native) are up to 1.2x for SparkR with STS, and up to 1.4x for ScootR.",
      "Both SparkR and ScootR outperform GNU-R and fastR, even for single-threaded execution on a single node.",
      "One possible direction for future work is to integrate other dynamic languages with Truffle support, such as JavasScript or Python."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://www.user.tu-berlin.de/akunft/paper/socc18-paper35.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 37135417
  },
  {
    "blog_id": "fast_r-cnn",
    "summary": [
      "What  The original R-CNN had three major disadvantages:  Two-staged training pipeline: Instead of only training a CNN, one had to train first a CNN and then multiple SVMs.",
      "Expensive training: Training was slow and required lots of disk space (feature vectors needed to be written to disk for all region proposals (2000 per image) before training the SVMs).",
      "Slow test: Each region proposal had to be handled independently.",
      "Fast R-CNN ist an improved version of R-CNN and tackles the mentioned problems.",
      "It no longer uses SVMs, only CNNs (single-stage).",
      "It does one single feature extraction per image instead of per region, making it much faster (9x faster at training, 213x faster at test).",
      "It is more accurate than R-CNN.",
      "How  The basic architecture, training and testing methods are mostly copied from R-CNN.",
      "For each image at test time they do:  They generate region proposals via selective search.",
      "They feed the image once through the convolutional layers of a pre-trained network, usually VGG16.",
      "For each region proposal they extract the respective region from the features generated by the network.",
      "The regions can have different sizes, but the following steps need fixed size vectors.",
      "So each region is downscaled via max-pooling so that it has a size of 7x7 (so apparently they ignore regions of sizes below 7x7...?).",
      "This is called Region of Interest Pooling (RoI-Pooling).",
      "During the backwards pass, partial derivatives can be transferred to the maximum value (as usually in max pooling).",
      "That derivative values are summed up over different regions (in the same image).",
      "They reshape the 7x7 regions to vectors of length F*7*7, where F was the number of filters in the last convolutional layer.",
      "They feed these vectors through another network which predicts:  The class of the region (including background class).",
      "Top left x-coordinate, top left y-coordinate, log height and log width of the bounding box (i.e. it fine-tunes the region proposal's bounding box).",
      "These values are predicted once for every class (so K*4 values).",
      "Architecture as image:  Sampling for training  Efficiency  If batch size is B it is inefficient to sample regions proposals from B images as each image will require a full forward pass through the base network (e.g. VGG16).",
      "It is much more efficient to use few images to share most of the computation between region proposals.",
      "They use two images per batch (each 64 region proposals) during training.",
      "This technique introduces correlations between examples in batches, but they did not observe any problems from that.",
      "They call this technique \"hierarchical sampling\" (first images, then region proposals).",
      "IoUs  Positive examples for specific classes during training are region proposals that have an IoU with ground truth bounding boxes of >=0.5.",
      "Examples for background region proposals during training have IoUs with any ground truth box in the interval (0.1, 0.5].",
      "Not picking IoUs below 0.1 is similar to hard negative mining.",
      "They use 25% positive examples, 75% negative/background examples per batch.",
      "They apply horizontal flipping as data augmentation, nothing else.",
      "Outputs  For their class predictions the use a simple softmax with negative log likelihood.",
      "For their bounding box regression they use a smooth L1 loss (similar to mean absolute error, but switches to mean squared error for very low values).",
      "Smooth L1 loss is less sensitive to outliers and less likely to suffer from exploding gradients.",
      "The smooth L1 loss is only active for positive examples (not background examples).",
      "(Not active means that it is zero.)",
      "Training schedule  The use SGD.",
      "They train 30k batches with learning rate 0.001, then 0.0001 for another 10k batches.",
      "(On Pascal VOC, they use more batches on larger datasets.)",
      "They use twice the learning rate for the biases.",
      "They use momentum of 0.9.",
      "They use parameter decay of 0.0005.",
      "Truncated SVD  The final network for class prediction and bounding box regression has to be applied to every region proposal.",
      "It contains one large fully connected hidden layer and one fully connected output layer (K+1 classes plus K*4 regression values).",
      "For 2000 proposals that becomes slow.",
      "So they compress the layers after training to less weights via truncated SVD.",
      "A weights matrix is approximated via  U (u x t) are the first t left-singular vectors of W.  Sigma is a t x t diagonal matrix of the top t singular values.",
      "V (v x t) are the first t right-singular vectors of W.  W is then replaced by two layers: One contains Sigma V^T as weights (no biases), the other contains U as weights (with original biases).",
      "Parameter count goes down to t(u+v) from uv.",
      "Results  They try three base models:  AlexNet (Small, S)  VGG-CNN-M-1024 (Medium, M)  VGG16 (Large, L)  On VGG16 and Pascal VOC 2007, compared to original R-CNN:  Training time down to 9.5h from 84h (8.8x faster).",
      "Test rate with SVD (1024 singular values) improves from 47 seconds per image to 0.22 seconds per image (213x faster).",
      "Test rate without SVD improves similarly to 0.32 seconds per image.",
      "mAP improves from 66.0% to 66.6% (66.9% without SVD).",
      "Per class accuracy results:  Fast_R-CNN__pvoc2012.jpg  Fixing the weights of VGG16's convolutional layers and only fine-tuning the fully connected layers (those are applied to each region proposal), decreases the accuracy to 61.4%.",
      "This decrease in accuracy is most significant for the later convolutional layers, but marginal for the first layers.",
      "Therefor they only train the convolutional layers starting with conv3_1 (9 out of 13 layers), which speeds up training.",
      "Multi-task training  Training models on classification and bounding box regression instead of only on classification improves the mAP (from 62.6% to 66.9%).",
      "Doing this in one hierarchy instead of two seperate models (one for classification, one for bounding box regression) increases mAP by roughly 2-3 percentage points.",
      "They did not find a significant benefit of training the model on multiple scales (e.g. same image sometimes at 400x400, sometimes at 600x600, sometimes at 800x800 etc.).",
      "Note that their raw CNN (everything before RoI-Pooling) is fully convolutional, so they can feed the images at any scale through the network.",
      "Increasing the amount of training data seemed to improve mAP a bit, but not as much as one might hope for.",
      "Using a softmax loss instead of an SVM seemed to marginally increase mAP (0-1 percentage points).",
      "Using more region proposals from selective search does not simply increase mAP.",
      "Instead it can lead to higher recall, but lower precision.",
      "Using densely sampled region proposals (as in sliding window) significantly reduces mAP (from 59.2% to 52.9%).",
      "If SVMs instead of softmaxes are used, the results are even worse (49.3%)."
    ],
    "author_id": "ALEJU",
    "pdf_url": "https://arxiv.org/pdf/1504.08083",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 68399070
  },
  {
    "blog_id": "a_neural_algorithm_for_artistic_style",
    "summary": [
      "What  The paper describes a method to separate content and style from each other in an image.",
      "The style can then be transfered to a new image.",
      "Examples:  Let a photograph look like a painting of van Gogh.",
      "Improve a dark beach photo by taking the style from a sunny beach photo.",
      "How  They use the pretrained 19-layer VGG net as their base network.",
      "They assume that two images are provided: One with the content, one with the desired style.",
      "They feed the content image through the VGG net and extract the activations of the last convolutional layer.",
      "These activations are called the content representation.",
      "They feed the style image through the VGG net and extract the activations of all convolutional layers.",
      "They transform each layer to a Gram Matrix representation.",
      "These Gram Matrices are called the style representation.",
      "How to calculate a Gram Matrix:  Take the activations of a layer.",
      "That layer will contain some convolution filters (e.g. 128), each one having its own activations.",
      "Convert each filter's activations to a (1-dimensional) vector.",
      "Pick all pairs of filters.",
      "Calculate the scalar product of both filter's vectors.",
      "Add the scalar product result as an entry to a matrix of size #filters x #filters (e.g. 128x128).",
      "Repeat that for every pair to get the Gram Matrix.",
      "The Gram Matrix roughly represents the texture of the image.",
      "Now you have the content representation (activations of a layer) and the style representation (Gram Matrices).",
      "Create a new image of the size of the content image.",
      "Fill it with random white noise.",
      "Feed that image through VGG to get its content representation and style representation.",
      "(This step will be repeated many times during the image creation.)",
      "Make changes to the new image using gradient descent to optimize a loss function.",
      "The loss function has two components:  The mean squared error between the new image's content representation and the previously extracted content representation.",
      "The mean squared error between the new image's style representation and the previously extracted style representation.",
      "Add up both components to get the total loss.",
      "Give both components a weight to alter for more/less style matching (at the expense of content matching).",
      "One example input image with different styles added to it.",
      "Rough chapter-wise notes  Page 1  A painted image can be decomposed in its content and its artistic style.",
      "Here they use a neural network to separate content and style from each other (and to apply that style to an existing image).",
      "Page 2  Representations get more abstract as you go deeper in networks, hence they should more resemble the actual content (as opposed to the artistic style).",
      "They call the feature responses in higher layers content representation.",
      "To capture style information, they use a method that was originally designed to capture texture information.",
      "They somehow build a feature space on top of the existing one, that is somehow dependent on correlations of features.",
      "That leads to a \"stationary\" (?)",
      "and multi-scale representation of the style.",
      "Page 3  They use VGG as their base CNN.",
      "Page 4  Based on the extracted style features, they can generate a new image, which has equal activations in these style features.",
      "The new image should match the style (texture, color, localized structures) of the artistic image.",
      "The style features become more and more abtstract with higher layers.",
      "They call that multi-scale the style representation.",
      "The key contribution of the paper is a method to separate style and content representation from each other.",
      "These representations can then be used to change the style of an existing image (by changing it so that its content representation stays the same, but its style representation matches the artwork).",
      "Page 6  The generated images look most appealing if all features from the style representation are used.",
      "(The lower layers tend to reflect small features, the higher layers tend to reflect larger features.)",
      "Content and style can't be separated perfectly.",
      "Their loss function has two terms, one for content matching and one for style matching.",
      "The terms can be increased/decreased to match content or style more.",
      "Page 8  Previous techniques work only on limited or simple domains or used non-parametric approaches (see non-photorealistic rendering).",
      "Previously neural networks have been used to classify the time period of paintings (based on their style).",
      "They argue that separating content from style might be useful and many other domains (other than transfering style of paintings to images).",
      "Page 9  The style representation is gathered by measuring correlations between activations of neurons.",
      "They argue that this is somehow similar to what \"complex cells\" in the primary visual system (V1) do.",
      "They note that deep convnets seem to automatically learn to separate content from style, probably because it is helpful for style-invariant classification.",
      "Page 9, Methods  They use the 19 layer VGG net as their basis.",
      "They use only its convolutional layers, not the linear ones.",
      "They use average pooling instead of max pooling, as that produced slightly better results.",
      "Page 10, Methods  The information about the image that is contained in layers can be visualized.",
      "To do that, extract the features of a layer as the labels, then start with a white noise image and change it via gradient descent until the generated features have minimal distance (MSE) to the extracted features.",
      "The build a style representation by calculating Gram Matrices for each layer.",
      "Page 11, Methods  The Gram Matrix is generated in the following way:  Convert each filter of a convolutional layer to a 1-dimensional vector.",
      "For a pair of filters i, j calculate the value in the Gram Matrix by calculating the scalar product of the two vectors of the filters.",
      "Do that for every pair of filters, generating a matrix of size #filters x #filters.",
      "That is the Gram Matrix.",
      "Again, a white noise image can be changed with gradient descent to match the style of a given image (i.e. minimize MSE between two Gram Matrices).",
      "That can be extended to match the style of several layers by measuring the MSE of the Gram Matrices of each layer and giving each layer a weighting.",
      "Page 12, Methods  To transfer the style of a painting to an existing image, proceed as follows:  Start with a white noise image.",
      "Optimize that image with gradient descent so that it minimizes both the content loss (relative to the image) and the style loss (relative to the painting).",
      "Each distance (content, style) can be weighted to have more or less influence on the loss function."
    ],
    "author_id": "ALEJU",
    "pdf_url": "http://arxiv.org/pdf/1508.06576",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 73180675
  },
  {
    "blog_id": "hmtl-multi-task-learning-for-state-of-the-art-nlp-245572bbb601",
    "summary": [
      "Sanh et al. (2018) propose a new method based on multi-task learning, trained in a hierarchical fashion, to achieve state of the art results on various NLP tasks such as Named Entity Recognition (NER), Entity Mention Detection (EMD) and Relation Extraction (RE).",
      "(See brief definitions of these tasks at the end of this article if you are not familiar with them.)",
      "The main question this paper aims to address is whether linguistic hierarchies and multi-task learning can be leveraged to improve results on the above-mentioned semantic-related tasks.",
      "Previously, multi-task frameworks have not been trained to leverage the strengths of inductive transfer to achieve more generalized capabilities.",
      "Complimentary aspects of a sentence (e.g., syntax and word order) can be combined to produce generalized sentence embeddings.",
      "This paper proposes a unified model that trains and combines four semantic NLP tasks on the basis that they share inter-dependencies with each other.",
      "For instance, in the example provided in the table below (highlighted in blue) you can observe that by resolving that \u201cDell\u201d and \u201cthe company\u201d refer to the same real-world entity, it is also likely to represent an organization as opposed to a person.",
      "This knowledge could be beneficial and transferred to other tasks such as EMD and NER.",
      "Image source  The proposed model (shown in the figure below) consists of a hierarchy between the tasks in the lower levels while encouraging complex interactions at deeper layers.",
      "This implies that simple supervised tasks will be placed at lower layers and more complex tasks at the higher layers.",
      "This is done in an end-to-end setting and without using hand-engineered features.",
      "A new sampling strategy for multi-task learning, called proportional sampling, is also proposed (more on this later).",
      "HMTL framework\u2014 Image source  Hierarchical Model  The input of the model consists of the concatenations of three type of word embeddings: fine-tuned GloVe embeddings, ELMo embeddings, and character-level embeddings.",
      "The first group of layers in the model are supervised by NER labels where the inputs are the concatenated embeddings and the output represents the hidden states produced by the biLSTMs.",
      "A CRF tagging layer represents the last layer in this group as seen in the model figure above.",
      "The second group of layers is supervised by EMD labels where the input is the concatenation of the output of lower layers and input embeddings, and the output represents sequence embeddings.",
      "Similar to NER, CRF is used to make tagging decisions.",
      "Note that the input contains information from the lower layers, establishing the hierarchical architecture.",
      "The highest level of the model is supervised by a Coreference resolution (CR) task where the input is the concatenated embeddings combined with the output of the lower layers, and the outputs are fed to the mention pair scorer.",
      "In this same level of the architecture, the model is also supervised by the RE task.",
      "The RE task involves identifying mentions and classifying their relations, thus it also tries to link mentions similar to the CR task (refer to the paper for more details).",
      "Experiments  Overall, two datasets are used for the experiments.",
      "For NER, the English portion of OntoNotes 5.0 ( Pradhan et al. 2013 ) is used.",
      "For CR, EMD, and RE, the ACE05 corpus ( Doddington et al. 2004 ) is used.",
      "Refer to the paper for more details on these datasets and how they were used.",
      "Data statistics can be found in the table below:  Image source  To avoid catastrophic forgetting (an issue common when training multi-task models), a simple yet effective training method is employed.",
      "Specifically, after each parameter update, a task from the pipeline is randomly selected and batches linked to this task are also randomly sampled.",
      "The sampling of a task is achieved using proportional sampling which is a function of the relative size of a dataset compared to the cumulative size of all datasets.",
      "Results  In summary, the proposed hierarchical and multi-task learning framework, coined HMTL, achieved state-of-the-art (SOTA) results on three tasks, namely NER ( +0.52 ), EMD( +3.8 ), and RE ( +6.8 ).",
      "The results are summarized in the table below:  Image source  The full model (A-GM) model (highlighted in blue) produces SOTA results for EMD and RE.",
      "These results suggest that having different type of information on different sentences produces valuable richer information.",
      "B, C, D, E, and E-GM are all single-task setups (highlighted in pink) which are outperformed by the full model (A) with exception of the EMD task.",
      "However, A-GM outperforms the single-task setup of EMD, keeping in mind that this model makes use of gold mentions.",
      "For the rest of the setups (e.g., F, G, etc.",
      "), varying combinations of tasks are used during training (highlighted in green).",
      "These results show how much much one task or tasks can contribute to the other/s.",
      "Note that the authors also experimented with the order of the tasks such (e.g., F vs. K) and how this decision influenced results.",
      "The table below shows the ablation study conducted on the input embeddings:  Image source  You can observe the strength of the contextualized ELMo embeddings by the differences shown in the metrics.",
      "In addition, the authors also discuss what the encoders and embeddings in the multi-task, hierarchical architecture are learning through various probing tasks (see full details in the paper).",
      "In the table below, you can also observe the differences in training time (defined by parameter updates) between a multi-task framework and single-task framework.",
      "The lower the values in the time column and the higher the performance the better the results.",
      "Image source  References  Paper: A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks \u2014 (Victor Sanh, Thomas Wolf, Sebastian Ruder)  Code: GitHub repo  Online Demo: HMTL for NLP  Detailed post (includes code snippets): HuggingFace Blog  Task Definitions  Named Entity Recognition (NER) aims to identify mentions of named entities in a sequence and classify them into predefined categories.",
      "Entity Mention Detection (EMD) is similar to NER but more general as it aims to identify all the mentions related to a real-life entity, whereas NER only focuses on the named entities.",
      "Coreference resolution (CR) aims to identify mentions that are referring to the same real-life entity and cluster them together.",
      "Relation Extraction (RE) aims to identify semantic relational structure between entity mentions in unstructured text."
    ],
    "author_id": "DAIR-AI",
    "pdf_url": "https://arxiv.org/pdf/1811.06031",
    "author_full_name": "Elvis Saravia",
    "source_website": "https://github.com/dair-ai/nlp_paper_summaries",
    "id": 2852538
  },
  {
    "blog_id": "towards-a-hands-free-query-optimizer-through-deep-learning",
    "summary": [
      "Towards a hands-free query optimizer through deep learning Marcus & Papaemmanouil, CIDR\u201919  Where the SageDB paper stopped\u2014 at the exploration of learned models to assist in query optimisation\u2014 today\u2019s paper choice picks up, looking exclusively at the potential to apply learning (in this case deep reinforcement learning) to build a better optimiser.",
      "Why reinforcement learning?",
      "Query optimisers are traditionally composed of carefully tuned and complex heuristics based on years of experience.",
      "Feedback from the actual execution of query plans can be used to update cardinality estimates.",
      "Database cracking, adaptive indexing , and adaptive query processing all incorporate elements of feedback as well.",
      "In this vision paper, we argue that recent advances in deep reinforcement learning (DRL) can be applied to query optimization, resulting in a \u201chands-free\u201d optimizer that (1) can tune itself for a particular database automatically without requiring intervention from expert DBAs, and (2) tightly incorporates feedback from past query optimizations and executions in order to improve the performance of query execution plans generated in the future.",
      "If we view query optimisation as a DRL problem, then in reinforcement learning terminology the optimiser is the agent, the current query plan is the state, and each available action represents an individual change to the query plan.",
      "The agent learns a policy which informs the actions it chooses under differing circumstances.",
      "Once the agent decides to take no further actions the episode is complete and the agent\u2019s reward is (ideally) a measure of how well the generated plan actually performed.",
      "There are a number of challenges, explored in this paper, with making this conceptual mapping work well in practice.",
      "Not least of which is that evaluating the reward function (executing a query plan to see how well it performs) is very expensive compared to e.g. computing the score in an Atari game.",
      "The ReJOIN join order enumerator  ReJOIN explores some of these ideas on a subset of the overall query optimisation problem: learning a join order enumerator.",
      "Each query sent to ReJOIN is an episode, the state represents subtrees of a binary join tree together with information about the query join and selection predicates.",
      "Actions combine two subtrees into a single tree.",
      "Once all input relations are joined the episode ends, and ReJOIN assigns a reward based on the optimiser\u2019s cost model.",
      "It\u2019s policy network is updated on the basis of this score.",
      "The final join ordering is passed to the optimiser to complete the physical plan.",
      "Using the optimiser\u2019s cost model as a proxy for the ultimate performance of a generated query plan enables join orderings to be evaluated much more quickly.",
      "The following chart shows how ReJOIN learns to produce good join orders during training.",
      "It takes nearly 9000 episodes (queries) to become competitive with PostgreSQL.",
      "Once ReJOIN has caught up with PostgreSQL, it goes on to surpass it, producing orderings with lower cost.",
      "Also of note here is that after training, ReJOIN produces its query plans faster than PostgreSQL\u2019s built-in join enumerator in many cases.",
      "The bottom-up nature of ReJOIN\u2019s algorithm is  , whereas PostgreSQL\u2019s greedy bottom-up algorithm is  .",
      "In addition to being limited to just join ordering, ReJOIN\u2019s use of the query optimiser\u2019s cost model to generate reward signals means that it is still dependent on a well-tuned cost model, which is a big part of the problem we wanted to solve in the first place.",
      "Ideally we\u2019d like to extend the approach to handle full physical plan generation, and also remove the dependency on having an existing cost model.",
      "Challenges in extending the approach  Once we go from just join ordering to the full search space including operator and access path selection etc., the approach from ReJOIN is unable to learn effective polities in reasonable time.",
      "An initial model failed to out-perform random choice even after 72 hours of training.",
      "If we use actual query execution time to generate the reward, then initial policies which will often generate very inefficient plans will take a long time to obtain a reward signal.",
      "Thus we learn the slowest at exactly the point when we\u2019d like to learn the fastest and the system takes a prohibitive amount of time to converge to good results.",
      "(An experiment with ReJOIN using real query latency instead of optimiser cost confirmed this).",
      "Finally, query latency as a reward signal doesn\u2019t meet the expectations of many DRL algorithms that the reward signal is dense and linear.",
      "A dense reward signal is one that provides incremental feedback with every action the agent takes (such as the score updating in an Atari game), not just at the end of the episode.",
      "The linear assumption means that an algorithm may attempt to maximise the sum of many small rewards within an episode.",
      "We have identified how the large search space, delayed reward signal, and costly performance indicators provide substantial hurdles to naive applications of DRL to query optimization.",
      "Should we just give up on the idea then?",
      "All is not lost yet!",
      "The last section of the paper details a number of alternative approaches that could overcome some of these hurdles.",
      "Research directions  Three techniques that may help to make DRL-based query optimisation practical again are learning from demonstration, cost-model bootstrapping, and incremental learning.",
      "We\u2019ll look at each of these briefly in turn next (there are no results from applying or evaluating these ideas as yet).",
      "Learning from demonstration  In learning from demonstration, a model is first trained to mimic the behaviour of an existing expert, and then goes on to learn directly from its actions.",
      "In the context of query optimisation, we would first train a model to mimic the actions taken by an existing optimiser (indexes, join orderings, pruning of bad plans etc.",
      "), and then switch to optimising queries directly bypassing the \u2018mentor\u2019 optimiser.",
      "In this second phase the agent fine-tunes its own policy.",
      "The advantage of this strategy is that mimicking the existing optimiser in the early stages helps the optimiser agent to avoid the \u2018obviously bad\u2019 parts of the search space.",
      "Since the behavior of the model in the second phase should not initially stray too far from the behavior of the expert system, we do not have to worry about executing any exceptionally poor query plans.",
      "Additionally, since the second training phase only needs to fine-tune an already-performant model, the delayed reward signal is of far less consequence.",
      "Cost model bootstrapping  Cost model bootstrapping uses a very similar in spirit two-phase approach.",
      "In the first phase, instead of learning to mimic the actions of an existing expert optimiser, the judgements of the existing expert optimiser (i.e., it\u2019s cost model) are used to bring the agent to an initial level of competence.",
      "The optimiser\u2019s cost model is used as the reward signal during initial training, and once the agent has learned to produce good policies according to the cost model, it is then switched to learning from a reward based on actual query latency.",
      "One complication is doing this is that the reward units (scale) need to be consistent across the costs produced by the query optimiser cost model, and the latency measurements of actual query executions.",
      "We could apply some kind of normalisation across the two, or alternatively transfer the weights from the first network to a new network (transfer learning).",
      "Incremental learning  Incremental learning attempts to mitigate the issues stemming from poor query plans early in the learning cycle by beginning learning on simpler problems:  \u2026 incrementally learning query optimization by first training a model to handle simple cases and slowly introducing more complexity.",
      "This approach makes the extremely large search space more manageable by dividing it into smaller pieces.",
      "In the context of query optimisation, we can make problems easier by reducing the number of relations, or by reducing the number of dimensions to be considered.",
      "That leads to a problem space that looks like this:  We could therefore try starting with a small number of pipeline phases and gradually introducing more, as shown in the following figure:  Or we could try starting with small examples and gradually focus on larger and larger queries.",
      "Maybe a hybrid strategy will be best, starting with join order and small queries, and gradually increasing sophistication in both dimensions."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://cidrdb.org/cidr2019/papers/p96-marcus-cidr19.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 53935450
  },
  {
    "blog_id": "statistical-foundations-of-virtual-democracy",
    "summary": [
      "Statiscal foundations of virtual democracy Kahng et al., ICML\u201919  This is another paper on the theme of combining information and making decisions in the face of noise and uncertainty \u2013 but the setting is quite different to those we\u2019ve been looking at recently.",
      "Consider a food bank that receives donations of food and distributes it to those in need.",
      "The goal is to implement an automated decision making system such that when a food donation is received, the system outputs the organisation (e.g. housing authority or food pantry) that should receive it.",
      "We could hard code a set of rules, but what should they be?",
      "And who gets to decide?",
      "A democratic solution to this would be to give each of the stakeholders a vote on every decision.",
      "In the food bank setting, identified classes of stakeholders include the donors, the recipients, the volunteers (who pick up food from the donor and deliver it to the recipient), and employees.",
      "Their votes encode their own preferences and biases, perhaps in a way that even the voters themselves couldn\u2019t neatly codify in a set of explicit rules.",
      "It\u2019s not really practical to have an actual vote with all stakeholders participating every time a food donation is made though!",
      "One of the most basic ideas underlying democracy is that complicated decisions can be made by asking a group of people to vote on the alternatives at hand.",
      "As a decision-making framework, this paradigm is versatile, because people can express a sensible opinion about a wide range of issues.",
      "One of its seemingly inherent shortcomings, though, is that voters must take the time to cast a vote\u2014 hopefully an informed one\u2014 every time a new dilemma arises.",
      "The big idea behind virtual democracy is that we learn the voting preferences of each stakeholder, essentially creating an agent which is able to vote in their place, a virtual voter.",
      "Then when we need to make a decision we ask those virtual voters to cast their votes (in the form of a preference ranking).",
      "The central question in this paper is this: given a set of preference rankings, how should we combine them to produce an actual decision?",
      "The procedure for doing this is known as the voting rule.",
      "\u2026 the choice of voting rule can have a major impact on the efficacy of the system.",
      "In fact, the question of which voting rule to employ is one of the central questions in computational social choice.",
      "It\u2019s one thing to come up with a voting rule that works well when we have the actual true preference rankings of all of the stakeholders.",
      "In a virtual democracy setting though, where we have learned approximations to those preference rankings, a highly desirable feature of a voting rule is that it is robust to noise.",
      "I.e., we want a voting rule whereby\u2026  \u2026 the output on the true preferences is likely to coincide with the output on noisy estimates thereof.",
      "Learning preferences  To learn voter preferences, voters are asked to make a set of pairwise comparisons (about 100) between alternatives.",
      "I.e., given this donation, should it be sent to recipient A or recipient B?",
      "Each alternative is presented as a set of pre-determined features.",
      "In the case of the food bank question voters are given information about the type of donation, and seven additional features such as distance between the donor and recipient, and when the recipient last received a donation.",
      "At the end of this process, the training data is used to learn a model of the preferences of the voter.",
      "This model is then used to predict the voter\u2019s preference ranking over many hundreds of recipients for a given donation.",
      "The Mallows model  To be able to compare the efficacy of various voting rules, we\u2019re going to need a way to compare how good their outputs are.",
      "The Kendall tau (KT) distance between two rankings (permutations) of a set is defined as the number of pairs of alternatives on which the rankings disagree.",
      "By disagree we mean that given a pair  one ranks  ahead of  , and the other ranks  ahead of  .",
      "For example, the KT distance between  and  is 2.",
      "The Mallows (1957) model was originally designed for use in situations where there is true ranking of the alternatives, and assigns a probability that a given voter is associated with a given alternative ranking.",
      "The probability decreases exponentially with the number of pairs of alternatives on which the true and alternative ranking disagree, i.e., their KT distance.",
      "A Mallows model is parameterised by a  parameter  .",
      "Our technical approach relies on the observation that the classic Mallows (1957).",
      "model is an unusually good fit with our problem.",
      "In the problem at hand, instead of a single true ranking, each voter has their own true ranking.",
      "When validating a learned  model, the test for accuracy is done using pairwise comparisons, just like in Mallows.",
      "Given an observed prediction accuracy  , we can relate this accuracy to an underlying Mallows model through a parameter  , where pairwise comparisons are drawn from within the top  ranked items in the true ranking.",
      "(See \u00a73 in the paper).",
      "Voting rules and the Borda count  The next piece of the puzzle is the selection of a voting rule to combine rankings and produce a final decision.",
      "The main result in the paper concerns the Borda count voting rule.",
      "Borda count is a positional scoring rule.",
      "Positional scoring rules give a score vector that assigns points to each position in a ranking.",
      "E.g. 5 points for being ranked first, 3 points for being ranked second, and so on.",
      "The score of an alternative is the sum of its ranking points across all of the voters.",
      "The alternative with the biggest score wins (break ties via random selection).",
      "The Borda count uses a very straightforward score vector: if there are  alternatives in the ranking, the score vector is defined as  .",
      "The heart of the paper is \u00a74, where drawing on the properties of the Mallows model, it\u2019s relationship to the predicted accuracy, and the Borda count rule, the authors show the Borda count is surprisingly robust to noise.",
      "I\u2019m going to happily skip over the proofs here and leave you to follow up on those if you\u2019re interested!",
      "\u2026 it is intuitive that the separation in Borda scores has to depend on  , but it is encouraging (and, to us, surprising) that his dependence is almost linear\u2026 the theorem implies that our noisy Borda ranking is highly unlikely to make mistakes on pairs of alternatives whose average score difference is linear in  .",
      "Other rules  So far so good, but what about other voting rules?",
      "Are they also robust to noise or is there something special about the Borda count?",
      "The main alternative to positional scoring rules are pairwise-majority consistent (PMC) rules, of which there are many examples (e.g., the ranked pairs method).",
      "The key result in \u00a75 of the paper is that all rules in this class are not robust to noise.",
      "It is instructive to contrast our positive result, Theorem 1, with this negative result.",
      "On a very high level, the former result asserts that \u201cif Borda count says that the gaps between alternatives are signi\ufb01cant, then the alternatives will not \ufb02ip under Borda count,\u201d whereas the latter says \u201ceven if a PMC rule says that the gaps between alternatives are very signi\ufb01cant, some alternatives are likely to \ufb02ip under that rule.\u201d  Borda count FTW  So there you have it: if you need to robustly combine noisy rankings of alternatives to make a decision, use the Borda count!",
      "Our theoretical and empirical results identify Borda count as an especially attractive voting rule for virtual democracy, from a statistical viewpoint.",
      "Another important feature of the Borda count rule is that the decisions it takes can be easily explained.",
      "An explanation consists of two elements: first the average position in the predicted preferences of each of the stakeholder groups, and second the features that were most important in achieving that ranking position (possible since alternatives are presented as vectors of features)."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://proceedings.mlr.press/v97/kahng19a/kahng19a.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 10587146
  },
  {
    "blog_id": "why-neurons-have-thousands-of-synapses-a-theory-of-sequence-memory-in-neocortex",
    "summary": [
      "Why neurons have thousands of synapses, a theory of sequence memory in neocortex Hawkins & Ahmad, Front.",
      "Neural Circuits 2016  It all began with a fascinating lunchtime conversation with Martin Thompson (@mjpt777), who mentioned to me a thought-provoking video he\u2019d seen online from Jeff Hawkins regarding models of behaviour in the brain.",
      "A few days later I watched the video, then I bought the book \u201c On Intelligence \u201d and devoured it in one sitting.",
      "That book was written in 2004, and I was curious to learn of developments in the theory since.",
      "So we\u2019re starting this week with a selection of three papers bringing the story up to date.",
      "There\u2019s also a link to the Turing Test that we finished up with last week.",
      "In the book, Hawkins stresses the central role of prediction in how our brain works, and ultimately therefore in what it means to be thinking:  Prediction is not just one of the things your brain does.",
      "It is the primary function of the neocortex, and the foundation of intelligence.",
      "The cortex is an organ of prediction.",
      "If we want to understand what intelligence is, what creativity is, how your brain works, and how to build intelligent machines, we must understand the nature of these predictions and how the cortex makes them.",
      "Even behavior is best understood as a by-product of prediction.",
      "And:  We can now see where Alan Turing went wrong, prediction, not behavior, is the proof of intelligence\u2026.",
      "the Turing Test, by equating intelligence with human behavior, limited our vision of what is possible.",
      "Today\u2019s paper choice looks at neurons in the brain, and the question of why they have so many excitatory synapses (thousands of them).",
      "Many of these aren\u2019t modelled in the neural networks of machine learning.",
      "What do they do?",
      "And with that understanding, what would it look like to extend our neural network models to include a similar capability?",
      "A neuron has a soma (cell body), an axon, and a network of dendrites .",
      "Synapses are the structures that pass signals between neurons.",
      "Synapses close to the cell body are called proximal.",
      "Those farther away are called distal.",
      "The activation of a distal synapse doesn\u2019t seem to have much effect at the soma, so what are they for?",
      "If several distal synapses close to each other are activated together (an active dendrite), then a local spike can depolarise the soma.",
      "A slightly depolarized cell fires earlier than it would otherwise if it subsequently receives sufficient feedforward input.",
      "By firing earlier it inhibits neighboring cells, creating highly sparse patterns of activity for correctly predicted inputs.",
      "Neurons as pattern recognisers  It takes 8-20 close together synapses to activate at the same time, which then combine in a non-linear fashion, to trigger a dendritic spike.",
      "Thus, a small set of neighboring synapses acts as a pattern detector.",
      "It follows that the thousands of synapses on a cell\u2019s dendrites act as a set of independent pattern detectors.",
      "The detection of any of these patterns causes an NDMA spike and subsequent depolarisation at the soma.",
      "When relatively few neurons are active relative to the population, then such pattern recognition is robust.",
      "That is, the chances of a false match become very low.",
      "By forming more synapses than the the minimum needed to trigger a spike, recognition also becomes robust to noise and variation.",
      "A single dendritic segment can contain several hundred synapses.",
      "If we assume an average of 20 synapses are allocated to recognize each pattern, and that a neuron has 6000 synapses, then a cell would have the ability to recognize approximately 300 different patterns.",
      "This is a rough approximation, but makes evident that a neuron with active dendrites can learn to reliably recognize hundreds of patterns within a large population of cells.",
      "Neurons as predictors  Neurons receive input from different sources segregated on different parts of the dendritic tree.",
      "There are typical several hundred proximal synapses which receive feedforward input and have a relatively large effect at the soma, and define the basic receptive field response of the neuron.",
      "The basal synapses receive contextual input from nearly cells in the same cortical region, and apical synapses receive feedback input.",
      "Spikes due to basal synapses activating depolarize the soma, but not enough to generate somatic action potential.",
      "We propose that this sub-threshold depolarization is an important state of the cell.",
      "It represents a prediction that the cell will become active shortly and plays an important role in network behavior.",
      "Apical synapses have a similar effect, and are used to establish a top-down expectation \u2013 which can be thought of as another form of prediction.",
      "Learning sequences of patterns  Because all tissue in the neocortex consists of neurons with active dendrites and thousands of synapses, it suggests there are common network principles underlying everything the neocortex does.",
      "This leads to the question, what network property is so fundamental that it is a necessary component of sensory inference, prediction, language, and motor planning?",
      "We propose that the most fundamental operation of all neocortical tissue is learning and recalling sequences of patterns\u2026  The neocortex is divided into cellular layers.",
      "Within layers we find mini-columns of cells.",
      "When an input is unexpected (i.e., it has not been predicted by pattern matching at the synapses leading to a spike and depolarisation) then all the cells in a column become active.",
      "This is the situation we find in (B) below when the sequences \u2018ABCD\u2019 and \u2018XBCY\u2019 have not yet been learned.",
      "( Enlarge )  After learning the sequences though, depolarised cells from the prediction of what should come next will fire first as they are \u2018primed\u2019, and this firing will inhibit the other cells nearby.",
      "( (C) in the figure above).",
      "Thus, a predicted input will lead to a very sparse pattern of cell activation that is unique to a particular element, at a particular location, in a particular sequence.",
      "As feedforward input arrives it activates cells, while basal input is generating predictions.",
      "So long as the next input matches the current prediction, the sequence continues.",
      "The network may make multiple simultaneous predictions without confusion.",
      "If an input matches any of the predictions it will result in the correct highly sparse representation.",
      "The apical dendrites connect neurons across layers 2, 3 and 5 in the neocortex.",
      "Their role seems to be to alert to deviations from expected sequences, as illustrated below.",
      "( Enlarge )  The HTM model neuron  The authors model the biology of the brain in software with Hierarchical Temporal Memory (HTM) neurons.",
      "We model a cell\u2019s dendrites as a set of threshold coincidence detectors; each with its own synapses.",
      "If the number of active synapses on a dendrite/coincidence detector exceeds a threshold the cell detects a pattern.",
      "The coincidence detectors are in three groups corresponding to the proximal, basal, and apical dendrites of a pyramidal cell.",
      "Networks built out of HTM neurons use continuous on-line learning with learning rules that are local to each neuron and no global objective function.",
      "For each dendritic segment a set of \u2018potential\u2019 synapses between the segment and other cells in the network is maintained.",
      "A scalar value called \u2018permanence\u2019 models the growth of the synapse.",
      "Permanence values close to zero indicate that although potential to grow a synapse exists, one has not started growing yet.",
      "Values close to one represent a large fully-formed synapse.",
      "The permanence value is incremented and decremented using a Hebbian-like rule .",
      "If the permanence value exceeds a threshold, such as 0.3, then the weight of the synapse is 1, if the permanence value is at or below the threshold, then the weight of the synapse is 0\u2026 Using a scalar permanence value enables on-line learning in the presence of noise.",
      "The figure below shows a network being fed a mixture of random elements and repeated sequences.",
      "The maximum possible average prediction accuracy based on the input data set is 50%, and this is only possible using higher-order representations.",
      "In (A) below the sequences in the data stream were changed after 3000 elements, and the network relearns the new sequences.",
      "In (B) varying proportions of the neurons are disabled after the network reaches a stable state, and the performance of the network can be seen to recover as it relearns using the remaining neurons.",
      "( Enlarge )  We\u2019ll look at HTM networks in more detail tomorrow."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://www.frontiersin.org/articles/10.3389/fncir.2016.00023/pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 17539393
  },
  {
    "blog_id": "fast-in-memory-transaction-processing-using-rdma-and-rtm",
    "summary": [
      "Fast In-memory Transaction Processing using RDMA and HTM \u2013 Wei et al. 2015  This paper tries to answer a natural question: with advanced processor features and fast interconnects, can we build a transaction processing system that is at least one order of magnitude faster than the state-of-the-art systems without using such features?",
      "The authors build a distributed transaction system, DrTM, that exploits Hardware Transactional Memory (HTM) and Remote Direct Memory Access (RDMA):  Hardware transactional memory (HTM) has recently come to the mass market in the form of Intel\u2019s restricted transactional memory (RTM).",
      "The features like atomicity, consistency and isolation (ACI) make it very promising for database transactions.",
      "Meanwhile, RDMA, which provides direct memory access (DMA) to the memory of a remote machine, has recently gained considerable interests in the systems community.",
      "With a 6-node cluster (20 cores/server), DrTM achieves 5.52M transactions per second on TPC-C.  It\u2019s interesting to compare that number to RIFL that we looked at last week \u2013 from Figure 14 in the RIFL paper we can see that RAMCloud with its kernel bypass transport (fastest configuration) does about 1250 txns per minute (about 21 tps) on TPC-C with a 6-node cluster (only 4 cores/server in this setup).",
      "RAMCloud is tuned for low latency of course, and has an average latency of 1ms.",
      "Using I-Confluence analysis, and extrapolating from the figures in Coordination Avoidance in Database Systems , Bailis et al. achieve about 480K transactions per second on TPC-C with a 6-node cluster (32 vcpus/server).",
      "That system scaled linearly up to 12.7M tps (with a 200 node cluster).",
      "We\u2019d have to debate whether the hardware was comparable, etc.",
      "(well, clearly it is not because one system is using direct hardware support, though fewer cores per server) \u2013 but an order of magnitude difference is significant.",
      "How fast could we go if we implemented an I-Confluent system on top of HTM and RDMA???",
      "To get this level of performance, DrTM depends on moving as much concurrency control as possible into HTM.",
      "One key challenge is the limited working set of HTM, and DrTM using transaction chopping to keep large transactions within it.",
      "A second key challenge is that RDMA cannot be used within an HTM region:  DrTM addresses this with a concurrency control protocol that combines HTM and two-phase locking (2PL)  to preserve serializability.",
      "Speci\ufb01cally, DrTM uses RDMA-based compare-and-swap (CAS) to lock and fetch the corresponding database records from remote machines before starting an HTM transaction.",
      "Thanks to the strong consistency of RDMA and the strong atomicity of HTM, any concurrent con\ufb02icting transactions on a remote machine will be aborted.",
      "DrTM leverages this property to preserve serializability among distributed transactions.",
      "To guarantee forward progress, DrTM further provides contention management by leveraging the fallback handler of HTM to prevent possible deadlock and livelock.",
      "HTM (RTM)  Intel\u2019s Restricted Transactional Memory (RTM) provides strong atomicity within a single machine, where a non-transactional code will unconditionally abort a transaction when their accesses con\ufb02ict.",
      "RTM uses the \ufb01rst-level cache to track the write set and an implementation-speci\ufb01c structure to track the read set, and relies on the cache coherence protocol to detect con\ufb02icts.",
      "Upon a con\ufb02ict, at least one transaction will be aborted.",
      "RTM provides a set of interfaces including XBEGIN, XEND and XABORT, which will begin, end and abort a transaction accordingly.",
      "The read/write set of an RTM transaction is limited in size due to the private cache and buffers on the CPU that are used to support it.",
      "Abort rates increase significantly as working set sizes increase, and a transaction that exceeds the hardware capacity will always be aborted.",
      "Any use of network I/O will also cause the transaction to be aborted.",
      "RDMA  Remote Direct Memory Access (RDMA) is a networking feature to provide cross-machine accesses with high speed, low latency and low CPU overhead.",
      "Much prior work has demonstrated the bene\ufb01t of using RDMA for in-memory stores and computing platforms.",
      "RDMA has three communication options.",
      "In order of increasing performance these are IP-emulation to enable socket-based code to be used unmodified, an MPI with SEND/RECV verbs, and \u2018one-sided RDMA\u2019 which provides one-way direct access to the memory of another machine bypassing the CPU.",
      "One-sided RDMA provides only read, write, and two atomic operations fetch_and_add, and compare _and_swap.",
      "DrTM  DrTM partitions data into shards spread across many machines connected by RDMA.",
      "It uses one worker-thread per core, each thread executes and commits a single transaction at a time.",
      "DrTM exposes a partitioned global address space, though a process still needs to distinguish between local and remote accesses.",
      "Remote access is primarily via one-sided RDMA operations for efficiency.",
      "The memory store layer of DrTM provides a general key-value store interface to the upper layers:  To optimize for different access patterns, DrTM provides both an ordered store in the form of a B+ tree and an unordered store in the form of a hash table.",
      "For the ordered store, we use the B+ tree in DBX, which uses HTM to protect the major B+ tree operations and was shown to have comparable performance with state-of-the-art concurrent B+ tree.",
      "For the unordered store, we further design and implement a highly optimized hash table based on RDMA and HTM.",
      "There is prior work on RDMA-optimised hash-tables, but nothing that exploits the combination of HTM and RDMA.",
      "DrTM leverages the strong atomicity of HTM and strong consistency of RDMA to design an HTM/RDMA- friendly hash table.",
      "First, DrTM decouples the race detection from the hash table by leveraging the strong atomicity of HTM, where all local operations (e.g., READ/WRITE/ INSERT/DELETE) on key-value pairs are protected by HTM transactions and thus any con\ufb02icting accesses will abort the HTM transaction.",
      "This signi\ufb01cantly simpli\ufb01es the data structures and operations for race detection.",
      "Second, DrTM uses one-sided RDMA operations to perform both READ and WRITE to remote key-value pairs without involv- ing the host machine.",
      "Finally, DrTM separates keys and values as well as its metadata into decoupled memory region, resulting in two-level lookups like Pilaf.",
      "This makes it ef\ufb01cient to leverage one-sided RDMA READ for lookups, as one RDMA READ can fetch a cluster of keys.",
      "Further, the separated key-value pair makes it possible to implement RDMA-friendly, location-based and host-transparent caching.",
      "DrTM uses cluster chaining for hashing.",
      "When caching lookup results, DrTM chooses to cache the key\u2019s location rather than value.",
      "This minimizes the lookup cost while still retaining strongly consistent reads and writes.",
      "To support distributed transactions, DrTM combines HTM with a higher-level two-phase locking (2PL) protocol:  \u2026to preserve serializability among con\ufb02icting transactions on multiple nodes, we design a 2PL-like protocol to coordinate accesses to the same database records from local and remote worker threads.",
      "To bridge HTM(which essentially uses OCC) and 2PL, DrTM implements the exclusive and shared locks using one-sided RDMA operations, which are cache- coherent with local accesses and thus provide strong consistency with HTM.",
      "The challenge is that any RDMA operation inside an HTM transaction will automatically cause it to abort.",
      "To this end, DrTM uses 2PL to safely accumulate all remote records into a local cache prior to the actual execution in an HTM transaction, and write back the committed updates to other machines until the local commit of the HTM transaction or discard temporal updates after an HTM abort.",
      "DrTM therefore requires prior knowledge of the read/write sets of transactions for locking and prefetching in the \u2018start\u2019 phase.",
      "For typical OLTP transactions such as TPC-C this is not normally a problem.",
      "On each individual machine DrTM uses HTM to provide transaction suppport.",
      "To mitigate the working set size limitations, DrTM uses transaction  chopping \u2018with optimisations\u2019 to decompose larger transactions into smaller pieces when needed.",
      "For read-only transactions with very large read sets DrTM provides a separate scheme to execute read-only transactions without HTM.",
      "DrTM transactions support strict serializability.",
      "Currently DrTM provides durability, but not high availability:  DrTM currently preserves durability rather than availability in case of machine failures, as done in recent in-memory databases.",
      "How to provide availability, e.g., through ef\ufb01ciently replicated logging, will be our future work.",
      "The source code of DrTM will soon be available at  [url]"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://sigops.org/sosp/sosp15/current/2015-Monterey/printable/158-wei.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 11313245
  },
  {
    "blog_id": "continuous-online-sequence-learning-with-an-unsupervised-neural-network-model",
    "summary": [
      "Continuous online sequence learning with an unsupervised neural network model Cui et al., Neural Computation, 2016  Yesterday we looked at the biological inspirations for the Hierarchical Temporal Memory (HTM) neural network model .",
      "Today\u2019s paper demonstrates more of the inner workings, and shows how well HTM networks perform on online sequence learning tasks as compared to other approaches (e.g., LSTM-based networks).",
      "The HTM model achieves comparable accuracy to other state-of-the-art algorithms.",
      "The model also exhibits properties that are critical for sequence learning, including continuous online learning, the ability to handle multiple predictions and branching sequences with high-order statistics, robustness to sensor noise and fault tolerance, and good performance without task-specific hyperparameter tuning.",
      "The HTM sequence memory model  Recall from yesterday that the HTM sequence memory model is organised around layers of mini-columns of cells.",
      "The network represents higher-order (dependent on previous time steps) sequences using a composition of two separate sparse representations.",
      "At the column level, each input element is encoded as a sparse distributed activation of columns: the top 2% of columns that receive the most active feedforward inputs are activated.",
      "Within an active column, a subset of the cells will be active \u2013 if a column contains predicted cells then only these cells will be active, otherwise all cells within the column become active.",
      "A network has  columns and  neurons per column.",
      "For each cell in the network we keep track of its activation state and its predictive state at time step  :  The  binary matrix  represents the activation state (on/off) of the cells:  is the activation state of the  th cell in the  th column.",
      "The  binary matrix  represents the predictive state (on/off) of the cells:  is the predictive state of the  th cell in the  th column.",
      "Each cell has  dendrite segments.",
      "To model synapse connections an  matrix  tracks the permanence of the synaptic connection at segment  .",
      "The values in the matrix are in the range 0 to 1.",
      "If the permanence value is above some threshold, then the synapse is considered connected.",
      "For convenience (since it can always be deduced from  ), the binary matrix  denotes the connected synapses.",
      "At time step  , a cell is in the predictive state if one of its dendritic segments receives enough input (is connected to enough active cells):  Threshold  represents the segment activation threshold and  represents element-wise multiplication.",
      "At each time  , columns are ordered based on their number of active proximal synapses, and the top 2% are activated.",
      "This set is denoted as  .",
      "In theory the proximal  synapse connections can also be adapted continuously during learning, but for this paper the proximal synapse connections are initialised such that each column is randomly connected to 50% of the inputs, and then fixed during learning.",
      "A cell is considered active if its column is active and it was in predictive state during the previous time step, or its column is active and none of the other cells in the same column were in a predictive state:  Learning is local and straight forward:  The lateral connections in the sequence memory model are learned using a Hebbian-like rule.",
      "Specifically, if a cell is depolarized and subsequently becomes active, we reinforce the dendritic segment that caused the depolarization.",
      "If no cell in ac active column is predicted, we select the cell with the most activated segment and reinforce that segment.",
      "Reinforcement of a dendritic segment involves decreasing permanence values of inactive synapses by a small value  and increasing the permanence for active synapses by a larger value  .",
      "Where  is a binary matrix with entries set to 1 iff the corresponding entry in  is positive.",
      "Cell that don\u2019t become active also receive a very small decay.",
      "The sequence memory operates with sparse distributed representations (SDRs).",
      "Original data is converted to SDRs using an encoder (in this case, a random encoder for categorical data, and scalar date-time encoders for a taxi data experiment).",
      "We can\u2019t use a simple one-hot encoding for categorical data because we also want to able to provide noise inputs drawn from a very large distribution.",
      "Decoding from SDRs is done using classifiers.",
      "Real-time streaming data analysis  In this paper HTMs are applied to real-time streaming data pattern matching problems where they have the following desirable characteristics:  they can learn online in a single pass without requiring a buffered data set  they can make higher-order predictions across multiple time steps, learning the order (number of previous steps to consider) automatically  they can make multiple simultaneous predictions in the case of ambiguity  they are robust to the loss of synapses and neurons (important for hardware implementations)  they do not require any hyperparameter tuning  Sequence prediction with artificial data  The first test compares the HTM sequence memory model with online sequential extreme machine learning (ELM), a time-delayed neural network (TDNN), and LSTMs.",
      "A data set is generated with sequences that require a network to maintain a context of at least the first two elements of a sequence in order to correctly predict the last element.",
      "Since the sequences are presented in a streaming fashion, and predictions are required continuously, this task represents a continuous online learning problem.",
      "For the LSTMs, retraining is done at regular intervals on a buffered data set of the previous time steps.",
      "The experiments include several LSTM models with varying buffer sizes.",
      "In the figure below, we see the prediction accuracy achieved by the various networks when the sequences are generated such that there is a single correct prediction.",
      "After the one thousandth element, we see how quickly the networks relearn when the sequences change.",
      "HTM is not the fastest initial learner, but it does achieve full prediction accuracy and recovers accuracy fastest after the sequence change.",
      "Remember though that HTM is working online seeing each example just once, whereas the LSTM\u2019s are periodically retrained (retraining points indicated by the vertical yellow lines in the figure below).",
      "HTMs determine the higher-order structure by themselves, whereas ELM and TDNN require the user to determine the number of steps to use as a temporal context.",
      "In the next experiment scenario the sequences have two or four possible endings, depending on the higher-order context.",
      "HTMs perform best at this task, and the more possible completions, the better their advantage appears to be.",
      "HTM sequence memory was tested with varying length sequences, and achieved perfect prediction performance up to 100-order sequences.",
      "The number of sequences that are required to achieve perfect prediction performance increase linearly as a function of the order of sequences.",
      "When deliberately damaging the network structure (removal of neurons), HTMs showed no impact with up to 30% cell death, whereas the ELM and LSTM networks declined more rapidly.",
      "(Techniques such as dropout were not used during training though).",
      "New York city taxi passenger demand prediction  The HTM sequence memory model was also tested against a variety of other approaches on a task of predicting taxi demand from streaming New York City taxi ride data.",
      "For this example, the parameters of the ESN, ELM, and LSTM network are extensively hand-tuned.",
      "The HTM model does not undergo any hyperparameter tuning.",
      "Limitations of HTMs  We have identified a few limitations of HTM\u2026  As a strict one-pass algorithm with access only to the current input, it may take longer for HTM to learn sequences with very long-term dependencies than algorithms with access to a larger history buffer.",
      "HTM is robust to spatial noise due to the use of sparse distributed representations, but proved sensitive to temporal noise (replacing elements in a sequence by random symbols).",
      "LSTMs are more robust to temporal noise.",
      "To improve the noise robustness of HTM, a hierarchy of sequence models operating on different timescales could be used.",
      "The HTM model did not perform as well as LSTM on grammar learning tasks \u2013 achieving 98.4% accuracy in a test, whereas the LSTM achieves 100%.",
      "It is future work to determine whether HTM can handle high-dimensional data such as speech and video streams  The work on HTMs also seems to be being reported outside of the usual machine learning conference venues.",
      "It\u2019s good to bring in ideas from further afield, but this may also mean they have received less scrutiny from the mainstream machine learning community.",
      "There has been some controversy surrounding this in the past:  [url]"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://arxiv.org/pdf/1512.05463.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 11791786
  },
  {
    "blog_id": "read-log-update-a-lightweight-synchronization-mechanism-for-concurrent-programming",
    "summary": [
      "Read-Log-Update: A Lightweight Synchronization Mechanism for Concurrent Programming \u2013 Matveev et al. 2015  An important paradigm in concurrent data structure scalability is to support read-only traversals: sequences of reads that execute without any synchronization (and hence require no memory fences and generate no contention).",
      "The gain from such unsynchronized traversals is significant because they account for a large fraction of operations in many data structures and applications.",
      "The popular read-copy-update (RCU) mechanism of McKenney and Slingwine provides scalability by enabling this paradigm.",
      "It allows read-only traversals to proceed concurrently with updates by creating a copy of the data structure being modified.",
      "Readers access the unmodified data structure while updaters modify the copy.",
      "The key to RCU is that once modifications are complete, they are installed using a single pointer modification in a way that does not interfere with ongoing readers.",
      "RCU has been supported in the Linux kernel since 2002, and the User-space RCU library is available for user-space applications.",
      "In this paper Matveev et al. take a big step forward beyond RCU with the introduction of  an extension to the base RCU ideas they call Read-Log-Update (RLU).",
      "RLU provides better performance and concurrency than RCU (it allows multiple simultaneous writers for example), but I think even more importantly it has a much simpler programming model meaning it can be applied to many more scenarios.",
      "For example, the RCU-based doubly-linked list in the Linux kernel only allows threads to traverse the list in the forward direction because of the limitations of RCU, whereas the RLU based alternative can easily support traversal in both directions.",
      "I\u2019m going to focus here on the core RLU algorithm.",
      "The paper also contains an extensive evaluation section which is a fascinating read, and has the by-product of reminding me just how big our field is and of the huge body of knowledge built up in data structures and algorithms.",
      "I collected a good number of additions to my \u2018known unknowns\u2019 list reading through it!",
      "For example: \u201cWe then apply the more advanced RLU scheme with the deferral mechanism of synchronize calls to the state-of-the-art RCU-based Citrus tree, an enhancement of the Bonsai tree of Clements et al. The Citrus tree uses both RCU and fine-grained locks to deliver the best performing search tree to date\u2026\u201d  Let\u2019s dive into how RLU works:  RLU provides support for multiple object updates in a single operation by combining the quiescence mechanism of RCU with a global clock and per thread object-level logs.",
      "The global clock and per-thread object logs will shortly be explained.",
      "Just in case the quiescence mechanism of RCU isn\u2019t front-and-centre in your mind right now, here\u2019s a quick refresher:  When [RCU readers] are not inside a critical section, readers are said to be in a quiescent state.",
      "A period of time during which every thread goes through at least one quiescent state is called a grace period.",
      "The key principle of RCU is that, if an updater removes an element from an RCU-protected shared data structure and waits for a grace period, there can be no reader still accessing that element.",
      "It is therefore safe to dispose of it.",
      "Waiting for a grace period can be achieved by calling synchronize rcu().",
      "The global clock is a logical clock (counter).",
      "All operations read the global clock value before they start, and  use this clock value to determine what version of a shared object they should read.",
      "A write thread first makes a copy of object to be updated in its  write-log, and then modifies the object in the log.",
      "This mechanism hides updates from concurrent reads.",
      "To avoid conflicts with concurrent writes, each object is also locked before it is duplicated and modified.",
      "Then, to commit the new object copies, a write operation increments the global clock, which effectively splits operations into two sets: (1) old operations that started before the clock increment, and (2) new operations that start after the clock increment.",
      "The first set of operations will read the old object copies while the second set will read the new object copies of this writer.",
      "Therefore, in the next step, the writer waits for old operations to finish by executing the RCU-style quiescence loop, while new operations \u201csteal\u201d new object copies of this writer by accessing the per thread write-log of this writer.",
      "After the completion of old operations, no other operation may access the old object memory locations, so the writer can safely write back the new objects from the writer-log into the memory, overwriting the old objects.",
      "It can then release the locks.",
      "Figure 3 from the paper provides a series of worked examples which is probably the best way to quickly grasp the idea.",
      "In each of the diagrams that follow, swim-lanes represent threads (T1, T2, T3) and time progress downwards.",
      "The global clock and fine-grained object locks are represented in the \u2018memory\u2019 column.",
      "Full pseudo-code for the algorithm is given in section 3.5 of the paper.",
      "Concurrent reads, no writers  In the example above, threads T1 and T2 both perform reads.",
      "They begin by copying the value of the global clock (22) to their local clocks.",
      "T1 reads O1, and T2 reads O1 and O2.",
      "None of these objects are locked, so reads proceed concurrently from memory.",
      "Concurrent reads with uncommitted writes  T2 now wants to update objects O2 and O3.",
      "Each of these objects is logged (copied into T2\u2019s write-log) and modified in the log.",
      "T2 locks these objects using the RLU shared locks (memory column) before logging them.",
      "T1 tries to read O2, after T2 has locked it.",
      "T1 sees that T2 holds the lock, and compares its local clock (22) with the value of T2\u2019s write clock (currently at &inf; since T2 has not committed its writes yet).",
      "From this comparison we see that this read has not started after a write clock increment (T1\u2019s clock is < T2\u2019s), and so T1 should not try to read O2 from T2\u2019s log (\u2018steal\u2019 it), but instead should read the object directly from memory.",
      "Committing writes  T2 begins the process of committing its writes.",
      "It increments the clock value and stores this first in its write-clock, and secondly in the global clock (the order matters).",
      "T2 now has to wait for a quiescent point in order to be able to install its updates \u2013 it must wait for \u2018old\u2019 operations to finish, those with local clock values < its own.",
      "In this worked example, that means waiting for T1 to finish.",
      "T3 has also started at this point, but has a local clock value of 23 (&geq; T2\u2019s), so T2 does not need to wait for it.",
      "When T1 completes, T2 can safely write back the objects in its log to memory and release the locks.",
      "T3 meanwhile wants to read O2 before T2 has been able to install its updates.",
      "It compares its local clock value (23) with the write-clock value of T2 (since T2 has the lock at this point), and since its local clock is &geq; T2\u2019s write-clock, it \u2018steals\u2019 the value from T2s write-log.",
      "An optimisation \u2013 deferring synchronisation  RLU synchronize deferral works as follows.",
      "On commit, instead of incrementing the global clock and executing RLU synchronize, the RLU writer simply saves the current write-log and generates a new log for the next writer.",
      "In this way, RLU writers execute without blocking on RLU synchronize calls, while aggregating write-logs and locks of objects being modified.",
      "The RLU synchronize call is actually only necessary when a writer tries to lock an object that is already locked.",
      "Therefore, only in this case, the writer sends a \u201csync request\u201d to the conflicting thread to force it to release its locks, by making the thread increment the global clock, execute RLU synchronize, write back, and unlock.",
      "This significantly reduces the amount of RLU synchronize calls, as well as contention on the global clock.",
      "The aggregration of write-logs and deferring of the global clock update also defers the stealing process to a later time, allowing more reads from memory."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://sigops.org/sosp/sosp15/current/2015-Monterey/printable/077-matveev.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 60253398
  },
  {
    "blog_id": "beat-asynchronous-bft-made-practical",
    "summary": [
      "BEAT: asynchronous BFT made practical Duan et al., CCS\u201918  Reaching agreement (consensus) is hard enough, doing it in the presence of active adversaries who can tamper with or destroy your communications is much harder still.",
      "That\u2019s the world of Byzantine fault tolerance (BFT).",
      "We\u2019ve looked at Practical BFT (PBFT) and HoneyBadger on previous editions of The Morning Paper.",
      "Today\u2019s paper, BEAT, builds on top of HoneyBadger to offer BFT with even better latency and throughput.",
      "Asynchronous BFT protocols are arguably the most appropriate solutions for building high-assurance and intrusion-tolerant permissioned blockchains in wide-are (WAN) environments, as these asynchronous protocols are inherently more robust against timing and denial-of-service (DoS) attacks that can be mounted over an unprotected network such as the Internet.",
      "The best performing asynchronous BFT protocol, HoneyBadger , still lags behind the partially synchronous PBFT protocol in terms of throughput and latency.",
      "BEAT is actually a family of five different asynchronous BFT protocols that start from the HoneyBadger baseline and make improvements targeted at different application scenarios.",
      "Unlike HoneyBadgerBFT, which was designed to optimize throughput only, BEAT aims to be flexible and versatile, providing protocol instances optimized for latency, throughput, bandwidth, or scalability (in terms of the number of servers).",
      "The BEAT protocols divide into two groups: those supporting full (general) state-machine replication (SMR), as required e.g. for smart contract use cases (BEAT0, BEAT1, BEAT2); and those that support BFT storage (append-only ledger) use cases only (BEAT3, BEAT4).",
      "The following table summarises the BEAT family and the key distinguishing features of each member.",
      "( Enlarge )  There\u2019s a lot of ground to cover here, but I\u2019ll do my best to give you an overview.",
      "Alongside the BEAT protocols themselves, the paper also includes two new building blocks: the generalized fingerprinted cross-checksum and an asynchronous verifiable information dispersal (AVID) algorithm.",
      "The HoneyBadger baseline  HoneyBadger supports ACS (the asynchronous common subset) meaning that it provides these guarantees:  Validity: if a correct server delivers a set  , then  and  contains the inputs of at least  correct servers.",
      "Agreement: if a correct server delivers a set  , then all correct servers deliver  .",
      "Totality: if  correct servers submit an input, then all correct servers deliver an output.",
      "HoneyBadger uses reliable broadcast (RBC) and asynchronous Byzantine binary agreement (ABA) protocols to achieve its aims.",
      "Threshold signatures are used to provide common coins for ABA, and threshold encryption is used to avoid censorship and achieve liveness.",
      "In a threshold scheme the partial outputs (e.g. decryption shares) of at least t participants need to be combined in order to recover (decrypt) the intended value.",
      "BEAT0: improved security and performance  BEAT0, our baseline protocol, incorporates a more secure and efficient threshold encryption, a direct instantiation of threshold coin-flipping (instead of using threshold signatures), and more flexible and efficient erasure-coding support.",
      "BEAT0\u2019s threshold encryption uses the TDH2 scheme by Shoup and , providing 128-bit security under elliptic curve cryptography.",
      "This gives stronger security and better performance than the scheme used in HoneyBadger.",
      "In place of the zfec erasure coding library used by HoneyBadger, which supports only Reed-Solomon codes and at most 128 servers, BEAT uses the Jerasure library giving access to more efficient erasure coding schemes and lifting the replica restriction.",
      "BEAT1: lower latency  Via a careful study of latency for each HoneyBadgerBFT subprotocol, we find that (1) most of the latency comes from threshold encryption and threshold signatures, and (2) somewhat surprisingly, when the load is small and there is low contention, erasure-coded reliable broadcast (AVID broadcast) causes significant latency.",
      "BEAT1 swaps out the AVID broadcast protocol of BEAT0 for a replication-based reliable broadcast protocol, Bracha\u2019s broadcast .",
      "Under small loads BEAT1 has lower latency.",
      "With small batch sizes BEAT1\u2019s throughput is higher than HoneyBadger / BEAT0, but with larger batch sizes throughput is down by 20-30%.",
      "BEAT2: causal ordering  BEAT2 builds on BEAT1 and also opportunistically moves the use of threshold encryption to the client side.",
      "In BEAT2, when the ciphertexts are delivered, it is too late for the adversary to censor transactions.",
      "Thus, the adversary does not know what transactions to delay, and can only delay transactions from specific clients.",
      "BEAT2 can be combined with anonymous communication networks to achieve full liveness.",
      "BEAT2 additionally achieves causal order, which prevents the adversary from inserting derived transactions before the original, causally prior transactions.",
      "BEAT3: higher throughput for storage use cases  BEAT3 is the first member of the BEAT family targeted for BFT-storage use cases (as opposed to general SMR).",
      "Recall that the safety and liveness properties of BFT storage remain the same as those of general SMR, with the only exception that the state may not be replicated at each server (but instead may be erasure-coded).",
      "BEAT3 can be used for blockchain applications that need append-only ledgers, and specific blockchains where the consensus protocol serves as an ordering service, such as Hyperledger Fabric.",
      "Whereas so far we\u2019ve been using a reliable broadcast protocol (AVID), BEAT3 replaces this with a bandwidth-efficient information dispersal scheme called AVID-FP.",
      "To disperse a block  , AVID requires bandwidth  , whereas AVID-FP can do it in  .",
      "To order transactions of size  , the communication complexity of BEAT0 is  , of BEAT1 and BEAT2 is  , and of BEAT3 is  .",
      "AVID-FP is a bandwidth-efficient AVID (asynchronous verifiable information dispersal) protocol using fingerprinted cross-checksum.",
      "In AVID-FP, given a block B to be dispersed, the dealer applies an (m,n) erasure coding scheme, where  and  \u2026 then it generates the corresponding fingerprinted cross-checksum for B with respect to the erasure coding scheme.",
      "Each server verifies the correctness of its fragment with respect to the fingerprint cross-checksum, \u201cand then, roughly speaking, leverages the (much smaller) fingerprinted cross-checksum in place of the fragment in the original AVID protocol.\u201d  An (n,m) fingerprinted cross-checksum contains a cross-checksum array of n values, and a fingerprint array of m values.",
      "The ith entry in the checksum array contains the hash of the ith coded fragment.",
      "See section 4 in the paper for details of the fingerprint array usage.",
      "BEAT4: partial reads  BEAT4 further reduces read bandwidth using a novel erasure-coded reliable broadcast protocol called AVID-FP-Pyramid.",
      "This supports use cases where clients only need to read a fraction of a data block.",
      "AVID-FD-Pyramid is based on pyramid codes, which trade space for access efficiency in erasure-coded storage systems (about 10% extra space requirement for a 50% drop in access overhead).",
      "Pyramid codes can be efficiently built from any (n, m) systematic and MDS (maximum distance separable) code.",
      "See section 4 in the paper for brief details, or Huang et al. for an in-depth treatment.",
      "BEAT4 uses a 2-level pyramid scheme which can tolerate one failure in each level, and is able to reduce read bandwidth by 50%.",
      "Full details are in section 9 of the paper.",
      "Evaluation  The evaluation is conducted on EC2 with up to 92 nodes from ten different regions in five different continents, using a variety of network sizes and batch sizes.",
      "In the figures that follow,  represents the network size such that BEAT0,1,2 & 3 require  nodes and BEAT4 requires  nodes.",
      "When f=1, BEAT0, BEAT1, BEAT2, and BEAT3 are around 2x faster than HoneyBadger, and when f becomes larger, they are even faster than HoneyBadger.",
      "When f = 1, BEAT4 is about as fast as HoneyBadger\u2026 As f increases, HoneyBadger is much slower than BEAT4.",
      "For throughput, BEAT0 slightly outperforms HoneyBadger.",
      "BEAT1 and BEAT2 achieve higher throughput than HoneyBadger with small batch sizes, but have 20-30% lower throughput at larger batch sizes.",
      "BEAT3 and BEAT4 outperform all the other protocols consistently.",
      "If this write-up has captured your interest, I highly encourage you to go an and read the full paper which contains significantly more detail than I was able to convey here."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://www.csee.umbc.edu/~hbzhang/files/beat.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 99341732
  },
  {
    "blog_id": "twitter-heron-towards-extensible-streaming-engines",
    "summary": [
      "Twitter Heron: Towards extensible streaming engines Fu et al., ICDE 2017  We previously looked at the initial Twitter Heron paper that announced Heron to the world.",
      "In this ICDE 2017 paper, the team give us an update based on the work done since as a result of open-sourcing Heron .",
      "\u2026 we discuss the challenges we faced when transforming Heron from a system tailored for Twitter\u2019s applications and software stack to a system using an extensible, modular architecture which provides flexibility to adapt to various environments and applications.",
      "We get a high level look at the architecture and an investigation into its performance (short version: having a modular architecture doesn\u2019t mean you have to sacrifice performance).",
      "The part I find the most interesting though is the comparisons between Twitter Heron\u2019s design and those of Storm and Spark Streaming.",
      "(Would be great to see Apache Flink included as well).",
      "Twitter\u2019s daily operations rely heavily on real-time processing of billions of event per day\u2026 Heron is now the de facto stream data processing system in Twitter and is used to support various types of applications such as spam detection, real time machine learning, and real time analytics, among others.",
      "Why a modular architecture?",
      "Moving Heron from an internal project to an open source project meant that Heron could no longer be tied to any element of Twitter\u2019s stack.",
      "To be externally useful to a broad audience, it needs to consider different environments (public/private cloud), a variety of software stacks, and diverse application workloads.",
      "A good example here is Twitter\u2019s use of Aurora for scheduling, whereas many other organisations may already have an alternate scheduler (e.g., YARN).",
      "Heron allows the application developer or system administrator to create a new implementation for a specific Heron module and plug it into the system without disrupting the remaining modules or the communication between them.",
      "The structure also allows different applications built on top of Heron to use different module implementations while operating on the same underlying resources.",
      "The high-level architecture of Heron  Heron\u2019s architecture is inspired by that of microkernel-based operating systems\u2026 due to the heterogeneity of today\u2019s cloud environments and Big Data platforms, we decided to design Heron using extensible self-contained modules that operate on top of a kernel which provides the basic functionality needed to build a streaming engine.",
      "There are seven main modules, as shown in the following figure:  At runtime, everything is container-based.",
      "Heron Instances are essentially the spouts or bolts that run on their own JVM.",
      "The Metrics Manager collects process metrics, and the Topology Master manages the directed graph of spouts and bolts (a topology).",
      "Let\u2019s now take a look at the remaining modules in more detail.",
      "Resource Manager  The resource manager is invoked on demand to manage resource assignments (CPU, memory, disk) for a particular topology.",
      "It does this by assigning Heron Instances to containers through a packing plan.",
      "An initial packing plan is created when a topology is first submitted, and can be repacked in response to user requests to scale up and down.",
      "Different resource management policies can be selected for the different topologies running on the same cluster.",
      "A generated packing plan is passed to the Scheduler.",
      "Scheduler  The Scheduler module interacts with an underlying scheduling framework such as YARN or Aurora to allocate the resources needed by a packing plan.",
      "Heron accommodates both stateful and stateless schedulers, depending on the level of support provided by the underlying scheduling framework.",
      "A stateful Scheduler regularly communicates with the underlying scheduling framework to monitor the state of the containers of the topology.",
      "In case a container has failed, the stateful Scheduler takes the necessary actions to recover from failure\u2026 A stateless Scheduler on the other hand, is not aware of the state of the containers while the topology is running.",
      "More specifically, it relies on the underlying scheduling framework to detect container failures and take the necessary actions to resolve them.",
      "Heron today has support for Aurora and YARN.",
      "A Mesos scheduler is being developed in the community.",
      "State Manager  The State Manager is used for distributed coordination and storing topology metadata.",
      "It stores for example topology definitions, packing plans, host and port information for all containers, and the location of the underlying scheduling framework.",
      "Heron provides a ZooKeeper based implementation, as well as a local file system version for local development and testing.",
      "Stream Manager  The Stream Manager handles all inter-process communication.",
      "It\u2019s implemented in C++ to provide tighter control over the memory and CPU footprint, and to avoid copying data between the native and JVM heaps.",
      "The Stream Manager uses several techniques to achieve high performance:  Protocol Buffers are allocated in memory pools and reused, avoiding new/delete operations  In-place updates of Protocol Buffers are performed  Lazy deserialization is used whenever possible  The communication layer offers two important configuration parameters to tune its behaviour for a given deployment: max spout pending determines the maximum number of tuple that can be pending on a spout task at any point in time, and cache drain frequency determines how often the tuple cache is drained.",
      "The tuple cache store incoming and outgoing data tuples before routing them to the appropriate Heron Instances.",
      "Heron vs Storm  Heron is designed with the goal of operating in a cloud environment on top of a scheduling framework such as Aurora or YARN (although it can also run in local mode).",
      "As a result, it leverages the resource isolation mechanisms implemented by these frameworks.",
      "Storm, on the other hand implements parts of the functionality of the Heron Resource Manager, the Heron Scheduler and the underlying scheduling framework in the same abstraction.",
      "In Storm, this creates confusion between Storm\u2019s scheduling decisions and those of any underlying scheduler.",
      "Furthermore, in Storm all resources for a cluster must be obtained up front, whereas Heron acquires resources on demand.",
      "Thus Storm clusters will tend to be over-provisioned.",
      "Heron provides resource isolation between topologies (through the underlying scheduling framework), and also between processes of the same topology.",
      "Storm can do neither \u2013 it packs multiple spout and bolt tasks into a single executor, and several executors share the same JVM.",
      "Finally, Heron\u2019s Stream Manager handles all data transfers separately from processing units, which helps to make the system scalable.",
      "Storm shares communication threads and processing threads in the same JVM.",
      "\u201cAs a result, it is much harder to isolate performance bottlenecks and thus optimize the overall performance.\u201d  Heron vs Spark Streaming  Spark Streaming depends on Spark itself for extensibility.",
      "Because Spark supports a wide diversity of use cases, it is not easy to customize it particularly for streaming.",
      "It is worth noting that Spark (and, as a result, Spark Streaming) has a similar architecture to Storm that limits the resource isolation guarantees it can provide\u2026 each executor process can run multiple tasks in different threads.",
      "As opposed to Heron, this model does not provide resource isolation among the tasks that are assigned to the same executor.",
      "All Spark Streaming communication relies on Spark, and is not customisable.",
      "Performance evaluation  If we compare Heron and Storm on a workload chosen to expose framework overheads we see that Heron beats storm on both throughput and latency:  The above figures are with acks enabled.",
      "Heron outperforms Storm by approximately 3-5x in terms of throughput and at the same time has 2-4x lower latency.",
      "With acks disabled, the throughput of Heron is 2-3x higher than Storm\u2019s.",
      "The following two charts show the impact of the max spout pending configuration parameter.",
      "As you might expect, allowing more queuing tasks increases throughput up to a point, at the expense of latency.",
      "See section V in the paper for a more detailed performance breakdown.",
      "Despite the benefits of general-purpose architectures, such as Heron\u2019s modular architecture, a common belief is that specialized solutions tend to outperform general-purpose ones because they are optimized for particular environments and applications.",
      "In this paper, we show that by carefully optimizing core components of the system, Heron\u2019s general-purpose architecture can actually provide better performance than specialized solutions such as Storm."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://www.microsoft.com/en-us/research/wp-content/uploads/2017/06/heron_icde-1.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 49333267
  },
  {
    "blog_id": "data-shapley",
    "summary": [
      "Data Shapley: equitable valuation of data for machine learning Ghorbani & Zou et al., ICML\u201919  It\u2019s incredibly difficult from afar to make sense of the almost 800 papers published at ICML this year !",
      "In practical terms I was reduced to looking at papers highlighted by others (e.g. via best paper awards), and scanning the list of paper titles looking for potentially interesting topics.",
      "For the next few days we\u2019ll be looking at some of the papers that caught my eye during this process.",
      "The now somewhat tired phrase \u201cdata is the new oil\u201d (something we can consume in great quantities to eventually destroy the world as we know it???)",
      "suggests that data has value.",
      "But pinning down that value can be tricky \u2013 how much is a given data point worth, and what framework can we use for thinking about that question?",
      "As data becomes the fuel driving technological and economic growth, a fundamental challenge is how to quantify the value of data in algorithmic predictions and decisions\u2026.",
      "In this work we develop a principled framework to address data valuation in the context of supervised machine learning.",
      "One of the nice outcomes is that once you\u2019ve understood what data has high value and what data has low value, you can use this insight to guide future data collection activities and improve your ROI on data gathering.",
      "It also turns out that examining data points with low values turns out to be a good way of discovering noisy and/or mislabelled data points.",
      "Removing these can improve your model\u2019s performance.",
      "It\u2019s most important to remember at all times when considering this work that we\u2019re talking about the value of data in the context of training a specific model.",
      "We can\u2019t give an objective valuation of a data point outside of that context, and a data point that is low value for one task may be high value for another.",
      "\u2026 we do not define a universal value for data.",
      "Instead, the value of each datum depend on the learning algorithm and the performance metric, as well as on other data in the training set.",
      "This dependency is reasonable and desirable in machine learning.",
      "How to judge the value of a data point  Assume we have some fixed training set  , a learning algorithm  , and a performance score function  , where  , which takes as input  a predictor trained on  and returns a performance score.",
      "Let  (sometimes written just as )  be a function that quantifies the value of the i-th datum.",
      "For an equitable data valuation function we want the following three properties to hold:  If a datapoint does not change the performance when it\u2019s added to any subset of the training data, then it should be given zero value.",
      "If two different data points, when individually added to any subset of the training data always produce exactly the same change in the predictor\u2019s score, then they should be given the same value by symmetry.",
      "When the overall prediction score is the sum of K separate predictions, the value of a datum should be the sum of its value for each prediction.",
      "These three conditions mirror the fairness conditions  from cooperative game-theory as defined by Shapley (in 1953), and constrain  to have the following form:  In game theory this is called the Shapley value, so here the authors call it the data Shapley value.",
      "Let\u2019s unpick that equation a little:  Take all possible subsets from  that do not include  , the data point we wish to value  For each of these subsets, compute the incremental value that arises when  is added back in  Sum all of those value increments, and divide by the number of subsets to yield the average value increment for a subset of the training data when data point  is added to it.",
      "Eqn.",
      "1 could be interpreted as a weighted sum of all possible \u201cmarginal contributions\u201d of  , where the weight is inverse the number of subsets of size  in  .",
      "This formulation is close to that of leave-one-out where instead of considering the last marginal contribution  , we consider each point\u2019s marginal contribution assuming that instead of the whole training set, a random subset of it is given.",
      "The constant  is an arbitrary scaling constant and doesn\u2019t affect any of the analysis.",
      "How to make it tractable  You might have spotted the challenge in computing a data Shapley value: it requires us to train and score a model (expensive) for every possible subset of the training data (exponentially many).",
      "And then of course we\u2019d like to do this for all of the points in our data set (typically large).",
      "So that\u2019s not going to work then.",
      "All is not lost.",
      "The authors introduce techniques for approximating data Shapley, tackling each of the three dimensions:  We can deal with the problem of exponentially many subsets by using Monte Carlo sampling: \u201cin practice, we generate Monte Carlo estimates until the average has empirically converged.\u201d  We can use early stopping (truncation) when calculating a data Shapley value for a sampled permutation  .",
      "Whenever  is within the performance tolerance (inherent noise) of  the marginal contribution is set to zero for the rest of the data points in the permutation.",
      "Putting these first two optimisations together gives Truncated Monte Carlo Shapley, TMC-Shapley.",
      "We can reduce the cost of training a model, where that model is trained using a variation of stochastic variant descent, but training the model for one epoch (one pass through the training data) only.",
      "This variant is called Gradient Shapley.",
      "We can reduce the number of data points we have to compute the data Shapley value for by instead computing the value for groups of points.",
      "\u201cFor example, in a heart disease prediction setting, we could group the patients into discrete bins based on age, gender, ethnicity and other features, and then quantify the data Shapley of each bin.\u201c  Data Shapley in action  There are some really interesting results in the evaluation section, that demonstrate the power of understanding the value of your data points (as well as the efficacy of data Shapley in helping you to do that).",
      "Using the UK Biobank data set and a model for predicting breast and skin cancer, the authors calculated data values for 1000 data points in a training set.",
      "Now, if you take data points away from the training set, starting with the most valuable data points, and retrain the model, you can see that model performance drops (Fig 1a).",
      "( Enlarge )  \u2026 points that data Shapley considers valuable are crucially important for the model performance while leave-one-out valuation is only slightly better than random valuation (i.e., removing random points).",
      "If you go in the other direction, and start by removing the lowest valued data points, the model performance improves!",
      "Points with a low Shapley value harm the model\u2019s performance and removing them improves accuracy (Fig 1b above).",
      "If we wanted to collect more data to improve, then adding data points similar to high value data points (assuming that\u2019s something we can control in data gathering) leads to much better performance improvements than adding low value data points (Fig 1c and d above).",
      "Another experiment trained data with noisy (incorrect) labels and found that the erroneous data points ended up with low data values.",
      "Thus by inspecting data points from least valuable to most valuable it\u2019s likely that you will find and be able to correct mislabeled examples.",
      "( Enlarge )  Deliberately adding noise to images reduces data values:  The last word  Data Shapley uniquely satis\ufb01es three natural properties of equitable data valuation.",
      "There are ML settings where these properties may not be desirable and perhaps other properties need to be added.",
      "It is a very important direction of future work to clearly understand these different scenarios and study the appropriate notions of data value.",
      "Drawing on the connections from economics, we believe the three properties we listed is a reasonable starting point.",
      "And an important reminder: \u201c\u2026 we have skipped over many important considerations about the intrinsic value of personal data, and we focused on valuation in the very specific context of a training set for supervised learning algorithms.",
      "\u201c"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://proceedings.mlr.press/v97/ghorbani19c/ghorbani19c.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 98757756
  },
  {
    "blog_id": "growing-a-protocol",
    "summary": [
      "Growing a protocol Ramasubramanian et al., HotCloud\u201917  I\u2019ve been really enjoying working my way through a selection of the HotCloud papers \u2013 they\u2019re relatively short, thought-provoking, and designed to promote discussion (each paper has a set of discussion questions at the very end \u2013 great if you\u2019re looking at them in a group of some kind).",
      "Today\u2019s paper is certainly no exception.",
      "The setting is a collaboration between Elastic and Peter Alvaro\u2019s research group at the University of California.",
      "The question under investigation is how best to implement and evolve distributed system protocols without introducing unintended bugs.",
      "In this paper, we argue that tool support for implementing and evolving fault-tolerant distributed systems needs to be rethought.",
      "We advocate exploration of the (sparse) middle ground between existing testing techniques practically inadequate for addressing fault tolerance concerns and traditional verification techniques ill-suited to the continual evolution of real-world evaluations.",
      "The status quo  If you want to solve a distributed systems problem these days, you can generally start by picking up a proven protocol for the issue at hand.",
      "For example, Paxos, Raft, Primary/Backup, Chain Replication, reliable broadcast, and so on.",
      "So we might naively expect that\u2026  \u2026modern system designers can merely take mechanisms \u201coff the shelf\u201d and enjoy the guarantees of hardened subsystems while constructing otherwise novel applications.",
      "Any practitioner, however, will quickly identify this as a fallacy.",
      "Even initial protocol implementations tend to differ significantly from their specification.",
      "(A sentiment echoed in the wonderful \u2018 Use of Formal Methods at Amazon Web Services \u2019 paper).",
      "But it\u2019s really the \u2018little\u2019 changes over the lifetime of the system, the protocol optimisations over time, that can have outsize unintended consequences.",
      "We end up with a system that is, for example, \u201cessentially Primary/Backup.\u201d And while it may have the essence of Primary/Backup, does it retain the guarantees?",
      "It\u2019s very hard to know.",
      "Such a circumstance places implementors in the bad position of deriving false confidence from assertions that their implementation is \u201cessentially Primary/Backup\u201d.",
      "What we normally rely on to ensure that changes to our system don\u2019t introduce new bugs is regression testing: \u201cregression testing techniques ensure future optimisations do not re-introduce bugs previously encountered in early stages of system development.\u201d In regression testing we check that inputs known to result in bad behaviour in the past no longer do.",
      "In the ideal case, the team would be using some kind of formal verification, and specifications would be co-evolved with the code and invariants proved afresh.",
      "In practice, this rarely happens.",
      "The correctness guarantees determined at initial verification time erode as protocols evolve.",
      "The authors point out that regression testing alone is not sufficient to assert fault tolerance properties.",
      "Inputs that trigger bugs in one version of a protocol are not guaranteed to trigger the same bug in a different version.",
      "In distributed systems, a large class of bugs are tied to the execution schedule, not (just) to the inputs.",
      "As a result, regression testing, as we currently employ it, is fundamentally too weak to prevent fault tolerance regression bugs.",
      "Root cause analysis is similarly inadequate, because a set of faults triggering bugs in later versions may fail to do so in an earlier version.",
      "This difference between input-based and schedule-based approaches necessitates the use of a different approach for fault tolerance testing and verification.",
      "We are left wanting something that works like verification, but feels like testing.",
      "Elastic meets LDFI  The team at Elastic faced a problem like the one we just described.",
      "They had a data replication protocol based on Primary/Backup, and were looking to introduce a faster variant that could synchronise individual operations rather than relying on file copying.",
      "The Elastic team then made a couple of really smart moves:  \u201cSince this was a new algorithm, Elastic was looking for ways to formally verify it.\u201d  \u201cElastic engaged our research team because they wanted a technique that strikes a balance between formal verification and testing \u2013 in particular the strong correctness guarantees of the former and the agility of the latter.\u201d  The project used Lineage Driven Fault Injection (LDFI), which builds a model based on good system execution and explores only fault scenarios capable of forcing the system into a bad state.",
      "(See the earlier write-up on The Morning Paper for more details).",
      "We implemented a sequence of versions of the replication protocol and used LDFI to incrementally verify them as part of continuous integration.",
      "LDFI helped to find existing bugs relating to the implementation of the new protocol, and also demonstrated the safety (or otherwise!)",
      "of a couple of subsequent optimisations.",
      "There are many instances in the software development cycle for a bug to be introduced, the first of which is when a protocol specification is converted to an implementation.",
      "During our case study, we found a bug which manifested precisely from such a transaction scenario.",
      "(The bug related to a primary failover with partially replicated writes in progress, see section 4.1 for details).",
      "LDFI generated the scenario automatically, rather than requiring testers to have the foresight to manually derive such test cases.",
      "Originally discovered in the context of concurrent writes, subsequent analysis showed that with the right inputs a variation can occur with just a single write.",
      "\u2026 the two bugs are similar, but do not manifest from the same fault scenarios.",
      "This reinforces the claim from our motivating example that techniques such as root cause analysis as they are generally deployed would not be effective in reasoning about the fault tolerance properties of distributed systems.",
      "A seemingly minor optimisation was explored to avoid extra processing: requests have monotonically increasing sequence numbers, and if a sequence number associated with a write request had been seen before, instead of processing it (again?",
      "), the payload should be dropped and the request simply acknowledged.",
      "It turns out there\u2019s a problem with this scheme that can occur during primary failover:  Fortunately, LDFI quickly and automatically discovers such a scenario by using the initial successful execution to test fault scenarios that may cause failures\u2026 Optimization carries the risk of introducing entirely new bugs capable of breaking the end-to-end properties of the system, which is best handled by verification-based tools.",
      "A second, substantially more complicated optimisation turned out not to produce any counterexamples when run against LDFI.",
      "Seemingly simple optimisations may break guarantees, while more complex ones may not \u2013 simplicity is not a guarantee of correctness.",
      "Improving distributed software quality  Our experience at Elastic suggests approaches like LDFI are a step towards improving the state of the art in distributed software quality.",
      "Where can we go from here?",
      "The team set out a number of future directions:  Integration with semantic-aware software model checkers: \u201cAn ideal tool solution would combine the best features of LDFI (which automatically builds models of domain knowledge, but ignore concurrency and asynchrony) with state-of-the-art combined approaches such as SAMC , since we know from Fischer et al. (FLP) that some of the most fundamental difficulties of distributed systems exist at the intersection of partial failure and asynchrony!\u201d  Exploiting the CALM theorem : \u201cwe are developing a prototype system that combines the Lineage-Driven approach (utilizing explanations of what went right to reason about what could go wrong) and CALM analysis (using static analysis to prove commutativity of message processing logic) to simultaneously prune the space of faults and re-orderings.\u201d  Using probabilistic system models to embrace rather than mask the inherent uncertainties in distributed executions.",
      "Improving the debugging experience using provenance to reason about distributed executions (\u201ca young research area capable of radical growth\u201d)  Our experience using LDFI at Elastic suggests the provision of high-level explanations of how a system achieves (or fails to achieve) good outcomes are a good starting point for taming the complexity of distributed debugging.",
      "There is wide scope for improving tools for implementing, evolving, and debugging distributed software:  The state of the art is so desperately poor that it should be easy for the research community to make an impact!",
      "Feedback  The authors of the paper would love your feedback.",
      "Do you agree that classical software quality techniques such as regression testing and root cause analysis do not extend to distributed systems in their current form?",
      "Can LDFI serve as a bridge between verification and testing, as explored in this real-world application?",
      "What other tools should the team be building, and what impact could the LDFI approach have on them?"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://www.usenix.org/system/files/conference/hotcloud17/hotcloud17-paper-ramasubramanian.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 42200029
  },
  {
    "blog_id": "deepcoder-learning-to-write-programs",
    "summary": [
      "DeepCoder: Learning to write programs Balog et al., ICLR 2017  I\u2019m mostly trying to wait until the ICLR conference itself before diving into the papers to be presented there, but this particular paper follows nicely on from yesterday, so I\u2019ve decided to bring it forward.",
      "In \u2018 Large scale evolution of image classifiers \u2018 we saw how it\u2019s possible to learn a model for an image classification problem instead of trying to hand-engineer it.",
      "(Which incidentally paves the way for bootstrapping sophisticated ML systems the same way that we might bootstrap a compiler).",
      "But that\u2019s a fairly constrained domain, surely the average programmer\u2019s job is safe \u2013 we can\u2019t learn to write any old program.",
      "Or can we?",
      "Well ok, no we can\u2019t, but DeepCoder can learn how to solve simple programming tasks of the kind found in the most basic problems on programming competition websites.",
      "And it\u2019s only going to get better!",
      "Here\u2019s an example:  This is the domain of Inductive Program Synthesis (IPS).",
      "DeepCoder is shown sets of inputs and outputs, and must learn a program that will produce the given outputs when presented with the given inputs (no memorization allowed!).",
      "Fundamentally this is a guided search across the space of all possible programs.",
      "To make that tractable we need to choose a language (a DSL) in which we\u2019re going to express programs \u2013 using a general purpose language such as C++ yields far too big a search space.",
      "DeepCoder\u2019s secret sauce is a neural network that is trained to predict the kinds of functions that might be useful when trying to recreate the outputs for a given set of inputs.",
      "Knowing the most likely functions the program will ultimately need to include guides the search and helps it find solutions much faster.",
      "The approach of integrating a learning module into an IPS system is called LIPS (Learning Inductive Program Synthesis).",
      "The DeepCoding DSL  DeepCoding programs are simple sequences of function calls, the result of each call initialising a fresh variable that holds either a single integer or an integer array.",
      "The output of the program is the return value of the last function call.",
      "The DSL contains the first-order functions head, last, take, drop, access, minimum, maximum, reverse, sort, and sum, and the higher-order functions map, filter, count, zipwith`, and `scanl.",
      "The pre-defined lambda functions which can be passed to these higher-order functions are:  for map: (+1), (-1), (*2), (/2)2 (*(-1))2 (**2), (*3), (/3), (*4), (/4)  for filter and count: (>0),\u00a0(<0), (%2 == 0), (%2 == 1)  for zipwith and scanl: (+), (-), (*), min, max  There\u2019s is no explicit looping, but of course many of the provided functions do provide branching and looping internally.",
      "The Learning Module  The learning module learns to predict the probability that each of the above functions is used in a given program, based on seeing an input-output pair.",
      "To take a simple example, given the input [3, 8, 1, 7] and the output [4, 12, 28, 32] the network should predict high probability for sort and (*4).",
      "The overall network structure looks like this:  With only three hidden layers, it\u2019s actually a fairly simple structure compared to some of the models we looked at last week.",
      "First, we represent the input and output types (singleton or array) by a one-hot-encoding, and we pad the inputs and outputs to a maximum length L with a special NULL value.",
      "Second, each integer in the inputs and in the output is mapped to a learned embedding vector of size E = 20.",
      "(The range of integers is restricted to a finite range and each embedding is parametrized individually.)",
      "Third, for each input-output example separately, we concatenate the embeddings of the input types, the inputs, the output type, and the output into a single (fixed-length) vector, and pass this vector through H = 3 hidden layers containing K = 256 sigmoid units each.",
      "The third hidden layer thus provides an encoding of each individual input-output example.",
      "Finally, for input-output examples in a set generated from the same program, we pool these representations together by simple arithmetic averaging.",
      "To network outputs a log-unnormalised array of probabilities of each function appearing in the source code:  To generate training examples, DSL programs are enumerated, pruned to remove obvious issues such as redundant variables, or the existence of shorter equivalent programs (approximated by identical behaviour on a set of inputs).",
      "Valid inputs for a program are generated by putting a constraint on the output values and propagating this constraint backward through the program.",
      "Input-output pairs are then generated by picking inputs from the pre-computed valid ranges and executing the program to obtain the output values.",
      "Searching for Solutions  At this point we have a set of inputs and outputs, and some clues as to which functions are most likely to be included in a program that generated those outputs.",
      "We know use this information to guide a search.",
      "The team evaluate a few different search strategies:  Depth-first search (DFS) over programs of some maximum length T. When the search extends a partial program, it considers functions in probability order (highest probability first of course!).",
      "A sort and add scheme which maintains a set of active functions and performs DFS with the active function set only.",
      "When the search fails, the next most probable function (or set of functions) is added to the active set and the search restarts.",
      "Use of the Sketch SMT-based program synthesis tool, with a similar sort-and-add scheme used to guide its search  Use of the  program synthesis tool also using a sort-and-add scheme  Let\u2019s generate some programs!",
      "The main experiment looked at programs of length T=5 ( a search space on the order of 1010, supported by a neural network trained on programs of length T=4.",
      "The table below shows the results when trying to find 100 programs (Sketch is dropped from this part of the evaluation as it is significantly slower than the other methods).",
      "The thing to remember when looking at the results is that there is no surprise that the search strategies can actually find successful programs.",
      "So what we\u2019re really interested in is how long does it take to do so, and how much if any the learning component actually helps.",
      "In the table above, the percentage columns tell you how long it took each variation to solve 20%, 40%, and 60% of the program generation challenges out of the 100 presented.",
      "The baseline row shows how long the search takes when given a function probability distribution that simply mirrors the global incidence of each function in the 500 program test set corpus.",
      "Thus the DeepCoder row is truly telling us what difference the neural network prediction make to search time.",
      "We hypothesize that the substantially larger performance gains on Sort and add schemes as compared to gains on DFS can be explained by the fact that the choice of attribute function (predicting presence of functions anywhere in the program) and learning objective of the neural network are better matched to the Sort and add schemes.",
      "Further experiments varying the length of the test programs (from 1 to 4 functions), and the length of the generated programs (from 1 to 5 functions) showed that the neural network can generalize to programs of different lengths than the programs in the training set.",
      "Our empirical results show that for many programs, this technique [neural network guided search] improves the runtime of a wide range of IPS baselines by 1-3 orders.",
      "We have found several problems in real online programming challenges that can be solved with a program in our language, which validates the relevance of the class of problems that we have studied in this work.",
      "In sum, this suggests that we have made significant progress towards being able to solve programming competition problems, and the machine learning component plays an important role in making it tractable.",
      "Of course, as the authors themselves point out, \u201cthere remain some limitations\u2026\u201c."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://arxiv.org/pdf/1611.01989.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 90525296
  },
  {
    "blog_id": "secure-coding-practices-in-java-challenges-and-vulnerabilities",
    "summary": [
      "Secure coding practices in Java: challenges and vulnerabilities Meng et al., ICSE\u201918  TL;DR : don\u2019t trust everything you read on Stack Overflow.",
      "Meng et al. conduct a study of Stack Overflow posts relating to secure coding practices in Java to find out the hot topics,  what people struggle with, and whether or not the accepted answers are actually following security best practices.",
      "We conducted an empirical study on Stack Overflow posts, aiming to understand developer\u2019s concerns on Java secure coding, their programming obstacles, and insecure coding practices.",
      "We observed a wide adoption of the authentication and authorization features provided by Spring Security \u2014 a third-party framework designed to secure enterprise applications\u2026  Well, how could I resist reading that!",
      "(Some readers may know that I was for many years the CTO of SpringSource).",
      "Spring Security does come in for some flak in this paper for the high volume of questions that are asked relating to it.",
      "There\u2019s no calibration though for underlying popularity.",
      "One of the reasons there are a lot of questions, I posit, is that there are an awful lot of users of Spring Security.",
      "Spring Boot applications will use Spring Security, and Spring Boot has been growing at an amazing rate these last few years (many millions of downloads every month, and still ticking along at over 300% y-o-y growth ).",
      "Another reason is that Spring Security has been around a long time, and the survey covers questions going back to 2008, when Spring Security used an older XML-based configuration style.",
      "Spring Security genuinely was hard to configure in the early days.",
      "In fact, the origins of Spring Security were in a project called \u201cAcegi\u201d created by the wonderful Ben Alex.",
      "Ben Alex himself introduced a phrase that became part of the SpringSource folklore when talking about a project to simplify Acegi configuration \u201cwhenever someone uses Acegi, a fairy dies.\u201d That was all a long time ago, and the modern Spring Security is a very different beast.",
      "The authors crawl 22,195 Stack Overflow posts containing the keywords \u2018Java\u2019 and \u2018security,\u2019 then filtered out those without accepted answers or with negative votes, and those without code snippets.",
      "After manually inspecting the remaining posts for relevance, the result is a set of 503 posts with dates from 2008 to 2016.",
      "The posts are then analysed to determine the most common security concerns, programming challenges, and vulnerabilities.",
      "What are people asking about?",
      "Most questions are to do with how to get something to work (implementation questions), rather than questions seeking to understand security design.",
      "The \u2018how do I\u2026\u2019 questions were further broken down based on the platform being discussed: Java platform security, Java EE security, or Spring Security.",
      "Spring Security gets the most questions (56%)!",
      "Seven major security topics emerge from a second-level classification of the implementation posts:  Java platform: cryptography, access control, & secure communication  Java EE security  Spring Security:  authentication, authorisation, and configuration  We can see the volume of questions in these topic areas growing and the distribution changing over time.",
      "During 2009-2011, most posts were about Java platform security.",
      "However, since 2012, the major security concern has shifted to securing enterprise Java applications (including both Java EE security and Spring Security).",
      "Specifically, Spring Security has taken up over 50% of the posts published every year since 2013.",
      "Common challenges (over the 8 year period)  Taking the five most popular topic areas (authentication, cryptography, Java EE security, and secure communication), the authors did a further analysis to understand common challenges.",
      "The biggest topic area by far is authentication (more than all the others combined).",
      "Recall that the authentication topic is specific to Spring Security.",
      "Here people want to know (i) how to integrate Spring Security with different application servers and frameworks, (ii) how to configure Spring Security using XML (84 q\u2019s) or Java (42 q\u2019s), and (iii) how to convert XML-based configurations to Java-based ones.",
      "The Spring family of projects originally all used exclusively XML-based configuration.",
      "But this has not been true for a long time.",
      "Today the preferred approach is Java based configuration, which has been supported in Spring Security since the 3.2.0 release in 2013.",
      "Even so,\u2026  \u2026 there are lots of annotations and APIs of classes, methods, and fields available to specify different configuration options\u2026 implicit constraints and subtle requirements are not well documented.",
      "(Here are the latest guides for the 5.0.6 release.",
      "If you\u2019re starting from scratch, go the Spring Boot way ).",
      "Developers also struggle converting older XML-based projects to use Java configuration.",
      "When it comes to cryptography (Java platform), users struggle with poor error messages, implicit constraints, and the difficulties involved in encrypting data using one programming language, and decrypting it in another.",
      "The cryptography posts were mostly about key generation and usage.",
      "Developers asked these questions mainly due to clueless error messages, cross-language data handling, and implicit API usage constraints.",
      "Access control posts mostly concerned how to restrict untrusted code from accessing certain packages, classes, and methods.",
      "There were also 9 posts on applets, which highlights some of limitations of a study that goes back to 2008.",
      "The only good answer to a question on applets in 2018 is \u201cdon\u2019t use applets!\u201d.",
      "Security communication posts mainly discussed the process of establishing SSL/TLS connections.",
      "This process contains so many steps that developers were tempted to accept a broken solution to simply bypass the security verification.",
      "Vulnerabilities  Does Stack Overflow give good advice?",
      "Sometimes, yes!",
      "But\u2026 \u201cwe identified security vulnerabilities in the accepted answers of three frequently discussed topics: Spring Security\u2019s csrf(), SSL/TLS, and password hashing\u201c.",
      "A common theme seems to be \u201cmy security policy is stopping me doing something,\u201d answered by \u201cdisable security!\u201d  Spring Security enables CSRF protection by default, and the corresponding token needs to be included in PATCH, POST, PUT and DELETE methods.",
      "Fail to do that, and things won\u2019t work as expected.",
      "Or you could just disable CSRF protection!",
      "(Don\u2019t).",
      "In one instance, after accepting the vulnerable solution, an asker commented \u201cAdding csfr().disable() solved the issue!!!",
      "I have no idea why it was enabled by default.\u201d  In the SSL/TSL topic area certification verification is a pain.",
      "That doesn\u2019t mean you don\u2019t need to do it though!",
      "9 out of 10 posts in this area had an accepted answer with an insecure solution bypassing security checks by trusting all certificates and/or allowing all hostnames.",
      "The implications are not always well understood.",
      "Disabling the SSL certificate validation process completely destroys the secure communication protocol, leaving clients susceptible to man-in-the-middle (MITM) attacks\u2026 A developer justified the verification-bypassing choice by stating \u201cI want my client to accept any certification (because I\u2019m only ever pointing to one server).\u201d  When it comes to password hashing, there\u2019s also a bunch of outdated and wrong advice around.",
      "3 out of 6 hashing-relevant posts accepted vulnerable solutions as correct answers, indicating that developers were unaware of best secure programming practices.",
      "Incorrect security information may propagate among Stack Overflow users and negatively influence software development.",
      "Recommendations  The authors have the following common sense  recommendations to make as a result of their study:  Developers should conduct security testing to check whether features work as expected.",
      "Security checks should not be disabled \u2013 even as a \u2018temporary\u2019 fix in dev or test.",
      "Be careful following advice found on Stack Overflow as some solutions may be out of date or insecure.",
      "Library designers should deprecate APIs not intended to be used anymore, improve error messages, and design simplified APIs with strong security defenses implemented by default.",
      "Tool builders can help by creating automatic tools to diagnose security errors, locate buggy code, and suggest security patches or solutions.",
      "You may not be surprised to hear that\u2026  \u2026 our future work is on building automatic or semi-automatic security bug detection and repair tools."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://arxiv.org/pdf/1709.09970",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 45518901
  },
  {
    "blog_id": "a-cloud-based-content-gathering-network",
    "summary": [
      "A cloud-based content gathering network Bhattacherjee et al., HotCloud\u201917  We all know what a content distribution network is, but what\u2019s a content gathering network?!",
      "CDNs are great, but their benefits are reduced for clients that have a relatively slow last mile connection \u2013 especially given that a typical web page request will involve many round trips.",
      "It is worth noting that such large last-mile latencies (e.g 100ms) are not atypical in many parts of the world.",
      "There is, of course, significant ongoing work both on cutting down the number of RTTs a web request takes, and on improving last-mile latencies.",
      "However, realizing these benefits across the large eco-system of web service providers and ISPs could still take many years.",
      "How do we improve performance for users in these environments today?",
      "The core idea of a CGN is to gather all the information needed for a page load in a place that has a short RTT time, and then transfer it to the client in (ideally) one round trip.",
      "At a cost of about $1 per user, the authors show that it can reduce the median page load time across 100 popular web sites by up to 53%.",
      "That\u2019s definitely something users would love!",
      "CDN vs CGN  CDNs locate customer\u2019s content as close to the end user as possible, whereas a CGN tries to locate (a proxy for) the end user as close to the content as possible.",
      "Until you take the number of round trips needed to load a web page into account, it doesn\u2019t seem on the surface like it would make much difference.",
      "The following picture helps to illustrate what\u2019s going on:  Suppose we have a web server serving some site on the east coast of the US.",
      "A CGN node in for example EC2 US-East will probably have very low latency access to the site.",
      "So it can make the multiple trips needed to access a page locally and then transfer the content to the client.",
      "The client and the CGN node use aggressive transport protocols, with large initial window sizes, and without the need of an additional handshake (unlike TCP).",
      "Here\u2019s the full picture: the client browser is configured to forward web requests to a local HTTP proxy service, which then forwards the URL to the CGN node nearest to the hosting web server.",
      "The CGN node runs a headless browser, which starts downloading the web page.",
      "As the content is downloaded, the CGN node forwards it to the client\u2019s local proxy in parallel using a TCP byte stream, and the local proxy serves the content to the browser.",
      "How does the proxy service know which CGN node is nearest to the hosting web server?",
      "A network of CGN nodes is deployed (e.g., at least one in each public cloud region), and each node makes independent latency measurements to a list of sites, and exchanges information with the other nodes.",
      "(This is a few MB, even for a million popular web sites).",
      "Clients obtain the mapping from CGN nodes.",
      "Experiments conducted by the authors show that the mappings are stable enough that updates can be infrequent (e.g., once a day).",
      "The rise of public clouds changes the game  We observe that present public cloud infrastructure provides enough global points of presence to be able to reach within close proximity of most popular Web services, and thus provides a natural platform to build a \u201ccontent gathering network,\u201d which operates on behalf of users.",
      "On the other side of the coin, with increasing numbers of websites themselves being cloud hosted, the distances between public cloud infrastructure and popular websites can be very small indeed.",
      "The authors set out to measure just how low latency might be\u2026  We deployed one node in each of Amazon\u2019s 14 data center regions.",
      "From each of these 14 nodes, we measure round trip times to each Web server hosting the top 100,000 web sites in Alexa\u2019s list of popular sites\u2026 For a smaller set of Web servers (top 10,000), we also similarly measured RTTs from clients in Lahore, Pakistan and Zurich, Switzerland.",
      "Both of these clients are university-hosted and \u201creal\u201d end-user connectivity is likely to be worse.",
      "The results?",
      "Taking the median (and 90th pecentile), for the top 10,000 domains, EC2 is within 4.8ms (39.5ms), Zurich within 39.8ms (215.8ms), and Lahore within 275.8ms (471.8ms).",
      "\u2026 median RTT from EC2 is 8x smaller than from Zurich, and 57x smaller than from Lahore.",
      "For the top 100,000 domains, the median RTT from EC2 is still only 7.2ms.",
      "Over a one-week period, the latencies from the EC2 node a domain is mapped to stayed very stable.",
      "Page load time improvements  For their evaluation, the team used m4.10xlarge EC2 instances as CGN nodes, and PhantomJS as the headless browser.",
      "Two CGN nodes were operated: one in North Carolina, USA, and one in Frankfurt, Germany.",
      "The test was done only with those domains which would have mapped to either of these two locations.",
      "The current implementation does not quite hit the goal of only one round trip between CGN and client due to differences in behaviours between client-side browsers and headless browsers, thus some content is still fetched from traditional CDN nodes.",
      "\u201cBut we do not believe that the 1-RTT goal is unreachable, and are working towards it.\u201d  Web pages were loaded with and without the CGN, from a client in Lahore.",
      "Even though CDN content requests still went directly to the CDN servers (for now), the results are still impressive.",
      "Out of the Alexa top-10K, we evaluate the top 100 and random 100 web pages which map to either of our two CGN nodes.",
      "For the top-100 set, the median page load time reduces from (the default) 16.1 seconds to 7.6 seconds with CGN \u2013 a reduction of 53%.",
      "For the random 100 set, median page load time is reduced by 43.2%.",
      "We also get a quick comparison with Google\u2019s Flywheel using the \u201cData Saver\u201d extension for Chrome.",
      "Testing against the top 30 domains, CGN still shows significant gains beyond Flywheel\u2019s.",
      "We note that Flywheel is a complex system with numerous optimizations, including compression, caching, and prefetching at the proxies.",
      "These techniques are all orthogonal to ours, and could readily be added to our approach for even more reduction in PLTs.",
      "Given how well CGNs work, the authors are moved to ask \u201ccan we leverage public cloud infrastructure and the observation that most content is hosted in (or near) this infrastructure to entirely eliminate CDNs from Web site delivery?\u201d (Note the scoping to web sites, characterised by large numbers of relatively small requests.",
      "For applications like video streaming, CDNs will always win).",
      "What about\u2026?",
      "How much might it cost to run a CGN network?",
      "Some back of the envelope calculations by the authors suggest that with a network shared by multiple users, the cost to provide the service comes in at about $1 per user per month.",
      "HTTPS.",
      "This is a big one for me.",
      "While technically supporting https is feasible, the client browser would need to trust the CGN node acting as at TLS/SSL proxy.",
      "\u201cThere is also scope for using techniques like Intel SGX to hide the content of client-server interactions from the CGN nodes as a privacy enhancement, although CGN nodes will still be able to see which servers a client connects to.\u201d The authors note that CDNs already operate under a similar trust model, but to me there\u2019s a big difference between shared static resources and dynamic content containing sensitive personal information.",
      "Commercial model  Who would host a CGN service.",
      "The authors have three suggestions:  Cloud providers could provide this as a service, to attract more web service providers to their infrastructure  Browser vendors competing for market share could run the service  Web service providers themselves may incur the expense  Rather than seeing this as a reduction in business for CDN vendors though, there seems to be an obvious fourth option to me: CDN vendors themselves could operate CGN capabilities alongside their existing CDN network."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://www.usenix.org/system/files/conference/hotcloud17/hotcloud17-paper-bhattacherjee.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 33714440
  },
  {
    "blog_id": "how-much-up-front-a-grounded-theory-of-agile-architecture",
    "summary": [
      "How Much Up-Front?",
      "A Grounded Theory of Agile Architecture \u2013 Waterman et al. 2015  It\u2019s time for something a little bit different, so this week I thought I\u2019d bring you a selection of papers from the recently held  ICSE\u201915 conference (International Conference on Software Engineering).",
      "To kick things off, today\u2019s choice looks at the question of how architecture fits into an agile process.",
      "Just how much effort should you spend up-front on architecture?",
      "And what happens if you defer architectural decisions to later?",
      "It\u2019s all a bit of a conundrum\u2026  Software architecture is the high-level structure and organisation of a software system.",
      "Because architecture defines system-level properties, it is difficult to change after development has started, causing a conflict with agile development\u2019s central goal of better delivering value through responding to change.",
      "To maximise agility, agile developers often avoid or minimise architectural planning, because architecture planning is often seen as delivering little immediate value to the customer.",
      "Too little planning however may lead to an accidental architecture that has not been carefully thought through, and may lead to the team spending a lot of time fixing architecture problems and not enough time delivering functionality (value).",
      "A common strategy is to do \u2018just enough\u2019 architecture up-front.",
      "But how much is just enough?",
      "This paper identifies six forces that shape the answer to that question, leading to five different agile architecture strategies that can be used.",
      "The grounded theory reference in the paper title is a reference to the approach used to derive these forces and strategies.",
      "I found it interesting to read the explanation of  grounded theory in the paper, and encourage you to take  look if this piques your interest (section II C).",
      "The short version is that it\u2019s an iterative bottom-up approach based on coding and clustering information given during structured interviews.",
      "The end result is a theory explaining how things relate to each other.",
      "This is in contrast to starting with a hypothesis and designing a test to prove or disprove it.",
      "We interviewed 44 participants in 37 interviews.",
      "Participants were gathered through industry contacts, agile interest groups and through direct contact with relevant organisations.",
      "Almost all participants were very experienced architects, senior developers, team leaders and development managers with at least six years\u2019 experience (twenty years was not uncommon), and most were also very experienced in agile development.",
      "The six forces and five strategies that emerged from the work are summarised in the figure below.",
      "Six Forces that Influence the Approach to Agile Architecture  Requirements Instability due to incomplete or changing requirements favours deferring detailed requirements gathering, analysis, and architecture design in return for getting early feedback from the customer based on an initial implementation.",
      "Technical Risk through challenging or demanding architecturally significant requirements, through having many integration points with other systems, and by involving legacy systems, is a force in favour of more up-front architecture.",
      "Participants also identified integration points, or interfaces to external systems, as a major source of complexity in the systems being developed, particularly when the other systems are legacy or are built from different technologies.",
      "Integration with other systems require data and communications to be mapped between the systems, adding to the up-front effort to ensure integration is possible with the technologies being used.",
      "For example: \u201cToday\u2019s systems tend to be more interconnected \u2013 they have a lot more interfaces to external systems than older systems which are typically standalone.",
      "They have a lot higher level of complexity for the same sized system.\u201d (P14, solutions architect)  Interestingly a similar argument can be made about the complexity introduced by microservices.",
      "Early Value : there is a need to start getting early value out of a system or product being built (not just feedback) before all functionality has been implemented.",
      "For example, to take advantage of a market opportunity or to use the revenue generated to fund further development.",
      "This force works against too much up-front architecture:  Teams that deliver early value must reduce the time to the first release by spending less time on up-front architecture design.",
      "They achieve this by reducing the planning horizon \u2013 how far ahead the team considers (high level) requirements for the purpose of architecture planning.",
      "Team Culture that is people-focused and collaborative is very important in fostering collaboration.",
      "\u201cA team without a trusting people-focused and collaborative culture has to rely on documentation for communication and formal plans, and hence requires more up-front effort to guide development.\u201d  Customer Agility is often required to match an agile approach to architecture.",
      "A customer must have an agile culture that is similar to the team\u2019s culture, whether the team is in-house or an ISV (independent software vendor), for the team to be truly agile.",
      "A highly-agile team will not fit in well with a heavyweight process-oriented organisation that prefers planning and formal communication.",
      "Experience enables pattern recognition that can short-circuit the amount of work needed to be done up-front.",
      "While generally important for all software development methods, experience is more important in agile development because the tacit knowledge and implicit decision-making that come with experience supports agile development\u2019s reduced process and documentation, and reduces the up-front effort: \u201cYou implement certain patterns without thinking [\u2026] you\u2019ve done this kind of pattern for solving this kind of a problem, without even thinking that this is the way that you are going.\u201d (P16b, head of engineering)  Five Strategies for Implementing Agile Architecture  In response to these forces there are five strategies that a team may choose from\u2026  Respond to change  A team\u2019s ability to use S1 (RESPOND TO CHANGE) is directly related to how agile it is.",
      "S1 increases the architecture\u2019s agility by increasing its modifiability and its tolerance of change, and allows the team to ensure the architecture continuously represents the best solution to the problem as it evolves.",
      "We are given five tactics for responding to change: keep designs simple, prove the architecture with code iteratively, use good design practices, delay-decision making, and plan for options.",
      "The first and last of these are in tension, which becomes more obvious when we juxtapose the descriptions in the paper:  Keeping designs simple means only designing for what is immediately required: no gold plating and no designing for what might be required or for what can be deferred\u2026 Planning for options means building in generality and avoiding making decisions that are unnecessarily constrained and which may close off possible future requirements without significant refactoring.",
      "Address Risk  S2 (ADDRESS RISK) reduces the impact of risk before it causes problems, and is usually done up-front, particularly for risk relating to system-wide decisions (for example, risk in selecting the technology stack or top-level styles).",
      "Using S2, a team designs the architecture in sufficient detail that it is comfortable that it is actually possible to build the system with the required Architecturally Significant Requirements with a satisfactory level of risk.",
      "Emergent Architecture is likely to be used when developing a minimum viable product in response to the Early Value force.",
      "The team makes only the minimal architectural decisions up-front such as selecting the technology stack and the highest level architectural styles and patterns.",
      "Big Design Up-Front  S4 (BIG DESIGN UP-FRONT) requires that the team acquires a full set of requirements and completes a full architecture design before development starts.",
      "There are no emergent design decisions, although the architecture may evolve during development.",
      "S4 is undesirable in agile development because it reduces the architecture\u2019s ability to use S1 (RESPOND TO CHANGE) by increasing the time to the first opportunity for feedback, increasing the chance that decisions will need to be changed later, and increasing the chance of over-engineering.",
      "While S4 may be considered the case of addressing risk (S2) taken to the extreme, in reality the use of S4 is driven primarily by an absence of CUSTOMER AGILITY (F5) rather than the presence of TECHNICAL RISK (F2).",
      "Using Frameworks and Template Architectures using software frameworks and the reference architectures from their creators provides the benefit of standard solutions to standard problems."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://ecs.victoria.ac.nz/foswiki/pub/Main/TechnicalReportSeries/ECSTR15-01.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 15084340
  },
  {
    "blog_id": "understanding-real-world-concurrency-bugs-in-go",
    "summary": [
      "Understanding real-world concurrency bugs in Go Tu, Liu et al., ASPLOS\u201919  The design of a programming (or data) model not only makes certain problems easier (or harder) to solve, but also makes certain classes of bugs easier (or harder) to create, detect, and subsequently fix.",
      "Today\u2019s paper choice studies concurrency mechanisms in Go.",
      "Before we dive in, it might be interesting to pause for a moment and consider your own beliefs about Go, which may well include some of the following:  Go was explicitly designed to make concurrent programming easier and less error-prone  Go makes concurrent programming easier and less error-prone  Go programs make heavy use of message passing via channels, which is less error prone than shared memory synchronisation  Go programs have less concurrency bugs  Go\u2019s built-in deadlock and data race detectors will catch any (most?)",
      "bugs you do let slip into your code  The first of those statements is true.",
      "For the remaining statements, you can use the data from this research to re-evaluate how strongly you want to hold those opinions\u2026  We perform the first systematic study on concurrency bugs in real Go programs.",
      "We studied six popular Go software [projects] including Docker, Kubernetes, and gRPC.",
      "We analyzed 171 concurrency bugs in total, with more than half of them caused by non-traditional, Go-specific problems.",
      "Apart from root causes of these bugs, we also studied their fixes, performed experiments to reproduce them, and evaluated them with two publicly-available Go bug detectors.",
      "The six applications studied were Docker, Kubernetes, etcd, CockroachDB, gRPC, and BoltDB, so that\u2019s a lot of important real-world Go-code right there.",
      "The analysis begins by studying how these applications actually make use of Go concurrency primitives, before going on to study concurrency related bugs from their issue trackers.",
      "These bugs are categorised on two main dimensions: the observed behaviour (blocking or non-blocking), and the type of concurrency primitive that is the cause (shared memory or message passing).",
      "Let\u2019s begin with a very quick recap of the main concurrency mechanisms in Go.",
      "Concurrency in Go  A major design goal of Go is to improve traditional multi-threaded programming languages and make concurrent programming easier and less error-prone.",
      "For this purpose, Go centers its multi-threading design around two principles: 1) making threads (called goroutines) lightweight and easy to create and 2) using explicit messaging (via channels) to communicate across threads.",
      "Goroutines are lightweight user-level threads (\u2018green\u2019 threads).",
      "A goroutine is created by adding the keyword go before a function call, including to an anonymous function.",
      "Local variables declared before an anonymous function are accessible within it and potentially shared.",
      "Channels are used to send data and states across goroutines, and may be buffered or unbuffered.",
      "When using unbuffered channels a goroutine will block on send (receive) until another goroutine is receiving (sending).",
      "The select statement allows a goroutine to wait on multiple channel operations, if more than one case is valid Go selects one at random.",
      "Go also has traditional synchronisation primitives including mutexes, condition variables, and atomic variables.",
      "How Go concurrency primitives are used in practice  The six applications make relatively heavy use of goroutines, especially with anonymous functions.",
      "An especially interesting comparison can be made in the case of gRPC, which has both a C implementation and a Go implementation.",
      "The following table shows the ratio of the number of goroutines created in gRPC-Go compared to gRPC-C, when processing the same number of requests.",
      "In this comparison goroutines tend to shorter lifetimes than the threads created in the C version, but are created much more frequently.",
      "This more frequent use of goroutines is to be expected as its something the Go language encourages.",
      "If we look at the use of concurrency primitives across the board in all of the applications, one more surprising finding is that shared memory synchronisation operations are still used more often than message passing:  The most frequently used message-passing primitive is chan, responsible for between 18.5% and 43% of all usages.",
      "So we have a situation in which traditional shared memory communication is still heavily used, in conjunction with significant amounts of message passing primitives.",
      "From a bug perspective that means we have all the exciting bug possibilities that shared memory communication affords, together with all of the bug possibilities message passing affords, and bugs caused be the interaction of these two styles!",
      "Go concurrency bugs  The authors searched the GitHub commit histories of the applications to find commits fixing concurrency bugs (3,211).",
      "From these 171 were randomly selected for study.",
      "Bugs are categorised into blocking bugs and non-blocking bugs.",
      "A blocking bug occurs when one or more goroutines are unintentionally stuck in their execution and cannot move forward.",
      "This definition is broader deadlocks and can include situations with no circular waits, but instead waits for resources that no other goroutines supply.",
      "85 of the bugs were blocking, and 86 were non- blocking.",
      "Bugs are also categorised in a second dimension according to whether they relate to shared memory protection (105 bugs) or message passing (66 bugs).",
      "Blocking bugs  Looking at the blocking bugs first of all, 42% of these are related to shared memory, and 58% are related to message passing.",
      "Recall as well that shared memory primitives are more widely used than message passing primitives.",
      "Contrary to the common belief that message passing is less error-prone, more blocking bugs in our studied Go applications are caused by wrong message passing than by wrong shared memory protection.",
      "The shared memory based bugs include all the usual suspects, with a few new twists due to Go\u2019s implementation of RWMutex and Wait (see \u00a75.1.1).",
      "For message-passing related bugs, many of these are due to missing sends or receives on channels, or closing channels.",
      "All blocking bugs caused by message passing are related to Go\u2019s new message passing semantics like channel.",
      "They can be difficult to detect especially when message passing operations are used together with other synchronization primitives.",
      "Contrary to common belief, message passing can cause more blocking bugs than shared memory.",
      "Investigating the fixes for these bugs showed that once understood, the fixes are fairly simple, and the type of fix is correlated with the bug cause.",
      "This suggests that fully-automated or semi-automated tools for fixing blocking bugs in Go may be a promising direction.",
      "Go\u2019s built-in deadlock detector was only able to detect two of the 21 blocking bugs reproduced in the study.",
      "Non-blocking bugs  More non-blocking bugs are caused by misuse of shared memory primitives than message passing.",
      "About half of these are caused by \u2018traditional\u2019 shared memory bug patterns.",
      "There are also several bugs caused by a lack of understanding of Go language features, especially the sharing of local variables declared before an anonymous function used in a goroutine, and the semantics of WaitGroups.",
      "New programming models and new libraries that Go introduced to ease multi-thread programming can themselves be the cause of more concurrency bugs.",
      "While message-passing based non-blocking bugs were comparatively less common, \u201cthe intricate design of message passing in a language can cause these bugs to be especially hard to find when combining with other language-specific features.\u201d  Interestingly, programmers fixing bugs caused by misuse of shared memory primitives showed a preference for using message passing to do so.",
      "Go\u2019s data race detector was able to detect half of the reproduced non-blocking bugs in the study.",
      "Wrapping up  Surprisingly, our study shows that it is as easy to make concurrency bugs with message passing as with shared memory, sometimes even more.",
      "Programmers have to have a clear understanding of:  goroutine creation with anonymous functions  the usage of buffered vs unbuffered channels  the non-determinism of waiting for multiple channel operations using select  correct use of the special library time  Although each of these features were designed to ease multi-threaded programming, in reality, it is difficult to write correct Go programs with them.",
      "I regret that I didn\u2019t have space in this write-up to include the many illustrative code samples highlighting details of bugs.",
      "If you\u2019re actively developing in Go, the full paper is well worth a read to study them."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://songlh.github.io/paper/go-study.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 43018598
  },
  {
    "blog_id": "learning-networking-by-reproducing-research-results",
    "summary": [
      "Learning networking by reproducing research results Yan & McKeown et al., SIGCOMM\u201917  Students taking Stanford\u2019s Advanced Topics in Networking class have to select a networking research paper and reproduce a result from it as part of a three-week pair project.",
      "At the end of the process, they publish their findings on the course\u2019s public Reproducing Network Research blog.",
      "It\u2019s well worth having a look around the blog: the students manage to achieve a lot in only three weeks!",
      "In the last five years, 200 students have reproduced results from 40 papers.",
      "In \u2018Learning networking by reproducing research results\u2019 the authors explain how this reproduction project came to be part of the course, what happens when students try to reproduce research, and the many benefits the students get from the experience.",
      "It\u2019s a wonderful and inspiring idea that I\u2019m sure could be applied more broadly too.",
      "We began the project as a means of teaching both engineering rigor and critical thinking, qualities that are necessary for careers in networking research and industry.",
      "We have observed that reproducing research can simultaneously be a tool for education and a means for students to contribute to the networking community.",
      "Why a reproducibility project?",
      "In high-school science students repeat well-known experiments in the lab, and it is widely agreed that this gives them a much deeper understanding of the underlying concepts.",
      "Our main goal for adapting this scientific approach to our networking class is for students to obtain a detailed, in-depth understanding of a significant paper, its key ideas, and its key results.",
      "Over and above just getting to the results, the act of recreation itself proves to be valuable and rewarding \u2013 perhaps more so even than whether or not identical results are actually achieved in the end.",
      "In fact, we find that students learn a huge amount when their experiments yield different results from the original research: they must figure out where the discrepancies lie and discern if there are unstated assumptions or inaccuracies in their own results or the published results.",
      "This is a fascinating and educational experience, and often a good lesson in diplomacy.",
      "Studying a paper in order to reproduce it leads to much deeper reflection on, and understanding of the paper.",
      "You could think of it as the logical fourth pass of Keshav\u2019s three-pass approach in \u2018 How to Read a Paper .\u2019 It also helps to instil in future researchers the principle that their results should be reproducible by others whenever possible.",
      "Publishing their own reproducible results on the course blog encourages the whole community to do likewise.",
      "How the projects are run  Students work in pairs and have three weeks out of a ten-week course to complete the assignment.",
      "ONE: The first step is to choose a central figure or table from a research paper of interest.",
      "To get the students started, we provide a list of suggested conferences and research publications that we think make good examples, and we encourage students to choose more recent works, or ones that have not yet been attempted by students in previous course offerings.",
      "At Stanford, we have had students successfully reproduce results ranging from widely cited papers such as Hedera and DCTCP to traditional papers like RED, to cutting-edge, as-yet unpublished work like SPDY.",
      "Here are the most popular chosen papers:  TWO: The students then need to figure out their strategy for trying to reproduce the results, with use of either the Mininet or Mahimahi emulators encouraged.",
      "THREE: The next step, which I\u2019m sure is where part of the magic happens, is that the students are helped to contact the paper authors.",
      "Opening up this communication channel between students and researchers has two main benefits: the first is for the student, who now has a primary source to contact regarding the tools, setup, workload and use-cases of the given experiment or research tool.",
      "The second is for the researcher, who is now aware that his or her work is being analyzed critically\u2026 the researcher will have additional feedback on the benefits, caveats, and persistence of his or her findings.",
      "FOUR: Then of course the students work with the instructors, peers, and teaching assistants to recreate the research.",
      "FIVE: A public blog is posted, which must contain all the code and workload in order for someone else to easily repeat experiments too.",
      "(And many do come to the blog for that purpose).",
      "SIX: The results in every blog post are verified using peer validation \u2013 every student group is required to replicate the results of another student group.",
      "\u201cThis reproduction effort is required to be an easy two-step process: (1) download and install any code, and (2) click \u2018run.\u2019 All code must be available in public code repositories.\u201d  Most projects are successful, although a few are not.",
      "Sometimes students can recreate the original work almost perfectly (subject to scaling or computational resource limits).",
      "Other times students find discrepancies between their own results and those in the paper, and investigate why.",
      "For example, one group found that the settings used for ECMP had a material impact on results, but the configuration used in the original testbed was not available.",
      "Sometimes other changes in the environment (such as TCP implementations moving on since the original date of publication of the paper) hamper recreation, and other times of course the students end up being a little bit too ambitious in their undertakings!",
      "The teaching staff try to coach the students to help them avoid this latter mistake.",
      "Benefits for participants and the research community  My favourite piece of the whole process is the way that it tears down a mental barrier that might otherwise hold back students from full engagement in the research community:  An unexpected outcome of this project is an increased role of students in the networking research community.",
      "While designing and running the experiments, students had to interact with the original authors, new researchers who came across our course blog, and even developers of the emulators or simulators.",
      "We believe the benefits of these interactions go both ways; the networking community at large can also benefit from these student research reproduction projects.",
      "Students themselves found multiple benefits from the course:  \u201cI specifically like the level of familiarity I got with the paper.",
      "There\u2019s a level you can only get by reproducing or implementing it.\u201d  It introduced them to cutting-edge research.",
      "For example, one pair were inspired to reproduce QJump .",
      "It gave them confidence to use research results in their own work.",
      "For example, one of the students working on the QJump project went on to implement a similar scheduler in her own research, \u201cwhich is something that I wouldn\u2019t have done if [I had just read] the paper.\u201d  It made them realise that papers could be understood, \u201ceven by students who have taken only two courses in networking, and results can be reproduced in part.\u201d  They felt good about publishing the blog posts as a contribution in their own right.",
      "\u201c[From an educational standpoint,] blog posts are easier to read than papers.",
      "If there is one cool idea from a paper that you can reproduce and put into a blog post, I think that could be very valuable.\u201d  The practical skills gained from the reproduction experiments proved useful when students went on to work in industry.",
      "The last word  We have found that the experience is rewarding and interesting for the students, and it gives them a chance to interact with researchers.",
      "In addition, we have learned that documenting the results of these reproduction studies is an essential resource for both future students and the research community at large.",
      "We hope that the materials presented in this editorial inspire you to consider offering similar projects in your graduate networking courses too.",
      "I wonder where else this approach might be applied.",
      "For example, in distributed systems students might try to model an algorithm using TLA+ (as Murat Demirbas teaches them to do in his course: see \u2018 My experience with using TLA+ in distributed systems classes \u2019), and in programming languages papers critical proofs could be reconstructed in Coq."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://ccronline.sigcomm.org/wp-content/uploads/2017/05/acmdl17-97.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 97291551
  },
  {
    "blog_id": "deep_residual_learning_for_image_recognition",
    "summary": [
      "Why  Deep plain/ordinary networks usually perform better than shallow networks.",
      "However, when they get too deep their performance on the training set decreases.",
      "That should never happen and is a shortcoming of current optimizers.",
      "If the \"good\" insights of the early layers could be transferred through the network unaltered, while changing/improving the \"bad\" insights, that effect might disappear.",
      "What residual architectures are  Residual architectures use identity functions to transfer results from previous layers unaltered.",
      "They change these previous results based on results from convolutional layers.",
      "So while a plain network might do something like output = convolution(image), a residual network will do output = image + convolution(image).",
      "If the convolution resorts to just doing nothing, that will make the result a lot worse in the plain network, but not alter it at all in the residual network.",
      "So in the residual network, the convolution can focus fully on learning what positive changes it has to perform, while in the plain network it first has to learn the identity function and then what positive changes it can perform.",
      "How it works  Residual architectures can be implemented in most frameworks.",
      "You only need something like a split layer and an element-wise addition.",
      "Use one branch with an identity function and one with 2 or more convolutions (1 is also possible, but seems to perform poorly).",
      "Merge them with the element-wise addition.",
      "Rough example block (for a 64x32x32 input):  input (64x32x32) --> identity --------------> + --> output (64x32x32)        |                                      ^        ------------> conv 3x3 --> conv 3x3 ---|  An example block when you have to change the dimensionality (e.g. here from 64x32x32 to 128x32x32):  to: 128x32x32 input (64x32x32) --> conv 1x1 -----------------------------> + --> output (128x32x32)        |                                                     ^        ------------> conv 1x1 ----> conv 3x3 --> conv 1x1 ---|                      to: 32x32x32                to: 128x32x32  The authors seem to prefer using either two 3x3 convolutions or the chain of 1x1 then 3x3 then 1x1.",
      "They use the latter one for their very deep networks.",
      "The authors also tested:  To use 1x1 convolutions instead of identity functions everywhere.",
      "Performed a bit better than using 1x1 only for dimensionality changes.",
      "However, also computation and memory demands.",
      "To use zero-padding for dimensionality changes (no 1x1 convs, just fill the additional dimensions with zeros).",
      "Performed only a bit worse than 1x1 convs and a lot better than plain network architectures.",
      "Pooling can be used as in plain networks.",
      "No special architectures are necessary.",
      "Batch normalization can be used as usually (before nonlinearities).",
      "Results  Residual networks seem to perform generally better than similarly sized plain networks.",
      "They seem to be able to achieve similar results with less computation.",
      "They enable well-trainable very deep architectures with up to 1000 layers and more.",
      "The activations of the residual layers are low compared to plain networks.",
      "That indicates that the residual networks indeed only learn to make \"good\" changes and default to \"if in doubt, change nothing\".",
      "Examples of basic building blocks (other architectures are possible).",
      "The paper doesn't discuss the placement of the ReLU (after add instead of after the layer).",
      "Activations of layers (after batch normalization, before nonlinearity) throughout the network for plain and residual nets.",
      "Residual networks have on average lower activations.",
      "Rough chapter-wise notes  (1) Introduction  In classical architectures, adding more layers can cause the network to perform worse on the training set.",
      "That shouldn't be the case.",
      "(E.g. a shallower could be trained and then get a few layers of identity functions on top of it to create a deep network.)",
      "To combat that problem, they stack residual layers.",
      "A residual layer is an identity function and can learn to add something on top of that.",
      "So if x is an input image and f(x) is a convolution, they do something like x + f(x) or even x + f(f(x)).",
      "The classical architecture would be more like f(f(f(f(x)))).",
      "Residual architectures can be easily implemented in existing frameworks using skip connections with identity functions (split + merge).",
      "Residual architecture outperformed other in ILSVRC 2015 and COCO 2015.",
      "(3) Deep Residual Learning  If some layers have to fit a function H(x) then they should also be able to fit H(x) - x (change between x and H(x)).",
      "The latter case might be easier to learn than the former one.",
      "The basic structure of a residual block is y = x + F(x, W), where x is the input image, y is the output image (x + change) and F(x, W) is the residual subnetwork that estimates a good change of x (W are the subnetwork's weights).",
      "x and F(x, W) are added using element-wise addition.",
      "x and the output of F(x, W) must be have equal dimensions (channels, height, width).",
      "If different dimensions are required (mainly change in number of channels) a linear projection V is applied to x: y = F(x, W) + Vx.",
      "They use a 1x1 convolution for V (without nonlinearity?).",
      "F(x, W) subnetworks can contain any number of layer.",
      "They suggest 2+ convolutions.",
      "Using only 1 layer seems to be useless.",
      "They run some tests on a network with 34 layers and compare to a 34 layer network without residual blocks and with VGG (19 layers).",
      "They say that their architecture requires only 18% of the FLOPs of VGG.",
      "(Though a lot of that probably comes from VGG's 2x4096 fully connected layers?",
      "They don't use any fully connected layers, only convolutions.)",
      "A critical part is the change in dimensionality (e.g. from 64 kernels to 128).",
      "They test (A) adding the new dimensions empty (padding), (B) using the mentioned linear projection with 1x1 convolutions and (C) using the same linear projection, but on all residual blocks (not only for dimensionality changes).",
      "(A) doesn't add parameters, (B) does (i.e. breaks the pattern of using identity functions).",
      "They use batch normalization before each nonlinearity.",
      "Optimizer is SGD.",
      "They don't use dropout.",
      "(4) Experiments  When testing on ImageNet an 18 layer plain (i.e. not residual) network has lower training set error than a deep 34 layer plain network.",
      "They argue that this effect does probably not come from vanishing gradients, because they (a) checked the gradient norms and they looked healthy and (b) use batch normaliaztion.",
      "They guess that deep plain networks might have exponentially low convergence rates.",
      "For the residual architectures its the other way round.",
      "Stacking more layers improves the results.",
      "The residual networks also perform better (in error %) than plain networks with the same number of parameters and layers.",
      "(Both for training and validation set.)",
      "Regarding the previously mentioned handling of dimensionality changes:  (A) Pad new dimensions: Performs worst.",
      "(Still far better than plain network though.)",
      "(B) Linear projections for dimensionality changes: Performs better than A.",
      "(C) Linear projections for all residual blocks: Performs better than B.",
      "(Authors think that's due to introducing new parameters.)",
      "They also test on very deep residual networks with 50 to 152 layers.",
      "For these deep networks their residual block has the form 1x1 conv -> 3x3 conv -> 1x1 conv (i.e. dimensionality reduction, convolution, dimensionality increase).",
      "These deeper networks perform significantly better.",
      "In further tests on CIFAR-10 they can observe that the activations of the convolutions in residual networks are lower than in plain networks.",
      "So the residual networks default to doing nothing and only change (activate) when something needs to be changed.",
      "They test a network with 1202 layers.",
      "It is still easily optimizable, but overfits the training set.",
      "They also test on COCO and get significantly better results than a Faster-R-CNN+VGG implementation."
    ],
    "author_id": "ALEJU",
    "pdf_url": "http://arxiv.org/pdf/1512.03385",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 88246188
  },
  {
    "blog_id": "a-case-for-managed-and-model-less-inference-serving",
    "summary": [
      "A case for managed and model-less inference serving Yadwadkar et al., HotOS\u201919  HotOS\u201919 is presenting me with something of a problem as there are so many interesting looking papers in the proceedings this year it\u2019s going to be hard to cover them all!",
      "As a transition from the SysML papers we\u2019ve been looking at recently, I\u2019ve chosen a HotOS position paper from the Stanford Platform Lab to kick things off.",
      "As we saw with the SOAP paper last time out, even with a fixed model variant and hardware there are a lot of different ways to map a training workload over the available hardware.",
      "In \u201cA case for managed and model-less inference serving\u201d Yadwadkar et al. look at a similar universe of possibilities for model serving at inference time, and conclude that it\u2019s too much to expect users to navigate this by themselves.",
      "Making queries to an inference engine has many of the same throughput, latency, and cost considerations as making queries to a datastore, and more and more applications are coming to depend on such queries .",
      "\u201cFor instance, Facebook applications issue tens-of-trillions of inference queries per day with varying performance, accuracy, and cost constraints.\u201d  If we want an increasing number of applications to use machine learning, we must automate issues that affect ease-of-use, performance, and cost efficiency for users and providers\u2026 Despite significant research, this is missing right now.",
      "Perhaps inspired by serverless in spirit and in terminology, the path forward proposed in this paper is towards a managed and model-less inference serving system.",
      "Managed here means that the system automates resource provisioning for models to match a set of SLO constraints (cf.",
      "autoscaling).",
      "Model-less is more confusing.",
      "First off there still is a model of course (but then there are servers hiding behind a serverless abstraction too!).",
      "Most of the discussion in the paper focuses on model families, i.e. selecting among variants of a given model transparently to the end user.",
      "But the vision clearly seems to include selection of the model itself.",
      "I get the former, but the latter feels to me much more like something you\u2019d be doing during model development and training rather than dynamically at inference time.",
      "Perhaps we are intended to develop and train multiple models with differing characteristics, and make all of these available to the inference serving system to dynamically select from at runtime??",
      "The paper is silent on this issue.",
      "\u2026 we argue for an interface to the managed inference serving system where users are able to focus on querying an inference for their tasks without needing to think of models, and the trade-offs offered by model-variants.",
      "We term this interface model-less.",
      "Usability expectations  Creating a managed and model-less inferencing platform faces many challenges: applications have diverse SLOs (e.g. throughput sensitive vs latency sensitive); query patterns change dynamically over time; the underlying available hardware resources are diverse and also change over time; and their are many possible variants of a given model to choose from.",
      "Model variants can be created by methods such as model compression, knowledge distillation, tuning of hyperparameter values, varying precision, optimising for different batch sizes, and so on.",
      "The following figure highlights how just one of these variables, batch size, impacts throughput and latency on ResNet50.",
      "Expectation 1: Model-variant selection should be hidden behind a high-level API that allows users to simply specify their performance and cost objectives.",
      "Different hardware architectures (CPUs, GPUs, TPUs, FPGAs, ASICs, \u2026) offer different performance and cost trade-offs.",
      "Performance may vary by up to a couple of orders of magnitude for example.",
      "Expectation 2: The choice of hardware should be hidden behind the same high level API for users.",
      "The system should select the right hardware type(s) to use at any point in time for meeting performance and cost SLOs.",
      "We\u2019d like to pack models as efficiently as possible on the underlying infrastructure.",
      "Different applications will have different needs (and resource requirements) in terms of throughput and latency.",
      "Today it is most common to provision separate resources for each model, but model multi-tenancy would allow us to make better use of the underlying resources.",
      "An evaluation conducted by the authors showed that most models experience minimal performance loss with up to 5-6 concurrent instances running on a shared GPU.",
      "A quick back-of-the-envelope calculation suggests cost savings around an order of magnitude or more from sharing of hardware resources.",
      "Expectation 3: Resource management to meet query cost and performance goals for different models under varying load should be abstracted away from users.",
      "To improve provider resource utilization and TCO, the system must transparently (i) share resources across different model instances, (ii) share models across users, and (iii) manage memory by replicating and evicting models based on observed load and popularity.",
      "Similar to the start-up latency of a cold function in the serverless world, there\u2019s start-up latency to be considered when first loading a model onto a given hardware platform.",
      "We pay this price when scaling up and when adding a new model to the system.",
      "Ideally we\u2019d hide this from the user as best as possible:  Expectation 4: Start-up latency arising due to (i) loading a model-variant into the target hardware\u2019s memory or storage and (ii) building an optimized model-variant, should be handled transparently.",
      "Keeping all model-variants warm all the time though is going to be very expensive, so there\u2019s a constraint:  Expectation 5: To prevent poor resource utilization, providers should not need to constantly keep model-variants running.",
      "How can we get there?",
      "The wonderful thing about position papers is that you don\u2019t have to show working implementations ;).",
      "Section 3 of the paper instead provides a sketch of approaches and research directions that can take us towards the vision.",
      "A user sends one or more queries for a prediction task, such as object recognition, with optional SLO constraints, for instance, 90 th percentile inference latency.",
      "The serving system takes care of the rest \u2013 automatic model-variant and target hardware selection, automatic model-variant generation, load-aware adaptive scaling, fault tolerance, monitoring, logging, maintaining security and privacy of models and queries.",
      "Based on the SLO of a query, and the dynamic state of the system, it will be mapped to a pairing of a model variant and target hardware for running that variant.",
      "Model variants can be generated on demand if needed.",
      "Ideally there would already be an instance of the variant running on the target hardware, if not we\u2019ll need to start one (presumably the start-up latency cost will be a factor in determining placement).",
      "\u201cHow to design mechanisms and policies to avoid or reduce this latency remains an open question.\u201d It\u2019s probably a combination of heuristics and learned behaviour to proactively launch variants, together with an eviction model to clean-up under-utilised model instances.",
      "Further opportunities come from considering model placement at the edge and middle tiers, not just in cloud datacenters.",
      "Different layers offer trade-offs in terms of resource capacity, cost and network latency, management overheads, and energy-efficiency.",
      "Utilizing the resources across this continuum of core-middle-edge computing opens new opportunities and research directions.",
      "The final discussion in the paper concerns security and privacy.",
      "Authentication and authorization should be in place to prevent unauthorized access to queries, their submission patterns, and inference outcomes, to other users and providers.",
      "When building personalized models by adding a user-specific layer on top of a number of generic layers, can we share  the generic layers across users?",
      "Are there privacy concerns in doing so?",
      "The last word  The growing importance of ML inference forces us to finally solve several problems together: management of heterogeneity for both hardware and models, designing user interfaces, and building SLO-driven systems.",
      "These challenges are non-trivial and create new avenues for research.",
      "The good news, however, is that it is a bounded problem (i.e., models and model-variants are immutable once created), thus giving us hope to get it right soon.",
      "Replace \u2018model\u2019 with \u2018datastore\u2019 and you\u2019ll see a very similar set of problems that we\u2019ve been chipping away at for a very long time.",
      "Hope that we can make meaningful progress soon, yes.",
      "But hope that we can \u2018get it right\u2019 soon and put this challenge behind us?",
      "My personal bet is that this is a longer road\u2026"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3317550.3321443?download=true",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 17601393
  },
  {
    "blog_id": "to-distribute-or-not-to-distribute-why-licensing-bugs-matter",
    "summary": [
      "To distribute or not to distribute?",
      "Why licensing bugs matter Vendome et al., ICSE\u201918  Software licensing can quickly get quite complicated, with over 100 known open source licenses out there, and distributions often including components with a mix of licenses.",
      "Unsurprisingly, developers find it hard to determine appropriate licenses for their work, and to interpret the implications of including third-party software under different licenses.",
      "We present a large-scale qualitative study aimed at characterizing licensing bugs, with the goal of understanding the types of licensing bugs developers face, their legal and technical implications, and how such bugs are fixed.",
      "The result is a helpful catalogue of seven different categories of licensing bugs, with 21 sub-categories in total between them.",
      "Although the authors are not lawyers (as far as I can tell), it still constitutes a very useful list of things to think about.",
      "\u201cOur proposed catalog can serve as a reference for developers and lawyers dealing with potential licensing issues.\u201d  The catalogue is drawn from an open coding exercise based on a statistically significant sample of 1,200 discussions randomly selected from a population of 59,426 discussions across a collection of issue trackers and mailing lists.",
      "The mailing lists were Apache\u2019s legal-discuss, Debian\u2019s debian-legal, Fedora\u2019s fedora-legal-list, Gnome\u2019s legal-last and OpenStack\u2019s open-discuss.",
      "For issue trackers, the authors looked for issues using the keyword license on all 136 Bugzilla issue trackers in the Bugzilla installation list, as well as the issue trackers of 86,032 GitHub projects (selected to try and make sure these were not toy projects).",
      "Who cares about licensing?",
      "Before diving into the catalogue itself, it\u2019s worth briefly reviewing the different stakeholders involved in licensing issues: there are holders of IP (e.g. trademark holders, patent holders, copyright holders), lawyers, and lawmakers, and then we can also call out:  Integrators, that reuse open source software within their own systems  Package maintainers, who are responsible for maintaining packages and integrating patches or bug fixes.",
      "Distributors \u2013 any individual or entity distributing software  Developers (in general)  Community \u2013 either people involved in a specific open source community, or the open source community as a whole.",
      "Catalog overview  The taxonomy is composed of 21 distinct sub-categories organised in 7 distinct high-level categories.",
      "Due to space limitations we only discuss a subset of the sub-categories (14).",
      "The complete taxonomy description and frequencies of each category can be found in the attached appendix.",
      "That appendix sounds like a useful resource.",
      "Unfortunately it\u2019s not included in the only openly hosted version of the paper I could find (on the first author\u2019s personal site, and linked at the top of this post).",
      "The descriptions we do get are still very useful though.",
      "It is important to remark that the results discuss the interpretation of developers and/or legal practitioners.",
      "Therefore it is possible that the legality of these interpretations or discussions may change (e.g., new interpretations can causes new legal precedents in the U.S.A.), on the enforceability may change in different jurisdictions.",
      "Selected licensing issues explored  Let\u2019s take a brief look inside each of the seven major categories.",
      "Laws and their interpretations  At the base level, there is confusion over what is copyrightable?",
      "Software is copyrightable, but higher level designs and ideas may fall out of scope.",
      "Disagreements on the scope of copyright can lead to difficulties.",
      "A related issue is understanding what is a derivative work?",
      "(A work partially owned by the copyright author on which it is originally based).",
      "\u201c\u2026 one of the most important features of open source licenses is that they should allow the creation and redistribution of derivative works.\u201d It\u2019s often unclear whether B should be treated as a derivative work of A, or just something that uses / bundles A.",
      "For example, Linus Torvalds asserts that merely using the kernel by making system calls does not constitute creating a derivative work.",
      "There is still plenty of disagreement even on this though.",
      "This is all further complicated by the fact that copyright, trademark, and patent laws are national in scope.",
      "Thus we often find clauses relating to choice of jurisdiction.",
      "\u2026 we observed that clauses related to choice of jurisdiction were a controversial topic within Debian in terms of their impact on software\u2019s freeness.",
      "However, the distribution may be impacted by external factors like trade restrictions to a particular country or distribution of what a country considers sensitive material.",
      "While organizations or communities may want to facilitate global reuse, the organizations and individuals must comply with these trade laws.",
      "Policies of the ecosystem  This category concerns issues relating to the licensing policies of specific open source communities such as the Apache Foundation, Eclipse Software Foundation, and Debian.",
      "These give community guidelines that projects within the foundation are expected to follow.",
      "For example, projects at Eclipse under the EPL cannot ship external libraries under the LGPL as part of their distribution.",
      "This makes for more complex user installation procedures if users have to assemble the last mile themselves.",
      "The FSF has specific guidelines on whether software with various licenses can be combined/derived alongside software licensed under the FSF licences.",
      "You need to think broader than just source code, images, fonts, databases, text files and so on all need consideration\u2026  Since IP clearance/evaluation extends to all bundled artifacts (not only source code and binaries), a non-free image or font could prevent the distribution of the software.",
      "Potential license violations  Some licenses are incompatible with each other , and issues can arise when including dependencies or reusing source code that is incompatible with either the declared license, or with the the license of other reused components.",
      "Generally such an issue impacts the ability to distributed the software.",
      "As a specific example, Apache License 2.0 is incompatible with GPL v2.",
      "(See the full list here ).",
      "Non-source code licensing  When evaluating license compliance, you also need to consider non-source code artefacts, and in particular the need to make the source of those artefacts available.",
      "In GPL for example, source is defined as \u201cthe preferred form of the work for making modifications to it\u201c.",
      "So if you distributed a PDF of a document, you would also need to distribute the source that generates that PDF.",
      "Documentation, like source code, is also protected by copyright.",
      "Even documentation shipped in HTML format has been questioned, since HTML is not the preferred form for making changes.",
      "Similar issues occur with other media such as fonts, images, and audio.",
      "An MP3 is likely not the preferred form for editing audio for example.",
      "Licensing content  A license inconsistency occurs when there is a mismatch between the documented license and the actual source code licensing, e.g. inconsistencies between software licensing an the spec file documenting included licenses.",
      "Other IP issues  Do you have the rights to use a contribution?",
      ".",
      "This is the arena of CLAs (Contributor License Agreements) and CTAs (Copyright Transfer Agreements).",
      "The fundamental difference between the two is that in the former case the author retains the copyright, and grants a license.",
      "In the latter case the author transfers the copyright.",
      "Without either of these, how can you protect the integrity of your software package?",
      "Projects that require CTAs/CLAs do it to reduce their legal risks\u2026 It is important to note that CLAs/CTAs are optional in the sense that an organization is not required to use them.",
      "However, it demonstrates that  these open source communities would rather reject contributions than increase the legal risk of distributing code that may contain a license violation.",
      "Another thorny area is patents.",
      "From the debian-legal mailing list.",
      "It\u2019s hard (bordering on impossible?)",
      "to know what patents may apply to a piece of software, including patents going through the approval process which may later be granted.",
      "A number of licenses include specific clauses relating to patents and their litigation.",
      "You also need to be careful to respect trademarks.",
      "Licensing semantics  The final category includes licensing bugs relating to difficulties and/or confusion over the use of dual licensing or understanding the implications of particular clauses.",
      "As an example, developers considering migration to GPL 2.0+ need to consider the \u201cor later\u201d clause.",
      "How do you know you will agree with the terms of a future version of the GPL?"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://www.christophervendome.com/wp-content/uploads/2018/05/ICSE18-LicensingBugsCRC.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 22030520
  },
  {
    "blog_id": "panopticon",
    "summary": [
      "Panopticon: An Omniscient Lock Broker for Efficient Distributed Transactions in the Datacenter \u2013 Tasci & Demirbas, 2015  Today we return to the theme of distributed transactions, and a paper that won a best paper award from IEEE Big Data in 2015.",
      "Panopticon is a centralized lock broker (like Chubby and ZooKeeper ) that manages distributed (decentralized) locks (unlike Chubby and ZooKeeper).",
      "Full distributed transaction support is built on top of this lock broker.",
      "A central idea is that the lock for a data item does not have to be held in same place as the data item itself.",
      "This allows locks to migrate (even if the corresponding data items don\u2019t), and improves the efficiency of the transaction commit protocol).",
      "The paper is an easy read, but I found myself wishing it was a little more formal at points \u2013 these things are hard to reason about!",
      "(Distributed) transactions have gained a reputation for being unscalable\u2026  The authors identify two main reasons for this:  The coordination required for acquiring multiple locks in a distributed setting (two-phase locking, two-phase commit) becomes increasingly costly as the number of servers and data items involved increases.",
      "There is a big latency difference between local and remote access.",
      "When the lock for a data item is tied to the same location as a data item (and techniques such as consistent hashing are used to place data without locality considerations) this penalises transaction latencies significantly, especially during the lock acquisition phase.",
      "Remember that we\u2019re taking as a baseline the fact that an application may need to read and write multiple data items as part of some business use case.",
      "The overhead we\u2019re concerned with here is doing so transactionally.",
      "Traditional distributed transactions employ two phase locking to prevent deadlocks, which requires that the server initiating the transaction to contact the other servers for locks in increasing order of locks.",
      "Instead of contacting other servers trying to acquire locks in increasing order in a serial manner, it is more efficient to go to the broker and test/set all the locks at once.",
      "In Panopticon, the lock request is sent at once to the broker, and the broker takes care of deadlock prevention.",
      "Panopticon divides locks into three categories based on their access patterns:  Locks that are accessed from multiple different servers  Locks that receive repetitive access from the same server  Locks that aren\u2019t accessed very much  Observe that:  It is best to host type 1 locks (locks that keep receiving across-server accesses) in the lock broker.",
      "And it is best to assign the type 2 locks to the requesting server to avoid the overheads of repetitive requests from that server to the broker.",
      "Panopticon uses heuristics to migrate locks so as position them in the best place possible.",
      "All locks start out (as type 3 locks) on the same server as the data item they protect.",
      "A server contacts the lock broker only if it requires a lock that is not kept locally.",
      "If a server s requests some lock l from the broker (i.e., a lock that is not local to s), then the lock is migrated to the broker if it is not already there.",
      "The broker then grants it to s. When s releases the lock, l stays resident centrally at the broker.",
      "l is now treated as a type 1 lock.",
      "If a lock held at the broker is not accessed for a long time, and the broker needs to save space, an LRU policy can be used to migrate type 1 locks back to the server owning the corresponding data item (becoming type 3).",
      "We use the following rule of thumb for declaring a lock to be of type 2 and migrating that lock to a server: If two consecutive requests for a given lock l (held at the broker) comes from the same server w, then the broker migrates lock l to server w. From that point on w treats l as its local lock, the lock locality of w is improved with this, since w does not need to contact the broker for l again.",
      "A type 2 lock can migrate back to the broker again if a request for it is received from some other server than the one it currently resides on.",
      "In this manner,  As the centralized authority for mediating access to data, the broker learns about the access patterns of transactions at runtime and manages the migration of locks to servers in a way that improves lock access locality.",
      "The lock broker, however, is oblivious to the state of the transactions, and the servers are the ultimate transaction managers.",
      "Transactions are initiated and executed by the servers distributedly after checking that all the locks are available at the server.",
      "Panopticon uses a form of two-phase locking to manage transactions.",
      "When a server initiates a distributed transaction, it requests all the locks it needs in a batch request sent to the broker.",
      "The broker orders the locks by data item ID so that all transactions always attempt to acquire locks in the same order (preventing deadlock).",
      "The broker works through the requested locks in order.",
      "If it already owns a lock, it forwards it to the requesting server.",
      "If the broker does not have the lock, it adds the requesting server\u2019s name to a request queue that the broker maintains for that lock, and forwards a request for the lock to the current owning server.",
      "When the lock becomes available, the broker forwards it to the server at the head of the queue.",
      "If the server initiating the transaction ultimately requires for example four locks \u2013 l1, l2, l3, and l4 in that order, it may be that at some point it holds locks 1,2, and 4.",
      "However lock l4 can be taken away from it at any point until the server has also acquired lock 3.",
      "Locks 1 & 2 are considered to be \u2018authoritatively owned\u2019 by the server and will never be stolen from it.",
      "After a transaction is finished, the server needs to unlock the data items, which means returning the locks back to the broker.",
      "In this phase we propose an optimization where the server lazy unlocks the locks.",
      "Lazy unlocking means that the locks are released locally, but not transmitted back to broker until \u03b4 time elapses, where \u03b4 is empirically determined.",
      "Lazy unlocking provides efficiency benefits for cases when the server needs to access the same data items used in the terminated transaction immediately in the next transaction.",
      "If a lazy-unlocked data item lock is requested in the meantime, it is immediately returned to the broker.",
      "In order to give more scalability and avoid bottlenecks when using extremely large number of servers and locks, we can employ a hierarchical composition of the brokers.",
      "For this we have k level-0 lock brokers each overseeing a cluster of servers and a top-level lock broker overseeing  these k lock brokers\u2026 Moreover, using hierarchical composition of brokers at different datacenters, the Panopticon system can provide a partial answer to the across-datacenter/WAN transactions problem.",
      "Providing an efficient and complete system for across-datacenter transactions remains part of our future work.",
      "Partition detection follows very simple rules, given that there is a central broker.",
      "If you can see the broker, you\u2019re in the main partition, and if you can\u2019t, you\u2019re not!",
      "Providing continued operation for data items locked by locks outside of the main partition is future work.",
      "Panopticon is built on top of Hazelcast, and compares favourably to the default distributed transaction protocol in Hazelcast.",
      "The tests use artificially generated workloads.",
      "It would be interesting to see how Panopticon performs with real workloads (in particular, the effectiveness of Panopticon is sensitive to the parameter Phist that determines how likely it is that a server uses the same objects in consecutive transactions \u2013 just how likely is this in real workloads with load balancing etc.?",
      "(genuine question on my part)).",
      "A few other things I\u2019m left wondering about as well: the prepare and commit phases still need to happen at every RM, even if the locks move around\u2026.",
      "what gets logged and where in the WAL?",
      "(Records normally hold information about locks held).",
      "How does recovery work?",
      "How do you prevent the broker from becoming a single point of failure?",
      "And if it isn\u2019t, do you have to run some kind of consensus protocol across multiple brokers\u2026?"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://www.cse.buffalo.edu/tech-reports/2014-06.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 63188694
  },
  {
    "blog_id": "large-scale-evolution-of-image-classifiers",
    "summary": [
      "Large-scale evolution of image classifiers Real et al., 2017  I\u2019m sure you noticed the bewildering array of network architectures in use when we looked at some of the top convolution neural network papers of the last few years last week ( Part 1 , Part2 , Part 3 ).",
      "With sufficient training data, these networks can achieve amazing feats, but how do you find the best network architecture for your problem in the first place?",
      "Discovering neural network architectures\u2026 remains a laborious task.",
      "Even within the specific problem of image classification, the state of the the art was attained through many years of focused investigation by hundreds of researchers.",
      "If you\u2019re an AI researcher and you come up against a difficult problem where it\u2019s hard to encode the best rules (or features) by hand, what do you do?",
      "Get the machine to learn them for you of course!",
      "If what you want to learn is a function, and you can optimise it through back propagation, then a network is a good solution (see e.g. \u2018 Network in Network \u2018, or \u2018 Learning to learn by gradient descent by gradient descent \u2018).",
      "If what you want to learn is a policy that plays out over some number of time steps then reinforcement learning would be a good bet.",
      "But what if you wanted to learn a good network architecture?",
      "For that the authors turned to a technique from the classical AI toolbox, evolutionary algorithms.",
      "As you may recall, evolutionary algorithms start out with an initial population, assess the suitability of population members for the task in hand using some kind of fitness function, and then generate a new population for the next iteration by mutations and combinations of the fittest members.",
      "So we know that we\u2019re going to start out with some population of initial model architectures (say, 1000 of them), define a fitness function based on how well they perform when trained, and come up with a set of appropriate mutation operations over model architectures.",
      "That\u2019s the big picture, and there\u2019s just one more trick from the deep learning toolbox that we need to bring to bear: brute force!",
      "If in doubt, overwhelm the problem with bigger models, more data, or in this case, more computation:  We used slightly-modified known evolutionary algorithms and scaled up the computation to unprecedented levels, as far as we know.",
      "This, together with a set of novel and intuitive mutation operators, allowed us to reach competitive accuracies on the CIFAR-10 dataset.",
      "This dataset was chosen because it requires large networks to reach high accuracies, thus presenting a computational challenge.",
      "The initial population consists of very simple (and very poorly performing) linear models, and the end result is a fully trained neural network with no post-processing required.",
      "Here\u2019s where the evolved models stand in the league table:  That\u2019s a pretty amazing result when you think about it.",
      "Really top-notch AI researchers are in very short supply, but computation is much more readily available on the open market.",
      "\u2018Evolution\u2019 evolved an architecture, with no human guidance, that beats some of our best models from the last few years.",
      "Let\u2019s take a closer look at the details of the evolutionary algorithm, and then we\u2019ll come back and dig deeper into the evaluation results.",
      "Evolving models  We start with a population of 1000 very simple linear regression models, and then use tournament selection.",
      "During each evolutionary step, a worker process (250 of them running in parallel) chooses two individuals at random and compares their fitness.",
      "The worst of the pair is removed from the population, and the better model is chosen as a parent to help create the next generation.",
      "A mutation is applied to the parent to create a child.",
      "The worker then trains the child, evaluates it on the validation set, and puts it back into the population.",
      "Using this strategy to search large spaces of complex image models requires considerable computation.",
      "To achieve scale, we developed a massively-parallel, lock-free infrastructure.",
      "Many workers operate asynchronously on different computers.",
      "They do not communicate directly with each other.",
      "Instead, they use a shared file-system, where the population is stored.",
      "Training and validation takes place on the CIFAR-10 dataset consisting of 50,000 training examples and 10,000 test examples, all labeled with 1 of 10 common object classes.",
      "Each training runs for 25,600 steps \u2013 brief enough so that each individual can be trained somewhere between a few seconds and a few hours, depending on the model size.",
      "After training, a single evaluation on the validation set provides the accuracy to use as the model\u2019s fitness.",
      "We need architectures that are trained to completion within an evolutionary experiment\u2026 [but] 25,600 steps are not enough to fully train each individual.",
      "Training a large enough model to completion is prohibitively slow for evolution.",
      "To resolve this dilemma, we allow the children to inherit the parents\u2019 weights whenever possible.",
      "The final piece of the puzzle then, is the encoding of model architectures, and the mutation operations defined over them.",
      "A model architecture is encoded as a graph (its DNA).",
      "Vertices are tensors or activations (either batch normalisation with ReLUs, or simple linear units).",
      "Edges in the graph are identity connections (for skipping) or convolutions.",
      "When multiple edges are incident on a vertex, their spatial scales or numbers of channels may not coincide.",
      "However, the vertex must have a single size and number of channels for its activations.",
      "The inconsistent inputs must be resolved.",
      "Resolution is done by choosing one of the incoming edges as the primary one.",
      "We pick this primary edge to be the one that is not a skip connection.",
      "Activation functions are similarly reshaped (using some combination of interpolation, truncation, and padding), and the learning rate is also encoded in the DNA.",
      "When creating a child, a worker picks a mutation at random from the following set:  Alter learning rate  Identity (effectively gives the individual further training time in the next generation)  Reset weights  Insert convolution (at a random location in the \u2018convolutional backbone\u2019).",
      "Convolutions are 3\u00d73 with strides of 1 or 2.",
      "Remove convolution  Alter stride (powers of 2 only)  Alter number of channels (of a random convolution)  Filter size (horizontal or vertical at random, on a random convolution, odd values only)  Insert one-to-one (adds a one-to-one / identity connection)  Add skip (identity between random layers)  Remove skip (removes a random skip)  Evaluation results  Here\u2019s an example of an evolution experiment, with selected population members highlighted:  Five experiment runs are done, and although not all models reach the same accuracy, they get pretty close.",
      "It took 9 x 1019 FLOPs on average per experiment.",
      "The following chart shows how accuracy improves over time during the experiments:  We observe that populations evolve until they plateau at some local optimum.",
      "The fitness (i.e. validation accuracy) value at this optimum varies between experiments (Above, inset).",
      "Since not all experiments reach the highest possible value, some populations are getting \u201ctrapped\u201d at inferior local optima.",
      "This entrapment is affected by two important meta-parameters (i.e. parameters that are not optimized by the algorithm).",
      "These are the population size and the number of training steps per individual.",
      "The larger the population size, the more thoroughly the space of models can be explored, which helps to reach better optima.",
      "More training time means that a model needs to undergo fewer identity mutations to reach a given level of training (remember that the end result of the evolution process is a fully trained model, not just a model architecture).",
      "Two other approaches to escaping local optima are increasing the mutation rate, and resetting weights.",
      "When it looks like members of the population are trapped in poor local optima, the team tried applying 5 mutations instead of 1 for a few generations.",
      "During this period some population members escape the local optimum, and none get worse:  To avoid getting trapped by poorer architectures that just happened to have received more training (e.g. through the identity mutation), the team also tried experiments in which the weights are simultaneously reset across all population members when a plateau is reached.",
      "The populations suffer a temporary degradation (as to be expected), but ultimately reach a higher optima."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://arxiv.org/pdf/1703.01041.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 85839366
  },
  {
    "blog_id": "kv-direct-high-performance-in-memory-key-value-store-with-programmable-nic",
    "summary": [
      "KV-Direct: High-performance in-memory key-value store with programmable NIC Li et al., SOSP\u201917  We\u2019ve seen some pretty impressive in-memory datastores in past editions of The Morning Paper, including FaRM , RAMCloud , and DrTM .",
      "But nothing that compares with KV-Direct:  With 10 programmable NIC cards in a commodity server, we achieve 1.22 billion KV operations per second, which is almost an order-of-magnitude improvement over existing systems, setting a new milestone for a general-purpose in-memory key-value store.",
      "Check out the bottom line in this comparison table from the evaluation:  ( Enlarge )  In addition to sheer speed, you might also notice that KV-Direct is 3x more power efficient than other systems, and the first general purpose KVS system to achieve 1 million KV operations per watt on commodity servers.",
      "Since the server CPU can also be used to run other workloads at the same time, you can make a case for KV-Direct being as much as 10x more power efficient than CPU-based systems.",
      "What we\u2019re seeing here is a glimpse of how large-scale systems software of the future may well be constructed.",
      "As the power ceiling puts a limit on multi-core scaling, people are now turning to domain-specific architectures for better performance.",
      "A first generation of key-value stores were built in a straightforward manner on top of traditional operating systems and TCP/IP stacks.",
      "More recently, as both the single core frequency scaling and multi-core architecture scaling are slowing down, a new research trend in distributed systems is to leverage Remote Direct Memory Access (RDMA) technology on NIC to reduce network processing cost.",
      "KV-Direct however, goes one step beyond.",
      "To support network virtualisation, more and more servers in data centers are now equipped with programmable NICS containing field-programmable gate arrays (FPGA).",
      "An embedded NIC chip connects to the network, and a PCIe connector attaches to the server.",
      "KV-Direct uses the FPGA in the NIC to implement key-value primitives directly.",
      "Like one-sided RDMA (Fig 1b below), KV-Direct bypasses the remote CPU.",
      "But it also extends the RDMA primitives from simple memory operations (READ and WRITE) to key-value operations (GET, PUT, DELETE and ATOMIC ops) \u2014 Fig 1c below.",
      "Compared with one-sided RDMA based systems, KV-Direct deals with the consistency and synchronization issues on the server-side, thus removing computation overhead in the client, and reducing network traffic.",
      "In addition, to support vector-based operations and reduce network traffic, KV-Direct also provides new vector primitives UPDATE, REDUCE, and FILTER, allowing users to define active messages and delegate certain computations to programmable NIC for efficiency.",
      "Design goals and challenges  Use cases for in-memory key-value stores have evolved beyond caching to things such as storing data indices, machine learning model parameters, nodes and edges in graph computing, and sequencers in distributed synchronisation.",
      "The role of the store shifts from object caching to a generic data structure store (c.f.",
      "Redis).",
      "This leads to the following design goals:  High batch throughput for small key-value pairs (e.g., model parameters, graph node neighbours).",
      "Predictable low-latency (e.g., for data-parallel computation,where tail latency matters)  High efficiency under write-intensive workloads (e.g., graph computations, and parameter servers)  Fast atomic operations  (e.g., for centralized schedulers, sequencers , counters and so on).",
      "Vector-type operations (for machine learning and graph computing workloads that often require operating on every element in a vector).",
      "The throughput constraint ends up being PCIe bandwidth:  In order to saturate the network with GET operations, the KVS on NIC must make full use of PCIe bandwidth and achieve close to one average memory access per GET.",
      "Getting to this level involves work on three fronts:  Minimising DMA (direct memory access) requests per KV operation.",
      "The two major components that drive random memory access are hash tables and memory allocation.",
      "Hiding PCIe latency while maintaining consistency, which entails pipelining requests.",
      "Care must be taken to respect causal dependencies here though.",
      "Balancing load between NIC DRAM and host memory.",
      "The NIC itself has a small amount of DRAM available, but it turns out not to be much faster than going over PCIe.",
      "So the trick turns out to be to use both in order to utilise the joint bandwidth.",
      "KV-Direct  KV-Direct enables remote direct key-value access.",
      "Clients send operation requests to the KVS server, and the programmable NIC processes requests and sends back results, bypassing the CPU.",
      "The following table shows the supported operations.",
      "The most interesting of course are the vector operations.",
      "KV-Direct supports two types of vector operations: sending a scalar to the NIC on the server, where the NIC applies the update to each element in the vector; and sending a vector to the server, where the NIC updates the original vector element-by-element.",
      "Furthermore, KV-Direct supports user-defined update functions as a generalisation to atomic operations.",
      "The update functions needs to be pre-registered and compiled to hardware logic before executing.",
      "When the user supplies an update function, the KV-Direct toolchain duplicates it several times to leverage FPGA parallelism and match computation with PCIe throughput, and then compiles it into reconfigurable hardware logic using a high-level synthesis (HLS) tool.",
      "These functions can be used for general stream processing on a vector value.",
      "The programmable NIC on the KVS server is reconfigured as a KV processor, which receives packets from the network, decodes vector operations, and buffers KV operations in a reservation station.",
      "The out-of-order engine then issues independent KV operations from the reservation station into the decoder.",
      "To minimise memory accesses, small KV pairs are stored inline in the hash table, while others are stored in dynamically allocated memory from a slab memory allocator.",
      "After a KV operation completes, the result is sent back to the out-of-order execution engine to find and execute matching KV operations in the reservation station.",
      "The reservation station is used to avoid dependencies between two KV operations leading to data hazards and a stalled pipeline.",
      "We borrow the concept of dynamic scheduling from computer architecture and implement a reservation station to track all in-flight KV operations and their execution context.",
      "To saturate PCIe, DRAM and the processing pipeline, up to 256 in-flight KV operations are needed.",
      "However, comparing 256 16-byte keys in parallel would take 40% of the logic resource of our FPGA.",
      "Instead, we store the KV operations in a small hash table in on-chip BRAM, indexed by the hash of the key.",
      "When a KV operation completes, the latest value is forwarded to the reservation station, where pending operations in the same hash slot are checked.",
      "Those with a matching key are executed immediately and removed from the station.",
      "Further design and implementation details can be found in sections 3 and 4 of the paper.",
      "Evaluation  The evaluation section contains a suite of microbenchmarks, followed by a system benchmark based on the YCSB workload.",
      "To simulate a skewed Zipf workload, skewness 0.99 was chosen.",
      "This is referred to as the long-tail workload in the figures.",
      "The testbed comprises eight servers with two 8-core CPUS per server,and one Arista switch.",
      "There is a total of 128 GiB of host memory per server.",
      "A programmable NIC is connected to the PCIe root complex of CPU 0, and its 40 Gbps Ethernet port is connected to the switch.",
      "The NIC has two PCIe Gen3 x8 links in a bifurcated Gen3 x16 physical connector.",
      "Here\u2019s the overall throughput achieved by the system.",
      "The throughput of a KV-Direct NIC is on-par with a state-of-the-art KVS server with tens of CPU cores.",
      "Without network batching, the tail latency ranges from 3-9  s depending on KV size, operation type, and key distribution.",
      "Network batching adds less than 1  s latency, but significantly improves performance.",
      "It is possible to attach multiple NICs per server.",
      "With 10 KV-Direct NICs on a server, one billion KV ops/s is readily achievable on a commodity server.",
      "Each NIC owns a disjoin partition of the keys.",
      "Multiple NICs suffer the same load imbalance problem as a multi-core KVS implementation, but for a relatively small number of partitions (e.g. 10) the load imbalance is not too great \u2013 1.5x of the average in the highest loaded NIC even for the long-tail highly skewed workload.",
      "KV-Direct throughput scales almost linearly with the number of NICS on a server.",
      "The last word:  After years of broken promises, FPGA-based reconfigurable hardware finally becomes widely available in main stream data centers.",
      "Many significant workloads will be scrutinized to see whether they can benefit from reconfigurable hardware, and we expect much more fruitful work in this general direction."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://lrita.github.io/images/blog/kv-direct.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 45795840
  },
  {
    "blog_id": "time-evolving-graph-processing-at-scale",
    "summary": [
      "Time evolving graph processing at scale Iyer et al., GRADES 2016  Here\u2019s a new (June 2016) paper from the distinguished AMPlab group at Berkeley that really gave me cause to reflect.",
      "The work addresses the problem of performing graph computations on graphs that are constantly changing (because updates flow in, such as a new follower in a social graph).",
      "Many graphs of interest have this property of constantly evolving.",
      "In part, that\u2019s what makes them interesting.",
      "You could always take a snapshot of e.g. the graph as it was at the end of the previous day and compute on that, but some applications need more up to date results (e.g.",
      "detecting traffic hotspots in cellular networks), and many applications would benefit from real-time results.",
      "GraphTau is a solution to this problem, implemented on top of GraphX which is in turn implemented on top of Spark\u2019s RDDs.",
      "It\u2019s a convergence of stream processing and graph processing.",
      "I\u2019m seeing a lot of streaming recently, and a lot of convergence.",
      "That topic probably warrants a separate post.",
      "In the meantime\u2026  Graph-structured data is on the rise, in size, complexity and the dynamism they exhibit.",
      "From social networks to telecommunication networks, applications that generate graph-structured data are ubiquitous\u2026 the dynamic nature of these datasets gives them a unique characteristic \u2013 the graph-structure underlying the data evolves over time.",
      "Unbounded, real-time data is fast becoming the norm , and thus it is important to process these time-evolving graph-structured datasets efficiently.",
      "(Aside: applications generating graph-structured data certainly are ubiquitous \u2013 pretty much any relational database has graph structure the minute you introduce foreign keys.",
      "It\u2019s applications generating graph-structured data and that require extensive traversals or graph-specific computations that we\u2019re really interested in here).",
      "For time-evolving graph-structured datasets, the authors identify three core requirements:  The ability to execute iterative graph algorithms in real-time  The ability to combine graph-structured data with unstructured and tabular data  The ability to run analytics over windows of input data  While some specialized systems for evolving-graph processing exist, these do not support the second and third requirements.",
      "GraphTau is \u201cthe first time-evolving graph processing system, to our knowledge, built on a general purpose dataflow framework.\u201d GraphTau is built on top of GraphX, which maintains graphs internally as a pair of RDDs: a vertex collection and an edge collection.",
      "(Note that Apache Flink has Gelly , which builds graph processing on top of a streaming dataflow core, but does not support iterative processing over evolving graphs to the best of my knowledge.)",
      "The main idea in GraphTau is to treat time-evolving graphs as a series of consistent graph snapshots, and dynamic graph computations as a series of deterministic batch computations on discrete time intervals.",
      "A graph snapshot is simply a regular graph, stored as two RDDs, the vertex RDD and the edge RDD.",
      "We define GraphStream as a sequence of immutable, partitioned datasets (graphs represented as two RDDs) that can be acted on by deterministic operators.",
      "User programs manipulate GraphStreams to produce new GraphStreams, as well as intermediate state in the form of RDDs or graphs.",
      "A DeltaRDD is an RDD whose elements are updates that need to be applied to a graph.",
      "A stream of such updates is called a DeltaDStream.",
      "GraphStreams can be built from a DeltaDStream or directly from a vertex DStream and an edge DStream.",
      "There are two supported computational models, called pause-shift-resume and online rectification.",
      "Pause-shift-resume  Some classes of graph algorithms can cope with the graph being modified while the algorithm is still converging.",
      "For example, if a graph changes during an evaluation of PageRank you\u2019ll still get an answer, which studies have shown will be within a reasonable error to the actual answer you\u2019d get if you started the algorithm again from scratch with the now current graph.",
      "Under these conditions, the pause-shift-resume (PSR) model is appropriate.",
      "In this model, GraphTau starts running a graph algorithm as soon as the first snapshot of a graph is available.",
      "Upon the availability of a new snapshot, it pauses the computation on the current graph, shifts the algorithm specific data to the new snapshot, and resumes the computation on the new graph.",
      "Online rectification  Algorithms such as connected-components will produce incorrect results under the PSR model (consider an edge or vertex that is removed during processing).",
      "For such algorithms, GraphTau proposes the online rectification model.",
      "In this model, GraphTau rectifies the errors caused by the underlying graph modificationts in an online fashion using minimal state.",
      "In the connected component example, it is necessary for every vertex to keep track of its component id over time.",
      "The approach works for any algorithm based on label propagation, at the expense of needing to keep algorithm-specific state.",
      "The question of time  GraphStream splits time into non-overlapping intervals, and stores all the inputs received during these intervals in batches (worker nodes are synchronized using NTP).",
      "Such intervals are based on receive time, there is also an option to process based on external timestamps (event time) which requires either the introduction of limited slack time to wait for late events, or application specific code to correct for late records.",
      "Each interval\u2019s updates reflects all of the input received until then.",
      "This is despite the fact that the DeltaRDD and its updated graph snapshot are distributed across nodes.",
      "As long as we process the whole batch consistently (e.g. ordered by timestamps), we will get a consistent snapshot.",
      "This makes distributed state much easier to reason about and is the same as \u201cexactly once\u201d processing of the graph updates even with faults or stragglers.",
      "GraphStream inherits its recovery mechanisms from GraphX and its RDDs: parallel recovery of lost state and speculative execution.",
      "Programming with GraphTau  The GraphStream interface supports transform, merge, streamingBSP, and forEachGraph operations as well an updateLocalState operator to allow for event processing and state tracking.",
      "mergeByWindow merges all graphs from a sliding window of past time intervals into one graph  forEachGraph applies a function to each graph generated from the GraphStream  transformWith combines two graph streams with various join and cogroup operators.",
      "the streamingBSP operator supports differential computation  This [streamingBSP] operator enables efficient implementation of a large class of incremental algorithms on time-evolving graphs.",
      "We signal the availability of the new graph snapshot using a variable in the driver program.",
      "In each iteration of Pregel , we check whether a new graph is available.",
      "If so, we do not proceed to the next iteration on the current graph.",
      "Instead, we resume computation on the new graph reusing the result, where only vertices in the new active set continue message passing.",
      "The new active set is a function of the old active set and the changes between the new graph and the old graph.",
      "For a large class of algorithms (e.g. incremental PageRank), the new active set includes vertices from the old set, any new vertices, and vertices with edge additions and deletions.",
      "Here\u2019s what the Page Rank example looks like:  Even on a simple six-node graph where one edge is added after 10 iterations, this saves 13/34 iterations overall.",
      "Here\u2019s another example GraphTau program, showing the ability to unify data and graph stream processing.",
      "This example computes top users in terms of triangle counts from a Twitter attention graph.",
      "A DStream ds is created from the external Twitter feed, and then a GraphStream is built from it.",
      "Triangle count is applied to each graph snapshot, and then we compute the number of times a user is a top user over a sliding window of ten seconds, outputting results every second.",
      "Preliminary evaluation shows that GraphTau\u2019s performances matches or exceeds that of specialized systems on a streaming connected components task based on a cellular dataset.",
      "The last word\u2026  In this paper, we presented GraphTau, a time-evolving graph processing system built on a data flow framework that addresses this demand.",
      "GraphTau represents time-evolving graphs as a series of consistent graph snapshots.",
      "On these snapshots, GraphTau enables two computational model, the Pause-Shift-Resume model and the Online Rectification model which allows the application of differential computation on a wide variety of graph algorithms.",
      "These techniques enable GraphTau to achieve significant performance improvements."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://www.cs.columbia.edu/~lierranli/publications/GraphTau-GRADES2016.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 5225266
  },
  {
    "blog_id": "improving-user-perceived-page-load-time-using-gaze",
    "summary": [
      "Improving user perceived page load time using gaze Kelton, Ryoo, et al., NSDI 2017  I feel like I\u2019m stretching things a little bit including this paper in an IoT flavoured week, but it does use at least bridge from the physical world to the virtual, if only via a webcam.",
      "What\u2019s really interesting here to me is what the paper teaches us about web page load performance.",
      "We know that faster loading pages are correlated with all sorts of user engagement improvements, but what exactly is a faster loading page?",
      "If we want faster loading pages because they drive better user engagement, then the ideal page load time metric should correspond with the way that a user perceives page load times.",
      "The most popular page load metrics, OnLoad, and Speed Index have the advantage of being easy to measure, but they it turns out they\u2019re not always a good match with the user-perceived page load time, uPLT.",
      "The OnLoad PLT metric measures the time taken for the browser OnLoad event to be fired (i.e., once all objects on the page are loaded).",
      "OnLoad tends to over-estimate page load time because users are often only interested in content \u2018above the fold\u2019 (visible without scrolling) when first waiting for a page to load.",
      "(Be warned though, with some page structures it can also under-estimate).",
      "The SpeedIndex PLT metric or Above Fold Time (AFT) measures the average time for all above-the-fold content to appear on the screen.",
      "\u201cIt is estimated by first calculating the visual completeness of a page, defined as the pixel distance between the current frame and the \u2018last\u2019 frame of the Web page.",
      "The last frame is when the Web page content no longer changes.",
      "Speed Index is the weighted average of visual completeness over time.",
      "The Speed Index value is lower (and better) if the browser shows more visual content earlier.\u201d The issue with Speed Index is that it doesn\u2019t take into account the relative importance of content.",
      "The authors conducted a study across 45 web pages and 100 different users using videos of pages loading to ensure that each user saw an identical experience for each page.",
      "Additional studies were conducted with simulations of different network speeds.",
      "The users were asked to press a key on the keyboard when they perceived the page to be loaded (see section 3 in the paper for full details of the study setup).",
      "Here\u2019s an example of how the different metrics look for the energystar.gov site:  And here are the overall results across all 45 web pages and 100 users:  OnLoad tends to either over-estimate (on average by 6.4 seconds) or under-estimate (by 2.1 seconds on average) when compared to true uPLT.",
      "Pages that are heavy in Javascript and/or images tend to have even larger OnLoad time gaps.",
      "Speed Index is about 3.5 seconds lower than uPLT for 87% of Web pages.",
      "Using gaze to improve page load times  Having understood that neither OnLoad nor the Speed Index metric is a good indicator of the true user-perceived page load time, the authors turn their attention to figuring out what to focus on in order to reduce perceived page load times.",
      "Intuitively, what a user is looking at (visual attention) should tell us what is important on the page.",
      "We can track this using gaze tracking software\u2026  Recently, advances in computer vision and machine learning have enabled low cost gaze tracking.",
      "The low cost trackers do not require custom hardware and take into account facial features, user movements, a user\u2019s distance from the screen, and other user differences.",
      "The study uses an off-the-shelf webcam based gaze tracker called GazePointer .",
      "A 50-user study is conducted using the GazePointer setup, across 45 web pages.",
      "An auxiliary study also used a much more expensive custom gaze tracker and confirmed that the results concur with the webcam-based solution.",
      "Each web page is divided into a set of regions, and the study tracks the regions associated with a users fixation points (i.e., when the user is focusing on something).",
      "For example, here are the visual regions for fcc.gov:  Here\u2019s a heat map across the 45 websites, showing where the attention budget is spent.",
      "For example, when looking at the first web site we see that 5 regions combined are fixated on by 90% of users, whereas the remaining 75% of regions are fixated on by less than half of the users.",
      "In general, we find that across the Web pages, at least 20% of the regions have a collective fixation of 0.9 or more.",
      "We also find that on average, 25% of the regions have a collective fixation of less than 0.3, i.e., 25% of regions are viewed by less than 30% of the users.",
      "This leads to the following hypothesis: prioritising loading the parts of a web page that hold users attention should result in faster perceived page load times.",
      "The WebGaze system collects gaze feedback from a subset of users as they browse, and uses the gaze feedback to determine which Web objects to prioritise during page load.",
      "(Good luck getting many people to opt-in to having their gaze tracked by webcam while they\u2019re browsing though!",
      "Let\u2019s just assume that getting sufficient gaze feedback is possible \u2013 e.g., from internal user testing).",
      "To identify which Web objects to prioritize, we use a simple heuristic: if a region has a collective fixation of over a prioritization threshold, then the objects in the region will be prioritized.",
      "In our evaluation, we set the prioritization threshold to be 0.7.",
      "Each visual region may have multiple objects.",
      "The CSS bounding rectangles for all objects visible in the viewport can be obtained via the DOM.",
      "An object is said to be in a given region if its bounding rectangle intersects with the region, when an object belongs to multiple regions, it is assigned the priority of the highest priority of those regions.",
      "Having found the visible Web objects to be prioritised, the next task is to extend the set of prioritised objects to include any other objects they may depend on.",
      "The WProf tool is used to extract dependencies.",
      "\u201cWhile the contents of sites are dynamic, the dependency information has been shown to be temporally stable.",
      "Thus, dependencies can be gathered offline.\u201d  To actually implement prioritization, WebGaze uses HTTP/2\u2019s Server Push functionality.",
      "Server Push decouples the traditional browser architecture in which Web objects are fetched in the order in which the browser parses the page.",
      "Instead, Server Push allows the server to preemptively push objects to the browser, even when the browser did not explicitly request these objects.",
      "Server Push helps (i) by avoiding a round trip required to fetch an object, (ii) by breaking dependencies between client side parsing and network fetching, and (iii) by better leveraging HTTP/2\u2019s multiplexing.",
      "In some pathological cases, Server Push can make things much worse!",
      "WebGaze reverts back to the default case without optimization when this is detected.",
      "This happened with 2 of the 45 pages in the study.",
      "Does gaze-based prioritisation actually improve perceived user page load times though?",
      "An evaluation compared WebGaze with three alternative strategies:  Default: the page loads as-is without prioritisation  Push-all: all of the objects on the Web page are pushed using Server Push  Klotski: the Klotksi algorithm is used to to push objects and dependencies with an objective of maximising the amount of above-the-fold content that can be delivered within 5 seconds.",
      "Figure 11 (below) shows the CDF of the percentage improvement in uPLT compared to alternatives.",
      "On an average, WebGaze improves uPLT 17%, 12%, and 9% over Default, Push-All, and Klotski respectively.",
      "At the 95% percentile, WebGaze improves uPLT by 64%, 44%, and 39% compared to Default, Push-All, and Klotski respectively.",
      "In about 10% of cases, WebGaze does worse than Klotski.",
      "In these cases, Klotski is sending less data than WebGaze.",
      "\u201cThis suggests we need to perform more analysis on determining the right amount of data that can be pushed without affecting performance.\u201d  The authors note the significant security and privacy concerns with deploying gaze tracking in the wild.",
      "If you want to experiment, my recommendation would be to conduct your own (opt-in, in-the-lab) user studies using gaze tracking, and then use the information gleaned to improve Web object prioritisation for production systems."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://www.usenix.org/system/files/conference/nsdi17/nsdi17-kelton.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 39038842
  },
  {
    "blog_id": "generalized-isolation-level-definitions",
    "summary": [
      "Generalized Isolation Level Definitions \u2013 Adya et al. 2000  Following on from yesterday\u2019s critique of ANSI SQL isolation levels , today\u2019s paper also gives a precise definition of isolation levels \u2013 but does so in a way that is inclusive of optimistic and general multi-version concurrency control strategies instead of being dependent on locking.",
      "Where Berenson et al. define phenomena P0..P3 in order to explain the ANSI SQL isolation levels (degree 1, 2, and 3), Adya et al.",
      "define generalised phenomena labelled G0, G1, G2 whose definitions are independent of implementation strategy.",
      "Based on these generalised phenomena they then provide portable isolation level definitions (PL) that accommodate all implementation strategies, not  just locking based ones.",
      "We now show that the preventative approach is overly restrictive since it rules out optimistic and multi-version implementations.",
      "As mentioned, this approach disallows all histories that would not occur in a locking scheme and prevents conflicting operations from executing concurrently\u2026 The real problem with the preventative approach is that the phenomena are expressed in terms of single-object histories.",
      "However, the properties of interest are often multi-object constraints.",
      "To avoid problems with such constraints, the phenomena need to restrict what can be done with individual objects more than is necessary.",
      "Our approach avoids this difficulty by using specifications that capture constraints on multiple objects directly.",
      "Furthermore, the definitions in the preventative approach are not applicable to multi-version systems since they are described in terms of objects rather than in terms of versions.",
      "On the other hand, our specifications deal with multi-version and single-version histories.",
      "Following a motivation section, the authors first build up a model of a database system, and then define different types of conflict (read and write dependencies, as well as something called an anti-dependency that we\u2019ll get too) that can occur within that model.",
      "This leads to the central notion of a Direct Serialization Graph (DSG) for a history.",
      "The phenomena and isolation levels are then specified as constraints on the allowable graphs.",
      "How to construct a Direct Serialization Graph  A DSG has one node for every committed transaction.",
      "(Directed) edges between these nodes represent read/write/anti-dependency conflicts.",
      "A transaction T2 depends on T1 if there is a path in the graph from T1 to T2.",
      "It directly depends on T1 if there is an edge from T1 to T2.",
      "In the description that follows, a delete is modelled as committing a special \u201cdead\u201d version of an object.",
      "To construct a DSG start with one node for every committed transaction, and for all pairs of distinct nodes T1 and T2, add a read dependency edge from T1 to T2 if any of the following conditions hold:  T1 commits a version xi of object x,  and T2 subsequently reads xi.",
      "T1 commits a change, and T2 then performs a predicate-based read such that the set of objects matched by the predicate are altered by T1\u2019s commit.",
      "Furthermore, T1 is the most recent transaction to have committed a change impacting T1\u2019s matches.",
      "Next add an anti-dependency edge from T1 to T2 if any of the following conditions hold:  T1 reads some version xi of object x, and T2 then commits the next version of x in the version history.",
      "T1 performs a predicate based read.",
      "T2 then commits a later (the next?)",
      "version of some object that would change the matches of T1.",
      "And finally add a write dependency edge from T1 to T2 if  T1 commits version xi of some object, and T2 then commits the next version of x in the version history.",
      "The dependency types and corresponding DSG edges are summarised below:  Phenomena and Isolation Levels  Isolation Level PL-1 (Portable definition of Read Uncommitted)  Phenomena P0 prevents dirty writes, and is motivated by a requirement to be able to serialize transactions based on writes, and to simplify recovery from aborts.",
      "The latter reason is not applicable to all systems, that may use other schemes for recovery.",
      "Portable Isolation level PL-1 provides for serialized transactions based on writes by disallowing Write Cycles.",
      "A Write Cycle is phenomenon G0.",
      "A history exhibits a write cycle if its direct serialization graph contains a cycle consisting entirely of write-dependency edges.",
      "Note that this definition neatly encompasses write cycles with more than 2 objects involved.",
      "Isolation Level PL-2 (Portable definition of Read Committed)  The essence of no dirty reads \u2013 without being overly restrictive on the reading and writing of objects written by transactions that are still uncommitted \u2013 is captured by phenomenon G1, which is comprised of three sub-phenomena G1a, G1b, and G1c.",
      "G1 subsumes G0, and PL-2 is the isolation level that prohibits G1 (and hence G0 also).",
      "G1a \u2013 Aborted Reads.",
      "T2 reads some object (including via predicates reads) modified by T1, and T1 aborts.",
      "To prevent aborted reads, if T2 reads from T1 and T1 aborts, T2 must also abort \u2013 known as a cascading abort.",
      "G1b \u2013 Intermediate Reads.",
      "T2 reads a version of some object (including via predicate reads) modified by T1, and it was not T1\u2019s final modification of that object.",
      "To prevent intermediate reads, transactions can only be allowed to commit if they have read the final versions of objects from other transactions.",
      "G1c \u2013 Circular Information Flow.",
      "The Direct Serialization Graph contains a directed cycle consisting entirely of (read and write) dependency edges.",
      "If T1 is affected by T2, then there is no path by which T2 can also affect T1.",
      "Proscribing G1 is clearly weaker than proscribing P1 since G1 allows reads from uncommitted transactions\u2026.",
      "Our PL-2 definition treats predicate-based reads like normal reads and provides no extra guarantees for them; we believe this approach is the most useful and flexible.",
      "Isolation Level PL-2.99 (Portable definition of Repeatable Read)  The level called Repeatable Read\u2026 provides less than full serializability with respect to predicates.",
      "Anti-dependency cycles due to predicates can occur at this level.",
      "Phenomenon G2-item, Item Anti-Dependency Cycles occurs when a DSG contains a directed cycle having one or more item anti-dependency edges.",
      "PL-2.99 forbids G2-item anomalies in addition to G1 (and hence G0).",
      "Isolation Level PL-3 (Portable definition of Serializable)  Consider the following history and DSG:  When T1 performs its query, there are exactly two employees, x and y, both in Sales.",
      "T1 sums up the salaries of these employees and compares it with the sum-of-salaries maintained for this department.",
      "However, before it performs the final check, T2 inserts a new employee, z2, in the Sales department, update the sum-of-salaries, and commits.",
      "Thus when T1 reads the new sum-of-salaries value it finds an inconsistency.",
      "This history is allowed at isolation level PL-2.99, but forbidden at isolation level PL-3, which prohibits phenomenon G2:  G2 Anti-dependency cycles occurs when a DSG contains a directed cycle with one or more anti-dependency edges (item or predicate).",
      "Proscribing G2 is weaker than providing P2, since we allow a transaction to modify object x even after another uncommitted transaction has read x\u2026.",
      "our specification for PL-3 provides conflict-serializability (this can be shown using theorems presented in [9]).",
      "All realistic implementations provide conflict-serializability; thus our PL-3 conditions provide what is normally considered as serializability.",
      "Mixing Isolation Levels  In a mixed system, each transaction specifies its level when it starts and this information is maintained as part of the history and used to construct a mixed serialization graph or MSG.",
      "Like a DSG, the MSG contains nodes corresponding to committed transactions and edges corresponding to dependencies, but only dependencies relevant to a transaction\u2019s level or obligatory dependencies show up as edges in the graph\u2026 For example, an anti-dependency edge from a PL-3 transaction to a PL-1 transaction is an obligatory edge since overwriting of reads matters at level PL-3.",
      "If an MSG is acyclic, and phenomena G1a and G1b do not occur for PL-2 and PL-3 transactions then a history is mixing correct.",
      "Such a history provides each transaction with the guarantees that pertain to its level.",
      "Broader Applicability  Our approach is applicable to other levels in addition to the ones discussed in the paper.",
      "We have developed implementation-independent specifications of commercial isolation levels such as Snapshot Isolation and Cursor Stability, and we have defined a new level called PL-2+; the details can be found in [1 \u2013 Adya\u2019s PhD thesis].",
      "PL-2+ is the the weakest level that guarantees consistent reads and causal consistency; it is useful in client-server systems and broadcast environments."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://bnrg.cs.berkeley.edu/~adj/cs262/papers/icde00.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 1873334
  },
  {
    "blog_id": "occupy-the-cloud-distributed-computing-for-the-99",
    "summary": [
      "Occupy the cloud: distributed computing for the 99% Jonas et al., SoCC\u201917  \u2018Occupy the cloud\u2019 won the best vision paper award at the recent ACM Symposium on Cloud Computing event.",
      "In the spirit of a vision paper, you won\u2019t find detailed implementation and evaluation  information here, but hopefully you\u2019ll find something to make you think.",
      "If I had to summarise the argument, I think it\u2019s this: speed of deployment and ease of use coupled with good enough runtime performance wins.",
      "DevOps are the new Kingmakers.",
      "Follow that line-of-thought, and the authors take you to an interesting proposition: serverless could be the centrepiece of a new distributed data processing platform.",
      "In this paper we argue that a serverless execution model with stateless functions can enable radically-simpler, fundamentally elastic, and more user-friendly distributed data processing systems.",
      "In this model, we have one simple primitive: users submit functions that are executed in a remote container; the functions are stateless as all the state for the function, including input, output is accessed from shared remote storage.",
      "At this point the alarm bells are probably ringing.",
      "How on earth can you make that even remotely efficient when all you have are stateless functions and hence you\u2019ll be serializing and deserializing all over the place?",
      "Surprisingly, we find that the performance degradation from using such an approach is negligible for many workloads and thus, our simple primitive is in fact general enough to implement a number of higher-level data processing abstractions, including MapReduce and parameter servers.",
      "What users want  Many more casual users of data processing platforms (the 99% from the paper title) don\u2019t want to spend hours understanding a complex stack of e.g. Spark, Scala, HDFS, Yarn and the JVM.",
      "Even the choice of instance types (70 on AWS at the time the paper was written), cluster sizes, regions and so on can be bewildering.",
      "\u2026 most software, especially in scientific and analytics applications, is not written by computer scientists, and it is many of these users who have been left out of the cloud revolution.",
      "What do these users want?",
      "To get vastly better performance than available on their laptop or workstation while taking minimal development time.",
      "For an interesting number of compute bound workloads, such as trying a large number of random initial seeds in a Monte Carlo simulations, or sweeping over a wide-range of hyperparameters in a machine learning setting (or even being guided through a set of input parameters by a system such as Vizier ), it\u2019s a big step forward to parallelise across functions, without necessarily worrying about intra-function optimisations.",
      "Therefore, a simple function interface that captures sufficient local state, performs computation remotely, and returns the result is more than adequate.",
      "Even for data-bound workloads, many users would be served by a simpler version of existing map-reduce frameworks with outputs persisted on object storage.",
      "How simple can we make it?",
      "Many of the problems with current cloud computing abstractions stem from the fact that they are designed for a server-oriented resource model.",
      "Having servers as the unit of abstraction ties together multiple resources like memory, CPU and network bandwidth.",
      "Further servers are also often long running and hence require DevOps support for maintenance.",
      "Our proposal is to to instead use a serverless architecture with stateless functions as the unifying abstraction for data processing.",
      "Users can run arbitrary (data processing) functions without setting up and configuring servers/frameworks etc.",
      "To make this work, you need a low overhead execution runtime, a fast scheduler, and high performance remote storage.",
      "The architecture at the base level is very simple by design and just includes the basics needed to execute functions.",
      "More complex abstractions such as dataflow and BSP are implemented on top.",
      "What makes this more tractable than in the past is that the benefits of colocation (of code and data) are diminishing.",
      "For example, on AWS EC2 writing to remote storage is faster than storing data on a single local SSD (though not than multiple SSDs).",
      "The gap between network bandwidth and storage I/O bandwidth continues to narrow though.",
      "While the developer has no control of where a stateless function runs\u2026 the benefits of colocating computation and data \u2013 a major design goal for prior systems like Hadoop, Spark and Dryad \u2013 have diminished.",
      "The PyWren prototype  It will probably come as no surprise to you to hear that the prototype is built on top of AWS Lambda.",
      "PyWren exposes a map primitive from Python on top of Lambda.",
      "On first encounter there\u2019s a slightly weird thing going on whereby instead of having the users deploy Lambda functions directly, PyWren has just a single common Lambda function.",
      "PyWren works by serializing the user written Python functions using cloudpickle and storing the results in S3 buckets.",
      "The single PyWren Lambda function then loads both the data to be processed and the function to process it from S3, with the result being serialized back to S3 at a pre-specified key.",
      "The stated reasons for doing it this way are:  to eliminate the majority of user overhead from deployment, packaging, and code versioning.",
      "to mitigate the high latency for function registration  to be able to execute functions that exceed Lambda\u2019s code size limit  I would anticipate those advantages diminishing over time.",
      "PyWren\u2019s map API mirrors the existing Python API for parallel processing, and so integrates easily with existing libraries for data processing and visualization.",
      "Calling map launches as many stateless functions as there are elements in the list that one is mapping over.",
      "The following benchmark results show the impact of using remote storage only and how that scales with worker counts.",
      "In our research group we have had students use PyWren for applications as diverse as computational imaging, scientific instrument design, solar physics, and object recognition.",
      "Working with heliphysicists at NASA\u2019s Solar Dynamics Observatory, we have used PyWren for extracting relevant features across 16TB of solar imaging data for solar flare prediction.",
      "Working with applied physics colleagues, we have used PyWren to design novel types of microscope point-spread functions for 3d superresolution microscopy.",
      "This necessitates rapid and repeat evaluation of a complex physics-based optical model inside an inner-loop.",
      "Beyond simple functions: dataflow and BSP  The next step beyond a simple parallel map, is a parallel map with a single global reduce stage running on one machine.",
      "This suits a number of classical machine learning workloads, and can handle learning problems up to 2TB in size.",
      "BSP algorithms can be implemented by adding data shuffles across stages using high bandwidth remote storage.",
      "The authors use both PySpark and PyWren to run a word count program over a dataset of 83.68M product reviews split into 333 partitions.",
      "PyWren is 17% slower (about 14s), but this doesn\u2019t count the 5-10 minutes needed to start the Spark cluster.",
      "The shuffle intensive Daytona sort benchmark is more challenging for PyWren.",
      "Finally using low-latency, high throughput key-value stores like Redis, RAMCloud, we can also implement parameter server style applications in PyWren.",
      "For example, we can implement HOGWILD!",
      "stochastic gradient descent by having each function compute the gradients based on the latest version of the shared model.",
      "Prototype limitations  You need to fit within the existing Lambda resource limits.",
      "Under current Lambda constraints this is enough to fill up the function\u2019s 1.5GB of memory in around 40 seconds.",
      "Assuming the same time to write results back out you have up to 80 seconds of I/O time, and therefore about 220 seconds of compute.",
      "The pricing works out at about 2x that of on-demand instances, if you assume 100% utilisation of those instances.",
      "There is no opportunity to influence the scheduling  Debugging is harder, relying on AWS CloudWatch logs.",
      "Large shuffle intensive workloads are not easily supported  It takes about 20-30 seconds to launch a function in the absence of caching \u2013 setting up the Python runtime, downloading the code to run from S3 etc..",
      "Applications requiring access to specialized hardware (GPU, FPGA) aren\u2019t supported in Lambda at the moment.",
      "Despite all of these, the list of applications that have been successfully deployed using PyWren (as we looked at earlier) is still quite impressive."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://arxiv.org/pdf/1702.04024",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 29474820
  },
  {
    "blog_id": "dremel-interactive-analysis-of-web-scale-datasets",
    "summary": [
      "Dremel: interactive analysis of web-scale datasets \u2013 Melnik et al. (Google), 2010.",
      "Dremel is Google\u2019s interactive ad-hoc query system that can run aggregate queries over trillions of rows in seconds.",
      "It scales to thousands of CPUs, and petabytes of data.",
      "It was also the inspiration for Apache Drill.",
      "Dremel borrows the idea of serving trees from web search (pushing a query down a tree hierarchy, rewriting it at each level and aggregating the results on the way back up).",
      "It uses a SQL-like language for query, and it uses a column-striped storage representation.",
      "It also supports a nested data model \u2013 Google\u2019s Protocol Buffers  Column stores have been adopted for analyzing relational data [1] but to the best of our knowledge have not been extended to nested data models.",
      "The columnar storage format that we present is supported by many data processing tools at Google, including MR, Sawzall, and FlumeJava.",
      "The way in which nested data structures are split into columns, and then re-assembled on demand in response to queries is central to Dremel, so let\u2019s explore how that works.",
      "Here\u2019s a sample Protocol Buffer schema for a Document entity:  message Document {     required int64 DocId;     optional group Links {         repeated int64 Backward;         repeated int64 Forward;     }     repeated group Name {         repeated group Language {             required string Code;             optional string Country;         }         optional String Url;     } }  Notice a few things about this: there are repeated and optional components, and there is nesting.",
      "The first part of splitting this into columns is pretty straight-forward: make a column for each field, using the nested path names.",
      "So, for the schema above we have columns DocId, Links.Backward, Links.Forward, Name.Language.Code, Name.Language.Country, and Name.Url.",
      "Focusing in on the Name.Language.Code column, we\u2019re going to take the Code entries from multiple \u2018rows\u2019 (Documents) and put them all into this column.",
      "The first problem we have to solve comes from the fact that \u2018Code\u2019 can be repeated several times within the same Document.",
      "In the \u2018DocId\u2019 column, each entry  represents a new Document, but in the Name.Language.Code column we need a way to know whether a given entry is a repeated entry from the current Document, or the start of a new Document.",
      "And if it is repeated,  where does it belong in the nesting structure?",
      "Furthermore, given the fact that some fields are optional (may be missing), we\u2019re going to need a way to take that into account too.",
      "Dremel solves these problems by keeping three  pieces of data for every column entry: the value itself, a repetition level, and a definition level.",
      "How it works is pretty subtle to wrap your head around, but I\u2019ll try to explain it as clearly as possible.",
      "Take a good look at the sketch below from my notebook.",
      "It shows a Document record that we want to split into columns, and to the right, the column entries that result within the Name.Language.Code column \u2013 where r represents the repetition level, and d the definition level.",
      "The first problem we mentioned was how to tell whether an entry is the start of a new Document, or another entry for the same column within the current Document.",
      "That\u2019s an easy one to solve: the repetition level is set to 0 for the first occurence of a field within a record.",
      "Hence \u2018en-us,\u2019 which is the first Code within the document, is encoded with repetition level 0.",
      "You might intuitively think that we\u2019d simply increment the repitition level for each occurence (and hence the \u2018en\u2019 entry would have repetition level 1, and \u2018en-gb\u2019 repetitionlevel 2).",
      "But that\u2019s not actually how it works.",
      "Notice that if we did that, we couldn\u2019t distinguish between a Code that is in a repeated Language element of a given Name (\u2018en\u2019 in our example), and a Code that is in a new Name element (the \u2018en-gb\u2019 case).",
      "So for all repeated column entries in a record after the first one, the repetition level value that is stored is instead the \u2018level\u2019 at which we\u2019re repeating.",
      "For the nesting Name.Language.Code, Name is level 1, Language is level 2, and Code is level 3.",
      "Still with me?",
      "Good, so, when we come to encode the \u2018en\u2019 value in the column, we give it repetition level 2, because it is inside a replicated Language element.",
      "And when we come to encode the \u2018en-gb\u2019 value, we give it repetition level 1, because Name is the first level at which we\u2019re repeating at this point in the record.",
      "And that NULL value you see in the column?",
      "That\u2019s there to record the fact that the second Name element doesn\u2019t include a Language.Code value at all.",
      "It\u2019s at repetition level 1 because the \u2018Name\u2019 element is the level we\u2019re repeating at.",
      "Now let\u2019s talk about the definition level value, d. Intuitively you might think this is just the nesting level in the schema (so 1 for DocId,  2 for Links.Forward, 3 for Name.Language.Code etc.)",
      "\u2013 but again that\u2019s not quite what it represents (and in fact, if we had access to the schema, it would be redundant to store that information with every column entry of course).",
      "Instead, the definition level indicates how many of the parent fields are actually defined.",
      "This is easier to understand by example.",
      "For the \u2018en-us\u2019 Code entry, it\u2019s within a Language field, within a Name field \u2013 so this gets definition level 2.",
      "The same is true for the \u2018en\u2019 and \u2018en-gb\u2019 entries.",
      "For the NULL entry though, the enclosing \u2018Name\u2019 is present, but there is no \u2018Language\u2019 component at all.",
      "Therefore this gets definition level 1.",
      "It turns out that by encoding these repitition and definition levels alongside the column value, it is possible to split records into columns, and subsequently re-assemble them efficiently.",
      "The algorithms for doing this are given in an appendix to the paper.",
      "Record assembly is pretty neat \u2013 for the subset of the fields the query is interested in, a Finite State Machine is generated with state transitions triggered by changes in repetition level.",
      "Let\u2019s cut to the chase now and look at what Google learned building and operating Dremel:  Scan-based queries can be executed at interactive speeds on disk-resident datasets of up to a trillion records.",
      "Near-linear scalability in the number of columns and servers is achievable for systems containing thousands of nodes.",
      "MR can benefit from columnar storage just like a DBMS.",
      "Record assembly and parsing are expensive.",
      "Software layers (beyond the query processing layer) need to be optimized to directly consume column-oriented data.",
      "MR and query processing can be used in a complementary fashion; one layer\u2019s output can feed another\u2019s input.",
      "In a multi-user environment, a larger system can benefit from economies of scale while offering a qualitatively better user experience.",
      "(Splitting the work into more parallel pieces reduced overall response time, without causing more underlying resource, e.g. CPU,  consumption)  If trading speed against accuracy is acceptable, a query can be terminated much earlier and yet see most of the data.",
      "The bulk of a web-scale dataset can be scanned fast.",
      "Getting to the last few percent within tight time bounds is hard.",
      "The last two points relate to the problems we looked at in The Tail at Scale \u2013 the outliers are always slower, and at enough scale you\u2019re going to get tripped up by that.",
      "Dremel allows you to specify the percentage of data processed at which you\u2019re happy to stop a query and return results.",
      "It sounds odd to say you want the results of a query without looking at all of the data \u2013 but consider for example a top-k query.",
      "Looking at 98% of the data makes it highly likely you\u2019ll get the right answer,  and cutting off the slow tail can give a response in under a minute as opposed to several minutes just waiting for that last 2%."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36632.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 21145042
  },
  {
    "blog_id": "sttr-a-system-for-tracking-all-vehicles-all-the-time-at-the-edge-of-the-network",
    "summary": [
      "STTR: A system for tracking all vehicles all the time at the edge of the network Xu et al., DEBS\u201918  With apologies for only bringing you two paper write-ups this week: we moved house, which turns out to be not at all conducive to quiet study of research papers!",
      "Today\u2019s smart camera surveillance systems are largely alert based, which gives two main modes of operation: either you know in advance the vehicles of interest so that you can detect them in real time, or you have to trawl through lots of camera footage post-facto (expensive and time-consuming).",
      "STTR is a system designed to track all of the vehicles all of the time, and store their trajectories for ever.",
      "I certainly have mixed feelings about the kinds of state surveillance and privacy invasions that enables (it\u2019s trivial to link back to individuals given trajectories over time), but here we\u2019ll just focus on the technology.",
      "Since the system is design with pluggable detection and matching algorithms, then given some calculations around volume it ought to be possible to use it to track objects other than vehicles.",
      "People for example?",
      "Assuming the availability of suitable detection and matching (figuring out if a vehicle is one you have seen before) algorithms, then the biggest challenge involved in tracking all of the vehicles all of the time is managing network bandwidth and storage.",
      "Storing and sending raw video clearly doesn\u2019t work, so STTR simply stores vehicle trajectories (presumably some window, e.g. 24 hours, of video could also be accommodated) and processes video at local nodes.",
      "An edge/fog node can have multiple connected cameras.",
      "Processing sensor streams (especially cameras) at the edge of the network is advantageous for three reasons: (a) reducing the latency for processing the streams, (b) reducing the backhaul bandwidth needed to send raw sensor streams to the cloud; and (c) preserving privacy concerns for the collected sensor data.",
      "(I know what they mean by (c), even though the literal interpretation says the opposite of what the authors intended!).",
      "Even when we\u2019re storing only trajectories and processing video locally, all trajectories for all vehicles for all time still sounds like a lot of storage, and worse, potentially unbounded.",
      "The first key insight is that there is a pragmatic bound if we make some assumptions about vehicle density and vehicle lifetime (e.g. age of vehicle or miles travelled).",
      "The second key insight is that any one node only needs to manage data for a finite \u2018activity region\u2019 (loosely, the area that it covers before other cameras take over).",
      "The finite size of the camera\u2019s activity region gives us a very good property, because at any given time, the number of vehicles that are active under this camera has to be finite, for vehicles need to occupy space.",
      "Meanwhile, each vehicle\u2019s life is also finite which indicates there exists trajectory upper bound for a given camera.",
      "Putting these facts together, we have the following simple idea \u2013 at any point in time, each camera stores the trajectory of vehicles that are active under its region.",
      "A back of the envelope calculation with 5 cameras every mile, up to 100 simultaneously active vehicles for a given camera, and a vehicle lifetime of 150,000 miles, gives a storage requirement of around 1.1GB per camera.",
      "Storing trajectories  The basic idea is to store the trajectory for a given vehicle at the node (camera) where it was last detected (active).",
      "Considering the following figure, the red car first seen at intersection  by camera  attached to fog node  begins with a trajectory  .",
      "When it moves through intersection  we append to its known trajectory and migrate the record to node  .",
      "When the vehicle then arrives at intersection  we migrate the record once more, this time to node  .",
      "The final trajectory record is  .",
      "It\u2019s easy to spot that as vehicle trajectories get longer over time, this process will result in a lot of network traffic to migrate trajectories from node to node.",
      "So instead of full trajectory migration, trajectories are partitioned across nodes and only aggregated once a node comes under storage pressure.",
      "(The current paper doesn\u2019t consider querying, so implications for query efficiency here are unaddressed).",
      "We end with a lazy version of the basic algorithm.",
      "Given the same vehicle movements as above the lazy trajectory aggregation might play out as follows:  At time  we create trajectory record (vertex)  at node  .",
      "At time  we create an a trajectory vertex at node  containing just the single step:  , and we also create an edge from the vertex at  to the newly created vertex at  (so we\u2019re building up a distributed graph structure).",
      "This process repeats when the vehicle arrives at  , such that we end up with three linked trajectory portions.",
      "If node  later comes under storage pressure we can aggregate its portion of the trajectory into the node at  , ending up with an edge from  directly to  , and a trajectory vertex at  of  .",
      "Once a vehicle is assumed to have reached end-of-life, its trajectory can be migrated to cloud storage or similar.",
      "Network communication via forward and backward propagation  Information about detected vehicles is propagated through the network in order to build up the distributed graph and to reduce resource requirements at nodes.",
      "The primary mechanism is forward propagation.",
      "Once a vehicle has been detected by a camera, the vehicle\u2019s signature (produced by an algorithm provided by domain experts) is sent to all cameras that are likely candidates for the vehicles to pass next.",
      "I infer that along with this message is sent the identifiers of all cameras in the propagation set.",
      "The cameras are then already primed to run the re-identification procedure when the vehicle is sighted.",
      "If a camera does detect a vehicle with a signature it received via forward propagation then it sends a sighting confirmation to all the other cameras in the propagation set, who can now drop it from their forward propagation local storage.",
      "Sometimes a camera might detect a vehicle that it has not been primed to expect via forward propagation.",
      "In this case, backward propagation is used to send a message to candidate upstream cameras given the direction of travel.",
      "If such a camera cannot immediately re-identify the detected object in the backward propagation message it can simply discard it.",
      "System architecture  The overall architecture of the STTR system looks like this:  Each fog node runs a collection of seven modules per camera (e.g., in a container).",
      "The detection module takes as input a raw video stream from the camera, and outputs a stream of detected object (signatures).",
      "The matching module implements the re-identification algorithm, looking for a match with a detected object in the candidate vehicle pool assembled from forward propagation messages  The camera map module maintains the geographical relationship between cameras  The forward and backward modules implement forward and backward propagation  The trajectory store manages trajectories as previously discussed and optimises network consumption  The policy module supports system configuration  The evaluation implementation is written in Python using ZeroMQ for communication between nodes and Redis as a persistent store.",
      "Evaluation  The evaluation is based on a simulation using the SUMO road traffic simulation package with a 10,000 second traffic flow emulation.",
      "Traffic is generated using a probability-based model.",
      "The fog computing topology is emulated using MaxiNet .",
      "The focus of the experiments is on understanding the storage and network bounds, and the impact of latency in forward and backward propagation.",
      "The short summary is that things pretty much work as you would expect.",
      "For example, if you have lower camera density then each camera uses more storage (since it is covering a larger area).",
      "The critical factor for enabling real-time surveillance turns out to be the speed of the re-identification algorithm.",
      "Plans for future work include:  Creation of efficient spatial and temporal index structures to support querying the network  Improving confidence in the calculated trajectories using a probabilistic approach to trajectory generation and maintenance  An on-campus deployment in conjunction with the campus police department"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3210284.3210291?download=true",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 66302705
  },
  {
    "blog_id": "barrier-enabled-io-stack-for-flash-storage",
    "summary": [
      "Barrier-enabled IO stack for flash storage Won et al., FAST\u201918  The performance of Flash storage has benefited greatly from concurrency and parallelism \u2013 for example, multi-channel controllers, large caches, and deep command queues.",
      "At the same time, the time to program an individual Flash cell has stayed fairly static (and even become slightly worse in some cases).",
      "That\u2019s ok until an application needs to guarantee ordering and/or durability of writes.",
      "Enforcing a storage order is achieved by an extremely expensive approach: dispatching the following request only after the block associated with preceding request has been completely transferred to the storage device and made durable.",
      "We call this mechanism Transfer-and-Flush.",
      "With transfer-and-flush, the parallelism and concurrency benefits are lost.",
      "As a consequence, on a single channel mobile storage SSD for smartphones, ordered writes offer only 20% of the performance (IOPS) of unordered writes.",
      "For a 32-channel Flash array, this ratio decreases to just 1%.",
      "In today\u2019s paper (which also won a best-paper award at FAST\u201918), the authors introduce a Barrier-enabled IO stack, which can enforce storage order between requests without the filesystem having to wait for a full transfer-and-flush.",
      "The headline performance gains certainly make you sit up and take notice:  SQLite performance increases by 270% and 75%, in server and in smartphone, respectively.",
      "In server storage BarrierFS brings performance gains of 43x and 73x in MySQL and SQLite respectively when compared to EXT4.",
      "There\u2019s some important fine-print in the headline numbers \u2013 the biggest gains come when ordering is preserved, but we don\u2019t wait for durability.",
      "The baseline being compared against offers both.",
      "Getting your ducks in order  The modern IO stack is said to be orderless\u2026  Consider a set of writes issued in some order (issue order) by a file system:  The IO scheduler may reorder and coalesce IO requests according so some scheduling principle (e.g., CFQ), before placing them onto a dispatch queue to be dispatched to the storage device (dispatch order).",
      "The storage controller receives the incoming commands and places them in its command queue.",
      "It is free to schedule commands for transfer as it sees fit.",
      "Errors, time-outs, and retries further impact the eventual ordering (transfer completion order).",
      "The overall persistence order is governed not by the order in which data blocks are made durable, but by the order in which the associated mapping table entries are updated.",
      "The two may not coincide.",
      "The long standing assumption (a holdover from the physical characteristics of rotating disks) is that the host cannot control the persistence order.",
      "The constraint that the host cannot control the persist order is a fundamental limitation in modern IO stack design.",
      "To guarantee ordering in the face of this, an expensive transfer-on-flush mechanism is used.",
      "If a needs to be ordered before b, then after dispatching a to the storage device, the caller needs to wait for a to be serviced (the DMA transfer to complete) \u2014 wait-on-transfer.",
      "Then the caller can issue a flush command and wait for its completion (wait-on-flush).",
      "Only once the flush has returned can the caller dispatch b.",
      "When ext4 (in its default Ordered mode) commits a journal transaction it uses two write requests: one for writing a coalesced chunk of journal descriptor block and log blocks (JD), and one for writing the commit block (JC).",
      "JD needs to be made durable before JC.",
      "Across transactions, ext4 also has to ensure that transactions are made durable in order.",
      "If either of these two ordering constraints are violated, the file system may recover incorrectly in the case of an unexpected failure.",
      "Order-preserving block devices with barriers  The \u2018cache barrier,\u2019 or \u2018barrier\u2019 for short, command is defined in the standard command set for mobile Flash storage.",
      "With the barrier command, the host can control the persist order without explicitly invoking cache flush.",
      "When the storage controller receives the barrier command, it guarantees that the data blocks transferred before the barrier command become durable after the ones that follow the barrier command do.",
      "The authors implement the barrier command concept using a new attribute REQ_BARRIER that can be set on a regular request object (to avoid the overhead of dispatching a dedicated command).",
      "For crash recovery they use a simple LFS style scheme.",
      "The actual implementation of the barrier command is not the main focus of the paper, it\u2019s how the barrier command is used and the benefits it brings that the authors focus on.",
      "(\u201cDeveloping a barrier-enabled SSD controller is an engineering exercise\u2026\u201d).",
      "With the barrier command in hand, it is possible to implement order-preserving dispatch:  Order-preserving dispatch is a fundamental innovation in this work.",
      "In order-preserving dispatch, the block device layer dispatches the following command immediately after it dispatches the preceding one and yet the host can ensure that that the two commands are serviced in order.",
      "We refer to this mechanism as wait-on-dispatch.",
      "Wait-on-dispatch eliminates the wait-on-transfer overhead.",
      "Using a barrier write request (i.e., a write request with both ORDERED and BARRIER attributes set), the existing command priority of the SCSI interface ensures that the barrier write is serviced only after the existing requests in the command queue are serviced and before any of the commands following the barrier write are serviced.",
      "The ordered priority command has rarely been used in existing block device implementations.",
      "This is because when the host cannot control the persist order, enforcing a transfer order with an ordered priority command barely carries any meaning from the perspective of ensuring the storage order.",
      "With the emergence of the barrier write, the ordered priority plays an essential role in making the entire IO stack an order-preserving one.",
      "Epoch-based scheduling (with epochs delineated by barrier writes)  ensures that:  the partial order between epochs is honoured  within an epoch, requests can be freely scheduled  orderless requests can be scheduled across epochs  Barrier-enabled filesystem (BFS)  The barrier-enabled IO stack adds two primitives, fbarrier() and fdatabarrier(), which synchronise the same set of blocks as fsync() and fdatasync() respectively, but return without ensuring the associated blocks become durable.",
      "I.e., they guarantee ordering but not durability.",
      "By interleaving write class with fdatabarrier an application can ensure that data blocks preceding the data barrier call are made durable ahead of data blocks that follow it.",
      "The authors modified ext4 to make it barrier enabled.",
      "Exploiting the order-preserving nature of the underlying block device, we physically separate the control plane activity (dispatching the write requests) from the data plane activity (persisting the associated data blocks and journal transaction) of a journal commit operation.",
      "Further, we allocate separate threads to each task so that the two activities can proceed in parallel with minimum dependency.",
      "The two threads are known as the commit thread  and the flush thread.",
      "We refer to this mechanism as Dual Mode Journaling.",
      "Evaluation  Section 6 in the paper contains evaluations of the block device layer, and the filesystem layer, but I\u2019m going to jump straight to the evaluation of what it means for applications running on top.",
      "For server workloads, the authors test varmail, a metadata-intensive workload known for heavy fsync() traffic, and an OLTP-insert workload using MySQL.",
      "When guaranteeing full durability, BFS improves varmail performance by between 10% and 60% depending on the type of SSD used.",
      "When guaranteeing only ordering, BFS gives a 36x performance improvement over ext4.",
      "For the MySQL workload, BFS gives a 12% performance boost with full durability guarantees, or a 43x performance boost with ordering only.",
      "With SQLite BFS gives a 75% performance boost on mobile storage with full durability, and 2.8x with just ordering.",
      "With higher powered server-side controllers employing higher degrees of parallelism the BFS advantage goes up to 73x.",
      "The last word  It is time to design a new IO stack for Flash storage that is free from the unnecessary constraint inherited from the old legacy that a host cannot control the persistence order\u2026.",
      "the \u201ccache barrier\u201d command is a necessity rather than a luxury.",
      "It should be supported in all Flash storage products ranging from mobile storage to high-performance Flash storage with supercap."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://www.usenix.org/system/files/conference/fast18/fast18-won.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 14819310
  },
  {
    "blog_id": "lets-talk-about-storage-and-recovery-methods-for-non-volatile-memory-database-systems",
    "summary": [
      "Let\u2019s talk about storage and recovery methods for non-volatile memory database systems Arulraj et al., SIGMOD 2015  Update: fixed a bunch of broken links.",
      "I can\u2019t believe I only just found out about this paper!",
      "It\u2019s exactly what I\u2019ve been looking for in terms of an analysis of the impacts of NVM on data storage systems, and the potential benefits of adapting storage algorithms to better exploit the technology.",
      "See my \u201c All change please \u201d post from earlier this year for a summary of NVM itself.",
      "Today\u2019s paper also contains some background on that, but I\u2019m going to focus on the heart of the material \u2013 in which the authors implement three of the common DBMS storage engine strategies and run them on an NVM system to see how they fare, before seeing what improvements can be made when the algorithms are then adapted to be optimised for NVM.",
      "Is NVM going to make a big difference, and will we need to rearchitect to get the most out of it?",
      "Yes (although it\u2019s not quite an order-of-magnitude difference):  We then present NVM-aware variants of these architectures that leverage the persistence and byte-addressability properties of NVM in their storage and recovery methods.",
      "Our experimental evaluation on an NVM hardware emulator shows that these engines achieve up to 5.5x higher throughput than their traditional counterparts while reducing the amount of wear due to write operations by up to 2x.",
      "A very short primer on NVM  NVM provides low latency reads and writes on the same order of magnitude as DRAM, coupled with persistent writes and a storage capacity similar to SSDs.",
      "NVM can scale beyond DRAM, and uses less power (DRAM consumes about 40% of the overall power consumed by a server).",
      "Flash-based SSDs use less power but are slower and only support block-based access.",
      "Although the advantages of NVM are obvious, making full use of them in an OLTP DBMS is non-trivial\u2026  Traditional DBMS engines are designed to cope with orders-of-magnitude differences in latency between volatile and non-volatile storage, and to optimise for block-level granularity with non-volatile storage.",
      "Many of the coping strategies employed are unnecessary in an NVM-only system.",
      "The authors perform their tests using an Intel Labs hardware emulator which is able to simulate varying hardware profiles expected from different NVM devices.",
      "The DBMS testbed  The authors study systems that are NVM-only (not two-level or greater storage hierarchies).",
      "They developed a single lightweight DBMS testbed into which multiple storage engines can be plugged.",
      "Using this consistent harness for all tests means that differences in performance will be due solely to the storage engines themselves.",
      "They develop traditional implementations of three storage engines, each using different approaches for supporting durable updates: (i) an in-place updates engine, (ii) a copy-on-write updates engine, and (iii) a log-structured updates engine.",
      "After taking a performance baseline for these engines using both YCSB and TPC-C workloads, they then develop NVM-aware derivatives of each of the engines and evaluate those for comparison.",
      "We get to see what changes are made, and the difference that they make.",
      "In-Place updates engine  The in-place updates engine keeps only a single version of each tuple at at all times.",
      "New values are written directly on top of old ones.",
      "The design of the in-place engine used in the study was based on VoltDB.",
      "A Write-Ahead Log (WAL) is used to assist in recovery from crashes and power failures, using a variant of ARIES adapted for main-memory DBMSs with byte-addressable storage engines.",
      "(See also MARS ).",
      "The standard in-place update engine has a high rate of data duplication \u2013 recording updates both in the WAL and in the table storage area.",
      "The logging infrastructure is designed on the assumption that the durable storage is much slower than memory, and thus batches updates (which increases response latency).",
      "Given this, we designed the NVM-InP engine to avoid these issues.",
      "Now when a transaction inserts a tuple, rather than copying the tuple to the WAL, the NVM-InP engine only records a non-volatile pointer to the tuple in the WAL.",
      "This is sufficient because both the pointer and the tuple referred to by the pointer are stored on NVM.",
      "Thus, the engine can use the pointer to access the tuple after the system restarts without needing to re-apply changes in the WAL.",
      "It also stores indexes as non-volatile B+trees that can be accessed immediately when the system restarts without rebuilding.",
      "There is no need to replay the log during recovery as committed transactions are durable immediately a transaction commits.",
      "The effects of uncommitted transactions that may be present in the database do need to be undone though.",
      "Copy-on-write updates engine  Instead of modifying the original tuple, a CoW engine creates a copy and then modifies that.",
      "It uses different look-up directories for accessing versions of tuples (aka shadow paging).",
      "There is no need for a WAL for recovery under this scheme.",
      "When a transaction commits, the engine updates the master record atomically to point to the new version of a tuple.",
      "In the study, the CoW engine uses LMDB\u2019s copy-on-write B-trees, storing directories on the filesystem with tuples in an HDD/SDD optimized format with all fields inlined.",
      "The CoW engine incurs a high overhead in propagating changes to the dirty directory \u2013 even if a transaction only modifies one tuple a whole block is copied to the filesystem.",
      "The NVM-CoW engine employs three optimizations to reduce these overheads.",
      "First, it uses a non-volatile copy-on-write B+tree that it maintains using the allocator interface.",
      "Second, the NVM-CoW engine directly persists the tuple copies and only records non-volatile tuple pointers in the dirty directory.",
      "Lastly, it uses the lightweight durability mechanism of the allocator interface to persist changes in the copy-on-write B+tree.",
      "It thus avoids the transformation and copying costs incurred by the original engine.",
      "Log-structured updates engine  The log-structured engine employs Log-structured merge (LSM) trees.",
      "Each tree consists of a collection of runs of data, each run is an ordered set of entries recording changes made to tuples.",
      "Runs reside either in volatile memory or stable storage with changes batched in memory and periodically cascaded out to stable storage.",
      "The contents of the memory table are lost on restart, so the engine maintains a WAL.",
      "The NVM version using a WAL stored on NVM.",
      "It avoids data duplication in the memory table and WAL by recording only non-volatile pointers to tuple modifications in the WAL.",
      "Instead of periodically flushing memory tables to stable storage tables (in stable storage optimised format), memory tables are simply marked as immutable .",
      "A few performance comparisons  Throughput comparisons at different latencies across the six engines for YCSB:  (Click for larger view)  On YCSB read-only workloads, NVM-InP is no faster than In-P, but NVM-CoW is 1.9-2.1x faster than straight CoW, and NVM-Log is 2.8x faster than Log.",
      "Under balanced and write-heavy workloads the NVM variants do much better, the NVM-CoW engine being 4.3-5.5x faster than straight CoW.",
      "Under TPC-C the NVM engines are 1.8-2.1x faster than the traditional engines.",
      "And for TPC-C:  Example of the difference in number of reads and writes (TPC-C):  On YCSB the NVM-aware engines perform up to 53% fewer loads, and 17-48% fewer stores on write-heavy workloads.",
      "For TPC-C, the NVM-aware engines perform 31-42% fewer writes.",
      "Total storage footprints:  NVM-InP and NVM-Log use 17-21% less storage on YCSB, and NVM-CoW uses 25% less.",
      "The NVM engines use 31-38% less storage on TPC-C \u2013 the space savings are more significant due to the write-intensive workload with long-running transactions.",
      "The takeaway  NVM access latency is the most significant factor in determining the runtime performance of the engines.",
      "The NVM aware variants achieve better absolute throughput (up to 5.5x), and perform fewer store operations (less than half for write-intensive workloads) which helps to extend device lifetime.",
      "They also use less storage (17-38% depending on workload) overall, which is important because early NVM products are expected to be relatively expensive.",
      "Overall, we find that the NVM-aware In-place engine performs the best across a wide set of workload mixtures and skew settings for all NVM latency configurations\u2026.",
      "It achieved the best throughput among all the engines with the least amount of wear on the NVM device."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://www.pdl.cmu.edu/PDL-FTP/NVM/storage.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 76874430
  },
  {
    "blog_id": "understanding-lifecycle-management-complexity-of-datacenter-topologies",
    "summary": [
      "Understanding lifecycle management complexity of datacenter topologies Zhang et al., NSDI\u201919  There has been plenty of interesting research on network topologies for datacenters, with Clos-like tree topologies and Expander based graph topologies both shown to scale using widely deployed hardware.",
      "This research tends to focus on performance properties such as throughput and latency, together with resilience to failures.",
      "Important as these are, note that they\u2019re also what\u2019s right in front of you as a designer, and relatively easy to measure.",
      "The great thing about today\u2019s paper is that the authors look beneath the surface to consider the less visible but still very important \u201clifecycle management\u201d implications of topology design.",
      "In networking, this translates into how easy it is to physically deploy the network, and how easy it to subsequently expand.",
      "They find a way to quantify the associated lifecycle management costs, and then use this to help drive the design of a new class of topologies, called FatClique.",
      "\u2026 we show that existing topology classes have low lifecycle management complexity by some measures, but not by others.",
      "Motivated by this, we design a new class of topologies, FatClique, that, while being performance-equivalent to existing topologies, is comparable to, or better than them by all our lifecycle management complexity metrics.",
      "Now, there\u2019s probably only a relatively small subset of The Morning Paper readers involved in designing and deploying datacenter network topologies.",
      "So my challenge to you as you read through this paper, is to think about where the hidden complexity and costs are in your own systems.",
      "Would you do things differently if these were made more visible?",
      "It would be great to see more emphasis for example on things like developer experience (DX) and operational simplicity \u2013 in my experience these kinds of attributes can have an outsize impact on the long-term success of a system.",
      "Anyway, let\u2019s get back to cables and switches\u2026  Physically deploying network topologies  When it comes to laying out a network topology for real in a datacenter, you need to think about packaging, placement, and bundling.",
      "Packaging is how you group things together, e.g. the arrangement of switches in racks, and placement concerns how these racks are physically placed on the datacenter floor.",
      "Placement in turn determines the kinds of cabling you need, and for optical cables the power of the transceivers.",
      "Within a rack we might package several connected switches into a single chassis using a backplane.",
      "At the other end of the scale, blocks are larger units of co-placement and packaging that combine several racks.",
      "With all those connections, it makes things a lot easier to group together multiple fibres all connecting the same two endpoints (racks) into bundles, which contain a fixed number of identical length fibres.",
      "Manufacturing bundles is simpler than manufacturing individual fibres, and handling such bundles significantly simplifies operational complexity.",
      "Patch panels make bundling easier by providing a convenient aggregation point to create and route bundles.",
      "Bundles and fibres are physically routed through the datacenter on cable trays.",
      "The trays themselves have capacity constraints of course.",
      "Here\u2019s an example of a logical Clos topology and its physical instantiation:  The authors identify three key metrics that together capture much of the deployment complexity in a topology:  The number of switches.",
      "More switches equals more packaging complexity.",
      "The number of patch panels, which is a function of topological structure and a good proxy for wiring complexity.",
      "The number of bundle types.",
      "This metric captures the other important part of wiring complexity \u2013 how many distinct bundle types are needed.",
      "A bundle type is represented by its capacity (how how many fibres) and its length.",
      "These complexity measures are complete.",
      "The number of cable trays, the design of the chassis, and the number of racks can be derived from the number of switches (and the number of servers and the datacenter floor dimensions, which are inputs to the topology design).",
      "The number of cables and transceivers can be derived from the number of patch panels.",
      "Here\u2019s how Clos and Expander (Jellyfish) representative topologies for the same number of servers stack up against these metrics:  The expander graph topology shows much higher deployment complexity in terms of the number of bundle types.",
      "Clos also exposes far fewer ports outside of a rack (it has better port hiding).",
      "Expanding existing networks  When you want to expand an existing network first you need to buy all the new gear and lay it out on the datacenter floor, and then you can begin a re-wiring process.",
      "This is all going on with live traffic flowing, so expansion is carried out in steps.",
      "During each step the capacity of the topology is guaranteed to be at least some percentage of the existing topology capacity.",
      "The percentage is sometimes known as the expansion SLO.",
      "During a step existing links to be re-wired are drained, then human operators physical rewire links at patch panels.",
      "The new links are tested and then undrained (strange word!",
      "), i.e., brought into service.",
      "For example, here\u2019s a logical expansion (top row) and its physical realisation:  The most time-consuming part of all this is the physical rewiring.",
      "The two metrics that capture expansion complexity are therefore:  The number of expansion steps, and  The average number of rewired links in a patch panel rack.",
      "Here\u2019s how Clos and Expander stack up on those metrics for the same networks we saw earlier:  This time the victory goes to Expander (Jellyfish).",
      "Jellyfish has a much higher north-to-south capacity ratio.",
      "Northbound links exit a block, and southbound links are to/from servers within a block.",
      "\u201cFat edges\u201d have more northbound than southbound links, and the extra capacity means you can accomplish more movement in each step.",
      "Clos topologies re-wire more links in each patch panel during an expansion step and require many steps because they have a low north-south capacity ratio.",
      "Enter the FatClique  Inspired by these insights, the authors define a new class of topologies called FatClique, which combine the hierarchical structure of Clos with the edge expansion capabilities of expander graphs.",
      "There are three levels in the hierarchy.",
      "A clique of switches form a sub-block.",
      "Cliques of sub-blocks come together to form blocks.",
      "And cliques of blocks come together to from the full FatClique topology.",
      "Four key design variables determine the particular instantiation of a FatClique topology: the number of ports in a switch that connect to other servers; the number of ports in a switch that connect to other sub-blocks in a block; the number of switches in a sub-block; and the number of sub-blocks in a block.",
      "A synthesis algorithm  takes a set of six input constraints (see \u00a75.1) and determines the values for these four design variables.",
      "There is plenty more detail in section 5 of the paper which I don\u2019t have the space to do justice too here.",
      "FatClique vs Clos vs Expander  The evaluation compares FatClique to Clos, Xpander, and Jellyfish at different network topology sizes, as shown in the table below.",
      "( Enlarge )  Here\u2019s how they stack up against the complexity metrics:  Number of switches  Number of patch panels  Number of bundle types  and associated cabling costs:  Number of expansion steps  Average number of rewired links  We find that FatClique is the best at most scales by all our complexity metrics.",
      "(The one exception is that at small and medium scales, Clos has slightly fewer patch panels).",
      "It uses 50% fewer switches and 33% fewer patch panels than Clos at large scale, and has a 23% lower cabling cost (an estimate we were able to derive from published cable prices).",
      "Finally, FatClique can permit fast expansion while degrading network capacity by small amounts (2.5-10%): at these levels, Clos can take 5x longer to expand the topology, and each step of Clos expansion can take longer than FatClique because the number of links to be rewired at each step per patch panel can be 30-50% higher.",
      "The one thing I couldn\u2019t find in the evaluation is any data to back up the opening claim that FatClique achieves all of this \u201cwhile being performance-equivalent to existing topologies.\u201d  The last word  As the management complexity of networks increases, the importance of designing for manageability will increase in the coming years.",
      "Our paper is only a first step in this direction\u2026"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://www.usenix.org/system/files/nsdi19-zhang.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 99384474
  },
  {
    "blog_id": "robinhood-tail-latency-aware-caching-dynamic-reallocation-from-cache-rich-to-cache-poor",
    "summary": [
      "RobinHood: tail latency aware caching \u2013 dynamic reallocation from cache-rich to cache-poor Berger et al., OSDI\u201918  It\u2019s time to rethink everything you thought you knew about caching!",
      "My mental model goes something like this: we have a set of items that probably follow a power-law of popularity.",
      "We have a certain finite cache capacity, and we use it to cache the most frequently requested items, speeding up request processing.",
      "Now, there\u2019s a long tail of less frequently requested items, and if we request one of these that\u2019s not in the cache the request is going to take longer (higher latency).",
      "But it makes no sense whatsoever to try and improve the latency for these requests by \u2018shifting our cache to the right.\u2019  Hence the received wisdom that unless the full working set fits entirely in the cache, then a caching layer doesn\u2019t address tail latency.",
      "So far we\u2019ve been talking about one uniform cache.",
      "But in a typical web application one incoming request might fan out to many back-end service requests processed in parallel.",
      "The OneRF page rendering framework at Microsoft (which serves msn.com, microsoft.com and xbox.com among others) relies on more than 20 backend systems for example.",
      "The cache is shared across these back-end requests, either with a static allocation per back-end that has been empirically tuned, or perhaps with dynamic allocation so that more popular back-ends get a bigger share of the cache.",
      "The thing about this common pattern is that we need to wait for all of these back-end requests to complete before returning to the user.",
      "So improving the average latency of these requests doesn\u2019t help us one little bit.",
      "Since each request must wait for all of its queries to complete, the overall request latency is defined to be the latency of the request\u2019s slowest query.",
      "Even if almost all backends have low tail latencies, the tail latency of the maximum of several queries could be high.",
      "(See \u2018 The Tail at Scale \u2019).",
      "The user can easily see P99 latency or greater.",
      "Techniques to mitigate tail latencies include making redundant requests, clever use of scheduling, auto-scaling and capacity provisioning, and approximate computing.",
      "Robin Hood takes a different (complementary) approach: use the cache to improve tail latency!",
      "Robin Hood doesn\u2019t necessarily allocate caching resources to the most popular back-ends, instead, it allocates caching resources to the backends (currently) responsible for the highest tail latency.",
      "\u2026RobinHood dynamically allocates cache space to those backends responsible for high request tail latency (cache-poor) backends, while stealing space from backends that do not affect the request tail latency (cache-rich backends).",
      "In doing so, Robin Hood makes compromises that may seem counter-intuitive (e.g., significantly increasing the tail latencies of certain backends).",
      "If you\u2019re still not yet a believer that caching can help with tail latencies, the evaluation results should do the trick.",
      "RobinHood is evaluated with production traces from a 50-server cluster with 20 different backend systems.",
      "It\u2019s able to address tail latency even when working sets are much larger than the cache size.",
      "In the presence of load spikes, RobinHood meets a 150ms P99 goal 99.7% of the time, whereas the next best policy meets this goal only 70% of the time.",
      "Look at that beautiful blue line!",
      "When RobinHood allocates extra cache space to a backend experience high tail latency, the hit ratio for that backend typically improves.",
      "We get a double benefit:  Since backend query latency is highly variable in practice, decreasing the number of queries to a backend will decrease the number of high-latency queries observed, improving the P99 request latency.",
      "The backend system will see fewer requests.",
      "As we\u2019ve studied before on The Morning Paper , small reductions in resource congestion can have an outsized impact on backend latency once a system has started degrading.",
      "Caching challenges  Why can\u2019t we just figure out which backends contribute the most to tail latency and just statically assign more cache space to them?",
      "Because the latencies of different backends tends to vary wildly over time: they are complex distributed systems in their own right.",
      "The backends are often shared across several customers too (either within the company, or perhaps you\u2019re calling an external service).",
      "So the changing demands from other consumers can impact the latency you see.",
      "Most existing cache systems implicitly assume that latency is balanced.",
      "They focus on optimizing cache-centric metrics (e.g., hit ratio), which can be a poor representation of overall performance if latency is imbalanced.",
      "Query latency is not correlated with query popularity, but instead reflects a more holistic state of the backed system at some point in time.",
      "An analysis of OneRF traces over a 24 hour period shows that the seventh most queried backend receives only about 0.06x as many queries as the most queried backend, but has 3x the query latency.",
      "Yet shared caching systems inherently favour backends with higher query rates (they have more shots at getting something in the cache).",
      "The RobinHood caching system  RobinHood operates in 5 second time windows, repeatedly taxing every backend by reclaiming 1% of its cache space and redistributing the wealth to cache-poor backends.",
      "Within each window RobinHood tracks the latency of each request, and chooses a small interval (P98.5 to P99.5) around P99 to focus on, since the goal is to minimise the P99 latency.",
      "For each request that falls within this interval, RobinHood tracks the ID of the backend corresponding to the slowest query in the request.",
      "At the end of the window RobinHood calculates the request blocking count (RBC) of each backend \u2013 the number of times it was responsible for the slowest query.",
      "Backends with a high RBC are frequently the bottleneck in slow requests.",
      "RobinHood thus considers a backend\u2019s RBC as a measure of how cache-poor it is, and distributes the pooled tax to each backend in proportion to its RBC.",
      "RBC neatly encapsulates the dual considerations of how likely a backend is to have high latency, and how many times that backend is queried during request processing.",
      "Since some backends are slow to make use of the additional cache space (e.g., if their hit rations are already high).",
      "RobinHood monitors the gap between the allocated and used cache capacity for each backend, and temporarily ignores the RBC of any backend with more than a 30% gap.",
      "When load balancing across a set of servers RobinHood makes allocation decisions locally on each server.",
      "To avoid divergence of cache allocations over time, RobinHood controllers exchange RBC data.",
      "With a time window of 5 seconds, RobinHood caches converge to the average allocation within about 30 minutes.",
      "The RobinHood implementation uses off-the-shelf memcached instances to form the caching layer in each application server.",
      "A lightweight cache controller at each node implements the RobinHood algorithm and issues resize commands to the local cache partitions.",
      "A centralised RBC server is used for exchange of RBC information.",
      "RBC components store only soft state (aggregated RBC for the last one million requests, in a ring buffer), so can quickly recover after a crash or restart.",
      "Key evaluation results  The RobinHood evaluation is based on detailed statistics of production traffic in the OneRF system for several days in 2018.",
      "The dataset describes queries to more than 40 distinct backend systems.",
      "RobinHood is compared against the existing OneRF policy, the policy from Facebook\u2019s TAO , and three research systems Cliffhanger , FAIR, and LAMA.",
      "Here are the key results:  RobinHood brings SLO violations down to 0.3%, compared to 30% SLO violations under the next best policy.",
      "For quickly increasing backend load imbalances, RobinHood maintains SLO violations below 1.5%, compared to 38% SLO violations under the next best policy.",
      "Under simultaneous latency spikes, RobinHood maintains less than 5% SLO violations, while other policies do significantly worse.",
      "Compared to the maximum allocation for each backend under RobinHood, even a perfectly clairvoyant static allocation would need 73% more cache space.",
      "RobinHood introduces negligible overhead on network, CPU, and memory usage.",
      "Our evaluation shows that RobinHood can reduce SLO violations from 30% to 0.3% for highly variable workloads such an OneRF.",
      "RobinHood is also lightweight, scalable, and can be deployed on top of an off-the-shelf software stack\u2026 RobinHood shows that, contrary to popular belief, a properly designed caching layer can be used to reduce higher percentiles of request latency."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://www.usenix.org/system/files/osdi18-berger.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 50388820
  },
  {
    "blog_id": "examining-gender-and-race-bias-in-sentiment-analysis-systems-b04b269a653",
    "summary": [
      "A pressing issue in machine learning (ML) research is to examine whether automated algorithms discriminate against gender or race when making decisions.",
      "In a recent study, Kiritchenko and Mohammad conducted an extensive analysis to determine whether hundreds of sentiment analysis systems accentuate inappropriate human biases and how these are manifested.",
      "Motivation  Automatic systems are beneficial to society but as they improve in predictive performance, comparable to human capabilities, they could perpetuate inappropriate human biases.",
      "In the context of sentiment analysis (SA), these biases may come in many forms.",
      "For instance, an SA system may consider the messages conveyed by a specific gender or race to be less positive simply because of their race and gender.",
      "Therefore, even when it is not clear how biases are manifested, it is the duty and responsibility of the natural language processing (NLP) system developer to address this problem.",
      "Contributions  In NLP, the sources of the bias may derive from the training data or language resources such as lexicons and word embeddings that a machine learning algorithm is designed to leverage for building the predictive model.",
      "There have been many works in computer vision and NLP that aim to detect these inappropriate biases, but few have focused on a large set of systems that aim to solve the same task, which is very common in the field of machine learning.",
      "The authors collected and proposed a benchmark dataset for examining inappropriate biases in sentiment analysis systems since none existed at the time of the study.",
      "Then 219 automatic sentiment analysis systems were examined for inappropriate biases using the dataset.",
      "The goal of the research was simply to examine whether the sentiment analysis systems were consistently assigning higher or lower sentiment intensity scores to sentence pairs involving a particular race or gender.",
      "Dataset  A benchmark dataset, coined as Equity Evaluation Corpus (EEC), was compiled to test the fairness of the sentiment analysis systems.",
      "Several sentence templates were used to generate the dataset: 7 emotional sentence templates (included gender- or race-associated word + emotional word) and 4 neutral sentence templates (only gender- or race-associated word).",
      "(See the 11 templates in the table below)  The <person> placeholder was instantiated by African American or European American names such as \u2018Latisha\u2019 and \u2018Ellen\u2019 and noun phrases such as \u2018my daughter\u2019 and \u2018my son\u2019.",
      "Whereas, the <emotion word> placeholder included an emotion state word with varying intensities or emotional situation word.",
      "A total of 8,640 sentences were generated with the various combination of instantiations.",
      "(Refer to the paper for the full list of noun phrases, names, and emotion words).",
      "For each template, sentence pairs were generated such as \u2018The conversation with my mom was heartbreaking\u2019 and \u2018The conversation with my dad was heartbreaking\u2019.",
      "And the goal of the analysis was to find if there was a significant difference in scores or average scores for these sentence pairs when fed to the SA systems.",
      "The Sentiment Analysis Systems  The 219 automatic sentiment analysis systems that participated in the SemEval-2018 Task 1: Affect in Tweets were individually evaluated for discrimination based on gender and race bias.",
      "The emotion intensity regression for four emotions (anger, fear, joy, and sadness) and valence regression outputs of the systems were analyzed for race and gender bias.",
      "The outputs were scores between 0 and 1.",
      "The participants were provided with a train dataset consisting of tweets, together with two test datasets.",
      "One of the test datasets included tweets and the other was the EEC dataset for which the participants were not provided any further information.",
      "Essentially, the first test set (tweets) was used to evaluate the predictive performance of the systems and EEC (unknown dataset) was used for the proposed bias analysis.",
      "Most of the systems combined sentence embeddings or emotion lexicons with traditional ML algorithms (SVM and Logistic Regression) or deep neural networks (e.g., LSTMs and Bi-LSTMs).",
      "The embeddings were obtained either through a distant supervision corpus , pre-trained models, or manually trained embeddings.",
      "The lexicons were often derived from the NRC emotion and sentiment lexicons.",
      "A baseline SVM system was also trained with word unigrams as features.",
      "To determine gender and race bias, each system\u2019s predicted score or average score on the respective sentence pairs generated by the templates were compared.",
      "Very high differences in scores indicated a higher bias by the systems.",
      "Gender Bias Results  The study reports that on the four emotion intensity prediction tasks a whopping 75% to 86% of the systems consistently scored sentences of one gender higher than another, even when all that changed was the noun phrases or names in the sentence pairs.",
      "This phenomenon was more evident in the emotions of anger and joy.",
      "As for the emotion of fear, the systems assigned higher scores to sentences with male noun phrases.",
      "Sadness was more balanced than the rest of the emotions.",
      "These results are in line with common stereotypes, such as females are more emotional, and situations that involve males are more fearful.",
      "Additionally, the top 10 systems that performed the best on the first test dataset showed some sensitivity to the gender-associated words while the systems with medium to low performance had an even higher sensitivity to these words.",
      "Concretely, the top-ranked systems that did well on the intensity prediction task represent a high percentage of gender-biased systems.",
      "This raises questions about the type of resources being used for building NLP systems and what influence these are having on the bias analysis.",
      "Race Bias Results  The race bias analysis produced similar results as compared to the gender bias analysis.",
      "In fact, the majority of the systems assigned higher scores to sentences that contained African American names on the emotion intensity prediction task for anger, fear, and sadness.",
      "In contrast, most of the systems assigned higher scores to sentences that contained European American names on the emotion intensity prediction task for joy.",
      "These results reflect the stereotypes found in a previous study which report that African Americans are commonly associated with more negative emotions.",
      "The Future  As future work, the authors will continue to investigate the systems (and its components) to locate the exact source/s of the identified biases.",
      "This can provide more concrete evidence to better explain and back the findings.",
      "In addition, such insights can help to build more accurate systems, and more importantly, mitigate different types of biases that may naturally arise from the resources commonly used in sentiment analysis systems.",
      "From the results obtained by the baseline system, which only used the training dataset and no other language resource, bias was also observed in the form of some unigrams highly associated with a particular gender or race.",
      "Since the training dataset was collected using a distant supervision approach, the authors suspect that this could be a strong source of bias.",
      "Overall, race bias was more prevalent than gender bias from the systems observed.",
      "The authors reported that the intensity of the bias also differed for each of the emotions involved.",
      "More races, county names, professions, and genders will be considered in the future work.",
      "(Update \u2014 8/11/2018)  Requests for Research  The EEC corpus only presents one setting to examine the fairness of sentiment analysis systems.",
      "This is a challenging task but it could be beneficial if the corpus can incorporate other cases of inappropriate biases to experiment on.",
      "Crowdsourcing efforts could help to improve the quality of the corpus.",
      "In addition, you can also try other types of systems besides SA.",
      "How about emotion recognition systems or emoji recognition systems?",
      "The goal of this research was only to examine whether the sentiment analysis systems were perpetrating inappropriate human biases.",
      "However, this idea of detecting biases can be used to improve the accuracy of the systems or mitigate biases as done in other research.",
      "I believe that including context (e.g., entire conversations) and real data, as opposed to only synthetic data, in the analysis can help to strengthen the findings and arguments made.",
      "In fact, more interesting insights may emerge from the datasets.",
      "Let me know if you want to work on any of these tasks.",
      "Reference: ACL 2018"
    ],
    "author_id": "DAIR-AI",
    "pdf_url": "https://www.aclweb.org/anthology/S18-2005.pdf",
    "author_full_name": "Elvis Saravia",
    "source_website": "https://github.com/dair-ai/nlp_paper_summaries",
    "id": 93195231
  },
  {
    "blog_id": "grand-pwning-unit-accelerating-microarchitectural-attacks-with-the-gpu",
    "summary": [
      "Grand Pwning Unit: Accelerating microarchitectural attacks with the GPU Frigo et al., IEEE Security & Privacy  The general awareness of microarchitectural attacks is greatly increased since meltdown and spectre earlier this year.",
      "A lot of time and energy has been spent in defending against such attacks, with a threat model that assumes attacks originate from the CPU.",
      "Frigo et al. open up an entirely new can of worms \u2013 modern SoCs contain a variety of special purpose accelerator units, chief among which is the GPU.",
      "GPUs are everywhere these days.",
      "Unfortunately, the inclusion of these special-purpose units in the processor today appears to be guided by a basic security model that mainly governs access control, while entirely ignoring the threat of more advanced microarchitectural attacks.",
      "I\u2019m sure you know where this is heading\u2026  It turns out the accelerators can also be used to \u201caccelerate\u201d microarchitectural attacks.",
      "Once more we find ourselves in a situation with widespread vulnerabilities.",
      "The demonstration target in the paper is a mobile phone running on the ARM platform, with all known defences, including any applicable advanced research defences, employed.",
      "Using WebGL from JavaScript, Frigo et al. show how to go from e.g.",
      "an advert on a web page to a fully compromised browser in under two minutes.",
      "Our end-to-end attack, named GLitch, uses all these GPU primitives in orchestration to reliably compromise the browser on a mobile device using only microarchitectural attacks in under two minutes.",
      "In comparison, even on PCs, all previous Rowhammer attacks from JavaScript require not default configurations (such as reduced DRAMh refresh rates or huge pages) and often take such a long time that some researchers have questioned their practicality.",
      "If only I could flip a bit\u2026  In Firefox, values stored in JavaScript ArrayObjects are 64 bits.",
      "The first 32 bits are used as a tag identifying the type of the object.",
      "When the tag value is below 0xffffff80 whole 64-bit work is considered as an IEEE-754 double, otherwise the last 32 bits are considered as a pointer to an object.",
      "(This strategy is known as NaN-boxing, encoding object pointers in IEEE-754 doubles as NaN values).",
      "So\u2026 if only we could flip bits within the first 25 bits of the tag, we could turn pointers into doubles, and vice-versa.",
      "The goal of the [GLitch] exploit is to obtain an arbitrary read/write primitive which can eventually lead to remote code execution.",
      "ArrayBuffers are the best fit to gain such a primitive since they provide the attacker with full control over their content.",
      "As a consequence, we want to create a reference to a fake ArrayBuffer whose data pointer we control.",
      "The GLitch exploit tool starts by storing a pointer to an inlined ArrayBuffer (header adjacent to data) in 1-to-0 bit-flip vulnerable location.",
      "Triggering a bit flip then turns this into a double that can be read, breaking ASLR (address space layout randomisation).",
      "Store a double in a 0-to-1 vulnerable cell in the ArrayBuffer, constructed in such a way that when a bit is flipped it becomes a pointer to a JSString, in turn pointing at the header (address obtained in step 1) for its immutable data.",
      "Read the value of the string to extract the content of the ArrayBuffer\u2019s header.",
      "Create a header for a fake ArrayBuffer (using the header information obtained in step 2) within the leaked ArrayBuffer and craft a reference to it using the same double-to-pointer bit flip technique that we used for the JSString.",
      "Now we have the desired arbitrary read/write primitive.",
      "Here\u2019s how long it takes for Glitch to break ASLR and compromise the browser on a Nexus 5:  On average, GLitch can break ASLR in only 27 seconds and fully compromise the browser remotely in 116s, making it the fastest known remote Rowhammer attack.",
      "So how does the GPU help us to flip bits (or just steal data in general using side-channel attacks)?",
      "Four attack primitives  We need to be able to either leak data (side-channel attacks) or corrupt data (e.g. Rowhammer attacks).",
      "A primary mechanism for leaking data using microarchitectural attacks is to time operations over resources shared with a victim process.",
      "The first attack primitive then is access to a high-resolution timer.",
      "There has been a bit of an arms race in CPU land with clever news ways of creating timers being devised and then blocked as best as possible.",
      "But the defences don\u2019t take into account GPUs.",
      "There are two explicit timer sources within the OpenGL / WebGL world that will do the job, available when the EXT_DISJOINT_TIMER_QUERY extension is present.",
      "Both GPU and CPU operations can be timed directly using the primitives it provides.",
      "It\u2019s also possible to craft your own timers using only standard (i.e., always available) WebGL2 functions: clientWaitSync and getSyncParameter.",
      "WebGL2 itself is not yet as widely supported as WebGL1 though.",
      "\u2026in order to comply with the WebGL2 specification none of these functions can be disable.",
      "Also, due to the synchronous nature of these timers, we can use them to measure both CPU and GPU operations.",
      "Here we can see for example clear timing differences between cached and uncached data using the EXT_DISJOINT_TIMER_QUERY extension:  A second attack primitive is having access to resources shared with other process.",
      "By figuring out the caching structure within their GPU (Adreno 330), the authors were able to figure out the sequence of operations needed to effectively bypass the GPU caches and measure memory page accesses for memory pages shared with the rest of the system.",
      "Internally the GPU has two levels of caching, and two ways of accessing memory (by inputting vertices to vertex shaders, or by fetching textures within shaders).",
      "Texture fetching turned out to be the easiest to control, and section IV.B of the paper describes in detail how the authors deduced an efficient strategy to evict cache sets from the GPU.",
      "The third attack primitive is knowledge of the physical location of allocated memory addresses: a requirement in order to understand which rows to hammer in a rowhammer atttack.",
      "When a row of memory is accessed we can tell if it was already in the row buffer or not by measuring the time the operation takes (buffer hits are faster).",
      "To carry out a reliable Rowhammer attack, three adjacent rows within a DRAM bank are required.",
      "Distinguishing between row buffer hits and misses enables us to determine whether allocations are contiguous or non-contiguous.",
      "(Details are in section VII.D, and the appendix covers the relationship between adjacency and contiguity).",
      "The fourth and final attack primitive is fast memory access needed to trigger bit flips with Rowhammer attacks.",
      "Using the knowledge of the GPU cache hierarchy gained via probing the authors derive efficient access patterns to perform double-sided Rowhammering attacks\u2026  Rowhammering  DRAM rows are composed of cells which store the value of a bit in a capacitor.",
      "The charge of a capacitor is transient, and therefore DRAM needs to be recharged within a precise interval (usually 64ms).",
      "Rowhammer is a software-based fault injection attack that can be considered a fallout of this DRAM property.",
      "By frequently activating specific rows an attacker can influence the charge in the capacitors of adjacent rows, making it possible to induce bit flips in a victim row without having access to its data.",
      "In a double-sided Rowhammer attack quick accesses to rows n-1 and n+1, impose high pressure on the capacitors in victim row n, triggering bit flips.",
      "\u201c\u2026 our novel GPU-based side-channel attack provides us with information about contiguous physical memory regions in JavaScript, allowing us to perform double-sided Rowhammer on ARM devices in the browser.\u201d  Mitigations  You could combine these primitives in a number of imaginative ways to construct different attacks, GLitch is but one end-to-end example.",
      "Eliminating known timers (e.g., disabling the EXT_DISJOINT_TIMER_QUERY) is still the best line of defence (though more timing strategies will likely be discovered).",
      "The WebGL2 getSyncParameter function can be disabled, and the clientWaitSync function could be replaced by a callback design.",
      "(This requires changes to the WebGL2 spec.).",
      "Stricter policies for memory reuse may also make it harder for an attacker to hammer valuable data.",
      "We showed that it is possible to perform advanced microarchitectural attacks directly from integrated GPUs found in almost all mobile devices\u2026 more alarming, these attacks can be launched from the browser.",
      "For example, we showed for the first time that with microarchitectural attacks from the GPU, an attacker can fully compromise a browser running on a mobile phone in less than 2 minutes\u2026 we hope our efforts make processor vendors more careful when embedding the next specialized unit into our commodity processors."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://www.vusec.net/wp-content/uploads/2018/05/glitch.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 61298922
  },
  {
    "blog_id": "diamond-automating-data-management-and-storage-for-wide-area-reactive-applications",
    "summary": [
      "Diamond: Automating data management and storage for wide-area, reactive applications Zhang et al., OSDI 2016  Diamond tackles the end-to-end problem of building reactive applications, defined here as those that update end-user visible state without requiring any explicit user action:  \u2026 today\u2019s popular applications are reactive: they provide users with the illusion of continuous synchronization across their device without requiring them to explicitly save, reload, and exchange shared data.",
      "Such applications have widely distributed processes sharing data across mobile devices, desktops, and cloud services.",
      "They make concurrent updates, may stop or fail at any time, and can be connected by slow or unreliable links.",
      "Distributed storage systems on their own don\u2019t provide a complete solution since programmers still need to synchronize updates between application processes and distributed storage.",
      "I didn\u2019t see it in the related work section, but for me SwiftCloud is another very interesting system that extends transactions all the way to the application / mobile client (although it doesn\u2019t emphasize the reactive nature in the same way as Diamond).",
      "Diamond aims to simplify development of such applications by combining a client-side library of reactive data types with a CRDT-aware data structure server (cf.",
      "Riak), push notifications, and client-side transaction support.",
      "A reactive data map (rmap) primitive lets applications create reactive data types and map them into the Diamond data management service  Reactive transactions automatically (re-)execute in response to shared data updates (push notifications)  A data-type aware form of optimistic concurrency control (DOCC) leverages the semantics of the supported data types to better cope with wide-area latencies (e.g., by concurrently committing transactions executing commutative operations).",
      "In case you were under any illusion that building such end-to-end reactive applications while maintaining safety was easy, \u00a72 in a paper does a good job of explaining otherwise.",
      "I\u2019m going to jump straight to the description of Diamond itself.",
      "System and data model  Diamond processes run on client device and on cloud servers.",
      "Every process is linked with a client-side library libdiamond, which provides access to the shared Diamond cloud.",
      "Front-end servers are scalable stateless nodes providing clients with access to Diamond\u2019s back-end storage, saving them from having to authenticate with or track the partitioning scheme of the back-end servers.",
      "Back-end storage servers themselves use Viewstamped Replication for fault tolerance.",
      "Diamond supports reactive data types for fine-grained synchronization, efficient concurrency control, and persistence.",
      "As with popular data structure stores such as Redis and Riak, we found that simple data types are general enough to support a wide range of application and provide the necessary semantics to enable commutativity and avoid false sharing\u2026.",
      "In addition to primitive data types, Diamond support simple Conflict-Free Replicated Data Types (CRDTs) and collection types with efficient type-specific interfaces.",
      "A Diamond instance provides a set of tables, where a table is simply a key -> data type map.",
      "Diamond maps its data types into application memory space through an operation called rmap.",
      "\u201cApplications call rmap with an application variable and a key to the Diamond record, giving them control over what data in their address space is shared and how it is organized.\u201d  Transaction model and implementation  Diamond supports both traditional style (application initiated) read-write transactions, and reactive transactions.",
      "A reactive transaction is essentially a state-change callback that runs inside a transaction and is triggered by a push notification from the Diamond \u2018cloud.\u2019 Reactive transaction may read, but not write, reactive data items.",
      "Reactive transactions help application processes automatically propagate changes made to reactive data types.",
      "Each time a read-write transaction modifies an rmapped variable in a reactive transaction\u2019s read set, the reactive transaction re-executes, propagating changes to derived local variables.",
      "As a result, reactive transactions provide a \u201clive\u201d view that gives the illusion of reactivity while maintaining an imperative programming style comfortable to application programmers.",
      "Further, because they read a consistent snapshot of rmapped data, reactive transactions avoid application-level bugs common to reactive programming models [48].",
      "Diamond\u2019s transaction protocol is similar to Spanner\u2019s , but uses DOCC instead of a locking mechanism, and commit timestamps from a timestamp service rather than TrueTime.",
      "When a read-write transaction commits, one or more reactive transactions may need to be executed:  The leader in the partition sends a publish request with the transaction\u2019s commit timestamp to each front-end subscribed to the related record  For each publish, the front-end server looks up the reactive transactions that have the updated record in their read sets and checks if the commit timestamp is bigger than the last notification sent to that client.",
      "If so, the front-end server sends a notify request to the client with the commit timestamp and the reactive transaction id  The client logs the notification on receipt, updates its latest known timestamp, and re-executes the reactive transaction at the commit timestamp.",
      "How does the front-end server know which reactive transactions will have the updated record in their read-set?",
      "When a reactive transaction is first registered by a client, libdiamond executes it immediately with the most recent data and records the reads that the transaction makes.",
      "This also means that push notifications can be made efficient as they can contain the data the reactive transaction is most likely to request.",
      "The discussion affords the fact that the read-set of a reactive transaction may change over time.",
      "If a reactive transaction doesn\u2019t have all of the data it wants to read pushed to it, it can always fetch it (and the read set will be updated for future go-rounds).",
      "What\u2019s not clear is how the front-end server would know to notify a client at all for some data that changes but the client has never requested before.",
      "My best guess is that it\u2019s up to the client to make sure it reads everything it might want to be notified about on that first run after registration.",
      "Reactive transactions in Diamond are similar to database triggers, events, and materialized views.",
      "They differ from these mechanisms because they modify local application state and execute application code rather than database queries that update storage state.",
      "Diamond\u2019s design draws on Thialfi; however, Thialfi cannot efficiently support data push notifications without insight into the application\u2019s access patterns.",
      "The Data-type (specific) Optimistic Concurrency Control mechanism (DOCC) uses fine-grained concurrency control based on the semantics of the different reactive data types.",
      "For example, allowing concurrent updates to different list elements.",
      "There\u2019s an (implicit) assumption baked in here though that there are no integrity constraints that need to be maintained across such elements.",
      "DOCC also makes use of CRDT properties when these types are used.",
      "The Diamond guarantee (100% satisfaction or your money back ;) )  The Diamond guarantee is described as \u201cACID+R\u201d and promises:  Atomicity \u2013 all or no updates to shared records in a r/w transaction succeed  Consistency \u2013 accesses in all transactions reflect a consistent view of shared records [Strongly consistent??]",
      "Isolation \u2013 accesses in all transactions reflect a global ordering of committed read-write transactions  Durability \u2013 updates to shared records in commited r/w transactions are never lost  Reactivity \u2013 accesses to modified records in registered reactive transactions will eventually re-execute.",
      "Diamond supports configurable isolation levels:  Evaluation  The team compared non-Diamond and Diamond-based versions of a number of reactive applications including \u2018the 100 game,\u2019 a chat room, a multiplayer scrabble game, and a twitter-clone.",
      "In addition to simplifying the design and programming of reactive apps, we found that Diamond facilitates the creation of general-purpose reactive libraries.",
      "As one example, Diamond transactions naturally lend themselves to managing UI elements.",
      "For instance, a check box usually maps a Boolean, re-draws a UI element in a reactive transaction, and writes to the Boolean in a read-write transaction when the user checks/unchecks the box.",
      "General findings when rewriting reactive apps to use Diamond include an elimination of bugs, and reduction in code size by around 30%.",
      "Performance evaluations show a low overhead in read-committed mode, through to a throughput reduction of about 50% when using strict serializability."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://www.usenix.org/system/files/conference/osdi16/osdi16-zhang-irene.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 81780960
  },
  {
    "blog_id": "eraser-a-dynamic-data-race-detector-for-multi-threaded-programs",
    "summary": [
      "Eraser: A dynamic data race detector for multi-threaded programs \u2013 Savage et al. 1997  Debugging a multithreaded program can be difficult.",
      "Simple errors in synchronization can produce timing-dependent data races that can take weeks or months to track down.",
      "Eraser dynamically detects data races in multi-threaded programs.",
      "There are two basic approaches to doing this, one based on \u2018happens-before\u2019 relationships, and one based on ensuring consistent locking discipline.",
      "Eraser uses the latter strategy.",
      "We have aimed Eraser specifically at the lock-based synchronization used in modern multithreaded programs.",
      "Eraser simply checks that all shared-memory accesses follow a consistent locking discipline.",
      "A locking discipline is a programming policy that ensures the absence of data races.",
      "For example, a simple locking discipline is to require that every variable shared between threads is protected by a mutual exclusion lock.",
      "We will argue that for many programs Eraser\u2019s approach of enforcing a locking discipline is simpler, more efficient, and more thorough at catching races than the approach based on happens-before.",
      "Let\u2019s get some definitions out of the way, briefly look at happens-before (the prior art), and then take a look at the core lockset algorithm that Eraser uses.",
      "Definitions  Lock:  A lock is a simple synchronization object used for mutual exclusion; it is either available, or owned by a thread.",
      "The operations on a lock mu are lock(mu) and unlock(mu).",
      "Thus it is essentially a binary semaphore used for mutual exclusion, but differs from a semaphore in that only the ownerof a lock is allowed to release it.",
      "Data Race:  A data race occurs when two concurrent threads access a shared variable and when at least one access is a write and the threads use no explicit mechanism to prevent the accesses from being simultaneous.",
      "Happens-before  Happens-before is a partial order of all events of all threads in a concurrent execution.",
      "For any single thread, events are ordered in the order in which they occur.",
      "Between threads, events are ordered according to the properties of the synchronization objects they access.",
      "If one thread accesses a synchronization object, and the next access to the object is by a different thread, then the first access is defined to happen before the second if the semantics of the synchronization object forbid a schedule in which these two interactions are exchanged in time.",
      "For example,  if one thread must release a lock before another thread can acquire it then we have a happens-before relationship.",
      "Without the presence of some sychnronization object, then we are at the mercy of data races.",
      "If two threads both access a shared variable, and the accesses are not ordered by the happens-before relation, then in another execution of the program in which the slower thread ran faster and/or the faster thread ran slower, the two accesses could have happened simultaneously; that is, adata race could have occurred, whether or not it actually did occur.",
      "All previous dynamic race detection tools that we know of are based on this observation.",
      "The authors of the paper point out that detectors based on happens-before are at the mercy of the interleaving produced by the scheduler for whether they can detect a race or not (see the easy to follow example in the paper).",
      "The Lockset algorithm used by Eraser does not have this property.",
      "The Lockset Algorithm  The basic version of the algorithm seeks to ensure that every access to a shared variable is protected by some lock.",
      "Whatever that lock is (since we don\u2019t know this up front), we want the lock to be held by any thread that accesses the variable.",
      "Eraser infers the protection relationship between lock and variable during program execution.",
      "For each shared variable v, Eraser maintains the set C(v) of candidate locks for v. This set contains those locks that have protected v for the computation so far.",
      "That is, a lock l is in C(v) if, in the computation up to that point, every thread that has accessed v was holding l at the moment of the access.",
      "When a new variable v is initialized, its candidate set C(v) is considered to hold all possible locks.",
      "When the variable is accessed, Eraser updates C(v) with the intersection of C(v) and the set of locks held by the current thread.",
      "This process, called lockset refinement, ensures that any lock that consistently protects v is contained in C(v).",
      "If some lock lconsistently protects v, it will remain in C(v) as C(v) is refined.",
      "If C(v) becomes empty this indicates that there is no lock that consistently protects v.  If locks_held(t) is the set of locks held by thread t,  then the algorithm can be succintly expressed as follows;  For each v, initialize C(v) to the set of all locks.",
      "On each access to v by thread t, set C(v) :=  the intersection of C(v) and locks_held(t); if C(v) = { }, then issue a warning.",
      "Which is pretty neat for something so simple!",
      "There are some refinements necessary to handle the case of initialization (which may often occur without locking),  variables that are read-only after initialization (final variables) and can be safely read without locks, and read-write locks that allow multiple simultaneous readers but  write exclusivity.",
      "These are all fairly easily dealt with:  For initialization, reporting of warnings is deferred until after initialization.",
      "The end of initialization is taken as the point at which a second thread first accesses the variable.",
      "To support final variables, races are only reported after a variable has become write-shared by more than one thread.",
      "To deal with multiple reader, single writer variables, locks held purely in read mode are removed from the candidate set when a write occurs, as these locks do not protect against a race between the writer and some other reader thread:  Many programs use single-writer, multiple-reader locks as well as simple locks.",
      "To accommodate this style we introduce our last refinement of the locking discipline: we require that for each variable v, some lock m protects v, meaning m is held in write mode for every write of v, and m is held in some mode (read or write) for every read of v.  One issue with Eraser is that it can create false alarms.",
      "These fall into three broad catagories:  Memory reuse:  if a program allocates its own memory blocks  and later on privately recycles them, Eraser has no way of knowing that the \u2018new\u2019 memory is now protected by a new set of locks.",
      "Private locks: using a locking mechanism other than pthreads, which the Eraser implementation described in the paper would then be unable to detect.",
      "Benign race: genuine races, but which do not affect the result of the program (some of these may be intentional)  Eraser introduced an annotation model to suppress unwanted warnings.",
      "Not implemented in Eraser, but a useful extension, is the ability to detect deadlocks.",
      "The paper gives us a sketch of how this can work:  A simple discipline that avoids deadlock is to choose a partial order among all locks and to program each thread so that whenever it holds more than one lock, it acquires them in ascending order.",
      "This discipline is similar to the locking discipline for avoiding data races: it is suitable for checking by dynamic monitoring, and it is easier to produce a test case that exposes a breach of the discipline than it is to produce a test case that actually causes a deadlock.",
      "A results of checking several programs with Eraser are reported \u2013 and as expected, it does a good job of flushing out bugs!",
      "A note on Valgrind  Today the most commonly used tool is arguably Valgrind, which implements data-race detection with DRD .",
      "This is what the Valgrind manual has to say on the topic:  There exist two different approaches for verifying the correctness of multithreaded programs at runtime.",
      "The approach of the so-called Eraser algorithm is to verify whether all shared memory accesses follow a consistent locking strategy.",
      "And the happens-before data race detectors verify directly whether all interthread memory accesses are ordered by synchronization operations.",
      "While the last approach is more complex to implement, and while it is more sensitive to OS scheduling, it is a general approach that works for all classes of multithreaded programs.",
      "An important advantage of happens-before data race detectors is that these do not report any false positives."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://www.cs.duke.edu/courses/cps210/spring06/papers/eraser.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 48141212
  },
  {
    "blog_id": "lemna-explaining-deep-learning-based-security-applications",
    "summary": [
      "LEMNA: explaining deep learning based security applications Guo et al., CCS\u201918  Understanding why a deep learning model produces the outputs it does is an important part of gaining trust in the model, and in some situations being able to explain decisions is a strong requirement.",
      "Today\u2019s paper shows that by carefully considering the architectural features of a given model, it\u2019s possible to co-design an explanatory model.",
      "The idea is applied to deep learning models in the security domain (to detect the start of functions within binaries, and to detect malware) where for reasons we\u2019ll look at next, the assumptions made by black-box explainers such as LIME don\u2019t apply.",
      "Like LIME, LEMNA approximates a local area of a complex deep learning decision boundary using a simple interpretable model.",
      "Unlike LIME, LEMNA can handle non-linear local boundaries, and feature dependencies (e.g., for a sequences fed into RNNs, which explicitly model dependencies in sequential data).",
      "Why explainability matters  While intrigued by the high accuracy, security practitioners are concerned about the lack of transparency of deep learning models, and thus hesitate to widely adopt deep learning classifiers in security and safety-critical areas.",
      "Explanations that are understandable by security analysts can help to build trust in trained models, and also to troubleshoot classification errors.",
      "\u201cWe argue that classifier reliability and trust do not necessarily come from a high classification accuracy on the training data\u2026 (instead) trust is more likely to be established by understanding model behavior .\u201d  Some of the ways that explanations can help to build trust include:  Demonstrating that the model has captured well-know heuristics (i.e., explanations match the knowledge of human experts)  Demonstrating that the model is able to capture new patterns of knowledge, which makes sense to human experts when explained.",
      "Explaining the reasons behind a false negative or false positive (i.e., highlighting the features that mislead the model), such that a human expert can understand why the mistake was made (and see that the mistake was in some way \u2018reasonable\u2019).",
      "Challenges explaining security DNNs  Unfortunately, existing explanation methods are not directly applicable to security applications\u2026 Security applications such as binary reverse engineering and malware analysis either have a high-level feature dependency (e.g., binary code sequences), or require high scalability.",
      "As a result, Recurrent Neural Networks (RNNs) or Multilayer Perceptron (MLP) models are more widely used.",
      "A blackbox explanation system such as LIME produces a Locally Interpretable Model Explanation using a linear model to approximate the detection boundary near the input to be explained.",
      "If the decision boundary is non-linear even in the local area, then this can introduce errors in the explanation process.",
      "Sampling (looking at points around the input) can easily land in areas beyond the linear region.",
      "Jumping ahead, a richer decision boundary as obtained by e.g., a mixture regression model will give higher fidelity.",
      "Furthermore, LIME assumes input features are independent, which does not hold when the input is a sequence (e.g. of byte codes from a binary) fed into an RNN model.",
      "Here the dependency (i.e., sequencing of instructions) matters very much.",
      "Handling feature dependencies  The fused lasso penalty term can be added to the loss function of models trained on features with a linear dependency (i.e. ordering dependency) among the input features.",
      "It works by restricting the coefficient weights of adjacent features to be within some small threshold.",
      "Consider producing an explanation for sentiment classification in next.",
      "With fused lasso, words next to each other in a sentence are likely to be grouped together, such that instead of an explanation simply pointing to the word \u2018not,\u2019 it can highlight instead a phrase (\u2018not worth the price\u2019).",
      "Handling non-linear boundaries  If one line isn\u2019t enough, then use several!",
      "A mixture regression model is a combination of multiple K linear regression models, where  is the weight assigned to each component model:  Given sufficient data samples, whether the classifier has a linear or non-linear decision boundary, the mixture regression model can nearly perfectly approximate the decision boundary (using a finite set of linear models).",
      "The LEMNA explanation system  LEMNA (Local Explanation Method using Nonlinear Approximation) combines fused lasso into the learning process of a mixture regression model, so that feature dependencies and decision boundary non-linearities can be handled at the same time.",
      "The mixture regression model is expressed in the form of probability distributions and trained using Expectation Maximisation (EM).",
      "To explain the classification of an input x the first step is to synthesize a set of data samples around x using the same approach as described in the LIME paper .",
      "This corpus of samples is then used to approximate the local decision boundary (for multi-class classification, a set of multiple mixture regression models are trained, each of which performs binary classification for one class).",
      "From this mixture model, we then identify the linear component that has the best approximation of the local decision boundary.",
      "The weights (or coefficients) in the linear model can be used to rank features.",
      "A small set of top features is selected as the explanation result.",
      "Targeted model patching  One neat application of LEMNA in the paper is classifier \u2018patching.\u2019 Given a misclassified instance, LEMNA\u2019s explanation can be used to pinpoint the small set of features  responsible for the miss.",
      "Often, such instances are outliers in the training data, and do not have enough \u201ccounter examples\u201d.",
      "To this end, our strategy is to augment the training data by adding related \u201ccounter examples,\u201d by replacing the feature values of  with random values.",
      "So long as we only introduce a small (e.g. 2-10) number of such samples for each case we can \u2018patch\u2019 the targeted errors without hurting the already high overall accuracy.",
      "The following table shows the result of patching the top 5 misleading features in a model and retraining for 40 epochs.",
      "These results demonstrate that by understanding the model behavior, we can identify the weaknesses of the model and enhance the model accordingly.",
      "Evaluation results  LEMNA is evaluated on an RNN-based model used to find the start of functions when reverse-engineering binaries, and on a malware classifier.",
      "Here\u2019s an example (on the RHS) of LEMNA highlighting the \u2018hot bytes\u2019 in the instruction sequence that led to the classification of x83 as the start of a function.",
      "LEMNA\u2019s local approximation accuracy for these classifiers has a root mean square error (RMSE) an order of magnitude smaller than LIME\u2019s.",
      "The saliency of the highlighted features in the explanation can be tested in three different ways.",
      "Given a set of highlighted features as an explanation for a classification as class C then:  removing those features from the input should lead to a different classification (feature deduction)  adding those features to some other sample not in class C should increase the chances of it being misclassified as a C (feature augmentation)  crafting synthetic inputs including those features should increase the likelihood of those inputs being classified as C  For each instance in the testing set for which an explanation is given, three samples are generated, one for each case above.",
      "The positive classification rate (PCR) then measures the ratio of samples still classified as the input\u2019s original label.",
      "In the feature deduction test, removing the top 5 features highlighted by LEMNA drops PCR to 25% or lower, indicating the small set of highlighted features are highly important to the classification.",
      "In the feature augmentation test, replacing the top 5 features highlighted by LEMNA caused 75% of test case for the PDF malware classifier to flip their labels.",
      "And using the synthetic inputs with the top 5 inputs, the synthetic instances have a 85%-90% chance of taking the original input\u2019s label.",
      "Focusing on the reverse-engineering function starts in binaries application, the explanation produced by LEMNA shows that the model does indeed capture well-known heuristics (C.W.H.",
      "), discovers new knowledge that makes sense to human experts (D.N.K.",
      "), and can provide insights into misclassification reasons for false negatives (R.F.N.)",
      "and false positives (R.F.P.",
      ")."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://people.cs.vt.edu/gangwang/ccs18.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 5550259
  },
  {
    "blog_id": "robust-learning-from-untrusted-sources",
    "summary": [
      "Robust learning from untrusted sources Konstantinov & Lampert, ICML\u201919  Welcome back to a new term of The Morning Paper!",
      "Just before the break we were looking at selected papers from ICML\u201919, including \u201cData Shapley.\u201d I\u2019m going to pick things up pretty much where we left off with a few more ICML papers\u2026  Data Shapley provides us with one way of finding and correcting or eliminating low-value (and potentially harmful) data points from a training set.",
      "In today\u2019s paper choice, Konstantinov & Lampert provide a way of assessing the value of datasets as a whole.",
      "The idea is that you might be learning e.g. a classifier by combining data from multiple sources.",
      "By assigning a value (weighting) to each of those data sources we can intelligently combine them to get the best possible performance out of the resulting trained model.",
      "So if you need more data in order to improve the performance of your model, \u2018Robust learning from untrusted sources\u2019 provides an approach that lets you tap into additional, potentially noisier, sources.",
      "It\u2019s similar in spirit to Snorkel which we looked at last year, and is designed to let you incorporate data from multiple \u2018weakly supervised\u2019 (i.e. noisy) data sources.",
      "Snorkel replaces labels with probability-weighted labels, and then trains the final classifier using those.",
      "The big idea  You have at least one data source that you trust to have reasonable quality, which we\u2019ll call the reference dataset.",
      "In addition you have N further data sources available, of varying quality.",
      "Two extremes of learning from all this data are to (a) use only the known high quality reference data source, discarding the others, and (b) train using all the available data regardless of quality.",
      "The goal of this paper is to find a middle-ground that lets us makes use of all of the data available,  without dragging the resulting model performance down due to noisy data.",
      "\u2026 we propose a method that automatically assigns weights to the sources\u2026 the weights are assigned to the sources according to the quality and reliability of the data they provide, quantified by an appropriate measure of trust we introduce.",
      "This is achieved by comparing the data from each source to a small reference dataset, obtained or trusted by the learner.",
      "In the evaluation, the resulting models consistently outperform models trained on either the reference dataset only, or all of the datasets, for any amount and type of data contamination considered in the study.",
      "How good is that data source?",
      "Intuitively, a good learning algorithm will assign more weight to sources whose distribution is similar to the target one, and less weight to those that provide different or low-quality data.",
      "The phrase \u2018similar to the target one\u2019 implies we\u2019re going to need a distance measure over distributions so that we can compare them.",
      "For this use case, the ideal distance measure would be one that gives us a strong correlation with the performance of a learned classifier.",
      "Now, there\u2019s nothing that correlates better with the performance of a learned classifier than\u2026 the performance of a learned classifier!",
      "The discrepancy between the distribution of a given dataset  and the reference dataset  is defined to be the difference in the expected loss of  a predictor trained on the reference dataset and a predictor trained on the additional dataset.",
      "Intuitively, the discrepancy between the two distributions is large, if there exists a predictor that performs well on one of them and badly on the other.",
      "On the other hand, if all functions in the hypothesis class perform similarly on both, then  and  have low discrepancy.",
      "Combining data sources  At this point we have a collection of datasets, and an indication of how \u2018good\u2019 each of those datasets is for the task in hand.",
      "Since we have a collection of trained classifiers, one per dataset, my intuition would be to use them as an ensemble and use weighted voting to combine their outputs.",
      "The natural first thing I would try is to have the weighting inversely proportional to the discrepancy score.",
      "That\u2019s sort of what happens here, but the authors  have a fancier way of determining the weights.",
      "The weights are chosen by minimising the following objective function:  Let\u2019s pick that apart a little bit\u2026  We have  different datasets to combine  is the weight assigned to the ith dataset  is the discrepancy score between dataset  and the reference dataset  .",
      "The first term therefore is learning weights based directly on the discrepancy score.",
      "The constraint that the learned weights must sum to 1 prevents the algorithm simply assigning a very low or zero weight to each source.",
      "The second term is a kind of regularisation parameter.",
      "is the number of data points in the ith dataset, so it\u2019s designed to encourage more confidence to be placed in the result of classifiers trained over larger datasets.",
      "Note that the first term is small whenever large weights are paired with small discrepancies and hence encourages trusting sources that provide data similar to the reference target sample.",
      "The second term is small whenever the weights are distributed proportionally to the number of samples per source.",
      "Thus, it acts as a form of regularization, by encouraging the usage of information from as many sources as possible.",
      "is a hyperparameter controlling the balance between the two.",
      "Incorporating private data  If one or more of the data sources is private or has constraints (e.g., it can\u2019t be moved to the location where the training is taking place) then it can still be incorporated into the overall process.",
      "If the reference (target) dataset can be moved to the location of the private data, then the discrepancy score can be calculated locally there.",
      "If the reference dataset also cannot be shared, then it turns out it is still possible to compute empirical discrepancies without observing the data from the source directly.",
      "This works because the equation for determining discrepancy breaks down into two independent terms, one concerning the reference dataset, and one concerning the comparison dataset:  Therefore, each discrepancy can be estimated by using a sequence of queries to the source about the gradient of a minibatch from its data with respect to a current candidate for the predictor\u2026  Evaluation  The first part of the evaluation is done using an Amazon products dataset with customer reviews for 957 Amazon products.",
      "The task is to  learn a classifier per product which can classify reviews for that product as either positive or negative.",
      "The experiment focuses in on reviews for books, and provides to the learning algorithm a dataset based on reviews for the book in question, together with data from reviews of other books, and data from reviews of other non-book products.",
      "Each run ends up with 10 data sources to combine: 600 reviews from the target book, and 100 labelled reviews from 9 other products, varying  , the number of these other products that are not books.",
      "Intuitively, when learning to classify book reviews and given access to reviews from both some books and some non-books, a good learning algorithm will be able to leverage all this information, while being robust to the potentially misleading data coming from the less relevant products.",
      "The resulting classifiers give much better results than either of the two extremes of (a) using only the reference dataset, and (b) just lumping all the data together in one unweighted pool.",
      "A variation on this looked across all 957 products, giving each classifier 100 reviews from its own dataset, and an additional set of 100 labelled reviews from every other product.",
      "The second part of the evaluation uses the \u2018Animals with Attributes 2\u2019 dataset with 37,322 images of 50 different animal classes.",
      "A sample is taken of clean data to form the reference dataset, and then corrupted additional datasets are created using a variety of manipulations:  Introducing label bias by setting labels of all corrupted samples to class 1  Shuffling labels  Shuffling features  Blurring images  Introducing dead pixels (for 30% of the image)  Swapping RGB channels  For a given combination of target attribute, corruption strategy, and blend of true and corrupted datasets, the learning experiment is repeated 100 times.",
      "The following table summarises the results over 85 different prediction tasks:  The results in table 2 show that our method performs significantly better than all baselines for many types of corruption and many values of n, especially for high levels of contamination, while essentially never performing significantly worse than any baseline."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://proceedings.mlr.press/v97/konstantinov19a/konstantinov19a.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 83267919
  },
  {
    "blog_id": "ipa",
    "summary": [
      "IPA: invariant-preserving applications for weakly consistent replicated databases Balegas et al., VLDB\u201919  IPA for developers, happy days!",
      "Last we week looked at automating checks for invariant confluence , and extending the set of cases where we can show that an object is indeed invariant confluent.",
      "I\u2019m not going to re-cover that background in this write-up, so I suggest you head over there for a quick catch-up before reading on if you missed it first time around.",
      "Today\u2019s paper is very much in same spirit, building on the same foundation of invariant confluence (I-Confluence) , and also on Indigo which introduced an annotation model for application invariants, a  invariant violation avoidance mechanism using lock reservations and escrows, and limited support for repairing violations that do happen.",
      "With Invariant-Preserving Applications (IPAs), Balegas et al. introduce new mechanisms for avoiding invariant violations and for repairing them when detected, based on CRDTs.",
      "There\u2019s also a very nice looking developer workflow to help ensure you\u2019ve got all the bases covered.",
      "At the end of the day, you get the dual benefit of higher throughput and lower latency (as compared to coordination-based approaches) coupled with knowing that there isn\u2019t some nasty invariant-violating concurrency bug waiting to bite you (so long as you specified your invariants and operation effects correctly of course!).",
      "Having your cake and eating it too  \u2026 it remains difficult to develop applications under weak consistency.",
      "Several studies show that, in many applications, concurrent executions lead to the violation of application invariants, resulting in inconsistent states.",
      "At this point you have a few choices:  The common path: turn a blind eye to the possibility and get bitten later on by strange inconsistencies in your data  The belt-and-braces approach: broadly constrain concurrency (i.e. coordinate heavily) in order to avoid the possibility of invariant violations \u2013 thus reducing availability and latency  The considered (and time-consuming in development) approach: take time to understand your invariants, figure out which parts of your system can be coordination free, and which parts still need it, and design accordingly.",
      "For bonus points, revisit your application design so that more parts of it can be coordination free.",
      "IPA takes the third way of course, and gives you new options for tweaking your application design, so that you can have your cake and eat it too:  This paper proposes a novel approach for preserving application invariants under weak consistency that does not impact the availability and latency of applications\u2026 To help programmers adopt our approach, we propose a methodology for modifying applications.",
      "The key element of the methodology is our invariant-preservation analysis (IPA) and static analysis tool that relies on information about the application, including invariants and operations, to identify which operations might lead to invariant violations and to suggest modifications to the operations to prevent those violations from occurring.",
      "Coordination avoidance  To avoid coordination overheads we need to ensure that all operations are invariant preserving under merge.",
      "One of IPA\u2019s neat tricks is allowing invariant violation within a transaction, by also including the logic to repair such a violation if it does occur.",
      "Our insight is that in many situations the effects to restore the database integrity can be applied preventively alongside the original operations, repairing the invariant violation automatically in a conflicting execution.",
      "An example makes all this much clearer.",
      "Consider an e-games platform with players that can take part in tournaments.",
      "Players can enroll and unenroll for tournaments, and admins can create and delete tournaments.",
      "But players shouldn\u2019t be able to enroll in tournaments that don\u2019t exist, tournaments may have a maximum capacity for players, and it shouldn\u2019t be possible to delete a tournament that has actively enrolled players.",
      "What happens if we have concurrent operations where a player enrols for a tournament while an admin is deleting it?",
      "As-is, the resulting state will violate our invariant.",
      "But we can ensure any violation will be automatically repaired by adding a new operation and updating merge semantics:  \u2026restoring a tournament to its previous state can be achieved by executing a touch operation in the tournament when executing the enroll, and adopting a conflict resolution policy where the touch wins over a concurrent delete.",
      "The touch operation has no observable effect, only updating the metadata to guarantee that the concurrent execution is detected and solved according to the defined conflict resolution policy.",
      "That\u2019s a pretty neat idea!",
      "Although there are still some open application design questions here for me: for example, how does the admin, who thought they just deleted a tournament, find out that in fact it still exists?",
      "Shouldn\u2019t they be notified so that they can take some action?",
      "I.e., there are application reasons why we might want to make compensating actions when a repair occurs.",
      "So I\u2019d probably want an event I could subscribe to to tell me when such a conflict-repairing merge occurs.",
      "Compensations / apologies  What about cases where a silent repair isn\u2019t possible or appropriate?",
      "For that we have compensations.",
      "With compensations, the idea is to check that the precondition holds when executing the operation in the initial replica, and to check that the invariants hold when operations are integrated remotely or when the state is read.",
      "Implementations of compensation mechanisms typically require re-executing operations multiple times, or using a leader to order operations, to ensure that replicas converge after applying a compensation.",
      "We implement compensations without any of these limitations by relying on CRDT convergence rules.",
      "Applying repairs and compensations  IPA relies on your application being built on top of CRDTs, with operations executed in causal order.",
      "Additional updates in an operation have to executed atomically  with the rest of the operation to ensure no inconsistencies can be observed.",
      "\u201cIn our prototype, we achieve this by relying on highly available transactions\u2026\u201d  Integrated developer workflow  Onto one of my favourite parts of the paper \u2013 the developer workflow!",
      "It all starts with the developer annotating their object model to specify  the application invariants and operation post-conditions, like this:  Then the IPA tool performs a developer-in-the-loop iterative analysis.",
      "In each iteration the tool identifies a pair of conflicting operations that might break an invariant when executed concurrently, and then it proposes a set of modifications to the application that will prevent this from happening.",
      "The developer chooses his or her preferred resolution, and the process repeats until no more conflicting operation pairs remain.",
      "The analysis returns a new specification of the application, which contains the selected modifications, comprising both the use fo appropriate conflict resolution policies for each object and the modification to operations to avoid invariant violations\u2026 Fully patched applications can then execute in any replicated system that provides causal consistency, highly available transactions, and the necessary type-specific conflict resolution policies.",
      "Here\u2019s the main algorithm used in the tool loop:  Section 5 in the paper goes into a lot more detail on this, and I wish I had the space here to do it justice.",
      "It\u2019s definitely worth checking out the full paper here if this work interests you.",
      "Behind the scenes: new CRDTs  To support the CRDT resolutions proposed by IPA, the authors developed new extensions to existing CRDTs:  An extension to Add-wins sets to support the touch operation  An extension to the Rem-wins set to support wildcard values in the remove operation (to support the application model equivalent of  cascading delete)  New compensation CRDTs, for example, Limited Size Set CRDTs that allow a programmer to specify a constraint that must be maintained, and the compensation to execute when the constraint is violated.",
      "Evaluation  The following table highlights the types of invariants covered by IPA, and their usage in four applications used in the evaluation.",
      "IPA can make unique identfier, aggregation inclusion, referential integrity and disjunction constraints I-Confluent through repair updates, and can additionally handle numerical and aggregation constraint invariants through compensations.",
      "Our IPA analysis and tool assist the programmer via static analysis to identify which operations might lead to an invariant violation, when executed concurrently, and by suggesting modifications to the operations.",
      "Our experimental evaluation shows that the static analysis can handle large applications in reasonable time for an offline process, and that the modified applications have similar performance to their unmodified counterparts that do not preserve invariants."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://www.vldb.org/pvldb/vol12/p404-balegas.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 87041789
  },
  {
    "blog_id": "sw-far-memory",
    "summary": [
      "Software-defined far memory in warehouse-scale computers Lagar-Cavilla et al., ASPLOS\u201919  Memory (DRAM) remains comparatively expensive, while in-memory computing demands are growing rapidly.",
      "This makes memory a critical factor in the total cost of ownership (TCO) of large compute clusters, or as Google like to call them \u201cWarehouse-scale computers (WSCs).\u201d  This paper describes a \u201cfar memory\u201d system that has been in production deployment at Google since 2016.",
      "Far memory sits in-between DRAM and flash and colder in-memory data can be migrated to it:  Our software-defined far memory is significantly cheaper (67% or higher memory cost reduction) at relatively good access speeds (6\u00b5s) and allows us to store a significant fraction of infrequently accessed data (on average, 20%), translating to significant TCO savings at warehouse scale.",
      "With a far memory tier in place operators can choose between packing more jobs onto each machine, or reducing the DRAM capacity, both of which lead to TCO reductions.",
      "Google were able to bring about a 4-5% reduction in memory TCO (worth millions of dollars!)",
      "while having negligible impact on applications.",
      "In introducing far memory Google faced a number of challenges: workloads are very diverse and change all the time, both in job mixes and in utilisation (including diurnal patterns), and there is near zero tolerance for application slowdown.",
      "If extra provisioned capacity was needed to offset a slowdown for example then this could easily offset all potential TCO savings.",
      "This boils down to a single digit \u00b5s latency toleration in the tail for far memory, and in addition to security and privacy concerns, rules out remote memory solutions.",
      "Google\u2019s \u201cfar\u201d memory it turns out is exactly the same memory, but with compressed data stored in it!",
      "The opportunity  One of the very earliest question to be addressed is how to define \u2018cold\u2019 memory, and given that, how much opportunity there is for moving cold memory into a far memory store.",
      "We focus on a definition that draws from the following two principles: (1) the value of temporal locality, by classifying as cold a memory page that has not been accessed beyond a threshold of T seconds; (2) a proxy for the application effect of far memory, by measuring the rate of accesses to cold memory pages, called promotion rate.",
      "With T set at 120 seconds, 32% of the memory usage in a Google WSC is cold on average.",
      "At this threshold applications access 15% of their total cold memory on average every minute.",
      "Across individual machines in a cluster, the percentage of cold memory varies for 1% to 52%.",
      "It also varies across jobs:  \u2026storing cold memory to cheaper but slower far memory has great potential of saving TCO in WSCs.",
      "But for this to be realized in a practical manner, the system has to (1) be able to accurately control its aggressiveness to minimize the impact on application performance, and (2) be resilient to the variation of cold memory behavior across different machines,clusters, and jobs.",
      "Enter zswap!",
      "Google use zswap to implement their far memory tier.",
      "Zswap is readily available and runs as a swap device in the Linux kernel.",
      "Memory pages moved to zswap are compressed (but the compressed pages stay in memory).",
      "Thus we\u2019re fundamentally trading (de)-compression latency at access time for the ability to pack more data in memory.",
      "Using zswap means that no new hardware solutions are required, enabling rapid deployment across clusters.",
      "\u2026quick deployment of a readily available technology and harvesting its benefits for a longer period of time is more economical than waiting for a few years to deploy newer platforms promising potentially bigger TCO savings.",
      "zswap\u2019s default control plane did not meet Googles strict performance slowdown and CPU overhead budgets though, so they built a new one to identify cold pages and proactively migrate them to far memory while treating performance as a first-class constraint.",
      "Cold memory pages are identified in the background and proactively compressed.",
      "Once accessed, a decompressed page stays in that state until it becomes cold again.",
      "The key to an efficient system is the identification of cold pages: the cold age threshold determines how many seconds we can go without a page being accessed before it is declared cold.",
      "The objective is to find the lowest cold age threshold that still allows the system to satisfy its performance constraints.",
      "A good proxy metric for the overhead introduce by the system is the promotion rate: the rate of swapping pages from far memory to near memory.",
      "For a given promotion rate, large jobs with more total memory are likely to see less of a slowdown than smaller jobs\u2026  \u2026 therefore we design our system to keep the promotion rate below P% of the application\u2019s working set size per minute, which serves as a Service Level Objective for far memory performance.",
      "From extensive A/B testing, P was empirically determined to be 0.2%/minute.",
      "At this level the compression/decompression overhead does not interfere with other colocated jobs on the same machine.",
      "What cold age threshold results in at 0.2%/min promotion rate though?",
      "Google maintain a promotion histogram for each job in the kernel, which records the total promotion rate of pages colder than the threshold T. This gives an indication of past performance, but we also want to be responsive to spikes.",
      "So the overall threshold is managed as follows:  The best cold age threshold is tracked for each 1 minute period, and the K-th percentile is used as the threshold for the next one (so we\u2019ll violate approximately 100-K% of the times under steady state conditions)  If jobs access more cold memory during the minute than the chosen K-th percentile then the best cold age threshold from the previous minute is used instead  Zswap is disabled for the first S seconds of job execution to avoid making decisions based on insufficient information.",
      "The system also collects per-job cold-page histograms for a given set of predefined cold age thresholds.",
      "These are used to perform offline analysis for potential memory savings under different cold-age thresholds.",
      "ML-based auto-tuning  To find optimal values for K and S, Google built a model for offline what-if explorations based on collected far-memory traces, that can model one week of an entire WSCs far memory behaviour in less than an hour.",
      "This model is used by a Gaussian Process (GP) Bandit machine learning model to guide the parameter search towards an optimal point with a minimal number of trials.",
      "The best parameter configuration found by this process is periodically deployed to the WSC with a carefully monitored phased rollout.",
      "The big advantage of the ML based approach is that it can continuously adapt to changes in the workload and WSC configuration without needing constant manual tuning.",
      "To the best of our knowledge, this is the first use of a GP Bandit for optimizing a WSC.",
      "Evaluation  The far memory system has been deployed in production since 2016.",
      "The following chart shows the change in cold memory coverage over that time, including the introduction of the autotuner which gave an additional 20% boost.",
      "Cold memory coverage varies over machine and time, but at the cluster level it remains stable.",
      "This enabled Google to convert zswap\u2019s cold memory coverage into lower memory provisioning, achieving a 4-5% reduction in DRAM CTO.",
      "\u201cThese savings are realized with no difference in performance SLIs.\u201d  There are very low promotion rates in practice, both before and after deployment of the autotuner.",
      "CPU overhead for compression and decompression is very low as well (0.001% and 0.005% respectively).",
      "One of the biggest consumers of DRAM is Bigtable, storing petabytes of data in memory and serving millions of operations per second.",
      "The following chart shows an A/B test result for Bigtable with and without zswap enabled.",
      "During this period site engineers monitored application-level performance metrics and observed no SLO violations.",
      "For Bigtable, zswap achieves 5-15% cold memory coverage.",
      "Our system has been in deployment in Google\u2019s WSC for several years and our results show that this far memory tier is very effective in saving memory CapEx costs without negatively impacting application performance\u2026 Ultimately an exciting end state would be one where the system uses both hardware and software approaches and multiple tiers of far memory (sub-\u00b5s tier-1 and single \u00b5s tier-2), all managed intelligently with machine learning and working harmoniously to address the DRAM scaling challenge."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3297858.3304053?download=true",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 88655260
  },
  {
    "blog_id": "polaris-faster-page-loads-using-fine-grained-dependency-tracking",
    "summary": [
      "Polaris: Faster Page Loads Using Fine-Grained Dependency Tracking \u2013 Netravali et al. 2016  Yesterday we looked at Shandian which promised faster web page load times, but required a modified client-side browser.",
      "Today we\u2019re sticking with the theme of reducing page load times with Polaris.",
      "Unlike Shandian, Polaris works with unmodified browsers, and in tests with content from 200 sites out of the top Alexa 500 it is able to reduce load times by 34% at the median, and 59% at the 95th percentile.",
      "To load a page, a browser must resolve the page\u2019s dependency graph.",
      "The dependency graph captures \u201cload-before\u201d relationships between a page\u2019s HTML, CSS, JavaScript, and image objects.",
      "Consider a browser parsing an HTML file that encounters a script tag.",
      "It has to halt the parsing and rendering of the page to download the linked .js file and evaluate it as this may alter the downstream HTML, or define JavaScript state required by later script files.",
      "Synchronously loading JavaScript files guarantees correctness, but this approach is often too cautious.",
      "For example, if first.js and second.js do not modify mutually observable state, the browser should be free to download and evaluate the files in whatever order maximizes the utilization of the network and the CPU.",
      "However, pages do not expose such fine-grained dependency information to browsers\u2026  (Yes, \u201c tags can be marked with async or defer attributes, but by default they have neither.",
      "In the test corpus of 200 popular sites, this accounts for 98.3% of all scripts\u2026).",
      "The Scout tool is used to load a page offline and produce a fine-grained dependency graph that is much more detailed than those produced by prior frameworks.",
      "\u201cFor 81% of the 200 real-world pages that we examined, our new graphs have different critical paths than those of graphs from prior work.\u201d  Polaris is a dynamic client-side scheduler that uses the dependency graphs created by Scout to reduce page load times:  When a user makes a request for a Polaris-enabled page, the server returns a scheduler stub instead of the page\u2019s original HTML.",
      "The scheduler stub includes the Polaris JavaScript library, the page\u2019s fine-grained dependency graph (as generated by Scout), and the original HTML.",
      "The Polaris library uses the Scout graph, as well as dynamic observations about network conditions, to load objects in an order that reduces page load time.",
      "Scout  Given simply lexical dependency information, then:  A script tag might read CSS style properties from the DOM tree, so CSS evaluation must block JavaScript execution  A script tag might change downstream HTML, so when a browser encounters such a tag it must block or transfer HTML parsing to a speculative thread  Two script tags that are lexically adjacent might exhibit a read/write dependency on JavaScript state.",
      "Thus browsers must execute the script tags serially\u2026  Scout finds out the true dependencies, not just the potential ones.",
      "It captures three types of data flows involving the JavaScript heap and the DOM state belonging to HTML and CSS:  Write/read dependencies: one object produces state that another object consumes.",
      "E.g. a global variable created by a.js and later read by b.js.",
      "Read/write dependencies: one object must read a piece of state before the value is updated by another object.",
      "For example, JavaScript code reading a DOM value before the value is changed by the HTML parser.",
      "\u201cAny reordering of object evaluations must ensure value equivalence for DOM queries \u2013 regardless of when a JavaScript file is executed, its DOM queries must return the same results.\u201d  Write/write dependencies: two objects update the same piece of state, and we must preserve the happens-before relationship.",
      "For example, CSS files update DOM state, changing the rules which govern a page\u2019s visual presentation.",
      "The CSS specification states that if two files update the same rule, the last writer wins.",
      "\u2026once we know the DOM dependencies and JavaScript heap dependencies for a script tag, the time at which the script can be evaluated is completely decoupled from the position of the script tag in the HTML \u2013 we merely have to ensure that we evaluate the script after its fine-grained dependencies are satisfied.",
      "Similarly, we can parse and render a piece of HTML at any time, as long as we ensure that we have blocked the evaluation of downstream objects in the dependency graph.",
      "The content of web pages is recorded using Mahimahi .",
      "Scout then rewrites each JavaScript and HTML file in the page adding instrumentation to log the fine-grained data flows across the JavaScript heap and the DOM.",
      "The page is then loaded in a regular browser and the log is used to generated the dependency graph.",
      "For a given page, a web server may generate a different dependency graph for different clients\u2026 The server-side logic must run Scout on each version of the dependency graph.",
      "We believe that this burden will be small in practice, since even customized versions of a page often share the same underlying graph structure (with different content in some of the nodes).",
      "An analysis of 200 sites from the Alexa top 500 showed that Scout finds 30% more edges at the median, and 118% more edges at the 95% percentile than existing dependency analysis tools (Klotski, WProf).",
      "Those additional edges have a dramatic impact on the characteristics of dependency graphs.",
      "For example, adding fine-grained dependencies alters the critical path length for 80.8% of the pages in our corpus.",
      "Polaris  Polaris is written completely in JavaScript and can be run on unmodified commodity browsers.",
      "It combines the dependency graph produced by Scout with observations about current network conditions to determine the dynamic critical path for a page.",
      "The dynamic critical path, i.e. the path which currently has the most unresolved objects, is influenced by the order and latency with which network fetches complete; importantly, the dynamic critical path may be different than the critical path in the static dependency graph.",
      "To load a page using Polaris, a web server is configured to respond to page requests with the Polaris scheduler stub HTML.",
      "This contains four components:  The scheduler itself, as inline JavaScript code  The Scout dependency graph, as a JavaScript variable inside the scheduler  DNS prefetch hints to indicate that the scheduler will be contacting certain hostnames in the near future.",
      "\u201cDNS hints allow Polaris to pre-warm the DNS cache in the same way that the browser does during speculative HTML parsing.\u201d  The page\u2019s original HTML, broken into chunks by Scout as determined by Scout\u2019s fine-grained dependency resolution.",
      "Across the 200 sites in test corpus, the schedule stub increased page size by 36.5KB (3%) at the median.",
      "Since modern browsers limit a page to at most six outstanding requests to a give origin, Polaris maintains per-origin priority queues.",
      "With the exception of the top-level HTML (which is included in the scheduler stub), each object in the dependency graph belongs to exactly one queue.",
      "Inside a queue, objects that are higher in the dependency tree receive a higher priority, since those objects prevent the evaluation of more downstream objects.",
      "At any given moment, the scheduler tries to fetch objects that reside in a dynamic critical path for the page load.",
      "However, if fetching the next object along a critical path would violate a per-origin network constraint, Polaris examines its queues, and fetches the highest priority object from an origin that has available request slots.",
      "How well does it work?",
      "\u2026 we demonstrate that Polaris can decrease page load times across a variety of web pages and network configurations: performance improves by 34% and 59% for the median and 95th percentile sites, respectively.",
      "Polaris\u2019 benefits grow as network latencies increase, because higher RTTs increase the penalty for bad fetch schedules.",
      "Thus, Polaris is particularly valuable for clients with cellular or low-quality wired networks.",
      "However, even for networks with moderate RTTs, Polaris can often reduce load times by over 20%.",
      "A closer look at three sites, Apple, ESPN, and Weather.com shows the impact the dependency graph has on the benefits that Polaris can bring:  Apple\u2019s home page has a flat dependency graph such that once the top-level HTML is loaded, all other objects can be fetched and evaluated in an arbitrary order.",
      "For low RTTs, this makes Polaris slower than the baseline.",
      "Weather.com has a much more complex dependency graph, which enables Polaris to beat the baseline handsomely.",
      "Polaris was also tested in conjunction with SPDY and found to be complementary: load times using Polaris over SPDY are 2.05%-4.03% faster than those with Polaris over HTTP/1.1."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://web.mit.edu/ravinet/www/polaris_nsdi16.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 8455503
  },
  {
    "blog_id": "the-why-and-how-of-nonnegative-matrix-factorization",
    "summary": [
      "The why and how of nonnegative matrix factorization Gillis, arXiv 2014 from: \u2018 Regularization, Optimization, Kernels, and Support Vector Machines .\u2019  Last week we looked at the paper \u2018 Beyond news content ,\u2019 which made heavy use of nonnegative matrix factorisation.",
      "Today we\u2019ll be looking at that technique in a little more detail.",
      "As the name suggests, \u2018The Why and How of Nonnegative matrix factorisation\u2019 describes both why NMF is interesting (the intuition for how it works), and how to compute an NMF.",
      "I\u2019m mostly interested in the intuition (and also out of my depth for some of the how!",
      "), but I\u2019ll give you a sketch of the implementation approaches.",
      "Nonnegative matrix factorization (NMF) has become a widely used tool for the analysis of high dimensional data as it automatically extracts sparse and meaningful features from a set of nonnegative data vectors.",
      "NMF was first introduced by Paatero andTapper in 1994, and popularised in a article by Lee and Seung in 1999.",
      "Since then, the number of publications referencing the technique has grown rapidly:  What is NMF?",
      "NMF approximates a matrix  with a low-rank matrix approximation such that  .",
      "For the discussion in this paper, we\u2019ll assume that  is set up so that there are  data points each with  dimensions, and every column of  is a data point, i.e. .",
      "We want to reduce the  original dimensions to  (aka, create a rank  approximation).",
      "So we\u2019ll have  and  .",
      "The interpretation of  is that each column is a basis element.",
      "By basis element we mean some component that crops up again and again in all of the  original data points.",
      "These are the fundamental building blocks from which we can reconstruct approximations to all of the original data points.",
      "The interpretation of  is that each column gives the \u2018coordinates of a data point\u2019 in the basis  .",
      "In other words, it tells you how to reconstruct an approximation to the original data point from a linear combination of the building blocks in  A popular way of measuring how good the approximation  actually is, is the Frobenius norm (denoted by the F subscript you may have noticed).",
      "The Frobenius norm is:  .",
      "An optimal approximation to the Frobenius norm can be computed through truncated Singular Value Decomposition (SVD).",
      "Why does it work?",
      "The intuition.",
      "The reason why NMF has become so popular is because of its ability to automatically extract sparse and easily interpretable factors.",
      "The authors give three examples of NMF at work: in image processing, text mining, and hyperspectral imaging.",
      "Image processing  Say we take a gray-level image of a face containing p pixels, and squash the data into a single vector such that the ith entry represents the value of the ith pixel.",
      "Let the rows of  represent the p pixels, and the n columns each represent one image.",
      "NMF will produce two matrices W and H. The columns of W can be interpreted as images (the basis images), and H tells us how to sum up the basis images in order to reconstruct an approximation to a given face.",
      "In the case of facial images, the basis images are features such as eyes, noses, moustaches, and lips, while the columns of H indicate which feature is present in which image.",
      "Text mining  In text mining consider the bag-of-words matrix representation where each row corresponds to a word, and each column to a document (for the attentive reader, that\u2019s the transpose of the bag-of-words matrix we looked at in \u2018 Beyond news content \u20199).",
      "NMF will produce two matrices W and H. The columns of W can be interpreted as basis documents (bags of words).",
      "What interpretation can we give to such a basis document in this case?",
      "They represent topics!",
      "Sets of words found simultaneously in different documents.",
      "H tells us how to sum contributions from different topics to reconstruct the word mix of a given original document.",
      "Therefore, given a set of documents, NMF identifies topics and simultaneously classifies the documents among these different topics.",
      "Hyperspectral unmixing  A hyperspectral image typically has 100 to 200 wavelength-indexed bands showing the fraction of incident light being reflected by the pixel at each of those wavelengths.",
      "Given such an image we want to identify the different materials present in it (e.g. grass, roads, metallic surfaces) \u2013 these are called the endmembers.",
      "Then we want to know which endmembers are present in each pixel, and in what proportion.",
      "For example, a pixel might be reflecting 0.3 x the spectral signal of grass, and 0.7 x the spectral signal of a road surface.",
      "NMF will produce two matrices W and H. The columns of W can be interpreted as basis endmembers.",
      "H tells us how to sum contributions from different endmembers to reconstruct the spectral signal observed at a pixel.",
      "\u2026given a hyperspectral image, NMF is able to compute the spectral signatures of the endmembers, and simultaneously the abundance of each endmember in each pixel.",
      "Implementing NMF  For a rank r factorisation, we have the following optimisation problem:  Though note that the Frobenius norm show here assumes Gaussian noise, and other norms may be used in practice depending on the distribution (e.g., Kullback-Leibler divergence for text-mining, the Itakura-Saito distance for music analysis, or the  norm to improve robustness against outliers).",
      "So far everything to do with NMF sounds pretty good, until you reach the key moment in section 3:  There are many issues when using NMF in practice.",
      "In particular, NMF is NP-hard.",
      "Unfortunately, as opposed to the unconstrained problem which can be solved efficiently using the SVD, NMF is NP-hard in general.",
      "Fortunately there are heuristic approximations which have been proven to work well in many applications.",
      "Another issue with NMF is that there is not guaranteed to be a single unique decomposition (in general, there might be many schemes for defining sets of basis elements).",
      "For example, in text mining you would end up with different topics and classifications.",
      "\u201cIn practice, this issue is tackled using other priors on the factors W and H and adding proper regularization terms in the objective function.\u201d  Finally, it\u2019s hard to know how to choose the factorisation rank, r. Some approaches include trial and error, estimation using SVD based of the decay of the singular values, and insights from experts (e.g., there are roughly so many endmembers you might expect to find in a hyperspectral image).",
      "Almost all NMF algorithms use a two-block coordinate descent scheme (exact or inexact), that is, they optimize alternatively over one of the two factors, W or H, while keeping the other fixed.",
      "The reason is that the subproblem in one factor is convex.",
      "More precisely, it is a nonnegative least squares problem (NNLS).",
      "Many algorithms exist to solve the NNLS problem; and NMF algorithms based on two-block coordinate descent differ by which NNLS algorithm is used.",
      "Some NNLS algorithms that can be plugged in include multiplicative updates, alternating least squares, alternating nonnegative least squares, and hierarchical alternating least squares.",
      "The following charts show the performance of these algorithms on a dense data set (left), and a sparse data set (right).",
      "You can initialise W and H randomly, but there are also alternate strategies designed to give better initial estimates in the hope of converging more rapidly to a good solution:  Use some clustering method, and make the cluster means of the top r clusters as the columns of W, and H as a scaling of the cluster indicator matrix (which elements belong to which cluster).",
      "Finding the best rank-r approximation of X using SVD and using this to initialise W and H (see section 3.1.8)  Picking r columns of X and just using those as the initial values for W.  Section 3.2 in the paper discusses an emerging class of polynomial time algorithms for NMF in the special case where the matrix X is r-separable.",
      "That is, there exist a subset of r columns such that all other columns of X can be reconstructed from them.",
      "In the text mining example for instance this would mean that each topic has at least one document focused solely on that topic.",
      "\u2026 we believe NMF has a bright future\u2026"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://arxiv.org/pdf/1401.5226",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 31021586
  },
  {
    "blog_id": "time-adaptive-sketches-ada-sketches-for-summarizing-data-streams",
    "summary": [
      "Time-adaptive sketches (Ada Sketches) for Summarizing Data Streams Shrivastava et al. SIGMOD 2016  More algorithm fun today, and again in the context of data streams.",
      "It\u2019s the 3 V\u2019s of big data, but not as you know it: Volume, Velocity, and Var\u2026 Volatility.",
      "Volatility here refers to changing patterns in the data over time, and that can make life awkward if you\u2019re trying to extract information from a stream.",
      "In particular, the authors study the heavy hitters problem, but with a twist: we want to give more weight to recent trends.",
      "In most applications that involve temporal data, most recent trends tend to be most informative for predictive purposes\u2026.",
      "For instance, most recent variations in credit history are much stronger indicators of a person\u2019s ability to make loan payments compared to variations in credit history from the distant past.",
      "Time-adaptive sketches generalize sketching algorithms and have the property that they retain counts of heavy hitters with good accuracy, while also providing provable time-adaptive guarantees.",
      "Coming in at 16 pages, the essence of the paper, especially if you\u2019re familiar with count-min sketches is this: instead of increasing counters by 1 every time you see an item, increase them by f(t), where f(t) is a monotone function in time.",
      "When you want to extract count estimates for time t, divide by f(t).",
      "The authors experiment with a linear function f(t) = at, for fixed a (0.5), and also an exponential function f(t) = at for fixed a (1.0015).",
      "Both gave good results.",
      "Finishing the write-up here though would be to short-change you.",
      "We\u2019re interested in why this works, and what guarantees it gives.",
      "Plus the paper also gives an excellent tour through some of the prior approaches to solving the heavy hitters problem.",
      "Let\u2019s start there, with a very quick recap on the basic Count-Min Sketch (CMS) algorithm.",
      "Count-Min Sketch  Create an integer array initialised to zeros that is w wide and d deep.",
      "Take d pairwise independent hash functions, h1,\u2026,hd and associate one with each row of the table, these functions should produce a value in the range 1..w. When a new value is seen, for each row of the table, hash the value with the corresponding hash function, and increment the counter in the indicated array slot.",
      "If you want to know the estimate of how many instances of a given value have been seen, hash the value as previously and look up the counter values that gives you in each row.",
      "Take the smallest of these as your estimate.",
      "Hokusai \u2013 nearly but not quite  Hokusai-sketching (Matusevych et al. 2012) introduced an item aggregation algorithm for constructing time-adaptive sketches.",
      "Hokusai uses a set of Count-Min sketches for different time intervals, to estimate the counts of any item for a given time or interval.",
      "To adapt the error rate temporally in limited space, the algorithm uses larger sketches for recent intervals and sketches of smaller size for older intervals.",
      "At the end of a time interval (e.g T), a sketch needs to be moved into the next-sized-down sketch, (the one for T\u20131).",
      "Hokusai has a very elegant way of doing this: at each rung on the ladder, sketch widths are halved.",
      "You can therefore compress a larger sketch into a smaller one by simply adding one half of the sketch to the other, and also halving the hash function ranges using modulo 2 operations.",
      "Although this idea of having different-size sketches for different time intervals is reasonable and yields accuracies that are time-adaptive, it comes with several inherent shortingcomings.",
      "Inspiration \u2013 Dolby noise reduction!",
      "This might date some of The Morning Paper readers \u2013 do you remember Dolby B noise reduction?",
      "And then the exciting introduction of Dolby C?",
      "Some of us grew up with music on cassette tapes, and Dolby Noise Reduction was ever present.",
      "When recording, Dolby systems employ pre-emphasis \u2013 artificially boosting certain parts of the input signal.",
      "On playback, the reverse de-emphasis translation restores the original signal levels.",
      "This process helps to improve the signal-to-noise ratio and combat tape hiss.",
      "We exploit the fact that Count-Min Sketch (CMS)\u2026 has better accuracy for heavy-hitters as compared to the rest of the items.",
      "While updating the sketch we apply pre-emphasis and artificially inflate the counts of more recent items compared to older ones, i.e., we make them heavier with respect to the older items.",
      "This is done by multiplying updates cit with f(t), which is any monotonically increasing function of time t. Thus, instead of updating the sketch with cit we update the sketch with _f(t) x cit.",
      "The tendency of the sketch is to preserve large values.",
      "This inflation thus preserves the accuracy of recent items, after artificial inflation, compared to the older ones.",
      "On querying of course, the de-emphasis process must be applied, which means dividing the results by f(t) to obtain the estimate of item i at time t. In the absence of collisions, as with the base CMS, counts are estimated exactly.",
      "Consider a CMS with only one row, and the case when two independent items i and j collide.",
      "We see cit instances of i, and cjt\u2019 instances of j.",
      "With plain CMS, we would over-estimate the count for i by cjt\u2019, whereas with the pre-emphasis process we overestimate by (f(t) x cjt)/f(t\u2019)).",
      "Therefore it is easy to see that more recent items suffer less compared to older items.",
      "Adaptive CMS  The Adaptive Count-Min Sketch algorithm (Ada-CMS), is just CMS but with the update and query mechanisms adapted to use the pre-emphasis and de-emphasis mechanism just described.",
      "Note that when f(t) = 1 we obtain the original CMS algorithm.",
      "By choosing appropriate f(t) functions, we can tailor the behaviour for different situations.",
      "One major question we are interested in is \"Given a fixed space and current state of time T, what are the values of time t \u2264 T where Ada-CMS is more accurate than vanilla CMS?",
      "For a given w and d, we can see as a start that the expected error of Ada-CMS will be less than CMS if:  For t=T this will always be true (due to the monotonicity requirement on f(t)).",
      "The upper bound on the error with vanilla CMS is &sqrt;T, so Ada-CMS wins when its error is less than this.",
      "To illustrate a reasonable scenario, suppose we want the errors with Ada-CMS to be never off by a factor \u03b3 away from that of vanilla CMS \u2200 t. This ensures that we guarantee accuracy within a factor \u03b3 of what the original CMS would achieve to even very old heavy hitters.",
      "In addition, we want to be more accurate than CMS on all recent time t > K, for some desirable choice of K.  With a couple of simple manoeuvres (see section 5.2), this turns into solving the following pair of simultaneous equations:  Other applications  The pre-emphasis and de-emphasis technique can be used in a number of other scenarios.",
      "The authors show an example with the Lossy Counting algorithm, and also how it can be applied to range queries (see \u00a76).",
      "Evaluation  Experimental evaluation is undertaken with two real-world streaming datasets from AOL (36M search queries with 3.8M unique terms) and Criteo (150K unique categorical terms).",
      "Comparison is undertaken between vanilla CMS, the Hokusai algorithm, Ada-CMS with a linear function (f(t) = 0.5t), and Ada-CMS with an exponential function (f(t)=1.0015t).",
      "In all cases d = 4, and w was varied from 210 to 223 to see the impact of varying range sizes.",
      "Here are the results for the AOL dataset:  Here\u2019s the standard deviation of those errors with w=218:  The Last Word  The proposed integration of sketches with pre-emphasis and de-emphasis, as we demonstrate, posseses strong theoretical guarantees on errors over time.",
      "Experiments on real datasets support our theoretical findings and show significantly superior accuracy and runtime overhead compared to the recently proposed Hokusai algorithm.",
      "We hope that our proposal will be adopted in practice, and it will lead to further exploration of the pre-emphasis and de-emphasis idea for solving massive data stream problems."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://research.microsoft.com/en-us/um/people/mbilenko/papers/16-ada-sketches.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 37479343
  },
  {
    "blog_id": "why-does-the-neocortex-have-columns-a-theory-of-learning-the-structure-of-the-world",
    "summary": [
      "Why does the neocortex have columns, a theory of learning the structure of the world Hawkins et al., bioRxiv preprint, 2017  Yesterday we looked at the ability of the HTM sequence memory model to learn sequences over time, with a model that resembles what happens in a single layer of the neocortex.",
      "But the neocortex has six layers.",
      "Today\u2019s paper builds on the previous work to show how pairs of layers can learn predictive models of static objects, when the sensory input changes due to our own movement.",
      "For example, when our fingers touch an object.",
      "Our research has focused on how the brain makes predictions of sensory inputs.",
      "Starting with the premise that all sensory regions make predictions of their constantly changing input, we deduced that each small area in a sensory region must have access to a location signal that represents where on an object the column is sensing.",
      "Building on this idea, we deduced the probable function of several cellular layers and are beginning to understand what cortical columns in their entirety might be doing.",
      "Anatomical evidence  A few general rules have been observed for cellular layers in the neocortex:  Cells in layers that receive direct feedforward input don\u2019t send their axons outside the local region, and don\u2019t form long distance horizontal connections within their own layer.",
      "Cells in layers driven by input layers do form long range connections within their layer, and also send an axonal branch outside of the region, representing an output.",
      "The two layer input-output circuit thus formed appears between layer 4 and layer 2/3 of the six layers in the neocortex.",
      "Layers 6 and 5 may be a second instance of the pattern.",
      "The prevalence of this two-layer connection motif suggests it plays an essential role in cortical processing.",
      "A key component of our theory is the presence in each column of a signal representing location.",
      "The location signal represents an \u201callocentric\u201d location, meaning it is a location relative to the object being sensed.",
      "The neurons effectively have to compute a predicted new location from the combination of current location, object orientation, and movement.",
      "That sounds a tall order, but we already know that grid cells in the entorhinal cortex perform these types of transformations, encoding the location of an animal\u2019s body relative to an external environment.",
      "These analogs, plus the fact that grid cells are phylogenetically older than the neocortex, lead us to hypothesize that the cellular mechanism used by grid cells were preserved and replicated in the sub-granular layers of each cortical column.",
      "Enough of the biology, let\u2019s now turn to the model it inspired.",
      "Multi-layer model  The current model consists of two layers of pyramidal neurons arranged in a column.",
      "The model has one or more of these columns.",
      "Each cortical column processes a subset of the sensory input space and is exposed to different parts of the world as the sensors move.",
      "The neurons used in the model are HTM model neurons as we looked at yesterday.",
      "Input layer  The input layer of each column consists of HTM neurons arranged in mini-columns.",
      "It receives a sensory input as a feedforward input, and a location input as a basal modulatory input.",
      "The sensory input is a sparse binary array representing the current feature in input space.",
      "During inference, cells that recognize both the modulatory location input and the feedforward driving input will inhibit other cells in the mini-column.",
      "In this way, the input layer forms a sparse representation that is unique for a specific sensory feature at a specific location on the object.",
      "Neurons in the input layer also receive feedback connections from the output layer.",
      "These carry information about the object detected, which combined with modularity input representing the anticipated new location, allow the input layer to more precisely predict the next sensory input.",
      "Output layer  The output layer is also made up of HTM neurons.",
      "The set of cells that are active in the output layer represent objects.",
      "Output layer cells receive feedforward input from the input layer, and modulatory input from other output cells representing the same object, both within the column and also from neighbouring columns.",
      "During learning, the set of cells representing an object remains active over multiple movements and learns to recognize successive patterns in the input layer.",
      "Thus, an object comprises a representation in the output layer, plus an associated set of feature/location representations in the input layer.",
      "Cells representing the same object positively bias each other.",
      "Say at time t, a column has feedforward support for objects A and B.",
      "And at time t+1 it has feedforward support for objects B and C. Due to the modulatory input from time t, the output layer will converge on the representation for B.",
      "Example: cubes and wedges  The following figure shows two layers of a single cortical column collaborating to disambiguate between a cube and a wedge shape that have shared features.",
      "The first sensed feature f1 is ambiguous so the output layer supports both object patterns, but with repeated sensations the output layer quickly narrows down on the correct choice (the cube in this case).",
      "( Enlarge )  Learning  Learning is based on Hebbian-style adaptation as we saw yesterday.",
      "The input layer learns specific feature/location combinations, and if the current combination has not yet been learned, then one cell from each mini-column (the one with the best modulatory input match) is chosen as the winner and becomes active.",
      "Winner cells learn by forming and strengthening modulatory connections with the current input location.",
      "The output layer learns representations that correspond to objects:  When the network first encounters a new object, a sparse set of cells in the output layer is chosen to represent the new object.",
      "These cells remain active while the system senses the object at different locations.",
      "Feed forward connections between the changing active cells in the input layer and unchanging active cells in the output layer are continuously reinforced.",
      "Simulation results  Networks are constructed with one or more two-layer cortical columns.",
      "In each column the input layer comprises 150 mini-columns, 16 cells tall each.",
      "The output layer consists of 4096 cells, which are not arranged in mini-columns.",
      "One-column and three-column variations of the network are trained on a library of 500 objects.",
      "As can be seen below, both variations converge on a single object representation over time, but the three-column version gets there faster.",
      "The capacity of the network is defined as the number of objects it can learn and recognise without confusion.",
      "It is influenced by the representational space of the network, the number of mini-columns in the input layer, the number of neurons  in the output layer, and the number of cortical columns.",
      "150 mini-columns with 16 cells per mini-column, and 10 simultaneously active mini-columns, turns out to be enough to uniquely represent about 10^15 sensory features, each represented at 16^10 unique locations.",
      "As the number of learned objects increases, neurons in the output layer form increasing numbers of connections to neurons in the input layer.",
      "If an output neuron connects to too many input neurons, it may be falsely activated by a pattern it was not trained on.",
      "Therefore, the capacity of the network is limited by the pooling capacity of the output layer.",
      "Mathematical analysis suggests that a single cortical column can store hundreds of objects before reaching this limit.",
      "Figure 5 below explores the various dimensions of network capacity.",
      "The network shows no drop in recognition accuracy with up to 20% noise in the sensory input, and 40% noise in the location input, though it does take longer to converge.",
      "Mountcastle\u2019s conjecture  In 1978 Mountcastle postulated that since the complex anatomy of cortical columns is similar in all of the neocortex, then all areas of the neocortex must be performing a similar function\u2026  The model of a cortical column presented in this paper is described in terms of sensory regions and sensory processing, but the circuitry underlying our model exists in all cortical regions.",
      "Thus if Mountcastle\u2019s conjecture is correct, even high-level cognitive functions, such as mathematics, language, and science would be implemented in this framework.",
      "It suggests that event abstract knowledge is stored in relation to some form of \u201clocation\u201d and that much of what we consider to be \u201cthought\u201d is implemented by inference and behavior generating mechanisms originally evolved to move and infer with fingers and eyes."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://www.frontiersin.org/articles/10.3389/fncir.2017.00081/full",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 35377831
  },
  {
    "blog_id": "bbr-congestion-based-congestion-control",
    "summary": [
      "BBR: Congestion-based congestion control Cardwell et al., ACM Queue Sep-Oct 2016  With thanks to Hossein Ghodse (@hossg) for recommending today\u2019s paper selection.",
      "This is the story of how members of Google\u2019s make-tcp-fast project developed and deployed a new congestion control algorithm for TCP called BBR (for Bandwidth Bottleneck and Round-trip propagation time), leading to 2-25x throughput improvement over the previous loss-based congestion control CUBIC algorithm.",
      "In fact, the improvements would have been even more significant but for the fact that throughput became limited by the deployed TCP receive buffer size.",
      "Increasing this buffer size led to a huge 133x relative improvement with BBR (2Gbps), while CUBIC remained at 15Mbps.",
      "BBR is also being deployed on YouTube servers, with a small percentage of users being assigned BBR playback.",
      "Playbacks using BBBR show significant improvement in all of YouTube\u2019s quality-of-experience metrics, possibly because BBR\u2019s behavior is more consistent and predictable\u2026 BBR reduces median RTT by 53 percent on average globally, and by more than 80 percent in the developing world.",
      "TCP congestion and bottlenecks  The Internet isn\u2019t working as well as it should, and many of the problems relate to TCP\u2019s loss-based congestion control, even with the current best-of-breed CUBIC algorithm.",
      "This ties back to design decisions taken in the 1980\u2019s when packet loss and congestion were synonymous due to technology limitations.",
      "That correspondence no longer holds so directly.",
      "When bottleneck buffers are large, loss-based congestion control keeps them full, causing bufferbloat.",
      "When bottleneck buffers are small, loss-based congestion control misinterprets loss as a signal of congestion, leading to low throughput.",
      "Fixing these problems requires finding an alternative to loss-based congestion control.",
      "From the perspective of TCP, the performance of an arbitrarily complex path is bound by two constraints: round-trip propagation time (RTprop), and bottleneck bandwidth, BtlBw (the bandwidth at the slowest link in each direction).",
      "Here\u2019s a picture to help make this clearer:  The RTprop time is the minimum time for round-trip propagation if there are no queuing delays and no processing delays at the receiver.",
      "The more familiar RTT (round-trip time) is formed of RTprop + these additional sources of noise and delay.",
      "Bandwidth Delay Product (BDP) is the maximum possible amount of data in transit in a network, and is obtained by multiplying the bottleneck bandwidth and round-trip propagation time.",
      "BDP is central to understanding network performance.",
      "Consider what happens to delivery rate as we gradually increase the amount of data inflight.",
      "When the amount of inflight data is less than BDP, then delivery rate increases as we send more data \u2013 delivery rate is limited by the application.",
      "Once the bandwidth at the bottleneck is saturated though, the delivery rate cannot go up anymore \u2013 we\u2019re pushing data through that pipe just as fast as it can go.",
      "The buffer will fill up, eventually we\u2019ll start dropping packets, but we still won\u2019t increase delivery rate.",
      "The optimum operating point is right on the BDP threshold (blue dot above), but loss-based congestion control operates at the BDP + Bottleneck Buffer Size point (green dot above).",
      "Now let\u2019s look at what happens to RTT as we increase the amount of data inflight.",
      "It can never be better than RTprop, so until we reach BDP, RTT ~= RTprop.",
      "Beyond BDP, as buffers start to fill, RTT goes up until buffers are completely full and we start dropping packets.",
      "Once more, the optimum operating point would be right on the BDP threshold.",
      "This was proved by Leonard Kleinrock in 1979, unfortunately about the same time Jeffrey M. Jaffe proved that it was impossible to create a distributed algorithm that converged to this operation point.",
      "Jaffe\u2019s result rests on fundamental measurement ambiguities.",
      "Although it is impossible to disambiguate any single measurement, a connection\u2019s behavior over time tells a clearer story, suggesting the possibility of measurement strategies designed to resolve ambiguity.",
      "Introducing BBR  BBR is a congestion control algorithm based on these two parameters that fundamentally characterise a path: bottleneck bandwidth and round-trip propagation time.",
      "It makes continuous estimates of these values, resulting in a distributed congestion control algorithm that reacts to actual congestion, not packet loss or transient queue delay, and converges with high probability to Kleinrock\u2019s optimal operating point.",
      "(BBR is a simple instance of a Max-plus control system , a new approach to control based on nonstandard algebra.",
      "This approach allows the adaptation rate [controlled by the max gain] to be independent of the queue growth [controlled by the average gain].",
      "Applied to this problem, it results in a simple, implicit control loop where the adaptation to physical constraint changes is automatically handled by the filters representing those constraints.",
      "A conventional control system would require multiple loops connected by a complex state machine to accomplish the same result.)",
      "Since RTT can never be less than RTprop, tracking the minimum RTT provides an unbiased and efficient estimator of the round-trip propagation time.",
      "The existing TCP acks provide enough information for us to calculate RTT.",
      "Unlike RTT, nothing in the TCP spec requires implementations to track bottleneck bandwidth, but a good estimate results from tracking delivery rate.",
      "The average delivery rate between a send and an ack is simply the amount of data delivered divided by the time taken.",
      "We know that this must be less than the true bottleneck delivery rate, so we can use the highest recorded delivery rate as our running estimate of bandwidth bottleneck.",
      "Putting this altogether leads to a core BBR algorithm with two parts: a protocol to follow on receiving an ack, and a protocol to following when sending.",
      "You\u2019ll find the pseudocode for these on pages 28 and 29-30.",
      "From my reading, there are a couple of small mistakes in the pseudocode (but I could be mistaken!",
      "), so I\u2019ve recreated clean versions below.",
      "Please do check against those in the original article if you\u2019re digging deeper\u2026  Here\u2019s the ack protocol:  (app_limited_until is set on the sending side, when the app is not sending enough data to reach BDP).",
      "This is what the sending protocol looks like:  The pacing_gain controls how fast packets are sent relative to BtlBw and is key to BBR\u2019s ability to learn.",
      "A pacing_gain greater than 1 increases inflight and decreases packet inter-arrival time, moving the connection to the right on the performance charts.",
      "A pacing_gain less than 1 has the opposite effect, moving the connection to the left.",
      "BBR uses this pacing_gain to implement a simple sequential probing state machine that alternates between testing for higher bandwidths and then testing for lower round-trip times.",
      "The frequency, magnitude, duration and structure of these experiments differ depending on what\u2019s already known (start-up or steady state) and the sending app\u2019s behaviour (intermittent or continuous).",
      "Most time is spent in the ProbeBW state probing bandwidth.",
      "BBR cycles through a sequence of gains for pacing_gain, using an eight-phase cycle with values 5/4, 3/4, 1, 1, 1, 1, 1, 1.",
      "Each phase lasts for the estimated round-trip propagation time.",
      "This design allows the gain cycle first to probe for more bandwidth with a pacing_gain above 1.0, then drain any resulting queue with a pacing_gain an equal distance below 1.0, and then cruise with a short queue using a pacing_gain of 1.0.",
      "The result is a control loop that looks like this plot below showing the RTT (blue), inflight (green) and delivery rate (red) from 700ms of a 10Mbps, 40-ms flow.",
      "Here\u2019s how BBR compares to CUBIC during the first second of a 10 Mbps, 40-ms flow.",
      "(BBR in green, CUBIC in red).",
      "We talked about the BBR benefits in Google\u2019s high-speed WAN network (B4) and in YouTube in the introduction.",
      "It also has massive benefits for low bandwidth mobile subscriptions.",
      "More than half of the world\u2019s 7 billion mobile Internet subscriptions connect via 8-to 114-kbps 2.5 G systems, which suffer well-documented problems because of loss-based congestion control\u2019s buffer-filling propensities.",
      "The bottleneck link for these systems is usually between the SGSN (serving GPRS support node)18 and mobile device.",
      "SGSN software runs on a standard PC platform with ample memory, so there are frequently megabytes of buffer between the Internet and mobile device.",
      "Figure 10 [ below] compares (emulated) SGSN Internet-to-mobile delay for BBR and CUBIC."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3012426.3022184?download=true",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 89628084
  },
  {
    "blog_id": "caa283af3c151372f4be86ed4c4b99",
    "summary": [
      "Machine Comprehension (MC) - given a natural language sentence, answer a natural language question.",
      "End-To-End MC - can not use language resources like dependency parsers.",
      "The only supervision during training is the correct answer.",
      "Query Regression Network (QRN) - Variant of Recurrent Neural Network (RNN).",
      "Related Work  Long Short-Term Memory (LSTM) and Gated Recurrence Unit (GRU) are popular choices to model sequential data but perform poorly on end-to-end MC due to long-term dependencies.",
      "Attention Models with shared external memory focus on single sentences in each layer but the models tend to be insensitive to the time step of the sentence being accessed.",
      "Memory Networks (and MemN2N)  Add time-dependent variable to the sentence representation.",
      "Summarize the memory in each layer to control attention in the next layer.",
      "Dynamic Memory Networks (and DMN+)  Combine RNN and attention mechanism to incorporate time dependency.",
      "Uses 2 GRU  time-axis GRU - Summarize the memory in each layer.",
      "layer-axis GRU - Control the attention in each layer.",
      "QRN is a much simpler model without any memory summarized node.",
      "QRN  Single recurrent unit that updates its internal state through time and layers.",
      "Inputs  qt - local query vector  xt - sentence vector  Outputs  ht - reduced query vector  xt - sentence vector without any modifications  Equations  zt = \u03b1(xt, qt)  &alpha is the update gate function to measure the relevance between input sentence and local query.",
      "h`t = \u03b3(xt, qt)  &gamma is the regression function to transform the local query into regressed query.",
      "ht = zt*h`t + (1 - zt)*ht-1  To create a multi layer model, output of current layer becomes input to the next layer.",
      "Variants  Reset gate function (rt) to reset or nullify the regressed query h`t (inspired from GRU).",
      "The new equation becomes ht = zt*rt*h`t + (1 - zt)*ht-1  Vector gates - update and reset gate functions can produce vectors instead of scalar values (for finer control).",
      "Bidirectional - QRN can look at both past and future sentences while regressing the queries.",
      "qtk+1 = htk, forward + htk, backward.",
      "The variables of update and regress functions are shared between the two directions.",
      "Parallelization  Unlike most RNN based models, recurrent updates in QRN can be computed in parallel across time.",
      "For details and equations, refer the paper .",
      "Module Details  Input Modules  A trainable embedding matrix A is used to encode the one-hot vector of each word in the input sentence into a d-dimensional vector.",
      "Position Encoder is used to obtain the sentence representation from the d-dimensional vectors.",
      "Question vectors are also obtained in a similar manner.",
      "Output Module  A V-way single-layer softmax classifier is used to map predicted answer vector y to a V-dimensional sparse vector v.  The natural language answer y is the arg max word in v.  Results  bAbI QA dataset used.",
      "QRN on 1K dataset with '2rb' (2 layers + reset gate + bidirectional) model and on 10K dataset with '2rvb' (2 layers + reset gate + vector gate + bidirectional) outperforms MemN2N 1K and 10K models respectively.",
      "Though DMN+ outperforms QRN with a small margin, QRN are simpler and faster to train (the paper made the comment on the speed of training without reporting the training time of the two models).",
      "With very few layers, the model lacks reasoning ability while with too many layers, the model becomes difficult to train.",
      "Using vector gates works for large datasets while hurts for small datasets.",
      "Unidirectional models perform poorly.",
      "The intermediate query updates can be interpreted in natural language to understand the flow of information in the network."
    ],
    "author_id": "shugan",
    "pdf_url": "http://arxiv.org/pdf/1606.04582",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 65102009
  },
  {
    "blog_id": "29486212643fd61f58a5a3eb5abb3c",
    "summary": [
      "Algorithm to derive similarity between 2 nodes of a graph (or graphical model derived from any other kind of dataset).",
      "SimRank  Input: A directed graph G = (V, E) where V represents vertices and E represents edges.",
      "SimRank defines similarity between 2 vertices (or nodes) i and j as the average of the similarity between their in-neighbours decayed by a constant factor C.  Any node is maximally similar to itself (with similarity = 1).",
      "PageRank analyses the individual vertices of the graph with respect to the global structure, while SimRank analyses the relationship between a pair of vertices (edges).",
      "SimRank scores are symmetric and can be defined between all pair of vertices.",
      "G2 is defined as the node pair graph such that each node in G2 corresponds to an ordered pair of nodes of G and there exists an edge between node pair (a, b) and (c, d) if there exists an edge between (a, c) and (b, d).",
      "In G2, similarity flows from node to node with singleton nodes (nodes of the form (a, a)) as the source of similarity.",
      "Variants  Minimax Variant  Defines similarity of nodes i and j as the minimum of maximum similarity between i and any in-neighbour of j and between j and any in-neighbour of i.  Computing SimRank  A naive solution can be obtained by iteration to a fixed point.",
      "Space complexity is O(n2) and time complexity is *O(kn2d) where k is the number of iterations, n is the number of vertices and d is the average of product of indegrees of pair of vertices.",
      "Optimisations can be made by setting the similarity between far off nodes as 0 and considering only nearby nodes for an update.",
      "Different Interpretations  Co-citation Score  The first iteration of SimRank produces results same as co-citation score between a pair of vertices.",
      "Successive iterations improve these initial scores.",
      "Random Surfer-Pairs Model  SimRank s(a, b) can be interpreted as the measure of how soon two random surfers are expected to meet at the same node if they start at nodes a and b and walk the graph backwards.",
      "Expected Meeting Distance (EMD) between 2 nodes a and b is the expected number of steps required before 2 surfers (starting at a and b) would meet if they walked randomly in locked step.",
      "Surfers are allowed to teleport with a small probability - to circumvent the infinite EMD problem.",
      "Expected-f Meeting Distance (EMD) - Given length l of a tour, compute f(l) (where f is a non-negative monotonic function) to bound the expected distance to a finite interval.",
      "Common choice for f is f(z) = Cz where C &epsilon; (0, 1)  The SimRank score for two nodes, with parameter C, is the expected-f meeting distance travelling back-edges with f(z) = Cz  Evaluation  Experiments on 2 datasets:  Corpus of scientific research papers from ResearchIndex.",
      "Transcripts of undergrad students at Stanford.",
      "Domain specific properties used to measure similarity and compared with SimRank scores.",
      "Results show improvement over co-citation scores.",
      "This comment has been minimized.",
      "Sign in to view  Copy link  Quote reply  tiankonghenlan20113046 commented  Oct 25, 2018  I read this paper before,but i cannot understand some aspects about the definition.",
      "can I  have a discussion wtih you ?"
    ],
    "author_id": "shugan",
    "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/775047.775126?download=true",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 31277852
  },
  {
    "blog_id": "a-first-look-at-the-usabilty-of-bitcoin-key-management",
    "summary": [
      "A first look at the usability of Bitcoin key management Eskandari et al., USEC 2015  This is the third of five papers from the ACM Queue Research for Practice selections on \u2018 Cryptocurrencies, Blockchains, and Smart Contracts .\u2019 And thankfully it\u2019s much easier to read and understand than yesterdays!",
      "The authors point out that a cryptocurrency intended for use by the general public also needs a means by which the general public can come to understand and manage collections of public and private keys.",
      "To date, this has proved elusive.",
      "As the authors of \u201c Why Johnny Can\u2019t Encrypt \u201d point out, \u201cSecurity mechanisms are effective only when used correctly.\u201d One of the interesting takeaways for me is that the familiar metaphors of \u2018coin\u2019 and \u2018password\u2019 can harm as much as they help.",
      "In all of the excitement surrounding Bitcoin, it is easy to forget that the decentralized currency assumes a solution to the longstanding problem of usable public key cryptography for user authentication.",
      "If you lose a Bitcoin private key, you lose the monetary value of the coins it protects.",
      "Moreover, Bitcoin users typically have no legal protection against loss or theft.",
      "So can we find a safe, usable key management approach for novice users?",
      "The findings from previous studies on the usability of public key cryptography include:  The metaphor and terminology behind public and private keys is confusing  It is difficult to correctly obtain other user\u2019s public keys  Key migration between devices is difficult  Many of the confusions seem to stem from the fact that users are used to passwords for protecting information, and public/private key pairs don\u2019t fit neatly into that mental model.",
      "The main bulk of the paper compares and contrasts six different approaches to Bitcoin key management, followed by a more detailed usability study of six actual Bitcoin clients, one from each category.",
      "Six approaches to Bitcoin key management  Here are six different approaches that you might use for managing private keys:  Keep them on device local storage.",
      "Bitcoin Core uses this approach.",
      "It\u2019s easy, but vulnerable to malware since the file storing private keys can be read by any process with applications to the user\u2019s folder.",
      "Private-key stealing malware has been known since at least 2011.",
      "Users need to be very careful not to inadvertently share this folder outside of their computer, but at the same time they need to periodically create new backups of the key storage file to ensure that new keypool keys are stored.",
      "Use a password-protected encrypted wallet.",
      "MultiBit uses this approach.",
      "These solutions encrypt the key storage file that is held on device local storage.",
      "The user is therefore protected against theft of the file (to the degree the password cannot be cracked), but malware on the machine will still be able to use e.g. a keystroke logger to capture the password.",
      "\u201cPassword-protected wallets may mislead the user to believe that the password itself provides access to their funds regardless of the location of the device storing the wallet.",
      "Offline storage of keys (e.g., Bitaddress) To protect against malware-based threats, wallets can be stored offline on portable media (e.g., a USB drive).",
      "Of course, this makes the wallet unavailable for immediate use, and the wallet is still exposed and potentially vulnerable to malware when mounted on a computational device.",
      "An interesting example here is paper wallets (e.g. [url]"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://users.encs.concordia.ca/~clark/papers/2015_usec.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 78841597
  },
  {
    "blog_id": "generating_images_with_recurrent_adversarial_networks",
    "summary": [
      "What  They describe a new architecture for GANs.",
      "The architecture is based on letting the Generator (G) create images in multiple steps, similar to DRAW.",
      "They also briefly suggest a method to compare the quality of the results of different generators with each other.",
      "How  In a classic GAN one samples a noise vector z, feeds that into a Generator (G), which then generates an image x, which is then fed through the Discriminator (D) to estimate its quality.",
      "Their method operates in basically the same way, but internally G is changed to generate images in multiple time steps.",
      "Outline of how their G operates:  Time step 0:  Input: Empty image delta C-1, randomly sampled z.",
      "Feed delta C-1 through a number of downsampling convolutions to create a tensor.",
      "(Not very useful here, as the image is empty.",
      "More useful in later timesteps.)",
      "Feed z through a number of upsampling convolutions to create a tensor (similar to DCGAN).",
      "Concat the output of the previous two steps.",
      "Feed that concatenation through a few more convolutions.",
      "Output: delta C0 (changes to apply to the empty starting canvas).",
      "Time step 1 (and later):  Input: Previous change delta C0, randomly sampled z (can be the same as in step 0).",
      "Feed delta C0 through a number of downsampling convolutions to create a tensor.",
      "Feed z through a number of upsampling convolutions to create a tensor (similar to DCGAN).",
      "Concat the output of the previous two steps.",
      "Feed that concatenation through a few more convolutions.",
      "Output: delta C1 (changes to apply to the empty starting canvas).",
      "At the end, after all timesteps have been performed:  Create final output image by summing all the changes, i.e. delta C0 + delta C1 + ..., which basically means empty start canvas + changes from time step 0 + changes from time step 1 + ....  Their architecture as an image:  Comparison measure  They suggest a new method to compare GAN results with each other.",
      "They suggest to train pairs of G and D, e.g. for two pairs (G1, D1), (G2, D2).",
      "Then they let the pairs compete with each other.",
      "To estimate the quality of D they suggest r_test = errorRate(D1, testset) / errorRate(D2, testset).",
      "(\"Which D is better at spotting that the test set images are real images?\")",
      "To estimate the quality of the generated samples they suggest r_sample = errorRate(D1, images by G2) / errorRate(D2, images by G1).",
      "(\"Which G is better at fooling an unknown D, i.e. possibly better at generating life-like images?\")",
      "They suggest to estimate which G is better using r_sample and then to estimate how valid that result is using r_test.",
      "Results  Generated images of churches, with timesteps 1 to 5:  Overfitting  They saw no indication of overfitting in the sense of memorizing images from the training dataset.",
      "They however saw some indication of G just interpolating between some good images and of G reusing small image patches in different images.",
      "Randomness of noise vector z:  Sampling the noise vector once seems to be better than resampling it at every timestep.",
      "Resampling it at every time step often led to very similar looking output images."
    ],
    "author_id": "ALEJU",
    "pdf_url": "http://arxiv.org/pdf/1602.05110v4",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 52492536
  },
  {
    "blog_id": "88ac9dbcac5523cb8b2d0a3d70f2d2",
    "summary": [
      "Large scale natural language understanding task - predict text values given a knowledge base.",
      "Accompanied by a large dataset generated using Wikipedia     Dataset  WikiReading dataset built using Wikidata and Wikipedia.",
      "Wikidata consists of statements of the form (property, value) about different items  80M statements, 16M items and 884 properties.",
      "These statements are grouped by items to get (item, property, answer) tuples where the answer is a set of values.",
      "Items are further replaced by their Wikipedia documents to generate 18.58M statements of the form (document, property, answer).",
      "Task is to predict answer given document and property.",
      "Properties are divided into 2 classes:  Categorical properties - properties with a small number of possible answers.",
      "Eg gender.",
      "Relational properties - properties with unique answers.",
      "Eg date of birth.",
      "This classification is done on the basis of the entropy of answer distribution.",
      "Properties with entropy less than 0.7 are classified as categorical properties.",
      "Answer distribution has a small number of very high-frequency answers (head) and a large number of answers with very small frequency (tail).",
      "30% of the answers do not appear in the training set and must be inferred from the document.",
      "Models  Answer Classification  Consider WikiReading as classification task and treat each answer as a class label.",
      "Baseline  Linear model over Bag of Words (BoW) features.",
      "Two BoW vectors computed - one for the document and other for the property.",
      "These are concatenated into a single feature vector.",
      "Neural Networks Method  Encode property and document into a joint representation which is fed into a softmax layer.",
      "Average Embeddings BoW  Average the BoW embeddings for documents and property and concatenate to get joint representation.",
      "Paragraph Vectors  As a variant of the previous method, encode document as a paragraph vector.",
      "LSTM Reader  LSTM reads the property and document sequence, word-by-word, and uses the final state as joint representation.",
      "Attentive Reader  Use attention mechanism to focus on relevant parts of the document for a given property.",
      "Memory Networks  Maps a property p and list of sentences x1, x2, ...xn in a joint representation by attention over the sentences in the document.",
      "Answer Extraction  For relational properties, it makes more sense to model the problem as information extraction than classification.",
      "RNNLabeler  Use an RNN to read the sequence of words and estimate if a given word is part of the answer.",
      "Basic SeqToSeq (Sequence to Sequence)  Similar to LSTM Reader but augmented with a second RNN to decode answer as a sequence of words.",
      "Placeholder SeqToSeq  Extends Basic SeqToSeq to handle OOV (Out of Vocabulary) words by adding placeholders to the vocabulary.",
      "OOV words in the document and answer are replaced by placeholders so that input and output sentences are a mixture of words and placeholders only.",
      "Basic Character SeqToSeq  Property encoder RNN reads the property, character-by-character and transforms it into a fixed length vector.",
      "This becomes the initial hidden state for the second layer of a 2-layer document encoder RNN.",
      "Final state of this RNN is used by answer decoder RNN to generate answer as a character sequence.",
      "Character SeqToSeq with pretraining  Train a character-level language model on input character sequence from the training set and use the weights to initiate the first layer of encoder and decoder.",
      "Experiments  Evaluation metric is F1 score (harmonic mean of precision and accuracy).",
      "All models perform well on categorical properties with neural models outperforming others.",
      "In the case of relational properties, SeqToSeq models have a clear edge.",
      "SeqToSeq models also show a great deal of balance between relational and categorical properties.",
      "Language model pretraining enhances the performance of character SeqToSeq approach.",
      "Results demonstrate that end-to-end SeqToSeq models are most promising for WikiReading like tasks."
    ],
    "author_id": "shugan",
    "pdf_url": "http://www.aclweb.org/anthology/P/P16/P16-1145.pdf",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 78861774
  },
  {
    "blog_id": "63eb099bb7a1ab4831cd37bffffb04",
    "summary": [
      "Build a supervised reading comprehension data set using news corpus.",
      "Compare the performance of neural models and state-of-the-art natural language processing model on reading comprehension task.",
      "Reading Comprehension  Estimate conditional probability p(a|c, q), where c is a context document, q is a query related to the document, and a is the answer to that query.",
      "Dataset Generation  Use online newspapers (CNN and DailyMail) and their matching summaries.",
      "Parse summaries and bullet points into Cloze style questions.",
      "Generate corpus of document-query-answer triplets by replacing one entity at a time with a placeholder.",
      "Data anonymized and randomised using coreference systems, abstract entity markers and random permutation of the entity markers.",
      "The processed data set is more focused in terms of evaluating reading comprehension as models can not exploit co-occurrence.",
      "Models  Baseline Models  Majority Baseline  Picks the most frequently observed entity in the context document.",
      "Exclusive Majority  Picks the most frequently observed entity in the context document which is not observed in the query.",
      "Symbolic Matching Models  Frame-Semantic Parsing  Parse the sentence to find predicates to answer questions like \"who did what to whom\".",
      "Extracting entity-predicate triples (e1,V, e2) from query q and context document d  Resolve queries using rules like exact match, matching entity etc.",
      "Word Distance Benchmark  Align placeholder of Cloze form questions with each possible entity in the context document and calculate the distance between the question and the context around the aligned entity.",
      "Sum the distance of every word in q to their nearest aligned word in d  Neural Network Models  Deep LSTM Reader  Test the ability of Deep LSTM encoders to handle significantly longer sequences.",
      "Feed the document query pair as a single large document, one word at a time.",
      "Use Deep LSTM cell with skip connections from input to hidden layers and hidden layer to output.",
      "Attentive Reader  Employ attention model to overcome the bottleneck of fixed width hidden vector.",
      "Encode the document and the query using separate bidirectional single layer LSTM.",
      "Query encoding is obtained by concatenating the final forward and backwards outputs.",
      "Document encoding is obtained by a weighted sum of output vectors (obtained by concatenating the forward and backwards outputs).",
      "The weights can be interpreted as the degree to which the network attends to a particular token in the document.",
      "Model completed by defining a non-linear combination of document and query embedding.",
      "Impatient Reader  As an add-on to the attentive reader, the model can re-read the document as each query token is read.",
      "Model accumulates the information from the document as each query token is seen and finally outputs a joint document query representation in the form of a non-linear combination of document embedding and query embedding.",
      "Result  Attentive and Impatient Readers outperform all other models highlighting the benefits of attention modelling.",
      "Frame-Semantic pipeline does not scale to cases where several methods are needed to answer a query.",
      "Moreover, they provide poor coverage as a lot of relations do not adhere to the default predicate-argument structure.",
      "Word Distance approach outperformed the Frame-Semantic approach as there was significant lexical overlap between the query and the document.",
      "The paper also includes heat maps over the context documents to visualise the attention mechanism.",
      "This comment has been minimized.",
      "Sign in to view  Copy link  Quote reply  yauhen-info commented  Apr 28, 2017  Thank you for sharing a good piece of work.",
      "Let me also ask if you had found a link to an implementation of the Attentive and Impatient Readers?"
    ],
    "author_id": "shugan",
    "pdf_url": "http://arxiv.org/pdf/1506.03340v3",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 77684000
  },
  {
    "blog_id": "why-do-recordreplay-tests-of-web-applications-break",
    "summary": [
      "Why do Record/Replay Tests of Web Applications Break?",
      "\u2013 Hammoudi et al. ICST \u201916  Your web application regression tests created using record/replay tools are fragile and keep breaking.",
      "Hammoudi et al. set out to find out why.",
      "If we knew that, perhaps we could design mechanisms to automatically repair broken tests, or to build more robust tests.",
      "The authors look at 300 different versions of five open source web applications, creating test suites for their initial versions using Selenium IDE and then following the evolution of the projects.",
      "When a test broke in a given version, it was repaired so that the process could continue.",
      "At the end of this process, data had been gathered on 722 individual test breakages.",
      "Using the data we gathered, we developed a taxonomy of the causes of test breakages that categorizes all of the breakages observed on the applications we studied.",
      "We then gathered 153 versions of three additional web applications and applied the foregoing process to them as well; this yielded data on 343 additional test breakages.",
      "We analyzed these in light of our taxonomy and were able to accommodate all of them without further changing the taxonomy; this provides evidence that our taxonomy may be more generally applicable.",
      "Hammoudi et al. had to create their own tests since, \u201cIn searching for web applications, we discovered that few open-source applications are provided with capture-replay test suites; in fact, few are provided with any test suites at all\u2026.\u201d  This will be a shorter paper write-up than usual.",
      "What you really need to know is your tests are breaking because the information used to locate page elements keeps breaking.",
      "After collating and clustering all of the breakages across the web tests, the authors create a taxonomy with 5 high level causes of test breakages:  Causes related to locators used in tests  Causes related to values and actions used in tests  Causes related to page reloading  Causes related to changes in user sessions times  Causes related to popup boxes.",
      "Locators are used by JavaScript and other languages, and by record/replay tools, to identify and manipulate elements.",
      "We identify two classes of locators, the second of which is composed of two sub-classes\u2026  Attribute-based locators use element attributes such as element ids and names.",
      "Structure-based locators rely on the structure of a web page and may locate elements via a hierarchy (e.g. xpaths or CSS selectors) or via an index in the case of multiple otherwise identical elements.",
      "Over 70% of all breakages are due to locator fragility in the face of change, and over 50% of all breakages are further due to attribute-based locators.",
      "Note that we cannot conclude from this that element attribute based location is inferior to other forms of location \u2013 even though it is responsible for most breakages \u2013 since we don\u2019t know the base rate of usage of the different location strategies.",
      "My personal suspicion is that attribute-based location is one of the more robust strategies.",
      "In terms of giving guidance to practitioners seeking to write more robust tests, it would be really nice to see this additional level of analysis.",
      "Our data suggests which categories of test breakages merit the greatest attention.",
      "Locators caused over 73% of the test breakages we observed, and attribute-based locators caused the majority of these.",
      "Clearly, addressing just this class of errors by finding ways to repair them if they break would have the largest overall impact on the reusability of tests across releases.",
      "The data also suggest where subsequent priorities should be placed in terms of finding methods for repairing test breakages."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7515470",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 27300559
  },
  {
    "blog_id": "learning-a-sat-solver-from-single-bit-supervision",
    "summary": [
      "The paper presents NeuroSAT, a message passing neural network that is trained to predict if a given SAT can be solved.",
      "As a side effect of training, the model also learns how to solve the SAT problem itself without any extra supervision.",
      "Background  Given an expression in the propositional logic, the task is to predict if there exists a substitution of variables that make the expression true.",
      "The expression itself can be written as a conjunction of disjunctions (\u201cand\u201d over \u201cor\u201d) where each conjunct is called a clause and each variable within a clause is called a literal.",
      "Invariants  The variables or clauses or literals (within the clauses) can be permuted.",
      "Every occurrence of a variable can be negated.",
      "Model  Given the SAT problem,  create an undirected graph of literals, their negations and the clauses they belong to.",
      "Put an edge between every literal and the clause to which it belongs and another kind of edge between every literal and its negation.",
      "Perform message passing between nodes to obtain vector representations corresponding to each node.",
      "Specifically, first, each clause received a message from its neighbours (literals) and updates its embeddings.",
      "Then every literal receives a message from its neighbours (both literals and clauses) and updates its embeddings.",
      "After T iterations, the nodes vote to decide the prediction of the model as a whole.",
      "The model is trained end-to-end using the cross-entropy loss between logit and the true label.",
      "Permutation invariance is ensured by operating on the nodes and the edges in the topological order and negation invariance is ensured by treating all literals as the same.",
      "Decoding Satisfying Assignment  The most interesting aspect of this work is that even though the model was trained to predict if the SAT problem can be satisfied, it is actually possible to extract the correct assignment from the classifier.",
      "In the early iterations, all the nodes vote \u201cunsolvable\u201d with low confidence.",
      "Then a few nodes start voting \u201csolvable\u201d and then a phase transition happens where most of the nodes start voting \u201csolvable\u201d with high confidence.",
      "The model never becomes highly confident that problem is \u201cunsolvable\u201d and almost never guesses \u201csolvable\u201d on an \u201cunsolvable\u201d problem.",
      "So in some sense, the model is looking for the combination of literals that actually solves the problem.",
      "The authors found that the 2 dimensional PCA projections of the literal embeddings are initially mixed up but become more and more linearly separable as the phase transition happens.",
      "Based on this insight, the authors propose to obtain cluster centres C1 and C2, partition the variables according to the cluster centres and then try assignments from both the partitions.",
      "This alone provides a satisfying solution in over 70% of the cases when though there is no explicit supervising signal about how to solve the problem.",
      "The other strengths of the paper includes  Generalizing to longer and more difficult SAT problems (than those seen during training).",
      "Generalizing to another kind of search problems like graph colouring, clique detection etc (over small random graphs).",
      "The paper also reports that by adding supervising signal about which clauses in the given expression are unsatisfiable, it is possible to decode the literals which prove the \u201cunsatisfiability\u201d of an expression at test time.",
      "Though not a lot of details have been provided about this part and would probably be covered in the next iteration of the paper."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1802.03685",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 91929013
  },
  {
    "blog_id": "pvanet",
    "summary": [
      "What  They present a variation of Faster R-CNN.",
      "Faster R-CNN is a model that detects bounding boxes in images.",
      "Their variation is about as accurate as the best performing versions of Faster R-CNN.",
      "Their variation is significantly faster than these variations (roughly 50ms per image).",
      "How  PVANET reuses the standard Faster R-CNN architecture:  A base network that transforms an image into a feature map.",
      "A region proposal network (RPN) that uses the feature map to predict bounding box candidates.",
      "A classifier that uses the feature map and the bounding box candidates to predict the final bounding boxes.",
      "PVANET modifies the base network and keeps the RPN and classifier the same.",
      "Inception  Their base network uses eight Inception modules.",
      "They argue that these are good choices here, because they are able to represent an image at different scales (aka at different receptive field sizes) due to their mixture of 3x3 and 1x1 convolutions.",
      "Representing an image at different scales is useful here in order to detect both large and small bounding boxes.",
      "Inception modules are also reasonably fast.",
      "Visualization of their Inception modules:  Concatenated ReLUs  Before the eight Inception modules, they start the network with eight convolutions using concatenated ReLUs.",
      "These CReLUs compute both the classic ReLU result (max(0, x)) and concatenate to that the negated result, i.e. something like f(x) = max(0, x <concat> (-1)*x).",
      "That is done, because among the early one can often find pairs of convolution filters that are the negated variations of each other.",
      "So by adding CReLUs, the network does not have to compute these any more, instead they are created (almost) for free, reducing the computation time by up to 50%.",
      "Visualization of their final CReLU block:  TODO  Multi-Scale output  Usually one would generate the final feature map simply from the output of the last convolution.",
      "They instead combine the outputs of three different convolutions, each resembling a different scale (or level of abstraction).",
      "They take one from an early point of the network (downscaled), one from the middle part (kept the same) and one from the end (upscaled).",
      "They concatenate these and apply a 1x1 convolution to generate the final output.",
      "Other stuff  Most of their network uses residual connections (including the Inception modules) to facilitate learning.",
      "They pretrain on ILSVRC2012 and then perform fine-tuning on MSCOCO, VOC 2007 and VOC 2012.",
      "They use plateau detection for their learning rate, i.e. if a moving average of the loss does not improve any more, they decrease the learning rate.",
      "They say that this increases accuracy significantly.",
      "The classifier in Faster R-CNN consists of fully connected layers.",
      "They compress these via Truncated SVD to speed things up.",
      "(That was already part of Fast R-CNN, I think.)",
      "Results  On Pascal VOC 2012 they achieve 82.5% mAP at 46ms/image (Titan X GPU).",
      "Faster R-CNN + ResNet-101: 83.8% at 2.2s/image.",
      "Faster R-CNN + VGG16: 75.9% at 110ms/image.",
      "R-FCN + ResNet-101: 82.0% at 133ms/image.",
      "Decreasing the number of region proposals from 300 per image to 50 almost doubles the speed (to 27ms/image) at a small loss of 1.5 percentage points mAP.",
      "Using Truncated SVD for the classifier reduces the required timer per image by about 30% at roughly 1 percentage point of mAP loss."
    ],
    "author_id": "ALEJU",
    "pdf_url": "https://arxiv.org/pdf/1608.08021",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 1907968
  },
  {
    "blog_id": "4eb32de8cabf21bda9a4ada15c46e8",
    "summary": [
      "The paper describes an unsupervised approach to train a generic, distributed sentence encoder.",
      "It also describes a vocabulary expansion method to encode words not seen at training time.",
      "Skip-Thoughts  Train an encoder-decoder model where the encoder maps the input sentence to a sentence vector and the decoder generates the sentences surrounding the original sentence.",
      "The model is called skip-thoughts and the encoded vectors are called skip-thought vectors.",
      "Similar to the skip-gram model in the sense that surrounding sentences are used to learn sentence vectors.",
      "Architecture  Training data is in form of sentence tuples (previous sentence, current sentence, next sentence).",
      "Encoder  RNN Encoder with GRU.",
      "Decoder  RNN Decoder with conditional GRU.",
      "Conditioned on encoder output.",
      "Extra matrices introduced to bias the update gate, reset gate and hidden state, given the encoder output.",
      "Vocabulary matrix (V) - Weight matrix having one row (vector) for each word in the vocabulary.",
      "Separate decoders for the previous and next sentence which share only V.  Given the decoder context h (at any time), encoder output, and list of words already generated for the output sentence, the probability of choosing w as the next word is proportional to exp(V(word)h)  Objective  Sum of the log-probabilities for the forward and backwards sentences conditioned on the encoder output.",
      "Vocabulary Expansion  Use a model like Word2Vec which can be trained to induce word representations and train it to obtain embeddings for all the words that are likely to be seen by the encoder.",
      "Learn a matrix W such that encoder(word) = cross-product(W, Word2Vec(word)) for all words that are common to both Word2Vec model and encoder model.",
      "Use W to generate embeddings for words are not seen during encoder training.",
      "Dataset  BookCorpus dataset having books across 16 genres.",
      "Training  uni-skip  Unidirectional auto-encoder with 2400 dimensions.",
      "bi-skip  Bidirectional model with forward (sentence given in correct order) and backward (sentence given in reverse order) encoders of 1200 dimensions each.",
      "combine-skip  concatenation of uni-skip and bi-skip vectors.",
      "Initialization  Recurrent matricies - orthogonal initialization.",
      "Non-recurrent matricies - uniform distribution in [-0.1,0.1].",
      "Mini-batches of size 128.",
      "Gradient Clipping at norm = 10.",
      "Adam optimizer.",
      "Experiments  After learning skip-thoughts, freeze the model and use the encoder as feature extractor only.",
      "Evaluated the vectors with linear models on following tasks:  Semantic Relatedness  Given a sentence pair, predict how closely related the two sentences are.",
      "skip-thoughts method outperforms all systems from SemEval 2014 competition and is outperformed only by dependency tree-LSTMs.",
      "Using features learned from image-sentence embedding model on COCO boosts performance and brings it at par with dependency tree-LSTMs.",
      "Paraphrase detection  skip-thoughts outperforms recursive nets with dynamic pooling if no hand-crafted features are used.",
      "skip-thoughts with basic pairwise statistics produce results comparable with the state-of-the-art systems that house complicated features and hand engineering.",
      "Image-sentence Ranking  MS COCO dataset  Task  Image annotation  Given an image, rank the sentences on basis of how well they describe the image.",
      "Image search - Given a caption, find the image that is being described.",
      "Though the system does not outperform baseline system in all cases, the results does indicate that skip-thought vectors can capture image descriptions without having to learn their representations from scratch.",
      "Classification  skip-thoughts perform about as good as bag-of-words baselines but are outperformed by methods where sentence representation has been learnt for the task at hand.",
      "Combining skip-thoughts with bi-gram Naive Bayes (NB) features improves the performance.",
      "Future Work  Variants to be explored include:  Fine tuning the encoder-decoder model during the downstream task instead of freezing the weights.",
      "Deep encoders and decoders.",
      "Larger context windows.",
      "Encoding and decoding paragraphs.",
      "Encoders, such as convnets."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1506.06726",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 91325283
  },
  {
    "blog_id": "cms-rcnn",
    "summary": [
      "What  They describe a model to locate faces in images.",
      "Their model uses information from suspected face regions and from the corresponding suspected body regions to classify whether a region contains a face.",
      "The intuition is, that seeing the region around the face (specifically where the body should be) can help in estimating whether a suspected face is really a face (e.g. it might also be part of a painting, statue or doll).",
      "How  Their whole model is called \"CMS-RCNN\" (Contextual Multi-Scale Region-CNN).",
      "It is based on the \"Faster R-CNN\" architecture.",
      "It uses the VGG network.",
      "Subparts of their model are: MS-RPN, CMS-CNN.",
      "MS-RPN finds candidate face regions.",
      "CMS-CNN refines their bounding boxes and classifies them (face / not face).",
      "MS-RPN (Multi-Scale Region Proposal Network)  \"Looks\" at the feature maps of the network (VGG) at multiple scales (i.e. before/after pooling layers) and suggests regions for possible faces.",
      "Steps:  Feed an image through the VGG network.",
      "Extract the feature maps of the three last convolutions that are before a pooling layer.",
      "Pool these feature maps so that they have the same heights and widths.",
      "Apply L2 normalization to each feature map so that they all have the same scale.",
      "Apply a 1x1 convolution to merge them to one feature map.",
      "Regress face bounding boxes from that feature map according to the Faster R-CNN technique.",
      "CMS-CNN (Contextual Multi-Scale CNN):  \"Looks\" at feature maps of face candidates found by MS-RPN and classifies whether these regions contains faces.",
      "It also uses the same multi-scale technique (i.e. take feature maps from convs before pooling layers).",
      "It uses some area around these face regions as additional information (suspected regions of bodies).",
      "Steps:  Receive face candidate regions from MS-RPN.",
      "Do per candidate region:  Calculate the suspected coordinates of the body (only based on the x/y-position and size of the face region, i.e. not learned).",
      "Extract the feature maps of the face region (at multiple scales) and apply RoI-Pooling to it (i.e. convert to a fixed height and width).",
      "Extract the feature maps of the body region (at multiple scales) and apply RoI-Pooling to it (i.e. convert to a fixed height and width).",
      "L2-normalize each feature map.",
      "Concatenate the (RoI-pooled and normalized) feature maps of the face (at multiple scales) with each other (creates one tensor).",
      "Concatenate the (RoI-pooled and normalized) feature maps of the body (at multiple scales) with each other (creates another tensor).",
      "Apply a 1x1 convolution to the face tensor.",
      "Apply a 1x1 convolution to the body tensor.",
      "Apply two fully connected layers to the face tensor, creating a vector.",
      "Apply two fully connected layers to the body tensor, creating a vector.",
      "Concatenate both vectors.",
      "Based on that vector, make a classification of whether it is really a face.",
      "Based on that vector, make a regression of the face's final bounding box coordinates and dimensions.",
      "Note: They use in both networks the multi-scale approach in order to be able to find small or tiny faces.",
      "Otherwise, after pooling these small faces would be hard or impossible to detect.",
      "Results  Adding context to the classification (i.e. the body regions) empirically improves the results.",
      "Their model achieves the highest recall rate on FDDB compared to other models.",
      "However, it has lower recall if only very few false positives are accepted.",
      "FDDB ROC curves (theirs is bold red):  Example results on FDDB:"
    ],
    "author_id": "ALEJU",
    "pdf_url": "http://arxiv.org/pdf/1606.05413v1",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 82314288
  },
  {
    "blog_id": "1607.05690",
    "summary": [
      "This paper derives an algorithm for passing gradients through a sample from a mixture of Gaussians.",
      "While the reparameterization trick allows to get the gradients with respect to the Gaussian means and covariances, the same trick cannot be invoked for the mixing proportions parameters (essentially because they are the parameters of a multinomial discrete distribution over the Gaussian components, and the reparameterization trick doesn't extend to discrete distributions).",
      "One can think of the derivation as proceeding in 3 steps:  1.",
      "Deriving an estimator for gradients a sample from a 1-dimensional density $f(x)$ that is such that $f(x)$ is differentiable and its cumulative distribution function (CDF) $F(x)$ is tractable:    $\\frac{\\partial \\hat{x}}{\\partial \\theta} = - \\frac{1}{f(\\hat{x})}\\int_{t=-\\infty}^{\\hat{x}} \\frac{\\partial f(t)}{\\partial \\theta} dt$    where $\\hat{x}$ is a sample from density $f(x)$ and $\\theta$ is any parameter of $f(x)$ (the above is a simplified version of Equation 6).",
      "This is probably the most important result of the paper, and is based on a really clever use of the general form of the Leibniz integral rule.",
      "2.",
      "Noticing that one can sample from a $D$-dimensional Gaussian mixture by decomposing it with the product rule $f({\\bf x}) = \\prod_{d=1}^D f(x_d|{\\bf x}_{<d})$ and using ancestral sampling, where each $f(x_d|{\\bf x}_{<d})$ are themselves 1-dimensional mixtures (i.e. with differentiable densities and tractable CDFs)  3.",
      "Using the 1-dimensional gradient estimator (of Equation 6) and the chain rule to backpropagate through the ancestral sampling procedure.",
      "This requires computing the integral in the expression for $\\frac{\\partial \\hat{x}}{\\partial \\theta}$ above, where $f(x)$ is one of the 1D conditional Gaussian mixtures and $\\theta$ is a mixing proportion parameter $\\pi_j$.",
      "As it turns out, this integral has an analytical form (see Equation 22).",
      "**My two cents**  This is a really surprising and neat result.",
      "The author mentions it could be applicable to variational autoencoders (to support posteriors that are mixtures of Gaussians), and I'm really looking forward to read about whether that can be successfully done in practice.",
      "The paper provides the derivation only for mixtures of Gaussians with diagonal covariance matrices.",
      "It is mentioned that extending to non-diagonal covariances is doable.",
      "That said, ancestral sampling with non-diagonal covariances would become more computationally expensive, since the conditionals under each Gaussian involves a matrix inverse.",
      "Beyond the case of Gaussian mixtures, Equation 6 is super interesting in itself as its application could go beyond that case.",
      "This is probably why the paper also derived a sampling-based estimator for Equation 6, in Equation 9.",
      "However, that estimator might be inefficient, since it involves sampling from Equation 10 with rejection, and it might take a lot of time to get an accepted sample if $\\hat{x}$ is very small.",
      "Also, a good estimate of Equation 6 might require *multiple* samples from Equation 10.",
      "Finally, while I couldn't find any obvious problem with the mathematical derivation, I'd be curious to see whether using the same approach to derive a gradient on one of the Gaussian mean or standard deviation parameters gave a gradient that is consistent with what the reparameterization trick provides."
    ],
    "author_id": "hlarochelle",
    "pdf_url": "http://arxiv.org/pdf/1607.05690v1",
    "author_full_name": "Hugo Larochelle",
    "source_website": "https://www.shortscience.org/user?name=hlarochelle",
    "id": 16280529
  },
  {
    "blog_id": "towards-usable-checksums-automating-the-integrity-verification-of-web-downloads-for-the-masses",
    "summary": [
      "Towards usable checksums: automating the integrity verification of web downloads for the masses Cherubini et al., CCS\u201918  If you tackled Monday\u2019s paper on BEAT you deserve something a little easier to digest today, and \u2018Towards usable checksums\u2019 fits the bill nicely!",
      "There\u2019s some great data-driven product management going on here as the authors set out to quantify current attitudes and behaviours regarding downloading files from the Internet, design a solution to improve security and ease-of-use, and then test their solution to gather feedback and prepare for a more widely deployed beta version.",
      "When I was growing up we were all taught \u201cDon\u2019t talk to strangers\u201d, and \u201cNever get in a stranger\u2019s car\u201d.",
      "As has been well noted by others, so much for that advice!",
      "Perhaps the modern equivalent is \u201cDon\u2019t download unknown files from the Internet!\u201d This paper specifically looks at applications made directly available from developer websites (vs downloads made through app stores).",
      "A popular and convenient way to download programs is to use official app stores such as Apple\u2019s Mac App Store and Microsoft\u2019s Windows Store.",
      "Such platforms, however, have several drawbacks for developers, including long review and validation times, technical restrictions (e.g., sandboxing), incompatibility with software licenses, and substantial commissions.",
      "Therefore, it is quite common that developers make their programs available directly from their websites.",
      "This is the case of popular programs such as VLC media player, OpenOffice, and GIMP.",
      "If you\u2019re reading The Morning Paper, you probably know what a checksum is for and how to use it to verify the integrity of a download.",
      "You\u2019re probably also well aware of the importance of doing so.",
      "Even so, I wouldn\u2019t be surprised if on at least one occasion it\u2019s been too awkward / you\u2019ve been in too much of a hurry to get some other task done you\u2019re focused on / you rated the risk as low enough, and so on, that you failed to do so.",
      "I\u2019ve seen up close the apparent struggles of very bright professional people without IT backgrounds to manage basics such as passwords.",
      "I hold out little hope of them navigating checksums\u2014 from \u201cWhat\u2019s a command-line?\u201d on up\u2014 even though I know they\u2019re more than capable of understanding if only it seemed sufficiently important to them.",
      "Yet checksums are an important line of defence to protect against adversaries tampering files to inject malware etc..  A popular way for developers to enable users to detect accidental or intentional modifications of their program files hosted on external platforms, such as mirrors and CDNs, is to provide so-called checksums on their websites.",
      "This practice is quite common in the open-source community but also for companies such as Google\u2026  For a restricted subset of downloadable assets -chiefly JavaScript and style sheets, included via script and link tags, integrated checksum support is available via the Subresource Integrity (SRI) specification introduced by the W3C in 2016.",
      "It\u2019s supported by all major browsers (including Edge, but not IE).",
      "If you\u2019re not already using it for externally hosted assets you include in your site then you really should look into it.",
      "Here\u2019s an example of the integrity attribute in action, taken from the MDN site:  <script src=\" [url]"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3243734.3243746?download=true",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 57228237
  },
  {
    "blog_id": "one-model-to-learn-them-all",
    "summary": [
      "The current trend in deep learning is to design, train and fine tune a separate model for each problem.",
      "Though multi-task models have been explored, they have been trained for problems from the same domain only and no competitive multi-task, multi-modal models have been proposed.",
      "The paper explores the possibility of such a unified deep learning model that can solve different tasks across multiple domains by training concurrently on them.",
      "Design Philosophy  Small, modality-specific subnetworks (called modality nets) should be used to map input data to a joint representation space and back.",
      "The joint representation is to be of variable size.",
      "Different tasks from the same domain share the modality net.",
      "MultiModel networks should use computational blocks from different domains even if they are not specifically designed for the task at hand.",
      "Eg the paper reports that attention and mixture-of-experts (MOE) layers slightly improve the performance on ImageNet even though they are not explicitly needed.",
      "Architecture  MulitModel Network consists of few, small modality nets, an encoder, I/O mixer and an autoregressive decoder.",
      "Encoder and decoder use the following computational blocks:  Convolutional Block  ReLU activations on inputs followed by depthwise separable convolutions and layer normalization.",
      "Attention Block  Multihead, dot product based attention mechanism.",
      "Mixture-of-Experts (MoE) Block  Consists of simple feed-forward networks (called experts) and a trainable gating network which selects a sparse combination of experts to process the inputs.",
      "For further details, refer the original paper .",
      "Encoder consists of 6 conv blocks with a MoE block in the middle.",
      "I/O mixer consists of an attention block and 2 conv blocks.",
      "Decoder consists of 4 blocks of convolution and attention with a MoE block in the middle.",
      "Modality Nets  Language Data  Input is the sequence of tokens ending in a termination token.",
      "This sequence is mapped to correct dimensionality using a learned embedding.",
      "For output, the network takes the decoded output and performs a learned linear mapping followed by Softmax.",
      "Image and Categorical Data  Uses residual convolution blocks.",
      "Similar to the exit flow for Xception Network  Audio Data  1-d waveform over time or 2-d spectrogram operated upon by stack of 8 residual convolution blocks.",
      "Tasks  WSJ speech corpus  ImageNet dataset  COCO image captioning dataset  WSJ parsing dataset  WMT English-German translation corpus  German-English translation  WMT English-French translation corpus  German-French translation  Experiments  The experimental section is not very rigorous with many details skipped (would probably be added later).",
      "While MultiModel does not beat the state of the art models, it does outperform some recent models.",
      "Jointly trained model performs similar to single trained models on tasks with a lot of data and sometimes outperformed single trained models on tasks with less data (like parsing).",
      "Interestingly, jointly training the model for parsing task and Imagenet tasks improves the performance of parsing task even though the two tasks are seemingly unrelated.",
      "Another experiment was done to evaluate the effect of components (like MoE) on tasks (like Imagenet) which do not explicitly need them.",
      "It was observed that either the performance either went down or remained the same when MoE component was removed.",
      "This indicates that mixing different components does help to improve performance over multiple tasks.",
      "But this observation is not conclusive as a different combination of say the encoder (that does not use MoE) could achieve better performance than one that does.",
      "The paper does not explore possibilities like these."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1706.05137",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 97932426
  },
  {
    "blog_id": "geometric-deep-learning",
    "summary": [
      "Notes  This paper surveys progress on adapting deep learning techniques to non-Euclidean data and suggests future directions.",
      "One of the strengths (and weaknesses) of deep learning\u2013specifically exploited by convolutional neural networks\u2013is that the data is assumed to exhibit translation invariance/equivariance and invariance to local deformations.",
      "Hence, long-range dependencies can be learned with multi-scale, hierarchical techniques where spatial resolution is reduced.",
      "However, this means that any information about the data that can\u2019t be learned when spatial resolution is reduced can get lost (I believe that residual networks aim to address this by the skip connections that are able to learn an identity operation; also, in computer vision, multi-scale versions of the data are often fed to CNNs).",
      "Key areas where this assumption about the data appears to be true is computer vision and speech recognition.",
      "Some quick background  The Laplacian, a self-adjoint (symmetric) positive semi-definite operator, which is defined for smooth manifolds and graphs in this paper, can be thought of as the difference between the local average of a function around a point and the value of the function at the point itself.",
      "It\u2019s generally defined as $\\triangle = -\\text{div} \\nabla$.",
      "When discretizing a continuous, smooth manifold with a mesh, note that the graph Laplacian might not converge to the continuous Laplacian operator with increasing sampling density.",
      "To be consistent, need to create a triangular mesh, i.e., represent the manifold as a polyhedral surface.",
      "Spectral methods  Fourier analysis on non-Euclidean domains is possible by considering the eigendecomposition of the Laplacian operator.",
      "A possible transformation of the Convolution Theorem to functions on manifolds and graphs is discussed, but is noted as not being shift-invariant.",
      "The Spectral CNN can be defined by introducing a spectral convolutional layer acting on the vertices of the graph and using filters in the frequency domain and the eigenvectors of the Laplacian.",
      "However, the spectral filter coefficients will be dependent on the particular eigenvectors (basis) - domain dependency == bad for generalization!",
      "The non-Euclidean analogy of pooling is graph coarsening- only a fraction of the vertices of the graph are retained.",
      "Strided convolutions can be generalized to the spectral construction by only keeping the low-frequency components - must recompute the graph Laplacian after applying the nonlinearity in the spatial domain, however.",
      "Performing matrix multiplications on the eigendecomposition of the Laplacian is expensive!",
      "Spectrum-free Methods  A polynomial of the Laplacian acts as a polynomial on the eigenvalues.",
      "ChebNet (Defferrard et al.)",
      "and Graph Convolutional Networks (Kipf et al.)",
      "boil down to applying simple filters acting on the r- or 1-hop neighborhood of the graph in the spatial domain.",
      "Some examples of generalizations of CNNs that define weighting functions for a locally Euclidean coordinate system around a point on a manifold are the  Geodesic CNN  Anisotropic CNN  Mixture Model network (MoNet)  What problems are being solved with these methods?",
      "Ranking and community detection on social networks  Recommender systems  3D geometric data in Computer Vision/Graphics  Shape classification  Feature correspondence for 3D shapes  Behavior of N-particle systems (particle physics, LHC)  Molecule design  Medical imaging  Open Problems  Generalization spectral analogues of convolution learned on one graph cannot be readily applied to other ones (domain dependency).",
      "Spatial methods generalize across different domains, but come with their own subtleties  Time-varying domains  Directed graphs non-symmetric Laplacian that do not have orthogonal eigendecompositions for interpretable spectral-domain constructions  Synthesis problems generative models  Computation extending deep learning frameworks for non-Euclidean data"
    ],
    "author_id": "pemami",
    "pdf_url": "https://arxiv.org/pdf/1611.08097",
    "author_full_name": "Patrick Emami",
    "source_website": "https://pemami4911.github.io/index.html",
    "id": 14310989
  },
  {
    "blog_id": "continuum-a-platform-for-cost-aware-low-latency-continual-learning",
    "summary": [
      "Continuum: a platform for cost-aware low-latency continual learning Tian et al., SoCC\u201918  Let\u2019s start with some broad approximations.",
      "Batching leads to higher throughput at the cost of higher latency.",
      "Processing items one at a time leads to lower latency and often reduced throughput.",
      "We can recover throughput to a degree by throwing horizontally scalable resources at the problem, but it\u2019s hard to recover latency.",
      "In many business scenarios latency matters, so we\u2019ve been seeing a movement overtime from batching through micro-batching to online streaming.",
      "Continuum looks at the same issues from the perspective of machine learning models.",
      "Offline (batch) trained models can suffer from concept drift (loss of accuracy over time) as a result of not incorporating the latest data.",
      "I.e., there\u2019s a business cost incurred for higher latency of update incorporation.",
      "Online models support incremental updates.",
      "Continuum determines the optimum time to retrain models in the presence of incoming data, based on user policy (best effort, cost-aware, or user-defined).",
      "There\u2019s some great data here about the need for and benefit of continual learning, and a surprising twist in the tale where it turns out that even if you can afford it, updating the model on every incoming data point is not the best strategy even when optimising for lowest latency of update incorporation.",
      "When good models go stale  There are a number of purpose-designed online learning algorithms (e.g. Latent Dirichlet Allocation for topic models, matrix factorization for recommender systems, and Bayesian inference for stream analytics).",
      "However, many mainstream ML frameworks including TensorFlow, MLib, XGBoost, scikit-learn and MALLET do not explicitly support continual model updating.",
      "It\u2019s up to the user to code custom training loops to manually trigger retraining.",
      "Often such models are updated on much slower timescales (e.g. daily, or maybe hourly) than the generation of the data they operate over.",
      "This causes a loss in model accuracy when concept drift occurs.",
      "Consider a Personalized PageRank (PPR) algorithm being fed data from Twitter.",
      "The following chart shows how L1 error and MAP metrics degrade over time as compared to a model trained on the very most recent data.",
      "The base model decays by about 10% in one hour.",
      "Using offline (batch) training to retrain the model from scratch every 10 minutes also takes orders of magnitude more compute than online learning, making short retraining windows using the full training data set impractical.",
      "When a topic modelling model (also trained using tweets) is updated once every ten minutes, we can see that it\u2019s perplexity (lower is better) decreases with every retraining.",
      "The performance of an ad recommendation system classifier similarly improves over time with model updating every five minutes.",
      "Data recency clearly matters in a number of applications.",
      "But writing your own continual learning training loops can be as much if not more work than implementing the key logic of online learning in the first place.",
      "An overview of Continuum  Continuum is designed to support continual learning a cross a broad set of ML frameworks.",
      "Based on an update policy, Continuum decides when to update the model.",
      "At runtime it looks like this:  Clients send data updates (or pointers to updated data) to Continuum  Continuum stores the updated data  Continuum evaluates the update policy and triggers retraining if needed  The backend fetches the updated data from data storage and trains the model  The backend notifies Continuum that an updated model is available.",
      "The updated model can then be shipped to a model serving system, e.g. Clipper .",
      "Continuum is about 4,000 lines of C++ and Python, and is available in open-source here:  [url]"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://www.cse.ust.hk/~weiwa/papers/huangshi-socc18.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 78499306
  },
  {
    "blog_id": "meta-learning-update-rules-for-unsupervised-representation-learning",
    "summary": [
      "Standard unsupervised learning aims to learn transferable features.",
      "The paper proposes to learn a transferable learning rule (in an unsupervised manner) that can generalize across tasks and architectures.",
      "Paper  Approach  Consider training the model with supervised learning - \u03c6t+1 = SupervisedUpdate(\u03c6t, xt, yt, \u03b8).",
      "Here t denotes the step, (x, y) denotes the data points, \u03b8 denotes the hyperparameters of the optimizer.",
      "Extending this formulation for meta-learning, one could say that t is the step of the inner loop, \u03b8 are the parameters of the meta learning model.",
      "Further, the paper proposes to use \u03c6t+1 = UnsupervisedUpdate(\u03c6t, xt, \u03b8) ie yt is not used (or even assumed to be available as this is unsupervised learning).",
      "The meta update rule is used to learn the weights of a meta-model by performing SGD on the sum of MetaObjective over the distribution of tasks (over the course of inner loop training).",
      "Model  Base model: MLP with parameters \u03c6t  To ensure that it generalizes across architectures, the update rule is designed to be neural-local ie updates are a function of pre and postsynaptic neurons though, in practice, this constraint is relaxed to decorrelate neurons by using cross neural information.",
      "Each neuron i in every layer l (in the base model) has an update network (MLP) which takes as input the feedforward activations, feedback weights and error signals.",
      "ie hbl(i) = MLP(xbl(i), zbl(i), vl+1, \u03b4l(i), \u03b8)  b - index of the minibatch  xl - pre non-linearity activations  zl - post non-linearity activations  vl - feedback weights  \u03b4l - error signal  All the update networks share the meta parameters \u03b8  The model is run in a standard feed-forward manner and the update network (corresponding to each unit) is used to generate the error signal \u03b4lb(i) = lin(hbl(i)).",
      "This loss is backpropogated using the set of learned backward weights vl instead of the forward weights wl.",
      "The weight update \u0394wl is also generated using a per-neuron update network.",
      "Meta Objective  The MetaObjective is based on fitting a linear regression model to labeled examples with a small number of data points.",
      "Given the emphasis on learning generalizable features, the weights (of linear regression) are estimated on one batch and evaluated on another batch.",
      "The MetaObjective is to reduce the cosine distance between yb and vTxbL  yb - Actual lables on the evaluation batch  xbL - Features of the evaluation batch (using the base model)  v - parameters of the linear regression model (learned on train batch)  Practical Considerations  Meta gradients are approximated using truncated backdrop through time.",
      "Increasing variation in the training dataset helps the meta optimization process.",
      "Data is augmented with shifts, rotations, and noise.",
      "Predicting these coefficients is an auxiliary (regression) task for training the meta-objective.",
      "Training the system requires a lot of resources - 8 days with 512 workers.",
      "Results  With standard unsupervised learning, the performance (on transfer task) starts declining after some time even though the performance (on the unsupervised task) is improving.",
      "This suggests that the objective function for the two tasks starts to mismatch.",
      "UnsupervisedUpdate leads to a better generalization as compared to both VAE and supervised learning (followed by transfer).",
      "UnsupervisedUpdate also leads to a positive transfer across domains (vision to language) when trained for a shorter duration of time (to ensure that the meta-objective does not overfit).",
      "UnsupervisedUpdate also generalizes to larger model architectures and different activation functions."
    ],
    "author_id": "shugan",
    "pdf_url": "https://www.mitpressjournals.org/doi/pdf/10.1162/089976602753712972",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 93544751
  },
  {
    "blog_id": "ml-for-genomics",
    "summary": [
      "Major points made by the article:  \u201cOur view is that to make genomic medicine a reality, we must develop computer systems that can accurately interpret the text of the genome just as the machinery inside the cell does\u201d.",
      "\u201cProtein-coding exons are the most understood regions in the genome (re: \u201cstart\u201d and \u201cstop\u201d codons\u201d).",
      "A long standing open problem is predicting whether a mutation will disrupt the stability or structure of the final protein molecule  \u201cPredicting phenotypes (e.g., traits and disease risks) from biomarkers such as the genome is, in principle, a supervised machine learning problem\u201d.",
      "The correct approach is not so simple; the computational model should be trained to predict measurable intermediate cell variables, also known as molecular phenotypes, and then these variables can be linked to phenotypes.",
      "Alternative Splicing (AS) is the selection and ligation of specific exons during post-transcriptional modification.",
      "On average, each protein-coding gene has approximately four transcripts (# of ways of selecting and combining available exons).",
      "We would like to be able to predict splicing by discovering the instructions that control splicing  Computational Model of Splicing  By accurately modeling splicing and AS computationally, researchers have been able to predict how it is affected by variations in the genome, and then to assess whether a mutation in the genome affects disease risk.",
      "Computational Model of Protein-DNA and Protein-RNA binding  \u201cAccurate models of protein-sequence binding are essential for interpreting the genome and for predicting the effects of mutations\u2026Biologists have developed high-throughput experiments that measure the sequence specificity of individual proteins.\u201d  Example computational model: inputs = genomic sequence, outputs is a binding score.",
      "One would like to predict the \u201cmotifs\u201d, or patterns, that a particular protein binds to.",
      "Specific Discussion Related to Deep Learning  Deep Learning has been used to improve predictive performance- see Feedforward NNs for AS patterns .",
      "CNNs have been used to improve predictive performance for binding specificity .",
      "Cellular processes are highly stochastic and hence the genotype of an individual may not be sufficient to completely determine their phenotype  Measuring hundreds of thousands of cell variable measurements per patient for a small group of people potentially gives a better chance at deciphering the genomic instructions of the cell.",
      "More data for a model to learn from.",
      "Necessary to use \u201clarge-scale machine learning\u201d  RNNs can be useful for the following  genome annotation  Modelling of cell variable dynamics through time  Creating a sequential state model of protein binding based on RNNs or LSTMs  Imputation of epigenomic tracks - seq2seq  Machine Learning models need to be more interpret-able for genomics!",
      "Notes  Since this is my first foray into computational biology, I\u2019m going to keep track of a lot of terminology here:  1.",
      "Protein-coding genes describe how to build large molecules made from amino-acid chains (human genome contains ~20,000) 2.",
      "Non-coding genes describe how to build small molecules made from ribonucleic acid (RNA) chains (human genome contains ~25,000) 3.",
      "Information structures making up alternating regions on a typical gene are known as Introns and Exons  4.",
      "Protein-sequence binding is the binding of proteins to nucleotide sequences 5.",
      "Position-Frequency Matrix - \"workhorse of binding site modeling\"  Strengths  Excellent paper for Machine Learning researchers to get a first look at diving into genomics."
    ],
    "author_id": "pemami",
    "pdf_url": "https://persagen.com/files/misc/leung2016machine.pdf",
    "author_full_name": "Patrick Emami",
    "source_website": "https://pemami4911.github.io/index.html",
    "id": 3855110
  },
  {
    "blog_id": "dilated_residual_networks",
    "summary": [
      "What  They change standard ResNets to contain higher resolution feature maps (i.e. more height/width) and to use dilated convolutions .",
      "This makes them more useful for e.g. segmentation.",
      "They identify a gridding related problem and how to fix it.",
      "How  Changes to ResNet  ResNets are organized in five blocks of (each multiple) residual convolutions.",
      "They increase the feature map resolutions of the fourth and fifth block (to 2x and 4x).",
      "They increase simultanously the dilation of each convolution in the fourth and fifth block (to dilation 2 and 4).",
      "This is a common practice and known as the \"\u00e1 trous trick\".",
      "Degridding  When using dilated convolutions one can end up with grid-like patterns in the generated feature maps.",
      "This happens when the image has higher-frequency content than the sampling of the dilated convolution.",
      "The below image shows an example for a convolution with dilation 2 and equal weights for all 3x3 parameters:  These grid-like patterns make the network less useful for segmentation tasks.",
      "They fix the gridding problems with two steps:  They remove the max pooling at the start of the network and replace it with convolutions.",
      "This is done, because max pooling led to high-frequency high-amplitude components in their tests.",
      "They add convolutions with less or no dilation after the dilated convolutions (i.e. at the end of the network).",
      "These \"smoothen\" the feature maps, thereby fixing the grid-like patterns.",
      "They get three networks:  DRN-A-18: ResNet with 18 layers and dilation (2 in block 4 and 4 in block 5).",
      "DRN-B-26: Like DRN-A-18, but max pooling is replaced by four residual convolutions (in two blocks, each two convs).",
      "They also add four residual convolutions at the end of the network (in two blocks, each two convs).",
      "DRN-C-26: Like DRN-B-26, but the four convolutions at the end of the network are not residual.",
      "The motivation to use non-residual convolutions in DRN-C-26 is that residual ones can propagate the problems (grid-like patterns) more easily to the output.",
      "Visualization of the architectures:  Visualization of the effect of using max pooling (DRN-A-XX) and replacing it with convolutions (DRN-B-XX):  Results  Example visualizations of the generated feature maps by each network:  ImageNet classification  Their networks perform quite a bit better than ResNets of comparable size.",
      "The ranking of their networks seems to be: 1.",
      "DRN-C-XX, 2.",
      "DRN-B-XX, 3.",
      "DRN-C-XX.",
      "The difference between B and C isn't that big, but between A and B it is.",
      "To a degree this is expected, as B and C have four more convolutions than A (due to max pooling being replaced by convs).",
      "The effect though seems to be a bit stronger than just that.",
      "Their DRN-C-42 network is only a bit less accurate than ResNet-101.",
      "Object Localization  They suggest a method for bounding box detection without explicit training for that (i.e. when only training on ImageNet for classification).",
      "Sounds like they just \"try\" every possible bounding box within a range of heights/widths at every location in the final feature map.",
      "Then they pick the one with highest activation, if it is above a threshold.",
      "When applying that method to their networks and ResNets they get significantly better results with their networks.",
      "Not that surprising, considering their final feature map resolutions are four times higher than in ResNet.",
      "Semantic Segmentation  They apply their models to the Cityscapes dataset.",
      "They get a 70.9 mean IoU for DRN-C-42, while the reported best value for ResNet-101 is 66.6.",
      "No information regarding runtimes in the paper.",
      "Dilated ResNets are most likely going to run slower as they work with higher resolution feature maps.",
      "They will also require more RAM."
    ],
    "author_id": "ALEJU",
    "pdf_url": "https://arxiv.org/pdf/1705.09914",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 71370947
  },
  {
    "blog_id": "weight_normalization",
    "summary": [
      "What it is  Weight Normalization (WN) is a normalization technique, similar to Batch Normalization (BN).",
      "It normalizes each layer's weights.",
      "Differences to BN  WN normalizes based on each weight vector's orientation and magnitude.",
      "BN normalizes based on each weight's mean and variance in a batch.",
      "WN works on each example on its own.",
      "BN works on whole batches.",
      "WN is more deterministic than BN (due to not working an batches).",
      "WN is better suited for noisy environment (RNNs, LSTMs, reinforcement learning, generative models).",
      "(Due to being more deterministic.)",
      "WN is computationally simpler than BN.",
      "How its done  WN is a module added on top of a linear or convolutional layer.",
      "If that layer's weights are w then WN learns two parameters g (scalar) and v (vector, identical dimension to w) so that w = gv / ||v|| is fullfilled (||v|| = euclidean norm of v).",
      "g is the magnitude of the weights, v are their orientation.",
      "v is initialized to zero mean and a standard deviation of 0.05.",
      "For networks without recursions (i.e. not RNN/LSTM/GRU):  Right after initialization, they feed a single batch through the network.",
      "For each neuron/weight, they calculate the mean and standard deviation after the WN layer.",
      "They then adjust the bias to -mean/stdDev and g to 1/stdDev.",
      "That makes the network start with each feature being roughly zero-mean and unit-variance.",
      "The same method can also be applied to networks without WN.",
      "Results:  They define BN-MEAN as a variant of BN which only normalizes to zero-mean (not unit-variance).",
      "CIFAR-10 image classification (no data augmentation, some dropout, some white noise):  WN, BN, BN-MEAN all learn similarly fast.",
      "Network without normalization learns slower, but catches up towards the end.",
      "BN learns \"more\" per example, but is about 16% slower (time-wise) than WN.",
      "WN reaches about same test error as no normalization (both ~8.4%), BN achieves better results (~8.0%).",
      "WN + BN-MEAN achieves best results with 7.31%.",
      "Optimizer: Adam  Convolutional VAE on MNIST and CIFAR-10:  WN learns more per example und plateaus at better values than network without normalization.",
      "(BN was not tested.)",
      "Optimizer: Adamax  DRAW on MNIST (heavy on LSTMs):  WN learns significantly more example than network without normalization.",
      "Also ends up with better results.",
      "(Normal network might catch up though if run longer.)",
      "Deep Reinforcement Learning (Space Invaders):  WN seemed to overall acquire a bit more reward per epoch than network without normalization.",
      "Variance (in acquired reward) however also grew.",
      "Results not as clear as in DRAW.",
      "Optimizer: Adamax  Extensions  They argue that initializing g to exp(cs) (c constant, s learned) might be better, but they didn't get better test results with that.",
      "Due to some gradient effects, ||v|| currently grows monotonically with every weight update.",
      "(Not necessarily when using optimizers that use separate learning rates per parameters.)",
      "That grow effect leads the network to be more robust to different learning rates.",
      "Setting a small hard limit/constraint for ||v|| can lead to better test set performance (parameter updates are larger, introducing more noise).",
      "Performance of WN on CIFAR-10 compared to BN, BN-MEAN and no normalization.",
      "Performance of WN for DRAW (left) and deep reinforcement learning (right)."
    ],
    "author_id": "ALEJU",
    "pdf_url": "http://arxiv.org/pdf/1602.07868",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 70889889
  },
  {
    "blog_id": "self-normalizing_neural_networks",
    "summary": [
      "What  They suggest a variation of ELUs, which leads to networks being automatically normalized.",
      "The effects are comparable to Batch Normalization, while requiring significantly less computation (barely more than a normal ReLU).",
      "How  They define Self-Normalizing Neural Networks (SNNs) as neural networks, which automatically keep their activations at zero-mean and unit-variance (per neuron).",
      "SELUs  They use SELUs to turn their networks into SNNs.",
      "Formula:  with alpha = 1.6733 and lambda = 1.0507.",
      "They proof that with properly normalized weights the activations approach a fixed point of zero-mean and unit-variance.",
      "(Different settings for alpha and lambda can lead to other fixed points.)",
      "They proof that this is still the case when previous layer activations and weights do not have optimal values.",
      "They proof that this is still the case when the variance of previous layer activations is very high or very low and argue that the mean of those activations is not so important.",
      "Hence, SELUs with these hyperparameters should have self-normalizing properties.",
      "SELUs are here used as a basis because:  They can have negative and positive values, which allows to control the mean.",
      "They have saturating regions, which allows to dampen high variances from previous layers.",
      "They have a slope larger than one, which allows to increase low variances from previous layers.",
      "They generate a continuous curve, which ensures that there is a fixed point between variance damping and increasing.",
      "ReLUs, Leaky ReLUs, Sigmoids and Tanhs do not offer the above properties.",
      "Initialization  SELUs for SNNs work best with normalized weights.",
      "They suggest to make sure per layer that:  The first moment (sum of weights) is zero.",
      "The second moment (sum of squared weights) is one.",
      "This can be done by drawing weights from a normal distribution N(0, 1/n), where n is the number of neurons in the layer.",
      "Alpha-dropout  SELUs don't perform as well with normal Dropout, because their point of low variance is not 0.",
      "They suggest a modification of Dropout called Alpha-dropout.",
      "In this technique, values are not dropped to 0 but to alpha' = -lambda * alpha = -1.0507 * 1.6733 = -1.7581.",
      "Similar to dropout, activations are changed during training to compensate for the dropped units.",
      "Each activation x is changed to a(xd+alpha'(1-d))+b.",
      "d = B(1, q) is the dropout variable consisting of 1s and 0s.",
      "a = (q + alpha'^2 q(1-q))^(-1/2)  b = -(q + alpha'^2 q(1-q))^(-1/2) ((1-q)alpha')  They made good experiences with dropout rates around 0.05 to 0.1.",
      "Results  Note: All of their tests are with fully connected networks.",
      "No convolutions.",
      "Example training results:  Left: MNIST, Right: CIFAR10  Networks have N layers each, see legend.",
      "No convolutions.",
      "121 UCI Tasks  They manage to beat SVMs and RandomForests, while other networks (Layer Normalization, BN, Weight Normalization, Highway Networks, ResNet) perform significantly worse than their network (and usually don't beat SVMs/RFs).",
      "Tox21  They achieve better results than other networks (again, Layer Normalization, BN, etc.).",
      "They achive almost the same result as the so far best model on the dataset, which consists of a mixture of neural networks, SVMs and Random Forests.",
      "HTRU2  They achieve better results than other networks.",
      "They beat the best non-neural method (Naive Bayes).",
      "Among all tested other networks, MSRAinit performs best, which references a network withput any normalization, only ReLUs and Microsoft Weight Initialization (see paper: Delving deep into rectifiers: Surpassing human-level performance on imagenet classification)."
    ],
    "author_id": "ALEJU",
    "pdf_url": "https://arxiv.org/pdf/1706.02515",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 75952682
  },
  {
    "blog_id": "born-again-neural-networks",
    "summary": [
      "The paper explores knowledge distillation (KD) from the perspective of transferring knowledge between 2 networks of identical capacity.",
      "This is in contrast to much of the previous work in KD which has focused on transferring knowledge from a larger network to a smaller network.",
      "The paper reports that these Born Again Networks (BANs) outperform their teachers by significant margins in many cases.",
      "Approach  The standard KD setting is as follows:  Start with an untrained network (or ensemble of networks) and train them for the given task.",
      "This network is referred to as the teacher network.",
      "Now start with another untrained network (generally of smaller size than the teacher network) and train it using the output of the teacher network.",
      "This network is referred to as the student network.",
      "The paper augments this setting with an extra cross-entropy loss between the output of the teacher and the student networks.",
      "The student tried to predict the correct answer while matching the output distribution of the teacher.",
      "The resulting student network is referred to as BAN - Born Again Network.",
      "The same approach can be used multiple times (with diminishing returns) where the kth generation student is initialized by knowledge transfer from (k-1)th generation student.",
      "The output of multiple generation BANs are combined via averaging to produce BANE (Born Again Network Ensemble).",
      "Dark Knowledge  Hinton et al suggested that even when the output of the teacher network is incorrect, it contains useful information about the similarity between the output classes.",
      "This information is referred to as the \u201cdark knowledge\u201d.",
      "The current paper observed that the gradient of the correct output dimension during distillation and normal supervised training resembles the original gradient up to a  weight factor.",
      "This sample specific weight is defined by the value of the teacher\u2019s max output.",
      "This suggests distillation may be performing some kind of importance weighing.",
      "To explore this further, the paper considers 2 cases:  Confidence Weighted By Teacher Max (CWTM) - where each example in the student\u2019s loss function is weighted by the confidence that the teacher has on the prediction for that sample.",
      "The student incurs a higher loss if the teacher was more confident about the example.",
      "Dark Knowledge with Permuted Predictions (DKPP) - The non-argmax output of teacher\u2019s predictive distribution are permuted thus destroying the information about which output classes are related.",
      "The key effect of these variations is that the covariance between the output classes is lost and classical knowledge distillation would not be sufficient to explain improvements (if any).",
      "Experiments  Image Data  Datasets  CIFAR10  CIFAR100  Baselines  ResNets  DenseNets  BAN Variants  BAN-DenseNet and BAN-ResNet  - Train a sequence of 2 or 3 BANs using DenseNets and ResNets.",
      "Different variants constrain BANs to be similar to their teacher or penalize l2-distance between student and teacher activations etc.",
      "Two settings with CWTM and DKPP as explained earlier.",
      "BAN-Resnet with DenseNet teacher and BAN-DenseNet with ResNet teacher  Text Data  Datasets:  PTB Dataset  Baselines  CNN-LSTM model  BAN Variant  LSTM  Results  BAN student models improved over their teachers in most of the configurations.",
      "Training BANs across multiple generations leads to saturating improvements.",
      "The student models exhibit improvements even in the control settings (CWTM and DKPP).",
      "One reason could be that the permutation procedure did not remove the higher order moments of output distribution.",
      "Improvements in the CWTM model suggests that the pre-trained models can be used to rebalance the training set by giving lesser weight for samples where the teacher\u2019s output distribution is more spread."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1805.04770",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 60887948
  },
  {
    "blog_id": "batch-renorm",
    "summary": [
      "BatchNorm  Batch Renormalization is a follow-up to the 2015 paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .",
      "The original motivation for BatchNorm came from the fact that the distribution of the inputs to each layer in a deep network changes throughout training as the parameters change.",
      "Since this slows down training, the authors reasoned that normalization of these distributions should allow for the use of higher learning rates and increased insensitivity to initializations.",
      "BatchNorm achieves the same accuracy as previous networks but with signifcantly fewer training steps.",
      "BatchNorm is an added \u201clayer\u201d to deep networks placed after the output of the layer transformation (e.g., the convolution operation) but before the nonlinearity (e.g., a ReLu layer).",
      "During training, the sample estimates for the mean and variance of the layer\u2019s outputs are generated from mini-batch statistics.",
      "A moving average estimate is maintained as well, and is used during inference.",
      "Some key points about BatchNorm:  Normalization cannot be interleaved with gradient descent optimization.",
      "This is because the gradient descent optimization wouldn\u2019t be taking into account the fact that normalization takes place.",
      "BatchNorm requires backpropagation to compute derivatives for the normalization w.r.t.",
      "minibatch statistics; otherwise, model parameters explode without the loss decreasing.",
      "Full whitening of each layer\u2019s inputs is costly and not everywhere differentiable, so each scalar feature is whitened independently  To prevent BatchNorm from changing what the network can represent, parameters are introduced that allow the BatchNorm layer to represent the identity transform.",
      "These parameters are also optimized with SGD  BatchRenorm  BatchNorm came with pros and cons.",
      "It is less effective when the training minibatches are small or do not consist of independent samples.",
      "Small minibatches mean that the sample estimates of the mean and variance during training are less accurate.",
      "These inaccuracies are compounded with depth.",
      "For non-iid minibatches at train-time, BatchNorm will tend to overfit to the specific distribution of the examples; BatchRenorm aims to break up the dependence between similar samples.",
      "Therefore, the goal of BatchRenorm is to provide the model with the capacity to overcome differences in activitations between training and inference.",
      "Essentially  parameters $r$ and $d$ are introduced that relate the output of the normalizations between train time (computed with minibatch statistics) and inference (computed with population statistics for entire training set).",
      "If $\\mu$ is an estimate of the mean of some particular node $x$, and $\\sigma$ is an estimate of its standard deviation perhaps computed as a moving average over the last several minibatches, we have  $r$ and $d$ are held constant during backprop  This transform is identity in expectation, and BatchNorm is $r = 1$, $d = 0$.",
      "This allows the layers to observe the \u201ccorrect\u201d activiations that would be seen during inference.",
      "In practice, you start with BatchNorm for a few thousand training steps, then switch to BatchRenorm.",
      "Questions for discussion  What is the cost of BatchRenorm?",
      "Slightly more model complexity because of the added $r$ and $d$, and slightly more complex backprop equations?",
      "Hyper-parameter search needed for $r_max$ and $d_max$?",
      "Doesn\u2019t seem to always be necessary to use BatchRenorm - in real world problems, however, training data probably won\u2019t be \u201cnice\u201d and iid  BatchNorm for Recurrent Networks ?",
      "This was used in Deep Reinforcement Learning to stabilize policy gradient methods!"
    ],
    "author_id": "pemami",
    "pdf_url": "https://arxiv.org/pdf/1702.03275",
    "author_full_name": "Patrick Emami",
    "source_website": "https://pemami4911.github.io/index.html",
    "id": 89581601
  },
  {
    "blog_id": "wgan",
    "summary": [
      "Explanation for all the math  More explanation for all the math  What  They suggest a slightly altered algorithm for GANs.",
      "The new algorithm is more stable than previous ones.",
      "How  Each GAN contains a Generator that generates (fake-)examples and a Discriminator that discriminates between fake and real examples.",
      "Both fake and real examples can be interpreted as coming from a probability distribution.",
      "The basis of each GAN algorithm is to somehow measure the difference between these probability distributions and change the network parameters of G so that the fake-distribution becomes more and more similar to the real distribution.",
      "There are multiple distance measures to do that:  Total Variation (TV)  KL-Divergence (KL)  Jensen-Shannon divergence (JS)  This one is based on the KL-Divergence and is the basis of the original GAN, as well as LAPGAN and DCGAN.",
      "Earth-Mover distance (EM), aka Wasserstein-1  Intuitively, one can imagine both probability distributions as hilly surfaces.",
      "EM then reflects, how much mass has to be moved to convert the fake distribution to the real one.",
      "Ideally, a distance measure has everywhere nice values and gradients (e.g. no +/- infinity values; no binary 0 or 1 gradients; gradients that get continously smaller when the generator produces good outputs).",
      "In that regard, EM beats JS and JS beats TV and KL (roughly speaking).",
      "So they use EM.",
      "EM  EM is defined as  (inf = infinum, more or less a minimum)  which is intractable, but following the Kantorovich-Rubinstein duality it can also be calculated via  (sup = supremum, more or less a maximum)  However, the second formula is here only valid if the network is a K-Lipschitz function (under every set of parameters).",
      "This can be guaranteed by simply clipping the discriminator's weights to the range [-0.01, 0.01].",
      "Then in practice the following version of the tractable EM is used, where w are the parameters of the discriminator:  The full algorithm is mostly the same as for DCGAN:  Line 2 leads to training the discriminator multiple times per batch (i.e. more often than the generator).",
      "This is similar to the max w in W in the third formula (above).",
      "This was already part of the original GAN algorithm, but is here more actively used.",
      "Because of the EM distance, even a \"perfect\" discriminator still gives good gradient (in contrast to e.g. JS, where the discriminator should not be too far ahead).",
      "So the discriminator can be safely trained more often than the generator.",
      "Line 5 and 10 are derived from EM.",
      "Note that there is no more Sigmoid at the end of the discriminator!",
      "Line 7 is derived from the K-Lipschitz requirement (clipping of weights).",
      "High learning rates or using momentum-based optimizers (e.g. Adam) made the training unstable, which is why they use a small learning rate with RMSprop.",
      "Results  Improved stability.",
      "The method converges to decent images with models which failed completely when using JS-divergence (like in DCGAN).",
      "For example, WGAN worked with generators that did not have batch normalization or only consisted of fully connected layers.",
      "Apparently no more mode collapse.",
      "(Mode collapse in GANs = the generator starts to generate often/always the practically same image, independent of the noise input.)",
      "There is a relationship between loss and image quality.",
      "Lower loss (at the generator) indicates higher image quality.",
      "Such a relationship did not exist for JS divergence.",
      "Example images:"
    ],
    "author_id": "ALEJU",
    "pdf_url": "https://arxiv.org/pdf/1701.07875",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 62302416
  },
  {
    "blog_id": "improving-cloud-service-resilience-using-brownout-aware-load-balancing",
    "summary": [
      "Improving Cloud Service Resilience using Brownout-aware Load Balancing \u2013 Klein et al 2014  This is what the previous two #themorningpaper selections have been building to.",
      "What happens when you apply brownout techniques to a set of load-balanced servers?",
      "We study how to extend the classical cloud service architecture composed of a load-balancer and replicas with a recently proposed self-adaptive paradigm called brownout.",
      "The following diagram (click to see larger view) tells a compelling story.",
      "Taking a load-balanced system with 5 servers (replicas) numbered 0-4, one replica is killed every 100 seconds until only 1 remains, then the replicas are brought back online one-at-a-time, also at 100 second intervals.",
      "The request rate is held constant throughout.",
      "The top plot shows what happens without brownout, and the bottom chart shows what happens with it.",
      "Note that without brownout controls in place, a large number of requests start timing out after the third replica has failed.",
      "These timeouts continue even when replicas are restored.",
      "This is attributed to \u2018rotten\u2019 requests that have timed out from the user perspective but are still active on the server-side \u2013 thus e.g. the database is wasting all its time on transactions that either will time out, or have already timed-out on the user side.",
      "The bottom plot shows the dramatic difference made to this particular system by applying brownout controls.",
      "The red line shows the automatically controlled setting of the dimmer switch (altering the ratio of responses that include optional content to those that don\u2019t).",
      "The blue line again shows the number of requests that time out: a dramatic improvement in user experience and system resiliency.",
      "It would be great to see a further study that also includes circuit-breakers.",
      "The basic brownout technique as applied to an individual server/replica we covered in the previous edition of #themorningpaper.",
      "If you are using a load-balancer that depends in some way on response times, the adaptive behaviour arising from the brownout dimmer switch may confuse it.",
      "The issue is to ensure that replica self-adaptivity would not confuse the load-balancing algorithm, overloading replicas that are already struggling with capacity shortage.",
      "Recall that for a brownout-compliant service, the mandatory part of the response is always computed, but the optional part is computed only with a certain probability governed by a control variable called the dimmer.",
      "Klein et.",
      "al.",
      "piggy-back the current dimmer-switch value from a replica onto the normal response flow so that it can be used in load-balancing decisions.",
      "Compared to the near-optimal (for non-adaptive workloads) Join the Shortest Queue (JSQ) algorithm,  results show that the resulting service can tolerate more replica failures and that the novel load-balancing algorithms improve the number of requests served with optional content by up to 5%.",
      "From the charts earlier therefore, a lot of the gains come from simply having brownout controlled replicas.",
      "The load-balancing enhancements are then the icing on the cake.",
      "Two algorithms are explored: the first is a variant of a PI controller which adjusts queue-offsets (for a base JSQ algorithm) based on dimmer switch settings; the second strives to keep the dimmer value the same across all replicas.",
      "In summary, adding brownout to a replicated service improves its resilience, even when using a brownout-unaware load-balancing algorithm\u2026 However, we observed that in scenarios featuring capacity heterogeneity, our algorithms performed better than shortest queue first (JSQ) with respect to the optional content ratio."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://people.cs.umu.se/hernandf/pubs/srds2014-preprint.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 51458146
  },
  {
    "blog_id": "imagination-augmented-agents-for-deep-reinforcement-learning",
    "summary": [
      "The paper presents I2A (Imagination Augmented Agent) that combines the model-based and model-free approaches leading to data efficiency and robustness even with imperfect models.",
      "I2A agent uses the predictions from a learned environment model as an additional context in deep policy networks.",
      "This leads to improved data efficiency and robustness to imperfect models.",
      "I2A agent has two main modules - Imagination module and the Policy module.",
      "Imagination Module  Environment Model  This is a recurrent model, trained in an unsupervised manner using the agent trajectories.",
      "It can be used to predict the future state given the current state and action.",
      "The environment model can be rolled out multiple times to obtain a simulated trajectory or an \u201cimagined\u201d trajectory.",
      "During each rollout, the actions are chosen using a rollout policy \u03c0r.",
      "Rollout Encoder  A rollout encoder E (LSTM) is used to process the entire imagined rollout.",
      "The imagination module is used to generate n trajectories.",
      "Each trajectory is a sequence of outputs of the environment model.",
      "These n trajectories are concatenated into a single \u201cimagination\u201d vector.",
      "The training data for the environment model is generated from trajectories of a partially trained model-free agent.",
      "Pretraining the environment model (instead of joint training with policy) leads to faster runtime.",
      "Policy Module  This module uses the output of both model-based path and model-free path as its input.",
      "It generates the policy vector and value function.",
      "Rollout Strategy  One rollout is performed for each possible action in the environment ie, the first action in the ith rollout is the ith action in the action set.",
      "Subsequent actions are generated using a shared rollout policy \u03c0\u2019  An effective strategy was to create a small model-free network \u03c0\u2019(ot) and then add a KL loss component that encourages \u03c0\u2019(ot)to be similar to the imagination augmented policy \u03c0(ot).",
      "Baselines  Model-free agent  Copy-model agent - same as I2A but the environment model is replaced by a \u201ccopy\u201d model that just returns the input observations.",
      "Environments  Sokoban  Task is to push a number of boxes onto given target locations.",
      "I2A outperforms the baselines and gains in performance as the number of unrolling steps increases (though at a diminishing rate).",
      "In case of poor environment models, the agent seems to be able to ignore the later part of the rollout when the error starts to accumulate.",
      "Monte Carlo search algorithm (without an explicit rollout encoder) performed poorly as compared to the model using rollout encoder.",
      "Predicting the reward along with value function and action seems to speed up training.",
      "If a near-perfect model is available, I2A agent\u2019s performance can be improved by performing Monte Carlo search with the trained I2A agent for the rollout policy.",
      "The agent plays entire episodes in simulation and tries to find a successful action sequence within 10 retries.",
      "MiniPacman  I2A agent is evaluated to see if a single model can be used to solve multiple tasks.",
      "A new environment is designed to define multiple tasks in an environment with shared state transitions.",
      "Each task is specified by a 5-dimensional reward vector that associates a reward with moving, eating food, eating a pill, eating a ghost and being eaten by a ghost.",
      "A single environment model is trained to predict both observations (frames) and events (eg \u201ceating a pill\u201d).",
      "This way, the environment model is shared across all tasks.",
      "Baseline agents and I2As are trained on each task separately.",
      "I2A architecture outperforms the standard agent in all tasks and the copy-model baseline in all but one task.",
      "The improvement in performance is higher for tasks where rewards are sparse and where the anticipation of ghost dynamics is especially important indicating that the I2A agent can use the environment model to explore the environment more effectively."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1707.06203",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 6777051
  },
  {
    "blog_id": "two-stage-synthesis-networks-for-transfer-learning-in-machine-comprehension",
    "summary": [
      "The paper proposes a two-stage synthesis network that can perform transfer learning for the task of machine comprehension.",
      "The problem is the following:  We have a domain DS for which we have labelled dataset of question-answer pairs and another domain DT for which we do not have any labelled dataset.",
      "We use the data for domain DS to train SynNet and use that to generate synthetic question-answer pairs for domain DT.",
      "Now we can train a machine comprehension model M on DS and finetune using the synthetic data for DT.",
      "SynNet  Works in two stages:  Answer Synthesis - Given a text paragraph, generate an answer.",
      "Question Synthesis - Given a text paragraph and an answer, generate a question.",
      "Answer Synthesis Network  Given the labelled dataset for DS, generate a labelled dataset of <word, tag> pair such that each word in the given paragraph is assigned one of the 4 tags:  IOBstart - if it is the starting word of an answer  IOBmid - if it is the intermediate word of an answer  IOBend - if it is the ending word of an answer  IOBnone - if it is not part of any answer  For training, map the words to their GloVe embeddings and pass through a Bi-LSTM.",
      "Next, pass them through two-FC layers followed by a softmax layer.",
      "For the target domain DT, all the consecutive word spans where no label is IOBnone are returned as candidate answers.",
      "Question Synthesis Network  Given an input paragraph and a candidate answer, Question Synthesis network generates question one word at a time.",
      "Map each word in the paragraph to their GloVe embedding.",
      "After the word vector, append a \u20181\u2019 if the word was part of the candidate answer else append a \u20180\u2019.",
      "Feed to a Bi-LSTM network (encoder-decoder) where the decoder conditions on the representation generated by the encoder as well as the question tokens generated so far.",
      "Decoding is stopped when \u201cEND\u201d token is produced.",
      "The paragraph may contain some named entities or rare words which do not appear in the softmax vocabulary.",
      "To account for such words, a copying mechanism is also incorporated.",
      "At each time step, a Pointer Network (CP) and a Vocabulary Predictor (VP) are used to generate probability distribution for the next word and a Latent Predictor Network is used to decide which of the two networks would be used for the prediction.",
      "At inference time, a greedy decoding is used where the most likely predictor is chosen and then the most likely word from that predictor is chosen.",
      "Machine Comprehension Model  Given any MC model, first train it over domain DS and then fine-tune using the artificial questions generated using DT.",
      "Implementation Details  Data Regularization - There is a need to alternate between mini batches from source and target domain while fine-tuning the MC model.",
      "At inference time, the fine-tuned MC model is used to get the distribution P(i=start) and P(i=end) (corresponding to the likelihood of choosing word I as the starting or ending word for the answer) for all the words and DP is used to find the optimal answer span.",
      "Checkpoint Averaging - Use the different checkpointed models to average the answer likelihood before running DP.",
      "Using the synthetically generated dataset helps to gain a 2% improvement in terms of F-score (from SQuAD -> NewsQA).",
      "Using checkpointed models further improves the performance to overall 46.6% F score which closes the gap with respect to the performance of model trained on NewsQA itself (~52.3% F score)"
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1706.09789.pdf",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 42443464
  },
  {
    "blog_id": "1609.06480",
    "summary": [
      "In this paper they prior the representation a logistic regression model using known protein-protein interactions.",
      "They do so by regularizing the weights of the model using the Laplacian encoding of a graph.",
      "Here is a regularization term of this form:  $$\\lambda ||w||_1 + \\eta w^T L w,$$  #### A small example:  Given a small graph of three nodes A, B, and C with one edge: {A-B} we have the following Laplacian:  $$ L = D - A =  \\left[\\array{ 1 & 0 & 0 \\\\ 0 & 1 & 0\\\\ 0 & 0 & 0}\\right] - \\left[\\array{ 0 & 1 & 0 \\\\ 1 & 0 & 0\\\\ 0 & 0 & 0}\\right]$$  $$L =  \\left[\\array{ 1 & -1 & 0 \\\\ -1 & 1 & 0\\\\ 0 & 0 & 0}\\right] $$  If we have a small linear regression of the form:  $$y = x_Aw_A + x_Bw_B + x_Cw_C$$  Then we can look at how $w^TLw$ will impact the weights to gain insight:  $$w^TLw $$  $$= \\left[\\array{ w_A & w_B & w_C}\\right] \\left[\\array{ 1 & -1 & 0 \\\\ -1 & 1 & 0\\\\ 0 & 0 & 0}\\right] \\left[\\array{ w_A \\\\ w_B \\\\ w_C}\\right]  $$  $$=  \\left[\\array{ w_A & w_B & w_C}\\right] \\left[\\array{ w_A -w_B \\\\ -w_A + w_B \\\\ 0}\\right]  $$    $$ =  (w_A^2 -w_Aw_B ) +  (-w_Aw_B + w_B^2) $$  So because all terms are squared we can remove them from consideration to look at what is the real impact of regularization.",
      "$$ =  (-w_Aw_B ) +  (-w_Aw_B) $$  $$ = -2w_Aw_B$$  The Laplacian regularization seems to increase the weight values of edges which are connected.",
      "Along with the squared terms and the $L1$ penalty that is also used the weights cannot grow without bound.",
      "#### A few more experiments:  If we perform the same computation for a graph with two edges: {A-B, B-C} we have the following term which increases the weights of both pairwise interactions:  $$ = -2w_Aw_B -2w_Bw_C$$  If we perform the same computation for a graph with two edges: {A-B, A-C} we have no surprises:   $$ = -2w_Aw_B -2w_Aw_C$$  Another thing to think about is if there are no edges.",
      "If by default there are self-loops then the degree matrix will have 1 on the diagonal and it will be the identity which will be an $L2$ term.",
      "If no self loops are defined then the result is a 0 matrix yielding no regularization at all.",
      "#### Contribution:  A contribution of this paper is to use the absolute value of the weights to make training easier.",
      "$$|w|^T L |w|$$  TODO: Add more about how this impacts learning.",
      "#### Overview  Here a high level figure shows the data and targets together with a graph prior.",
      "It looks nice so I wanted to include it.",
      "[url]"
    ],
    "author_id": "joecohen",
    "pdf_url": "http://arxiv.org/pdf/1609.06480v1",
    "author_full_name": "Joseph Cohen",
    "source_website": "https://www.shortscience.org/user?name=joecohen",
    "id": 27086937
  },
  {
    "blog_id": "end_to_end_learning_for_self-driving_cars",
    "summary": [
      "What  They describe a model that automatically steers cars on roads.",
      "The model runs in realtime.",
      "The model is trained from images and corresponding (correct) steering wheel angles.",
      "How  Architecture  Their model is just a standard CNN with a few convolutional and fully connected layers.",
      "They have a single output: the correct steering wheel angle predicted by the model.",
      "(The model does not predict whether to accelerate or brake.)",
      "To be more precise: The steering wheel angle is predicted as 1/r, where r is the turning radius of the car.",
      "This way, the prediction is more independent of the used car.",
      "They use 1/r instead of r in order to avoid getting infinity when driving straight.",
      "Their input images are in YUV color space.",
      "They use a hard coded (not learned) normalization layer (no further explanations in paper).",
      "Visualization:  Data collection  They drive on roads with cars, collecting photos and corresponding steering wheel angles.",
      "They drive on different road settings (e.g. highway, tunnels, residential roads) and weather (sunny, cloudy, foggy, ...).",
      "They collected about 72 hours of driving.",
      "They annotate each example with the road center location, road type and lane switching information (\"stays in lane\"/\"is switching lanes\").",
      "Data augmentation  Training on just the collected images is not going to work, because they only show correct driving.",
      "Sooner or later the model would make a mistake and end up going slightly off road.",
      "This would then be a situation that it has never seen before and the predicted output becomes more or less random, leading to crashes.",
      "They attached two more cameras to the cars during data collection, one pointing to the left and one to the right.",
      "They use these images as examples of bad situations and change the ground truth steering wheel angle so that it would steer the car back onto the road within 2 seconds.",
      "They also generate additional images between the center and left/right cameras.",
      "They do this by viewpoint transformation and assume that all point above the horizon line are infinitely far away, while all below the horizon line are on flat grounds.",
      "(No further explanation here on how they transform images and annotations exactly.",
      "And where do they get the horizon line from?)",
      "They also seem to apply random rotations and translations to the images (or maybe that is meant by viewpoint transformations?).",
      "Training  They only train on examples where the driver stays in a lane (as opposed to switching lanes).",
      "They subsample the input at 10fps (collection was at 30fps) in order to not get too similar examples.",
      "Training and application happens in Torch7.",
      "The system can predict at 30fps.",
      "Results  Simulation  They simulate on the collected data how the model would steer.",
      "I.e. they predict steering wheel angles and then transform future images as if that angle had been chosen.",
      "They measure how far the simulated car would be away from the road center.",
      "They measure an virtual, human intervention (of 6 seconds) whenever the car gets too far away from the center.",
      "They can then estimate the simulated time that the car would have been autonomous.",
      "The actual number though is nowhere in the paper.",
      "x(  They drive in Monmouth County, being autonomous roughly 98% of the time.",
      "They drive 16.09km on Garden State Parkway, being autonomous 100% of the time."
    ],
    "author_id": "ALEJU",
    "pdf_url": "https://arxiv.org/pdf/1604.07316",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 55410278
  },
  {
    "blog_id": "adversarially_learned_inference",
    "summary": [
      "What  They suggest a new architecture for GANs.",
      "Their architecture adds another Generator for a reverse branch (from images to noise vector z).",
      "Their architecture takes some ideas from VAEs/variational neural nets.",
      "Overall they can improve on the previous state of the art (DCGAN).",
      "How  Architecture  Usually, in GANs one feeds a noise vector z into a Generator (G), which then generates an image (x) from that noise.",
      "They add a reverse branch (G2), in which another Generator takes a real image (x) and generates a noise vector z from that.",
      "The noise vector can now be viewed as a latent space vector.",
      "Instead of letting G2 generate discrete values for z (as it is usually done), they instead take the approach commonly used VAEs and use continuous variables instead.",
      "That is, if z represents N latent variables, they let G2 generate N means and N variances of gaussian distributions, with each distribution representing one value of z.",
      "So the model could e.g. represent something along the lines of \"this face looks a lot like a female, but with very low probability could also be male\".",
      "Training  The Discriminator (D) is now trained on pairs of either (real image, generated latent space vector) or (generated image, randomly sampled latent space vector) and has to tell them apart from each other.",
      "Both Generators are trained to maximally confuse D.  G1 (from z to x) confuses D maximally, if it generates new images that (a) look real and (b) fit well to the latent variables in z (e.g. if z says \"image contains a cat\", then the image should contain a cat).",
      "G2 (from x to z) confuses D maximally, if it generates good latent variables z that fit to the image x.",
      "Continuous variables  The variables in z follow gaussian distributions, which makes the training more complicated, as you can't trivially backpropagate through gaussians.",
      "When training G1 (from z to x) the situation is easy: You draw a random z-vector following a gaussian distribution (N(0, I)).",
      "(This is basically the same as in \"normal\" GANs.",
      "They just often use uniform distributions instead.)",
      "When training G2 (from x to z) the situation is a bit harder.",
      "Here we need to use the reparameterization trick here.",
      "That roughly means, that G2 predicts the means and variances of the gaussian variables in z and then we draw a sample of z according to exactly these means and variances.",
      "That sample gives us discrete values for our backpropagation.",
      "If we do that sampling often enough, we get a good approximation of the true gradient (of the continuous variables).",
      "(Monte Carlo approximation.)",
      "Results  Images generated based on Celeb-A dataset:  Left column per pair: Real image, right column per pair: reconstruction (x -> z via G2, then z -> x via G1)  Reconstructions of SVHN, notice how the digits often stay the same, while the font changes:  CIFAR-10 samples, still lots of errors, but some quite correct:"
    ],
    "author_id": "ALEJU",
    "pdf_url": "http://arxiv.org/pdf/1606.00704",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 50538257
  },
  {
    "blog_id": "reasonet-learning-to-stop-reading-in-machine-comprehension",
    "summary": [
      "In the domain of machine comprehension, making multiple passes over the given document is an effective technique to extract the relation between the given passage, question and answer.",
      "Unlike previous approaches, which perform a fixed number of passes over the passage, Reasoning Network (ReasoNet) uses reinforcement learning (RL) to decide how many times a document should be read.",
      "Every time the document is read, ReasoNet determines whether the document should be read again or has the termination state been reached.",
      "If termination state is reached, the answer module is triggered to generate the answer.",
      "Since the termination state is discrete and not connected to the final output, RL approach is used.",
      "Datasets  CNN, DailyMail Dataset  SQuAD  Graph Reachability Dataset  2 synthetic datasets to test if the network can answer questions like \u201cIs node_1 connected to node_12\u201d?",
      "Architecture  Memory (M) - Comprises of the vector representation of the document and the question (encoded using GRU or other RNNs).",
      "Attention - Attention vector (xt) is a function of current internal state st and external memory M. The state and memory are passed through FCs and fed to a similarity function.",
      "Internal State (st) - Vector representation of the question state computed by a RNN using the previous internal state and the attention vector xt  Termination Gate (Tt) - Uses a logistic regression model to generate a random binary variable using the current internal state st.  Answer - Answer module is triggered when Tt = 1.",
      "For CNN and DailyMail, a linear projection of GRU outputs is used to predict the answer from candidate entities.",
      "For SQuAD, the position of the first and the last word from the answer span are predicted.",
      "For Graph Reachability, a logistic regression module is used to predict yes/no as the answer.",
      "Reinforcement Learning - For the RL setting, reward at time t, rt = 1 if Tt = 1 and answer is correct.",
      "Otherwise rt = 0  Workflow - Given a passage p, query q and answer a:  Extract memory using p  Extract initial hidden state using q  ReasoNet executes all possible episodes that can be enumerated by setting an upper limit on the number of passes.",
      "These episodes generate actions and answers that are used to train the ReasoNet.",
      "Result  CNN, DailyMail Corpus  ReasoNet outperforms all the baselines which use fixed number of reasoning steps and could benefit by capturing the word alignment signals between query and passage.",
      "SQuAD  At the time of submission, ReasoNet was ranked 2nd on the SQuAD leaderboard and as of 9th July 2017, it is ranked 4th.",
      "Graph Reachability Dataset  ReasoNet - Standard ReasoNet as described above.",
      "ReasoNet-Last - Use the prediction from the Tmax  ReasoNet > ReasoNet-Last > Deep LSTM Reader  ReasoNet converges faster than ReasoNet-Last indicating that the terminate gate is useful.",
      "Notes  As such there is nothing discouraging the ReasoNet to make unnecessary passes over the passage.",
      "In fact, the modal value of the number of passes = upper bound on the number of passes.",
      "This effect is more prominent for large graph indicating that the ReasoNet may try to play safe by performing extra passes.",
      "It would be interesting to see if the network can be discouraged from making unnecessary passed by awarding a small negative reward for each pass."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1609.05284",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 37171167
  },
  {
    "blog_id": "emotional-chatting-machine-emotional-conversation-generation-with-internal-and-external-memory",
    "summary": [
      "The paper proposes ECM (Emotional Chatting Machine) which can generate both semantically and emotionally appropriate responses in a dialogue setting.",
      "More specifically, given an input utterance or dialogue and the desired emotional category of the response, ECM is to generate an appropriate response that conforms to the given emotional category.",
      "Much of the recent, deep learning based work on conversational agents has focused on the use of encoder-decoder framework where the input utterance (given sequence of words) is mapped to a response utterance (target sequence of words).",
      "This is the so-called seq2seq family of models.",
      "ECM model can sit within this framework and introduces 3 new components:  Emotion Category Embedding  Embed the emotion categories into a real-valued, low-dimensional vector space.",
      "These embeddings are used as input to the decoder and are learnt along with rest of the model.",
      "Internal Memory  Physiological, emotional responses are relatively short-lived and involve changes.",
      "ECM accounts for this effect by adding an Internal Memory which captures this dynamics of emotions during decoding.",
      "It starts with \u201cfull\u201d emotions in the beginning and keeps decaying the emotion value over time.",
      "How much of the emotion value is to be decayed is determined by a sigmoid gate.",
      "By the time the sentence is decoded, the value becomes zero, signifying that the emotion has been completely expressed.",
      "External Memory  Emotional responses are expected to carry emotionally strong words along with generic, neutral words.",
      "An external memory is used to include the emotionally strong words explicitly by using 2 non-overlapping vocabularies - generic vocabulary and the emotion vocabulary (read from the external memory).",
      "Both these vocabularies are assigned different generation probabilities and an output gate controls the weights of generic and emotion words.",
      "This way the emotion words are included in an otherwise neutral response.",
      "Loss function  The first component is the cross-entropy loss between predicted and target token distribution.",
      "A regularization term on internal memory to make sure the emotional state decays to 0 at the end of the decoding process.",
      "Another regularization term on external memory to supervise the probability of selection of a generic vs emotion word.",
      "*Dataset  STC Dataset (~220K posts and ~4300K responses) annotated by the emotional classifier.",
      "Any error on the part of the classifier degrades the quality of the training dataset.",
      "NLPCC Dataset - Emotion classification dataset with 23105 sentences.",
      "Metric  Perplexity to evaluate the model at the content level.",
      "Emotion accuracy to evaluate the model at the emotional level.",
      "ECM achieves a perplexity of 65.9 and emotional accuracy of 0.773.",
      "Based on human evaluations, ECM statistically outperforms the seq2seq baselines on both naturalness (likeliness of response being generated by a human) and emotion accuracy.",
      "Notes  It is an interesting idea to let the sigmoid gate decide how the emotion \u201cvalue\u201d be spent while decoding.",
      "It seems similar to the idea of how much do we want to \u201cattend\u201d to the emotion value the key difference being that your total attention is limited.",
      "It would be interesting to see the shape of the distribution of how much of the emotion value is spent at each decoding time step.",
      "If the curve is highly biased towards say using most of the emotion value towards the end of the decoding process, maybe another regularisation term is needed to ensure a more balanced distribution of how the emotion is spent."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1704.01074",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 90600191
  },
  {
    "blog_id": "analysis-of-join-the-shortest-queue-routing",
    "summary": [
      "Analysis of join-the-shortest queue routing for web server farms \u2013 Gupter et al 2007  What\u2019s the best way to balance web requests across a set of servers?",
      "Round-robin is the simple algorithm that everyone knows best, but there is a better way\u2026 This paper analyzes the Join the Shortest Queue (JSQ) routing policy and shows that it delivers near-optimal results.",
      "It is assumed that the load balancer (dispatcher) immediately routes requests to one of the servers in the farm, and that servers equally split their capacity over the requests they are processing.",
      "This is known a a processor sharing (PS) scheduling policy.",
      "We are thus interested in an PS server farm with immediate dispatch.",
      "Although the paper considers the serving of static web pages, the immediate dispatch and PS server policy are equally applicable in the case of web apps and REST APIs etc.",
      "The JSQ routing policy is widely used in commercial dispatchers.",
      "Under JSQ, an incoming request is routed to the server with the least number of unfinished requests.",
      "Thus, JSQ strives to balance load across the servers reducing the probability of one server having several jobs while another server sits idle.",
      "From the point of view of a new arrival it is a greedy policy for the case of PS servers, because the arrival would prefer sharing with as few jobs as possible.",
      "Gupter et al. build a queueing model conditioned by the average request arrival rate (assume a Poisson distribution), number of servers, and (mean) time taken by a job to run on a server in isolation.",
      "In the typical queueing theory description, this is an M/G/K/JSQ/PS model : Poisson arrival rate M, General distribution of service processing times, K servers, JSQ dispatching, and PS serving.",
      "Despite the ubiquity of JSQ/PS server farms, no-one has yet analyzed the performance of JSQ in this setting.",
      "The full analysis is heavy reading at points, but thankfully the key results are all summarized for us.",
      "Firstly it is shown that a single queue approximation (focusing on what happens at just one of the servers) can be used to understand the behaviour of the system as a whole, and furthermore that this approximation is exact if job-size distribution is exponential.",
      "Does the behaviour of the system depend on the distribution of job sizes (I.e. the mix of long and short time to service requests)?",
      "The JSQ/PS system shows near insensitivity to the variability of the job-size distribution\u2026 This is a non-trivial result since very similar routing policies for PS server farms, like Least-Work-Left (sending the job to the host with the least total work), or Round-Robin, are highly sensitive to the job-size distribution.",
      "The model was tested against extensive simulations and found to be highly accurate:  \u2026our analytical approximation method is always within 2.5% of simulation estimates for mean queue length and response time, under all job-size distributions examined.",
      "(which also means you could use this model for capacity planning and/or response time estimations).",
      "So JSQ has some very nice properties, but is it actually a good choice for your load-balancing algorithm?",
      "We show, via simulation, that it is unlikely there is a routing policy that outperforms JSQ by more than about 10%  So there you have it: easy to implement, stable under a wide variety of request distributions, useful for modelling, and near-optimal.",
      "If you need to implement a load-balancing function for PS servers, JSQ is the way to go\u2026"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://www.cs.cmu.edu/~harchol/Papers/peva07.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 54139177
  },
  {
    "blog_id": "rank_ordered_autoencoders",
    "summary": [
      "What  Autoencoders typically have some additional criterion that pushes them towards learning meaningful representations.",
      "E.g. L1-Penalty on the code layer (z), Dropout on z, Noise on z.",
      "Often, representations with sparse activations are considered meaningful (so that each activation reflects are clear concept).",
      "This paper introduces another technique that leads to sparsity.",
      "They use a rank ordering on z.",
      "The first (according to the ranking) activations have to do most of the reconstruction work of the data (i.e. image).",
      "How  Basic architecture:  They use an Autoencoder architecture: Input -> Encoder -> z -> Decoder -> Output.",
      "Their encoder and decoder seem to be empty, i.e. z is the only hidden layer in the network.",
      "Their output is not just one image (or whatever is encoded), instead they generate one for every unit in layer z.",
      "Then they order these outputs based on the activation of the units in z (rank ordering), i.e. the output of the unit with the highest activation is placed in the first position, the output of the unit with the 2nd highest activation gets the 2nd position and so on.",
      "They then generate the final output image based on a cumulative sum.",
      "So for three reconstructed output images I1, I2, I3 (rank ordered that way) they would compute final image = I1 + (I1+I2) + (I1+I2+I3).",
      "They then compute the error based on that reconstruction (reconstruction - input image) and backpropagate it.",
      "Cumulative sum:  Using the cumulative sum puts most optimization pressure on units with high activation, as they have the largest influence on the reconstruction error.",
      "The cumulative sum is best optimized by letting few units have high activations and generate most of the output (correctly).",
      "All the other units have ideally low to zero activations and low or no influence on the output.",
      "(Though if the output generated by the first units is wrong, you should then end up with an extremely high cumulative error sum...)  So their z coding should end up with few but high activations, i.e. it should become very sparse.",
      "The cumulative generates an individual error per output, while an ordinary sum generates the same error for every output.",
      "They argue that this \"blurs\" the error less.",
      "To avoid blow ups in their network they use TReLUs, which saturate below 0 and above 1, i.e. min(1, max(0, input)).",
      "They use a custom derivative function for the TReLUs, which is dependent on both the input value of the unit and its gradient.",
      "Basically, if the input is >1 (saturated) and the error is high, then the derivative pushes the weight down, so that the input gets into the unsaturated regime.",
      "Similarly for input values <0 (pushed up).",
      "If the input value is between 0 and 1 and/or the error is low, then nothing is changed.",
      "They argue that the algorithmic complexity of the rank ordering should be low, due to sorts being O(n log(n)), where n is the number of hidden units in z.",
      "Results  They autoencode 7x7 patches from CIFAR-10.",
      "They get very sparse activations.",
      "Training and test loss develop identically, i.e. no overfitting."
    ],
    "author_id": "ALEJU",
    "pdf_url": "https://arxiv.org/pdf/1605.01749",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 60516406
  },
  {
    "blog_id": "planet",
    "summary": [
      "What  They describe a convolutional network that takes in photos and returns where (on the planet) these photos were likely made.",
      "The output is a distribution over locations around the world (so not just one single location).",
      "This can be useful in the case of ambiguous images.",
      "How  Basic architecture  They simply use the Inception architecture for their model.",
      "They have 97M parameters.",
      "Grid  The network uses a grid of cells over the planet.",
      "For each photo and every grid cell it returns the likelihood that the photo was made within the region covered by the cell (simple softmax layer).",
      "The naive way would be to use a regular grid around the planet (i.e. a grid in which all cells have the same size).",
      "Possible disadvantages:  In places where lots of photos are taken you still have the same grid cell size as in places where barely any photos are taken.",
      "Maps are often distorted towards the poles (countries are represented much larger than they really are).",
      "This will likely affect the grid cells too.",
      "They instead use an adaptive grid pattern based on S2 cells.",
      "S2 cells interpret the planet as a sphere and project a cube onto it.",
      "The 6 sides of the cube are then partitioned using quad trees, creating the grid cells.",
      "They don't use the same depth for all quad trees.",
      "Instead they subdivide them only if their leafs contain enough photos (based on their dataset of geolocated images).",
      "They remove some cells for which their dataset does not contain enough images, e.g. cells on oceans.",
      "(They also remove these images from the dataset.",
      "They don't say how many images are affected by this.)",
      "They end up with roughly 26k cells, some of them reaching the street level of major cities.",
      "Visualization of their cells:  Training  For each example photo that they feed into the network, they set the correct grid cell to 1.0 and all other grid cells to 0.0.",
      "They train on a dataset of 126M images with Exif geolocation information.",
      "The images were collected from all over the web.",
      "They used Adagrad.",
      "They trained on 200 CPUs for 2.5 months.",
      "Album network  For photo albums they develop variations of their network.",
      "They do that because albums often contain images that are very hard to geolocate on their own, but much easier if the other images of the album are seen.",
      "They use LSTMs for their album network.",
      "The simplest one just iterates over every photo, applies their previously described model to it and extracts the last layer (before output) from that model.",
      "These vectors (one per image) are then fed into an LSTM, which is trained to predict (again) the grid cell location per image.",
      "More complicated versions use multiple passes or are bidirectional LSTMs (to use the information from the last images to classify the first ones in the album).",
      "Results  They beat previous models (based on hand-engineered features or nearest neighbour methods) by a significant margin.",
      "In a small experiment they can beat experienced humans in geoguessr.com.",
      "Based on a dataset of 2.3M photos from Flickr, their method correctly predicts the country where the photo was made in 30% of all cases (top-1; top-5: about 50%).",
      "City-level accuracy is about 10% (top-1; top-5: about 18%).",
      "Example predictions (using in coarser grid with 354 cells):  Using the LSTM-technique for albums significantly improves prediction accuracy for these images."
    ],
    "author_id": "ALEJU",
    "pdf_url": "http://arxiv.org/pdf/1602.05314",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 86675541
  },
  {
    "blog_id": "inception_v4",
    "summary": [
      "Overview  Inception v4 is like Inception v3, but  Slimmed down, i.e. some parts were simplified  One new version with residual connections (Inception-ResNet-v2), one without (Inception-v4)  They didn't observe an improved error rate when using residual connections.",
      "They did however oberserve that using residual connections decreased their training times.",
      "They had to scale down the results of their residual modules (multiply them by a constant ~0.1).",
      "Otherwise their networks would die (only produce 0s).",
      "Results on ILSVRC 2012 (val set, 144 crops/image):  Top-1 Error:  Inception-v4: 17.7%  Inception-ResNet-v2: 17.8%  Top-5 Error (ILSVRC 2012 val set, 144 crops/image):  Inception-v4: 3.8%  Inception-ResNet-v2: 3.7%  Architecture  Basic structure of Inception-ResNet-v2 (layers, dimensions):  Image -> Stem -> 5x Module A -> Reduction-A -> 10x Module B -> Reduction B -> 5x Module C -> AveragePooling -> Droput 20% -> Linear, Softmax  299x299x3 -> 35x35x256 -> 35x35x256 -> 17x17x896 -> 17x17x896 -> 8x8x1792 -> 8x8x1792 -> 1792 -> 1792 -> 1000  Modules A, B, C are very similar.",
      "They contain 2 (B, C) or 3 (A) branches.",
      "Each branch starts with a 1x1 convolution on the input.",
      "All branches merge into one 1x1 convolution (which is then added to the original input, as usually in residual architectures).",
      "Module A uses 3x3 convolutions, B 7x1 and 1x7, C 3x1 and 1x3.",
      "The reduction modules also contain multiple branches.",
      "One has max pooling (3x3 stride 2), the other branches end in convolutions with stride 2.",
      "From top to bottom: Module A, Module B, Module C, Reduction Module A.",
      "Top 5 eror by epoch, models with (red, solid, bottom) and without (green, dashed) residual connections.",
      "Rough chapter-wise notes  Introduction, Related Work  Inception v3 was adapted to run on DistBelief.",
      "Inception v4 is designed for TensorFlow, which gets rid of some constraints and allows a simplified architecture.",
      "Authors don't think that residual connections are inherently needed to train deep nets, but they do speed up the training.",
      "History:  Inception v1 - Introduced inception blocks  Inception v2 - Added Batch Normalization  Inception v3 - Factorized the inception blocks further (more submodules)  Inception v4 - Adds residual connections  Architectural Choices  Previous architectures were constrained due to memory problems.",
      "TensorFlow got rid of that problem.",
      "Previous architectures were carefully/conservatively extended.",
      "Architectures ended up being quite complicated.",
      "This version slims down everything.",
      "They had problems with residual networks dieing when they contained more than 1000 filters (per inception module apparently?).",
      "They could fix that by multiplying the results of the residual subnetwork (before the element-wise addition) with a constant factor of ~0.1.",
      "Training methodology  Kepler GPUs, TensorFlow, RMSProb (SGD+Momentum apprently performed worse)  Experimental Results  Their residual version of Inception v4 (\"Inception-ResNet-v2\") seemed to learn faster than the non-residual version.",
      "They both peaked out at almost the same value.",
      "Top-1 Error (ILSVRC 2012 val set, 144 crops/image):  Inception-v4: 17.7%  Inception-ResNet-v2: 17.8%  Top-5 Error (ILSVRC 2012 val set, 144 crops/image):  Inception-v4: 3.8%  Inception-ResNet-v2: 3.7%"
    ],
    "author_id": "ALEJU",
    "pdf_url": "http://arxiv.org/pdf/1602.07261v1",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 45435119
  },
  {
    "blog_id": "emergence-of-grounded-compositional-language-in-multi-agent-populations",
    "summary": [
      "The paper provides a multi-agent learning environment and proposes a learning approach that facilitates the emergence of a basic compositional language.",
      "The language is quite rudimentary and is essentially a sequence of abstract discrete symbols.",
      "But it does comprise of a defined vocabulary and syntax.",
      "Setup  Cooperative, partially observable Markov game (multi-agent extension of MDP).",
      "All agents have identical action and observation spaces, use the same policy and receive a shared reward.",
      "Grounded Communication Environment  Physically simulated 2-D environment in continuous space and discrete time with N agents and M landmarks.",
      "The agents and the landmarks would occupy some location and would have some attributes (colour, shape).",
      "Within the environment, the agents can go to a location, look at a location or do nothing.",
      "Additionally, they can utter communication symbols c (from a shared vocabulary C).",
      "Agents themselves learn to assign a meaning to the symbols.",
      "Each agent has an internal goal (which could require interaction with other agents to complete) which the other agents cannot see.",
      "Goal for agent i consists of an action to perform, a landmark location where to perform the action and another agent who should be performing the action.",
      "Since the agent is continuously emitting symbols, a memory module is provided and simple additive memory updates are done.",
      "For interaction, the agents could use verbal utterances, non-verbal signals (gaze) or non-communicative strategies (pushing other agents).",
      "Approach  A model of all agent and environment state dynamics is created over time and the return gradient is computed.",
      "Gumbel-Softmax distribution is used to obtain categorical word emission c.  A multi-layer perceptron is used to model the policy which returns action, communication symbol and the memory update for each agent.",
      "Since the number of agents (and hence the number of communication streams etc) can vary across instantiations, an identical model is instantiated per agent and per communication stream.",
      "The output of individual processing modules are pooled into feature vectors corresponding to communication and physical observations.",
      "These pooled features and the goal vectors are fed to the final processing module from which actions and categorical symbols are sampled.",
      "In practice, using an additional task (each agent predicts the goal for another agent) encouraged more meaningful communication utterances.",
      "Compositionality and Vocabulary Size  Authors recommend using a large vocabulary with a soft penalty that discourages use of too many words.",
      "This leads to use of a large vocabulary in the intermediate state which converges to a small vocabulary.",
      "Along the lines of rich gets richer dynamics, the communication symbol c\u2019s are modelled as being generated by a Dirichlet process.",
      "The resulting reward across all agents is the log-likelihood of all communication utterances to have been generated by a Dirichlet process.",
      "Since the agents can only communicate in discrete symbols and do not have a global positioning reference, they need to unambiguously communicate landmark references to other agents.",
      "Case I - Agents can not see each other  Non-verbal communication is not possible.",
      "When trained with just 2 agents, symbols are assigned for each landmark and action.",
      "As the number of agents is increased, additional symbols are used to refer to agents.",
      "If the agents of the same colour are asked to perform conflicting tasks, they perform the average of conflicting tasks.",
      "If distractor locations are added, the agents learn to ignore them.",
      "Non-verbal communication  Agents are allowed to observe other agents\u2019 position, gaze etc.",
      "Now the location can be pointed to using gaze.",
      "If gaze is disabled, the agent could indicate the goal landmark by moving to it.",
      "Basically even when the communication is disabled the agents can come up with strategies to complete the task."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1703.04908",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 30189879
  },
  {
    "blog_id": "distributed-consensus-and-the-implications-of-nvm-on-database-management-systems",
    "summary": [
      "Distributed consensus and the implications of NVM on database management systems Fournier, Arulraj, & Pavlo ACM Queue Vol 14, issue 3  As you may recall, Peter Bailis and ACM Queue have started a \u201cResearch for Practice\u201d series introducing \u201cexpert curated guides to the best of CS research.\u201d Aka, reading lists for The Morning Paper!",
      "I previously covered the papers from the first edition (blog entries dated June 14th-21st, 2016).",
      "Today we\u2019re turning our attention to the the second edition:  I am thrilled to introduce our second instalment of Research for Practice, which provides highlights from two critical areas in storage and large-scale services: distributed consensus and non-volatile memory.",
      "Distributed consensus  The first topic area is Distributed Consensus, with papers selected by Camille Fournier.",
      "\u201cThe three papers included in this selection address the real world of consensus system: Why are they needed?",
      "Why are they difficult to understand?",
      "What happens when you try to implement.",
      "Them?",
      "Is there an easier way, something that more developers can understand and therefore implement?\u201d  Fournier\u2019s three choices are:  Paxos made live \u2013 an engineering perspective  The Chubby lock service for loosely coupled distributed systems  In search of an understandable consensus algorithm  All of which will be familiar to regular readers of The Morning Paper ;) (Links above are to my write-ups).",
      "If you want more of this kind of thing, I did a two-week mini-series on consensus back in March of last year.",
      "Here are three additional picks of my own:  Viewstamped Replication Revisited  Raft Refloated  And yesterday\u2019s paper, Flexible Paxos  Implications of NVM on database management systems  Joy Arulrja and Andrew Pavlo introduce a selection of three papers looking at the implications of NVM for database management systems:  The advent of non-volatile memory (NVM) will fundamentally change the dichotomy between memory and durable storage in a database management systems (DBMS).",
      "This is a topic area that really caught my attention earlier this year, and I wrote a short post entitled \u201c All change please \u201d summarizing some of the hardware advances hitting our data centers, including NVM.",
      "On the subject of NVM itself and its implications, the papers I\u2019ve covered so far can be found by searching on the blog for the keyword \u2018NVM\u2019 .",
      "The first of Arulja and Pavlo\u2019s picks is  From ARIES to MARS  Which looks at the classic ARIES recovery protocol and how it can be optimized for NVM.",
      "Their second and third paper choices are ones that I haven\u2019t covered before.",
      "So we\u2019ll be looking at those papers in the next two days.",
      "The links below will go live as each days\u2019 post goes up.",
      "Let\u2019s talk about storage and recovery methods for non-volatile memory database systems \u2013 a wonderful tour of common DBMS storage engine designs, and how they can be adapted to NVM.",
      "How did I miss this paper first time around???",
      "It\u2019s a real gem.",
      "Write-limited sorts and joins for persistent memory , which looks at the implications of read/write cost imbalance and limited write endurance in NVM.",
      "As Arulja and Pavlo say,  The common theme for these papers is that you cannot just run an existing DBMS on NVM and expect it to leverage its unique set of properties.",
      "The only way to achieve that is to come up with novel architectures, protocols, and algorithms that are tailor-made for NVM.",
      "The third edition of Reseach for Practice must be due out soon \u2013 I\u2019m very much looking forward to seeing where it goes next!"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/2956641.2967618?download=true",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 27051635
  },
  {
    "blog_id": "fast_scene_understanding_for_autonomous_driving",
    "summary": [
      "What  They suggest a method to predict for images the semantic segmentation maps, instance segmentation maps and depth maps.",
      "Semantic segmentation = \"assign color X to all pixels showing a car, color Y to all pixels showing people, ...\"  Instance segmentation = \"assign color X to all pixels showing car number 1, color Y to all pixels showing car number 2, ...\"  Their method is optimized to run fast and with low memory demands.",
      "The method is aimed at self-driving cars.",
      "How  Architecture  They base their model on ENet.",
      "ENet is a network for fast segmentation.",
      "It uses three blocks of (several) residual convolutions for downscaling, followed by two blocks of upscaling.",
      "Some convolutions are dilated.",
      "Number of filters does not exceed 128.",
      "They share two blocks of downscaling between all three branches (semantic segmentation / instance segmentation / depth maps).",
      "Each branch gets one unique downscaling module and two upscaling modules.",
      "Losses  Semantic Segmentation: Pixelwise cross-entropy.",
      "Instance Segmentation:  They do not directly predict some kind of instance flag per pixel.",
      "Instead they generate per pixel an embedding vector.",
      "These are supposed to be similar for pixels belonging to the same instance (and dissimilar for different instances).",
      "They can then cluster these vectors to find all pixels belonging to an instance (they use GPU based mean shift clustering for that).",
      "In order to train the network to generate such embeddings, they view each instance as a class and design the losses so that the intra-class variance (distances) is minimized, while the intra-class distances are maximized.",
      "So they get a variance term/loss and a distance term/loss.",
      "Both of these are hinged (so e.g. once the variance goes below a certain threshold, the corresponding loss just becomes zero).",
      "L_var = intraclass variance loss, L_dist = interclass variance loss, L_reg = regularization loss  ||.|| = L2 distance, [.",
      "]_+ = max(0, x) = hinge  C = class, N_c = pixels in class, mu_c = mean embedding, x_i = pixel embedding, delta_v = variance hinge, delta_d = interclass distance hinge  Depth Estimation:  Common loss here (according to the authors) would be the L2 distance with a scale invariance term and local structure similarity term.",
      "But they don't use these as it performs worse than their loss.",
      "Instead they use a reverse Huber loss, which seems to have some similarity with a combination of L1 and L2 loss:  Other  Input image size is 1024x512.",
      "They use Adam with learning rate 5e-4 and batch size 10.",
      "They keep the batch norm parameters fixed (no explanation why).",
      "Results  For semantic segmentation on Cityscapes they reach the same score as ENet (59.3 class IoU, 80.4 category IoU).",
      "For instance segmentation on Cityscapes they reach 21.0 AP as opposed to 46.9 AP of Mask R-CNN.",
      "For depth map prediction on Cityscapes their result differ by distance of objects.",
      "The difference is lower for close objects (MAE of 1.5m for objects <25m away) and higher for the ones further apart (MAE of 7.5m for objects <100m away).",
      "Visualization of predicted depth vs real depth:  Example results:  They reach 21fps (that should be around 3x or so faster than Mask R-CNN) and 1.2GB memory footprint.",
      "Training all branches jointly in one network (as described above) vs. training them completely separately (fully disjoint networks) improves accuracy by a bit."
    ],
    "author_id": "ALEJU",
    "pdf_url": "https://arxiv.org/pdf/1708.02550",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 19050347
  },
  {
    "blog_id": "efficient-lifelong-learning-with-a-gem",
    "summary": [
      "Contributions  A new (and more realistic) evaluation protocol for lifelong learning where each data point is observed just once and a disjoint set of tasks are used for training and validation.",
      "A new metric that focuses on the efficiency of the models - in terms of sample complexity and computational (and memory) costs.",
      "Modification of Gradient Episodic Memory ie GEM which reduces the computational overhead of GEM without compromising on the results.",
      "Empirical validation that using task descriptors help lifelong learning models and improve their few-shot learning capabilities.",
      "Link to the code  Learning Protocol  Two group of datasets - one for training and evaluation (DEV) and other for cross validation (DCV).",
      "Data can be sampled multiple times for cross-validation dataset but only once from the training dataset.",
      "Each group of dataset (say DEV or DCV) is a list of task-specific datasets Dk (k is the task index).",
      "Each sample in Dk is of the form (x, t, y) where x is the data, t is the task descriptor and y is the output.",
      "Dk contains Bk minibatches of data.",
      "Metrics  Accuracy  ak,i,j = accuracy on test task j after training on ith minibatch of training task k.  Ak = mean over all j = 1 to k (ak, Bk, j) ie train the model on data for task k and then test it on all the tasks.",
      "Forgetting Measure  fjk = forgetting on task j after training on all minibatches upto task k.  fjk = max over all l = 1 to k-1 (al, Blj - ak, Bkj)  Forgetting = Fk = mean over all j = 1 to k-1 (fjk)  LCA - Learning Curve Area  Zb = average b shot performance where b is the minibatch number.",
      "Zb = mean over all k = 0 to T (ak, b, k)  LCA\u03b2 = mean over all b = 0 to \u03b2 (Zb)  One special case is LCA0 which is the forward transfer performance or performance on the unseen task.",
      "In experiments, \u03b2 is kept small as we want the model to learn from few examples.",
      "Model  GEM has been shown to be very effective in single epoch setting but introduces a very high computational overhead.",
      "Average GEM (AGEM) reduces this overhead by sampling (and using) only some examples from the episodic memory instead of using all the examples.",
      "While GEM provides better guarantees in terms of worst-case forgetting, AGEM provides better guarantees in terms of average accuracy.",
      "Joint Embedding Model Using Compositional Task Descriptors  Compositional Task Descriptors are used to speed training on the subsequent tasks.",
      "A matrix specifying the attribute value of objects (to be recognized in the task) are used.",
      "A joint-embedding space between image features and attribute embeddings is learned.",
      "Experiments  Datasets  Permuted MNIST  Split CIFAR  Split CUB  Split AWA  Setup  Integer task descriptors for MNIST and CIFAR and class attributes as descriptors for CUB and AWA  Baselines include GEM , iCaRL , Elastic Weight Consolidation , Progressive Neural Networks etc.",
      "Results  AGEM outperforms other models on all the datasets expect MNIST where the Progressive Neural Networks lead.",
      "One reason could be that MNIST has a large number of training examples per task.",
      "But Progressive Neural Networks lead to bad utilization of capacity.",
      "While AGEM and GEM have similar performance, GEM has a much higher computational and memory overhead.",
      "Use of task descriptors improves the accuracy for all the models.",
      "It seems that AGEM offers a good tradeoff between average accuracy performance and efficiency - in terms of sample efficiency, memory requirements and computational costs."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1812.00420",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 59320034
  },
  {
    "blog_id": "convolutional_networks_with_adaptive_inference_graphs",
    "summary": [
      "What  They introduce a dynamic gating mechanism for ResNet that decides on-the-fly which layers to execute for a given input image.",
      "This improves performance, classification accuracy and robustness to adversarial attacks.",
      "Note that the decision of whether to execute a layer during training is stochastic, making it closely related to stochastic depth.",
      "How  Their technique is based on residual connections, i.e. on ResNet.",
      "They add a gating function, so that x_l = x_{l-1} + F(x_{l-1}) becomes x_l = x_{l-1} + gate(x_{l-1}) * F(x_{l-1}).",
      "Gating  The gating function produces a discrete output (0 or 1), enabling to completely skip a layer.",
      "They implement the gating function by applying the following steps to an input feature map:  Global average pooling, leads to Cx1x1 output.",
      "Flatten to vector  Apply fully connected layer with d outputs and ReLU  Apply fully connected layer with 2 outputs  Apply straight-through gumbel-softmax to output  Straight-through Gumbel-softmax is a standard technique that is comparable to softmax, but produces discrete 0/1 outputs during the forward pass and uses a softmax during the backward pass to approximate gradients.",
      "For small d, the additional execution time of the gating function is negligible.",
      "Visualization:  They also introduce a target rate loss, which pushes the network to execute t percent of all layers (i.e. let t percent of all gates produce 1 as the output):  where z_l is the fraction of gates that produced 1 for layer l within the minibatch  and t is the target rate.",
      "Note that this will probably cause problems for small minibatches.",
      "Results  CIFAR-10  They choose d=16, adds 0.01% FLOPS and 4.8% parameters to ResNet-110.",
      "At t=0.7 they observe that on average 82% of all layers are executed.",
      "The model chooses to always execute downsampling layers.",
      "They seem to be important.",
      "When always executing all layers and scaling their outputs with their likelihood of being executed (similar to Dropout, Stochastic Depth), they achieve higher score than a network trained with stochastic depth.",
      "This indicates that their results do not just come from a regularizing effect.",
      "ImageNet  They compare gated ResNet-50 and ResNet-110.",
      "In ResNet-110 they always execute the first couple of layers (ungated), in ResNet-50 they gate all layers.",
      "Trade-off between FLOPs and accuracy (ResNet-50/101 vs their model \"ConvNet-AIG\" at different t):  Their model becomes more accurate as t is lowered down from 1.0 to 0.7.",
      "After that the accuracy starts to drop.",
      "Visualization of layerwise execution rate:  The model learns to always execute downsampling layers.",
      "The model seems to treat all man-made objects and all animals as two different groups with similar layers.",
      "Execution distribution and execution frequency during training:  The model quickly learns to always execute the last layer.",
      "The model requires especially many layers to classify non-iconic views of objects (e.g. dog face from unusual perspective instead of whole dog from the side).",
      "When running adversarial attacks on ResNet-50 and their ConvNet-AIG-50 (via Fast Gradient Sign Attack) they observe that:  Their model is overall more robust to adversarial attacks, i.e. keeps higher accuracy (both with/without protection via JPEG compression).",
      "The executed layers remain mostly the same in their model.",
      "They seem to not get attacked."
    ],
    "author_id": "ALEJU",
    "pdf_url": "http://openaccess.thecvf.com/content_ECCV_2018/papers/Andreas_Veit_Convolutional_Networks_with_ECCV_2018_paper.pdf",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 86524123
  },
  {
    "blog_id": "rasmusbhvr15",
    "summary": [
      "This paper describes a learning algorithm for deep neural networks that can be understood as an extension of stacked denoising autoencoders.",
      "In short, instead of reconstructing one layer at a time and greedily stacking, a unique unsupervised objective involving the reconstruction of all layers is optimized jointly by all parameters (with the relative importance of each layer cost controlled by hyper-parameters).",
      "In more details:  * The encoding (forward propagation) adds noise (Gaussian) at all layers, while decoding is noise-free.",
      "* The target at each layer is the result of noise-less forward propagation.",
      "* Direct connections (also known as skip-connections) between a layer and its decoded reconstruction are used.",
      "The resulting encoder/decoder architecture thus ressembles a ladder (hence the name Ladder Networks).",
      "* Miniature neural networks with a single hidden unit and skip-connections are used to decode the left and top layers into a reconstruction.",
      "Each network is applied element-wise (without parameter sharing across reconstructed units).",
      "* The unsupervised objective is combined with a supervised objective, corresponding to the regular negative class log-likelihood objective (using an output softmax layer).",
      "Two losses are used for each input/target pair: one based on the noise-free forward propagation (which also provides the target of the denoising objective) and one with the noise added (which also corresponds to the encoding stage of the unsupervised autoencoder objective).",
      "Batch normalization is used to train the network.",
      "Since the model combines unsupervised and supervised learning, it can be used for semi-supervised learning, where unlabeled examples can be used to update the network using the unsupervised objective only.",
      "State of the art results in the semi-supervised setting are presented, for both the MNIST and CIFAR-10 datasets.",
      "#### My two cents  What I find most exciting about this paper is its performance.",
      "On MNIST, with only 100 labeled examples, it achieves 1.13% error!",
      "That is essentially the performance of stacked denoising autoencoders, trained on the entire training set (though that was before ReLUs and batch normalization, which this paper uses)!",
      "This confirms a current line of thought in Deep Learning (DL) that, while recent progress in DL applied on large labeled datasets does not rely on any unsupervised learning (unlike at the \"beginning\" of DL in the mid 2000s), unsupervised learning might instead be crucial for success in low-labeled data regime, in the semi-supervised setting.",
      "Unfortunately, there is one little issue in the experiments, disclosed by the authors: while they used few labeled examples for training, model selection did use all 10k labels in the validation set.",
      "This is of course unrealistic.",
      "But model selection in the low data regime is arguably, in itself, an open problem.",
      "So I like to think that this doesn't invalidate the progress made in this paper, and only suggests that some research needs to be done on doing effective hyper-parameter search with a small validation set.",
      "Generally, I really hope this paper will stimulate more research on DL methods to the specific case of small labeled dataset / large unlabeled dataset.",
      "While this isn't a problem that is as \"flashy\" as tasks such as the ImageNet Challenge which comes with lots of labeled data, I think this is a crucial research direction for AI in general.",
      "Indeed, it seems naive to me to expect that we will be able to collect large labeled dataset for each and every task, on our way to real AI."
    ],
    "author_id": "hlarochelle",
    "pdf_url": "http://papers.nips.cc/paper/5947-semi-supervised-learning-with-ladder-networks.pdf",
    "author_full_name": "Hugo Larochelle",
    "source_website": "https://www.shortscience.org/user?name=hlarochelle",
    "id": 7157784
  },
  {
    "blog_id": "critical_learning_periods_in_deep_neural_networks",
    "summary": [
      "What  They find that artificial neural networks show critical periods, similar to biological neural networks.",
      "Critical periods in biological neural networks are phases in the network's development during which a malformed sensory input can irreversibly harm the capabilities of the network.",
      "E. g. a disease causing blurry vision for some time during early development can permanently harm the ability to interpret visual stimuli.",
      "The same disease very early or later in life will cause no permanent damage (after being healing).",
      "How  Blur  They train networks in CIFAR10 and blur all input images by down- and then upscaling.",
      "They vary when the blurring is removed.",
      "They always train for 300 epochs more after the blurring was removed.",
      "So the network always sees the unaltered input images for at least the same amount of epochs.",
      "Vertical flipping  Same as blurring, but they vertically flip the image.",
      "This is expected to keep low and mid level statistics the same.",
      "Only the final layers have to change.",
      "This is expected to be easy to adapt to, even if the flipping is removed fairly late into the training.",
      "Label permutation  They randomly permute the class labels and remove the effect after some epochs.",
      "This is expected to only affect the last layer and hence should have similar effects to vertical flipping.",
      "Sensory deprivation  They test the effect of sensory deprivation on neural nets.",
      "They make the input of the networks uninformative by replacing it with gaussian noise.",
      "This is assumed to have less effect than adding blur, because the network does not learn significantly wrong statistics (due to the input being uninformative).",
      "Mutual Information Noise  They add random gaussian noise to each layer's output (or to the weights - not really clear).",
      "They allow the network to learn the variance of that noise.",
      "They add a regularization based on mutual information.",
      "This adds a \"cost proportional to the quantity of mutual information I(w;D) that the weights retain about the training data D after the learning process\" (?).",
      "So the network can retain more information, but has to pay for that.",
      "It is expected to set the variance to low values for layers which are critical for the predictions.",
      "Results  Blur  Removing the blur early leads to only a small loss in final accuracy.",
      "Removing the blur too late leads to a large and permanent loss in final accuracy.",
      "The decline in accuracy is not linear with respect to when the blur is removed.",
      "The effect is similar to biological neural nets.",
      "Making the network deeper does not help, but instead worsens the effect.",
      "Fixing the learning rate doesn't help either.",
      "This is basically like starting to let the network learn once the blur is removed, but using a weirdly bad initialization.",
      "(Weird in the sense that it starts with great accuracy, but is barely able to improve.)",
      "Vertical flipping  As expected, adding vertical flips does not significantly affect long term accuracy.",
      "Label permutation  Same as for vertical flipping, only minor effect.",
      "Sensory deprivation  This has worse effects than vertical flipping / label permutation.",
      "Overall less decrease in accuracy than with blur.",
      "The effect is more linear with respect to the epoch (remove early: hardly any decline in accuracy, remove after half of training: medium decline, remove late: strong decline).",
      "Mutual Information Noise  Without deficit, the network will put most weight (least amount of noise) on the middle layers (3-5 of 7).",
      "With deficit, it will put more weight on the last layers and is only able to partially reconfigure if the deficit is removed early enough."
    ],
    "author_id": "ALEJU",
    "pdf_url": "https://arxiv.org/pdf/1711.08856",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 36272485
  },
  {
    "blog_id": "744ab6c17a2289ca139ea586d1d65e",
    "summary": [
      "The paper introduces a novel technique to explain the predictions of any classifier in an interpretable and faithful manner.",
      "It also proposes a method to explain models by obtaining representative individual predictions and their explanations.",
      "Demo  Desired Characteristics for Explanations  Interpretable  Take into account user limitations.",
      "Since features in a machine learning model need not be interpretable, the input to the explanations may have to be different from input to the model.",
      "Local Fidelity  Explanation should be locally faithful, ie it should correspond to how the model behaves in the vicinity of the instance being predicted.",
      "Model Agnostic  Treat the original, given model as a black box.",
      "Global Perspective  Select a few predictions such that they represent the entire model.",
      "LIME  Local Interpretable Model-agnostic Explanations  Interpretable Data Representations  For text classification, an interpretable representation could be a binary vector indicating the presence or absence of a word (or bag or words).",
      "For image classification, an interpretable representation may be a binary vector indicating the \"presence\" of a super-pixel.",
      "x \u2208 Rd is the original representation of an instance being explained while x` \u2208 {0, 1}d denotes a binary vector for its representation.",
      "Fidelity-Interpretability Trade-off  Define an explanation as a model g \u2208 G, where G is a class of potentially interpretable models and g acts over absence/presence of the interpretable components  Define \u2126(g) as a measure of complexity (as opposed to interpretability) of the explanation g \u2208 G.  Define f to be the model being explained.",
      "Define \u03c0x(z) as a proximity measure between an instance z to x (to define locality around x).",
      "Define L(f, g, \u03c0x) as a measure of how unfaithful g is in approximating f in the locality defined by \u03c0x.",
      "To ensure both interpretability and local fidelity, we minimise L(f, g, \u03c0x) while having \u2126(g) be low enough to be interpretable.",
      "Sampling for Local Exploration  Since f is treated as a black box, the local behaviour of L(f, g, \u03c0x) is approximated by drawing samples weighted by \u03c0x.",
      "Given an instance x`, generate a dataset of perturbed samples Z and optimise the LIME model loss, L(f, g, \u03c0x).",
      "The paper proposes to use sparse linear explanations with the locally weighted square loss as L. This could be a problem in the case of highly non-linear models.",
      "Submodular Pick for Explaining Models  Global understanding of the model by explaining a set of individual instances.",
      "Define B to the number of explanations to be generated.",
      "Pick Step - the task of selecting B instances for the user to inspect.",
      "Aim to obtain non-redundant explanations that represent how the model behaves globally.",
      "Given a matrix of n explanations, using d features (also called explanation matrix), rank the features such that the feature which explains more instances gets a higher score.",
      "When selecting instances, avoid instances with similar explanation and try to increase coverage.",
      "Conclusion  The paper evaluates its approach on a series of simulated and human-in-the-loop tasks to check:  Are explanations faithful to the model.",
      "Could the predictions be trusted.",
      "Can the model be trusted.",
      "Can users select the best classifier given the explanations.",
      "Can user (non-experts) improve the classifier by means of feature selection.",
      "Can explanations lead to insights about the model itself.",
      "Future Work  Need to define a way of finding (and ranking) compatible features across images for SP-LIME.",
      "It could be difficult to define the relevant features for model explanation in certain cases - for example, single words may not be a good feature in sentiment analysis models."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1602.04938",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 26277001
  },
  {
    "blog_id": "hierarchical-graph-representation-learning-with-differentiable-pooling",
    "summary": [
      "Most existing GNN (Graph Neural Network) methods are inherently flat and are unable to process the information in a hierarchical manner.",
      "The paper proposes a differentiable graph pooling operation, DIFFPOOL, that can generate hierarchical graph representations and can be easily plugged into many GNN architectures.",
      "Key Idea  CNNs have spatial pooling operation that allows for deep CNN architectures to operate on coarse graph representations of input images.",
      "This notion cannot be applied as-is to graphs as they do not have a natural notion of spatial locality like images do.",
      "DIFFPOOL attempts to resolve this problem by learning a differentiable soft-assignment at each layer which is equivalent to pooling the cluster of nodes to obtain a sparse representation.",
      "Approach  Given a graph G(A, F), where A is the adjacency matrix and F is the feature matrix.",
      "Given a permutation invariant GNN that follows the message passing architecture.",
      "The output of this GNN can be expressed as Z = GNN(A, X) where X is the current feature matrix.",
      "Goal is to stack L GNN layers on top of each other such that the lth layer uses coarsened output from the (l-1)th layer.",
      "This coarsening operation uses a cluster assignment matrix S.  The learned cluster assignment matrix at layer l is denoted at Sl  Given Sl, the embedding matrix for the (l+1)th layer is given as transpose(Sl)Zl and adjancecy matrix is given by transpose(Sl)AlSl  A new GNN, called as GNNpool is used to produce the assignment matrix S by taking a softmax over GNNpool(Al, Xl)  As long as the GNN model is permutation invariant, the resulting DIFFPOOL model is also permutation invariant.",
      "Auxiliary Losses  The paper uses 2 auxiliary losses to push the model away from spurious local minima early in the training.",
      "Link prediction objective - at each layer, link prediction loss ( = A - S(transpose(S))) is minimized with the intuition that the nearby nodes should be pooled together.",
      "Ideally, the cluster assignment for each node should be a one-hot vector so the entropy for cluster assignment per node is regularized.",
      "Baselines  GNN based models  GraphSage  Mean pooling  Set2Set pooling  Sort pooling  Structure2vec  Edge conditioned filters in CNN  PatchySan  Kernel based models  Graphlet, shortest path etc  Model Variants  GraphSage  Mean pool + Diff pool (3 or 2 layers)  Structure2Vec + Diffpool  Diffpool-Det  The assignment matrix S are generated using graph clustering algorithms.",
      "Diffpool-NoLP  The link prediction objective function is turned off.",
      "At each DiffPool layer, the number of classes is set to 25% of the number of nodes before the DiffPool layer.",
      "Results  DiffPool obtains the highest average performance across all the pooling approaches and improves upon the base GraphSage architecture by an average of around 7%.",
      "In terms of runtime complexity, the paper reports that DiffPool does not incur any significant additional running time.",
      "But given that now there are 2 GNN models per layer, the size of the model should increase.",
      "DiffPool can capture hierarchical community structure even when trained on just the graph classification loss.",
      "One advantage of DiffPool is that the nodes are pooled in a non-uniform way so densely connected group of nodes would collapse into one cluster while sparsely connected nodes can retain their identity."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1806.08804",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 69481324
  },
  {
    "blog_id": "hierarchical_deep_reinforcement_learning",
    "summary": [
      "What  They present a hierarchical method for reinforcement learning.",
      "The method combines \"long\"-term goals with short-term action choices.",
      "How  They have two components:  Meta-Controller:  Responsible for the \"long\"-term goals.",
      "Is trained to pick goals (based on the current state) that maximize (extrinsic) rewards, just like you would usually optimize to maximize rewards by picking good actions.",
      "The Meta-Controller only picks goals when the Controller terminates or achieved the goal.",
      "Controller:  Receives the current state and the current goal.",
      "Has to pick a reward maximizing action based on those, just as the agent would usually do (only the goal is added here).",
      "The reward is intrinsic.",
      "It comes from the Critic.",
      "The Critic gives reward whenever the current goal is reached.",
      "For Montezuma's Revenge:  A goal is to reach a specific object.",
      "The goal is encoded via a bitmask (as big as the game screen).",
      "The mask contains 1s wherever the object is.",
      "They hand-extract the location of a few specific objects.",
      "So basically:  The Meta-Controller picks the next object to reach via a Q-value function.",
      "It receives extrinsic reward when objects have been reached in a specific sequence.",
      "The Controller picks actions that lead to reaching the object based on a Q-value function.",
      "It iterates action-choosing until it terminates or reached the goal-object.",
      "The Critic awards intrinsic reward to the Controller whenever the goal-object was reached.",
      "They use CNNs for the Meta-Controller and the Controller, similar in architecture to the Atari-DQN paper (shallow CNNs).",
      "They use two replay memories, one for the Meta-Controller (size 40k) and one for the Controller (size 1M).",
      "Both follow an epsilon-greedy policy (for picking goals/actions).",
      "Epsilon starts at 1.0 and is annealed down to 0.1.",
      "They use a discount factor / gamma of 0.9.",
      "They train with SGD.",
      "Results  Learns to play Montezuma's Revenge.",
      "Learns to act well in a more abstract MDP with delayed rewards and where simple Q-learning failed.",
      "Rough chapter-wise notes  (1) Introduction  Basic problem: Learn goal directed behaviour from sparse feedbacks.",
      "Challenges:  Explore state space efficiently  Create multiple levels of spatio-temporal abstractions  Their method: Combines deep reinforcement learning with hierarchical value functions.",
      "Their agent is motivated to solve specific intrinsic goals.",
      "Goals are defined in the space of entities and relations, which constraints the search space.",
      "They define their value function as V(s, g) where s is the state and g is a goal.",
      "First, their agent learns to solve intrinsically generated goals.",
      "Then it learns to chain these goals together.",
      "Their model has two hiearchy levels:  Meta-Controller: Selects the current goal based on the current state.",
      "Controller: Takes state s and goal g, then selects a good action based on s and g. The controller operates until g is achieved, then the meta-controller picks the next goal.",
      "Meta-Controller gets extrinsic rewards, controller gets intrinsic rewards.",
      "They use SGD to optimize the whole system (with respect to reward maximization).",
      "(3) Model  Basic setting: Action a out of all actions A, state s out of S, transition function T(s,a)->s', reward by state F(s)->R.",
      "epsilon-greedy is good for local exploration, but it's not good at exploring very different areas of the state space.",
      "They use intrinsically motivated goals to better explore the state space.",
      "Sequences of goals are arranged to maximize the received extrinsic reward.",
      "The agent learns one policy per goal.",
      "Meta-Controller: Receives current state, chooses goal.",
      "Controller: Receives current state and current goal, chooses action.",
      "Keeps choosing actions until goal is achieved or a terminal state is reached.",
      "Has the optimization target of maximizing cumulative reward.",
      "Critic: Checks if current goal is achieved and if so provides intrinsic reward.",
      "They use deep Q learning to train their model.",
      "There are two Q-value functions.",
      "One for the controller and one for the meta-controller.",
      "Both formulas are extended by the last chosen goal g.  The Q-value function of the meta-controller does not depend on the chosen action.",
      "The Q-value function of the controller receives only intrinsic direct reward, not extrinsic direct reward.",
      "Both Q-value functions are reprsented with DQNs.",
      "Both are optimized to minimize MSE losses.",
      "They use separate replay memories for the controller and meta-controller.",
      "A memory is added for the meta-controller whenever the controller terminates.",
      "Each new goal is picked by the meta-controller epsilon-greedy (based on the current state).",
      "The controller picks actions epsilon-greedy (based on the current state and goal).",
      "Both epsilons are annealed down.",
      "(4) Experiments  (4.1) Discrete MDP with delayed rewards  Basic MDP setting, following roughly: Several states (s1 to s6) organized in a chain.",
      "The agent can move left or right.",
      "It gets high reward if it moves to state s6 and then back to s1, otherwise it gets small reward per reached state.",
      "They use their hierarchical method, but without neural nets.",
      "Baseline is Q-learning without a hierarchy/intrinsic rewards.",
      "Their method performs significantly better than the baseline.",
      "(4.2) ATARI game with delayed rewards  They play Montezuma's Revenge with their method, because that game has very delayed rewards.",
      "They use CNNs for the controller and meta-controller (architecture similar to the Atari-DQN paper).",
      "The critic reacts to (entity1, relation, entity2) relationships.",
      "The entities are just objects visible in the game.",
      "The relation is (apparently ?)",
      "always \"reached\", i.e. whether object1 arrived at object2.",
      "They extract the objects manually, i.e. assume the existance of a perfect unsupervised object detector.",
      "They encode the goals apparently not as vectors, but instead just use a bitmask (game screen heightand width), which has 1s at the pixels that show the object.",
      "Replay memory sizes: 1M for controller, 50k for meta-controller.",
      "gamma=0.99  They first only train the controller (i.e. meta-controller completely random) and only then train both jointly.",
      "Their method successfully learns to perform actions which lead to rewards with long delays.",
      "It starts with easier goals and then learns harder goals."
    ],
    "author_id": "ALEJU",
    "pdf_url": "https://arxiv.org/pdf/1604.06057",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 23929837
  },
  {
    "blog_id": "recursive-programming",
    "summary": [
      "Recursive Programming \u2013 Dijkstra 1960  * Updated link to one that is not behind a paywall \u2013 thanks to Graham Markall for the catch *  This paper deals with something we take so much for granted that it\u2019s hard to imagine a time when it had yet to be introduced to the world.",
      "That time was 1960, the concept is the language runtime stack, and the author of the paper is none other than Dijkstra.",
      "In fact, we\u2019re so familiar with the stack, that it takes a while to get your head around what went before: each subroutine in a program had its own private fixed working space.",
      "That is, suppose you write a program that contains 30 subroutines \u2013 then there will be 30 reserved areas in memory, one for each subroutine to use.",
      "Dijkstra points out a couple of difficulties with this arrangement:  In the first place, the storage allocations for all the subroutines together will, in general, occupy much more memory space than they ever need simultaneously, and the available memory space is therefore used rather uneconomically.",
      "Furthermore \u2013 and this is a more serious objection \u2013 it is then impossible to call a subroutine while one or more of the previous activations of the same subroutine have not yet come to an end, without losing the possibility of finishing them off properly later on.",
      "In other words, if each subroutine has its own fixed storage area, then recursive programming is not possible (you\u2019ll overwrite the state in the one fixed storage area for the subroutine).",
      "Since this rather limits the design space of programs, Dijkstra was interested in finding a technique that could eliminate the restriction:  The basic concept of the method is the so-called stack.",
      "Dijkstra describes how to build a stack with a block of memory and a stack pointer \u2013 I\u2019m going to assume you\u2019re all familiar with the idea!",
      "The following insight is key to the use of the stack in this context:  If we mark off, on a time axis, the moments when a unit is added to or removed from the stack, by using an opening bracket for the addition of a unit, and a closing bracket for its removal, then we obtain a correctly nested bracket structure, in which the opening and closing brackets form pairs in the same way as they do in a normal algebraic expression involving brackets.",
      "This is closely related to the circumstance that we can use a stack for storing the intermediate results formed in the evaluation of an arbitrary algebraic expression by means of elementary algebraic operations.",
      "In this case, the interest is always restricted to the most recent element in the stack.",
      "As the intermediate results are used only once, use of an element implies its removal from the stack.",
      "Take an expression  A + (B - C) * (D|E + F)  We know that we can record this in reverse polish notation as :  A, B, C, -, D, E, |, F, +, *, +  The above is well-known, and so elegant that we could not refrain from trying to extend this technique by consistent application of its principles.",
      "The example above assumes that A, B, C etc are numerical values that can be found in memory.",
      "But Dijkstra points out that C could equally have been an expression (for example C = (P/(Q-R + S*T )), and by the time we have done evaluating C, the net result would be the same as if we had the value of C accessible directly.",
      "In other words, it is immaterial to the \u201csurroundings\u201d in which the value C is used, whether the value C can be found ready-made in memory, or whether it is necessary to make temporary use of a number of the next stack locations for its evaluation.",
      "Now suppose it wasn\u2019t an expression that appears for the value of C, but a function (to be evaluated by a subroutine): \u201cthis provides a strong argument for arranging the subroutine in such a way that it operates in the first free places of the stack, in just the same way as a compound term written out in full.\u201d  The stack can be used by a subroutine for its parameters, its local variables, and even anonymous intermediate results created during execution of the subroutine.",
      "Inside the subroutine we store the most anonymous intermediate results in the \u201ctop\u201d of the stack in just the same way.",
      "Every reference to a local quantiity, however, implies one is interested in a place that is situated deeper within the stack, and here one is interested in random access to the stack places, in other words we must be able to give the places deeper in the stack some kind of address.",
      "The value of that reference point is, \u2018to be derived from the value of the stack pointer at the moment of the call.\u2019 With a final flourish, Dijkstra goes on to show that \u2018link\u2019 information must be preserved  in the stack when calling subroutines, so that regardless of complexity we can pick up exactly where we left off (in the ALU)- this is to include a return address.",
      "We can now follow what happens when subroutine A calls subroutine B, and observe that:  In this process, nothings forbids A from being identical with B.",
      "The subroutine only has to appear in the memory once, but it may have more than one simultaneous \u201cincarnation\u201d from a dynamic point of view: the \u201cinnermost\u201d activation causes the same piece of text to work in a higher part of the stack.",
      "Thus the subroutine has developed into a defining element that can be used completely recursively.",
      "Tada!",
      "Note: 1960 is also the year of \u201cRecursive Functions of Symbolic Expressions and their Computation by Machine,\u201d McCarthy\u2019s famous paper which \u201cdescribes a formalism for defining functions recursively,\u201d and then shows how it can be implemented in the LISP programming system for the IBM 704.",
      "\"Recursive Functions of Symbolic Expressions\u2026\" McCarthy, 1960 (LISP)  [url]"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://oai.cwi.nl/oai/asset/9253/9253A.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 74547298
  },
  {
    "blog_id": "1605.06065",
    "summary": [
      "This paper proposes a variant of Neural Turing Machine (NTM) for meta-learning or \"learning to learn\", in the specific context of few-shot learning (i.e. learning from few examples).",
      "Specifically, the proposed model is trained to ingest as input a training set of examples and improve its output predictions as examples are processed, in a purely feed-forward way.",
      "This is a form of meta-learning because the model is trained so that its forward pass effectively executes a form of \"learning\" from the examples it is fed as input.",
      "During training, the model is fed multiples sequences (referred to as episodes) of labeled examples $({\\bf x}_1, {\\rm null}), ({\\bf x}_2, y_1), \\dots, ({\\bf x}_T, y_{T-1})$, where $T$ is the size of the episode.",
      "For instance, if the model is trained to learn how to do 5-class classification from 10 examples per class, $T$ would be $5 \\times 10 = 50$.",
      "Mainly, the paper presents experiments on the Omniglot dataset, which has 1623 classes.",
      "In these experiments, classes are separated into 1200 \"training classes\" and 423 \"test classes\", and each episode is generated by randomly selecting 5 classes (each assigned some arbitrary vector representation, e.g. a  one-hot vector that is consistent within the episode, but not across episodes) and constructing a randomly ordered sequence of 50 examples from within the chosen 5 classes.",
      "Moreover, the correct label $y_t$ of a given input ${\\bf x}_t$ is always provided only at the next time step, but the model is trained to be good at its prediction of the label of ${\\bf x}_t$ at the current time step.",
      "This is akin to the scenario of online learning on a stream of examples, where the label of an example is revealed only once the model has made a prediction.",
      "The proposed NTM is different from the original NTM of Alex Graves, mostly in how it writes into its memory.",
      "The authors propose to focus writing to either the least recently used memory location or the most recently used memory location.",
      "Moreover, the least recently used memory location is reset to zero before every write (an operation that seems to be ignored when backpropagating gradients).",
      "Intuitively, the proposed NTM should learn a strategy by which, given a new input, it looks into its memory for information from other examples earlier in the episode (perhaps similarly to what a nearest neighbor classifier would do) to predict the class of the new input.",
      "The paper presents experiments in learning to do multiclass classification on the Omniglot dataset and regression based on functions synthetically generated by a GP.",
      "The highlights are that:  1.",
      "The proposed model performs much better than an LSTM and better than an NTM with the original write mechanism of Alex Graves (for classification).",
      "2.",
      "The proposed model even performs better than a 1st nearest neighbor classifier.",
      "3.",
      "The proposed model is even shown to outperform human performance, for the 5-class scenario.",
      "4.",
      "The proposed model has decent performance on the regression task, compared to GP predictions using the groundtruth kernel.",
      "**My two cents**  This is probably one of my favorite ICML 2016 papers.",
      "I really think meta-learning is a problem that deserves more attention, and this paper presents both an interesting proposal for how to do it and an interesting empirical investigation of it.",
      "Much like previous work\u00a0[\\[1\\]][1] [\\[2\\]][2], learning is based on automatically generating a meta-learning training set.",
      "This is clever I think, since a very large number of such \"meta-learning\" examples (the episodes) can be constructed, thus transforming what is normally a \"small data problem\" (few shot learning) into a \"big data problem\", for which deep learning is more effective.",
      "I'm particularly impressed by how the proposed model outperforms a 1-nearest neighbor classifier.",
      "That said, the proposed NTM actually performs 4 reads at each time step, which suggests that a fairer comparison might be with a 4-nearest neighbor classifier.",
      "I do wonder how this baseline would compare.",
      "I'm also impressed with the observation that the proposed model surpassed humans.",
      "The paper also proposes to use 5-letter words to describe classes, instead of one-hot vectors.",
      "The motivation is that this should make it easier for the model to scale to much more than 5 classes.",
      "However, I don't entirely follow the logic as to why one-hot vectors are problematic.",
      "In fact, I would think that arbitrarily assigning 5-letter words to classes would instead imply some similarity between classes that share letters that is arbitrary and doesn't reflect true class similarity.",
      "Also, while I find it encouraging that the performance for regression of the proposed model is decent, I'm curious about how it would compare with a GP approach that incrementally learns the kernel's hyper-parameter (instead of using the groundtruth values, which makes this baseline unrealistically strong).",
      "Finally, I'm still not 100% sure how exactly the NTM is able to implement the type of feed-forward inference I'd expect to be required.",
      "I would expect it to learn a memory representation of examples that combines information from the input vector ${\\bf x}_t$ *and* its label $y_t$.",
      "However, since the label of an input is presented at the following time step in an episode, it is not intuitive to me then how the read/write mechanisms are able to deal with this misalignment.",
      "My only guess is that since the controller is an LSTM, then it can somehow remember ${\\bf x}_t$ until it gets $y_t$ and appropriately include the combined information into the memory.",
      "This could be supported by the fact that using a non-recurrent feed-forward controller is much worse than using an LSTM controller.",
      "But I'm not 100% sure of this either.",
      "All the above being said, this is still a really great paper, which I hope will help stimulate more research on meta-learning.",
      "Hopefully code for this paper can eventually be released, which would help in popularizing the topic.",
      "[1]:  [url]"
    ],
    "author_id": "hlarochelle",
    "pdf_url": "http://arxiv.org/pdf/1605.06065v1",
    "author_full_name": "Hugo Larochelle",
    "source_website": "https://www.shortscience.org/user?name=hlarochelle",
    "id": 45612165
  },
  {
    "blog_id": "same-different-problems-strain-convolutional-neural-networks",
    "summary": [
      "Same-different problems strain convolutional neural networks Ricci et al., arXiv 2018  Since we\u2019ve been looking at the idea of adding structured representations and relational reasoning to deep learning systems, I thought it would be interesting to finish off the week with an example of a problem that seems to require it: detecting whether objects in a scene are the same or different.",
      "This image containing a flute was correctly classified by a CNN trained on millions of photographs.",
      "On ImageNet the network even surpassed the accuracy of a human observer.",
      "This image contains two shapes that are the same, a relationship that is immediately obvious to a human observer.",
      "\u201cYet, the CNN failed to learn this relation even after seeing millions of training examples.\u201d  The above is an example of a same-different (SD) visual relation problem (output whether the objects in the scene are the same, or different).",
      "Spatial relation (SR) problems ask whether objects follow a certain spatial relation, e.g. in a line, horizontally stacked, vertically stacked, and so on.",
      "For example:  The synthetic visual reasoning test (SVRT) contains a collection of 23 binary classification problems along these lines.",
      "In each case opposing classes differ based on whether their stimuli obey an abstract rule.",
      "If you train a bunch of CNNs (with different depths, filter sizes, etc.)",
      "on these tasks an interesting pattern pops out.",
      "The CNNs really struggled on problems where the abstract rule required detecting whether things were the same or different (congruent up to some transformation), whereas they achieved good accuracy on spatial relation problems.",
      "The resulting dichotomy across the SVRT problems is striking.",
      "CNNs fare uniformly worse on SD problems than they do on SR problems.",
      "Many SR problems were learned satisfactorily, whereas some SD problems (e.g. problems 20 and 7) resulted in accuracy not substantially above chance.",
      "For SR problems, all the CNNs did pretty well, regardless of network configuration.",
      "But for SD problems larger networks performed noticeably better than smaller ones.",
      "This suggests that something about the SD problems is straining the capacity of the CNNs.",
      "Probing further  To dig deeper into this apparent difference between same-different and spatial-relation problems the authors construct a new visual-relation benchmark called PSVRT.",
      "The dataset is parameterised so that the size of the items in the scene, the number of scene items, and the size of the whole image can all be controlled.",
      "Scene items are just binary bit patterns placed on a blank background.",
      "For any given configuration of parameters, the resulting scene can be used for both an SD problem and an SR problem, simply based on labelling.",
      "Our goal was to examine how hard it is for a CNN architecture to learn relations for visually different but conceptually equivalent problems.",
      "If CNNs can truly learn the \u201crule\u201d underlying these problems, then one would expect the models to learn all problems with more-or-less equal ease.",
      "However, if the CNN only memorize the distinguishing features of the two image classes, then learning should be affected by the variability of the example images in each category.",
      "A baseline architecture was established with four convolutional layers, that was able to easily learn both the same-different and spatial-relation PSVRT problems with item size 4, image size 60, and two items in the image.",
      "This baseline CNN was then trained from scratch on a variety of PSVRT problems, each time using 20 million training images and a batch size of 50.",
      "There were three sub-experiments:  Fixing item size (m) at 4, number of items (k) at 2, and varying image size (n) between 30 and 180.",
      "Fixing image size at 60, number of items at 2, and varying item size between 3 and 7.",
      "Fixing image size at 60, item size at 4, and varying the number of items between 2 and 6.",
      "In all conditions, we found a strong dichotomy between SD and SR conditions.",
      "In SR, across all image parameters and in all trials, the model immediately learned at the start of training and quickly approached 100% accuracy, producing consistently high and flat mean ALC curves.",
      "In SD, however, we found that the overall ALC was significantly lower than SR.  Digging deeper, when learning did occur in SD, increasing item size never strained performance.",
      "But increasing the overall image size, or increasing the number of items did.",
      "(Gray bars in the above figures indicate the number of trials in which learning failed).",
      "The results suggest that straining is not simply a direct outcome of an increase in image variability.",
      "Using CNNs with more than twice the number of kernels (wide), or twice as many layers (deep) did not change the observed trend.",
      "What\u2019s going on?",
      "The authors hypothesise that the CNNs learn \u2018subtraction templates\u2019 when tackling SD problems: filters with one positive region and one negative region.",
      "Each relative arrangement of items requires a different subtraction template since each item must lie in on of the template\u2019s two regions.",
      "If identical items lie in opposing regions, they are subtracted by the synaptic weights.",
      "The difference is used to choose the appropriate same/different label.",
      "A strategy like this doesn\u2019t require memorizing specific items, so item size doesn\u2019t make much of a difference.",
      "However, image size (the biggest straining factor) exponentially increases the possible number of arrangements of items.",
      "Our results indicate that visual-relation problems can quickly exceed the representational capacity of feedforward networks.",
      "While learning templates for individual objects appears to be tractable for today\u2019s deep networks, learning templates for arrangements of objects becomes rapidly intractable because of the combinatorial explosion in the number of features to be stored\u2026 Given the vast superiority of humans over modern computers in their ability to detect visual relations, we see the exploration of attentional and grouping mechanisms as an important next step in our computational understanding of visual reasoning."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://arxiv.org/pdf/1802.03390",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 38469042
  },
  {
    "blog_id": "e0baa075b4a917c0a69edc575772a8",
    "summary": [
      "Knowledge Bases (KBs) are effective tools for Question Answering (QA) but are often too restrictive (due to fixed schema) and too sparse (due to limitations of Information Extraction (IE) systems).",
      "The paper proposes Key-Value Memory Networks, a neural network architecture based on Memory Networks that can leverage both KBs and raw data for QA.",
      "The paper also introduces MOVIEQA, a new QA dataset that can be answered by a perfect KB, by Wikipedia pages and by an imperfect KB obtained using IE techniques thereby allowing a comparison between systems using any of the three sources.",
      ".",
      "Related Work  TRECQA and WIKIQA are two benchmarks where systems need to select the sentence containing the correct answer, given a question and a list of candidate sentences.",
      "These datasets are small and make it difficult to compare the  systems using different sources.",
      "Best results on these benchmarks are reported by CNNs and RNNs with attention mechanism.",
      "Key-Value Memory Networks  Extension of Memory Networks Model .",
      "Generalises the way context is stored in memory.",
      "Comprises of a memory made of slots in the form of pair of vectors (k1, v1)...(km, vm) to encode long-term and short-term context.",
      "Reading the Memory  Key Hashing - Question, x is used to preselect a subset of array (kh1, vh1)...(khN, vhN) where the key shares atleast one word with x and frequency of the words is less than 1000.",
      "Key Addresing - Each candidate memory is assigned a relevance probability:  phi = softmax(A\u03c6X(x).A\u03c6K(khi))  \u03c6 is a feature map of dimension D and A is a dxD matrix.",
      "Value Reading - Value of memories are read by taking their weighted sum using addressing probabilites and a vector o is returned.",
      "o = sum(phiA\u03c6V(vhi))  Memory access process conducted by \"controller\" neural network using q = A\u03c6X(x) as the query.",
      "Query is updated using  q2 = R1(q+o)  Addressing and reading steps are repeated using new Ri matrices to retrive more pertinent information in subsequent access.",
      "After a fixed number of hops, H, resulting state of controller is used to compute a final prediction.",
      "a = argmax(softmax(qH+1TB\u03c6Y(yi))) where yi are the possible candidate outputs and B is a dXD matrix.",
      "The network is trained end-to-end using a cross entropy loss, backpropogation and stochastic gradient.",
      "End-to-End Memory Networks can be viewed as a special case of Key-Value Memory Networks by setting key and value to be the same for all the memories.",
      "Variants of Key-Value Memories  \u03c6x and \u03c6y - feature map corresponding to query and answer are fixed as bag-of-words representation.",
      "KB Triple  Triplets of the form \"subject relation object\" can be represented in Key-Value Memory Networks with subject and relation as the key and object as the value.",
      "In standard Memory Networks, the whole triplet would have to be embedded in the same memory slot.",
      "The reversed relations \"object is_related_to subject\" can also be stored.",
      "Sentence Level  A document can be split into sentences with each sentence encoded in the key-value pair of the memory slot as a bag-of-words.",
      "Window Level  Split the document in the windows of W words and represent it as bag-of-words.",
      "The window becomes the key and the central word becomes the value.",
      "Window + Centre Encoding  Instead of mixing the window centre with the rest of the words, double the size of the dictionary and encode the centre of the window and the value using the second dictionary.",
      "Window + Title  Since title of the document could contain useful information, the word window can be encoded as the key and document title as the value.",
      "The key could be augmented with features like \"window\" and \"title\" to distinguish between different cases.",
      "MOVIEQA Benchmark  Knowledge Representation  Doc - Raw documents (from Wikipedia) related to movies.",
      "KB - Graph based KB made of entities and relations.",
      "IE - Performing Information Extraction on Wikipedia to create a KB.",
      "The QA pairs should be answerable by both raw document and KB so that the three approaches can be compared and the gap between the three solutions can be closed.",
      "The dataset has more than 100000 QA pairs, making it much larger than most existing datasets.",
      "Experiments  MOVIEQA  Systems Compared  Bordes et al's QA system  Supervised Embeddings(without KB)  Memory Networks  Key-Value Memory Networks  Observations  Key-Value Memory Networks outperforms all methods on all data sources.",
      "KB > Doc > IE  The best memory representation for directly reading documents uses \"Window Level + Centre Encoding + Title\".",
      "KB vs Synthetic Document Analysis  Given KB triplets, construct synthetic \"Wikipedia\" articles using templates, conjuctions and coreferences to determine the causes for gap in performance when using KB vs doc.",
      "Loss in One Template sentences are due to difficulty of extracting subject, relation and object from the artifical docs.",
      "Using multiple templates does not detoriate performance much.",
      "But conjuctions and coreferences cause a dip in performance.",
      "WIKIQA  Given a question, select the sentence (from Wikipedia document) that best answers the question.",
      "Key-Value Memory Networks outperforms all other solutions though it is only marginally better than LDC (Sentence Similarity Learning by Lexical Decomposition and Composition) and attentive models based on CNNs and RNNs.",
      "This comment has been minimized.",
      "Sign in to view  Copy link  Quote reply  ylytju commented  Mar 7, 2018  In the sentence level, what do play the role of the key and value?",
      "In this case, what's the difference between the Key-value MM and the traditional MM?",
      "This comment has been minimized.",
      "Sign in to view  Copy link  Quote reply  Owner Author  shagunsodhani commented  Mar 8, 2018  In some cases, where your key is same as your value, there is no difference between key-value and traditional memory networks.",
      "Now let us say that in the sentence case, the key and memory both are the same bag of words, then there is no difference.",
      "But now you could have the key different from the memory."
    ],
    "author_id": "shugan",
    "pdf_url": "https://arxiv.org/pdf/1606.03126",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 22744554
  },
  {
    "blog_id": "wavefs",
    "summary": [
      "The Design and Implementation of the Wave Transactional Filesystem \u2013 Escriva & Sirer 2015  Since we\u2019ve been looking at various combinations of storage and transactions, it seemed appropriate to start this week with the Wave Transactional Filesystem.",
      "Throughout the paper you\u2019ll find this abbreviated as WTF, but my brain can\u2019t read that without supplying the alternate meaning so I shall refer to it as Wave FS during this post except when quoting directly from the paper.",
      "Distributed filesystems are a cornerstone of modern data processing applications\u2026 Yet current distributed filesystems exhibit a tension between retaining the familiar semantics of local filesystems and achieving high performance in the distributed setting.",
      "Often designs will compromise consistency for performance, require special hardware, or artificially restrict the filesystem interface.",
      "The Wave Transactional Filesystem (Wave FS) is a distributed filesystem that provides a transactional model.",
      "A transaction can span multiple files and contain any combination of reads, writes, and seeks.",
      "The key to this is a file-slicing abstraction, on top of which both a traditional POSIX interface and a file-slice aware interface are provided.",
      "A broad evaluation shows that WTF achieves throughput and latency similar to industry-standard HDFS, while simultaneously offering stronger guarantees and a richer API.",
      "A sample application built with file slicing outperforms traditional approaches by a factor of four by reducing the overall I/O cost.",
      "The ability to make transactional changes to multiple files at scale is novel in the distributed systems space, and the file slicing APIs enable a new class of applications that are difficult to implement efficiently with current APIs.",
      "Together, these features are a potent combination that enables a new class of high performance applications.",
      "The basic idea is very easy to understand: file data is kept in immutable, arbitrarily-sized, byte-addressable, sequences of bytes called slices.",
      "Think of this like an old fashioned film reel containing a sequence of video frames.",
      "If a portion of the file is to be overwritten, the new bytes are written to a new slice (like recording a new take of a scene).",
      "We then go the the cutting room floor to splice the new slice into the original at the desired point.",
      "This is where the analogy breaks down a little, because Wave FS never changes the original bytes on disk.",
      "Instead information about the slices and splice-points is kept in metadata separate to the slices themselves.",
      "By reading the metadata and following the instructions there it is possible to reconstitute the current state of the file.",
      "A file in Wave FS is a sequence of slices and their associated offsets.",
      "A worked example should help to make this clear.",
      "Let\u2019s consider the history of a file foo, which we happen to write/update in 1MB chunks.",
      "The first write creates a 2MB slice, \u2018A\u2019.",
      "Then we append 2MB to the end of the file by writing slice \u2018B\u2019 and updating the metadata.",
      "Another process overwrites 2MB in the center of the file:  The next update overwrites the third MB:  And so does the final update:  Compaction compresses the metadata, and garbage collection can later kick-in and delete slice D.  Thus we have immutable slices, and mutable metadata.",
      "This representation has some inherent advantages over block-based designs.",
      "Specifically, the abstraction provides a separation between metadata and data that enables filesystem-level transactions to be implemented using, solely, transactions over the metadata.",
      "Data is stored in the slices, while the metadata is a sequence of slices.",
      "WTF can transactionally change these sequences to change the files they represent, without having to rewrite the data.",
      "Custom storage servers hold filesystem data and handle the bulk of the I/O requests.",
      "They know nothing about the structure of the filesystem and treat all data as opaque slices.",
      "References to the slices and the metadata that describes how to reconstitute them into files is kept in HyperDex.",
      "The procedures for reading and writing follow directly from the abstraction.",
      "A writer creates one or more slices on the storage servers, and overlays them at the appropriate positions within the file by appending their slice pointers to the metadata list.",
      "Readers retrieve the metadata list, compact it, and determine which slices must be retrieved from the storage servers to fulfill the read.",
      "The correctness of this design relies upon the metadata storage providing primitives to atomically read and append to the list.",
      "HyperDex natively supports both of these operations.",
      "Because each writer writes slices before appending to the metadata list, it is guaranteed that any transaction that can see these immutable slices is serialized after the writing transaction commits.",
      "It can then retrieve the slices directly.",
      "The transactional guarantees of WTF extend directly from this design as well: a WTF transaction will execute a single HyperDex transaction consisting of multiple append and retrieve operations.",
      "To support arbitrarily large files and efficient operations on the list of pointers, partitions a file into fixed-size regions, each with its own list.",
      "Wave FS also implements transaction retry in its client library on top of HyperDex.",
      "This allows for example, an append operation to succeed even if the underlying file length has changed (the semantics of append depend on adding the bytes at the end, not at a certain index position).",
      "The slice-aware alternative API provides yank, paste and append calls that are analogous to read, write, and append but operate on slices instead of bytes.",
      "There is also a punch verb that zeros out bytes and frees the underlying storage, as well as concat and copy functions built on these primitives.",
      "Wave FS employs a locality-aware slice placement algorithm to improve disk locality for nearby file ranges, and uses replication to add a configurable degree of fault-tolerance.",
      "To accomplish this, it augments the metadata list such that each entry references multiple slice pointers that are replicas of the data.",
      "On the write path, writers create multiple replica slices and append their pointers atomically.",
      "Readers may read from any of the replicas, as they hold identical data.",
      "The metadata storage derives its fault tolerance from the strong guarantees offered by HyperDex."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://arxiv.org/pdf/1509.07821v1.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 61603670
  },
  {
    "blog_id": "derflow-distributed-deterministic-dataflow-programming-for-erlang",
    "summary": [
      "Derflow: Distributed Deterministic Dataflow programming for Erlang \u2013 Bravo et al. 2014  Today\u2019s choice is part of the work of the SyncFree European research project on large-scale computation without synchronisation.",
      "Non-determinism makes it very difficult to reason about distributed applications.",
      "So Bravo et al. figured life might be easier if we could just make them deterministic instead.",
      "How do you do that?",
      "Just go with Derflow\u2026  Concurrent programs in non-deterministic languages are notoriously hard to prove correct and have led to well-known disasters\u2026.",
      "we believe that limiting the ability to write non-deterministic code provides a reasonable alternative to exhaustively checking our applications for correctness.",
      "Derflow is based on the idea of a distributed single-assignment data store, and is built on top of riak core.",
      "Given the same input values, a program written in deterministic dataflow style will always return the same output values, or never return.",
      "These input values can be data streams as well, which is a natural generalization of functional programming to the concurrent setting.",
      "Our proposed solution provides a distributed deterministic dataflow solution which operates transparently over distributed Erlang, providing the ability to have highly-available, fault-tolerant, deterministic computations.",
      "The basic Derflow model is easy to understand.",
      "Think of a single assignment store where the keys are variable identifiers, and the values are either as-yet-unassigned, a reference to another variable, or an Erlang term.",
      "Once you\u2019ve assigned a value to a variable, you can never change it.",
      "There\u2019s also a whole bunch of hidden metadata associated with each variable that the system keeps track of:  what variables are bound to this one  what processes are waiting for the variable to be bound to a value  if a variable is part of a stream, what the \u2018next\u2019 value in the stream is (see below)  any processes waiting on a reader before setting the value (to support lazy production of values)  any monitors tracking the availability/reachability of the variable (to handle partitions etc.)",
      "The basic programming model consists of declare() (creates a new variable and returns its id), bind(x,v) (bind a variable x to value v) , and read(x).",
      "A process reading an unbound variable will block until a value is bound.",
      "Derflow also makes use of Erlang\u2019s spawn to support concurrency.",
      "Streaming data is supported by treating the stream as a list of dataflow variables:  Streams are a useful technique which allow threads, or processes, to communicate and synchronize in concurrent programming.",
      "A stream is represented here as a list of dataflow variables, with an unbound dataflow variable as the final element of the list.",
      "The operation produce extends the tail of the stream by binding a new value to the last element, and creating a new unbound last variable (\u2018next\u2019) which it returns.",
      "consume reads an element from the stream and also returns the id of the next variable in the stream.",
      "Failures need to be carefully handled if they are not to introduce non-determinism.",
      "determinism and dataflow variables provide a very useful property for failure handling: redundant computation will not affect the correctness of a deterministic dataflow program.",
      "We propose a failure handling model where failed processes or temporarily unreachable processes, can be restarted while still providing the guarantees of the deterministic programming model.",
      "If a computing process fails, it is just re-executed.",
      "In the Derflow model, duplicate processing cannot alter the outcome.",
      "(Though of course this relies on programs being side-effect free too).",
      "If a process is blocked because a variable it is waiting to read never becomes available, then restarts have to cascade more deeply:  The re-execution of blocked process will result in the process immediately blocking again.",
      "Therefore we must provide a way to identify dependencies between processes and dataflow variables in order to provide a deterministic restart strategy which guarantees progress.",
      "A common strategy to ensure progress in this situation is to restart the process that declared the failed dataflow variable.",
      "In addition, all the processes depending on the restarted process should also be restarted.",
      "The implementation of Derflow builds on riak core.",
      "mnesia was considered but rejected, in part because of its behaviour under network partitions:  Problems arise in the presence of network partitions where the mnesia nodes on either side of the network partition are able to make progress independently.",
      "Currently, no mechanisms exist for reconciling the changes made to the database when nodes reconnect, nor reasoning about concurrent or causally influenced operations.",
      "Several examples of Derflow programs are given, which show that the outcome of the program is independent of the degree of concurrency used in computing it.",
      "In Derflow, any function that uses dataflow variables can be run in a different process while keeping the final result same.",
      "Thus, programmers can transparently add concurrency to their programs (either parallelism or distribution) in a secure way without thinking about data races and possible bugs.",
      "However, this concurrency is not managed automatically via the runtime \u2013 programmers must explicitly specify it (contrast this to e.g. an execution planner for datalog).",
      "The semantics of Derflow programs are very clearly explained in the paper,  and the deterministic model of concurrent execution certainly looks interesting.",
      "Yet I found it hard to assess just from this one paper whether Derflow would be an interesting way of writing real systems, or whether the restrictions would feel limiting.",
      "Perhaps the answers to this question are found in the literature on Kahn Process Networks on which Derflow is based.",
      "Deterministic dataflow was first proposed by Gilles Kahn in 1974, in a programming model that is now known as Kahn networks.",
      "In 1977, a lazy version of this same model was proposed by Kahn and David MacQueen.",
      "However, up until recently this model has never become part of mainstream concurrent programming.",
      "This may be due to either the model\u2019s inability to express non-determinism or the simultaneous invention of two other models for handling concurrent programming: the actor model (message passing) and monitors (shared state).",
      "(Note that Derflow also introduces a mechanism to support explicit introduction of small amounts of non-determinism where you really need them)."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://www.info.ucl.ac.be/~pvr/erlang14cameraready.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 13135318
  },
  {
    "blog_id": "joint_training_of_a_convnet_and_a_pgm_for_hpe",
    "summary": [
      "What  They describe a model for human pose estimation, i.e. one that finds the joints (\"skeleton\") of a person in an image.",
      "They argue that part of their model resembles a Markov Random Field (but in reality its implemented as just one big neural network).",
      "How  They have two components in their network:  Part-Detector:  Finds candidate locations for human joints in an image.",
      "Pretty standard ConvNet.",
      "A few convolutional layers with pooling and ReLUs.",
      "They use two branches: A fine and a coarse one.",
      "Both branches have practically the same architecture (convolutions, pooling etc.).",
      "The coarse one however receives the image downscaled by a factor of 2 (half width/height) and upscales it by a factor of 2 at the end of the branch.",
      "At the end they merge the results of both branches with more convolutions.",
      "The output of this model are 4 heatmaps (one per joint?",
      "unclear), each having lower resolution than the original image.",
      "Spatial-Model:  Takes the results of the part detector and tries to remove all detections that were false positives.",
      "They derive their architecture from a fully connected Markov Random Field which would be solved with one step of belief propagation.",
      "They use large convolutions (128x128) to resemble the \"fully connected\" part.",
      "They initialize the weights of the convolutions with joint positions gathered from the training set.",
      "The convolutions are followed by log(), element-wise additions and exp() to resemble an energy function.",
      "The end result are the input heatmaps, but cleaned up.",
      "Results  Beats all previous models (with and without spatial model).",
      "Accuracy seems to be around 90% (with enough (16px) tolerance in pixel distance from ground truth).",
      "Adding the spatial model adds a few percentage points of accuracy.",
      "Using two branches instead of one (in the part detector) adds a bit of accuracy.",
      "Adding a third branch adds a tiny bit more.",
      "Example results.",
      "Part Detector network.",
      "Spatial Model (apparently only for two input heatmaps).",
      "Rough chapter-wise notes  (1) Introduction  Human Pose Estimation (HPE) from RGB images is difficult due to the high dimensionality of the input.",
      "Approaches:  Deformable-part models: Traditionally based on hand-crafted features.",
      "Deep-learning based disciminative models: Recently outperformed other models.",
      "However, it is hard to incorporate priors (e.g. possible joint- inter-connectivity) into the model.",
      "They combine:  A part-detector (ConvNet, utilizes multi-resolution feature representation with overlapping receptive fields)  Part-based Spatial-Model (approximates loopy belief propagation)  They backpropagate through the spatial model and then the part-detector.",
      "(3) Model  (3.1) Convolutional Network Part-Detector  This model locates possible positions of human key joints in the image (\"part detector\").",
      "Input: RGB image.",
      "Output: 4 heatmaps, one per key joint (per pixel: likelihood).",
      "They use a fully convolutional network.",
      "They argue that applying convolutions to every pixel is similar to moving a sliding window over the image.",
      "They use two receptive field sizes for their \"sliding window\": A large but coarse/blurry one, a small but fine one.",
      "To implement that, they use two branches.",
      "Both branches are mostly identical (convolutions, poolings, ReLU).",
      "They simply feed a downscaled (half width/height) version of the input image into the coarser branch.",
      "At the end they upscale the coarser branch once and then merge both branches.",
      "After the merge they apply 9x9 convolutions and then 1x1 convolutions to get it down to 4xHxW (H=60, W=90 where expected input was H=320, W=240).",
      "(3.2) Higher-level Spatial-Model  This model takes the detected joint positions (heatmaps) and tries to remove those that are probably false positives.",
      "It is a ConvNet, which tries to emulate (1) a Markov Random Field and (2) solving that MRF approximately via one step of belief propagation.",
      "The raw MRF formula would be something like <likelihood of joint A per px> = normalize( <product over joint v from joints V> <probability of joint A per px given a> * <probability of joint v at px?> + someBiasTerm).",
      "They treat the probabilities as energies and remove from the formula the partition function (normalize) for various reasons (e.g. because they are only interested in the maximum value anyways).",
      "They use exp() in combination with log() to replace the product with a sum.",
      "They apply SoftPlus and ReLU so that the energies are always positive (and therefore play well with log).",
      "Apparently <probability of joint v at px?> are the input heatmaps of the part detector.",
      "Apparently <probability of joint A per px given a> is implemented as the weights of a convolution.",
      "Apparently someBiasTerm is implemented as the bias of a convolution.",
      "The convolutions that they use are large (128x128) to emulate a fully connected graph.",
      "They initialize the convolution weights based on histograms gathered from the dataset (empirical distribution of joint displacements).",
      "(3.3) Unified Models  They combine the part-based model and the spatial model to a single one.",
      "They first train only the part-based model, then only the spatial model, then both.",
      "(4) Results  Used datasets: FLIC (4k training images, 1k test, mostly front-facing and standing poses), FLIC-plus (17k, 1k ?",
      "), extended-LSP (10k, 1k).",
      "FLIC contains images showing multiple persons with only one being annotated.",
      "So for FLIC they add a heatmap of the annotated body torso to the input (i.e. the part-detector does not have to search for the person any more).",
      "The evaluation metric roughly measures, how often predicted joint positions are within a certain radius of the true joint positions.",
      "Their model performs significantly better than competing models (on both FLIC and LSP).",
      "Accuracy seems to be at around 80%-95% per joint (when choosing high enough evaluation tolerance, i.e. 10px+).",
      "Adding the spatial model to the part detector increases the accuracy by around 10-15 percentage points.",
      "Training the part detector and the spatial model jointly adds ~3 percentage points accuracy over training them separately.",
      "Adding the second filter bank (coarser branch in the part detector) adds around 5 percentage points accuracy.",
      "Adding a third filter bank adds a tiny bit more accuracy."
    ],
    "author_id": "ALEJU",
    "pdf_url": "http://arxiv.org/pdf/1406.2984",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 30485653
  },
  {
    "blog_id": "artistic_style_transfer_for_videos",
    "summary": [
      "What  The paper describes a method to transfer the style (e.g. choice of colors, structure of brush strokes) of an image to a whole video.",
      "The method is designed so that the transfered style is consistent over many frames.",
      "Examples for such consistency:  No flickering of style between frames.",
      "So the next frame has always roughly the same style in the same locations.",
      "No artefacts at the boundaries of objects, even if they are moving.",
      "If an area gets occluded and then unoccluded a few frames later, the style of that area is still the same as before the occlusion.",
      "How  Assume that we have a frame to stylize x and an image from which to extract the style a.",
      "The basic process is the same as in the original Artistic Style Transfer paper , they just add a bit on top of that.",
      "They start with a gaussian noise image x' and change it gradually so that a loss function gets minimized.",
      "The loss function has the following components:  Content loss (old, same as in the Artistic Style Transfer paper)  This loss makes sure that the content in the generated/stylized image still matches the content of the original image.",
      "x and x' are fed forward through a pretrained network (VGG in their case).",
      "Then the generated representations of the intermediate layers of the network are extracted/read.",
      "One or more layers are picked and the difference between those layers for x and x' is measured via a MSE.",
      "E.g. if we used only the representations of the layer conv5 then we would get something like (conv5(x) - conv5(x'))^2 per example.",
      "(Where conv5() also executes all previous layers.)",
      "Style loss (old)  This loss makes sure that the style of the generated/stylized image matches the style source a.  x' and a are fed forward through a pretrained network (VGG in their case).",
      "Then the generated representations of the intermediate layers of the network are extracted/read.",
      "One or more layers are picked and the Gram Matrices of those layers are calculated.",
      "Then the difference between those matrices is measured via a MSE.",
      "Temporal loss (new)  This loss enforces consistency in style between a pair of frames.",
      "The main sources of inconsistency are boundaries of moving objects and areas that get unonccluded.",
      "They use the optical flow to detect motion.",
      "Applying an optical flow method to two frames (i, i+1) returns per pixel the movement of that pixel, i.e. if the pixel at (x=1, y=2) moved to (x=2, y=4) the optical flow at that pixel would be (u=1, v=2).",
      "The optical flow can be split into the forward flow (here fw) and the backward flow (here bw).",
      "The forward flow is the flow from frame i to i+1 (as described in the previous point).",
      "The backward flow is the flow from frame i+1 to i (reverse direction in time).",
      "Boundaries  At boundaries of objects the derivative of the flow is high, i.e. the flow \"suddenly\" changes significantly from one pixel to the other.",
      "So to detect boundaries they use (per pixel) roughly the equation gradient(u)^2 + gradient(v)^2 > length((u,v)).",
      "Occlusions and disocclusions  If a pixel does not get occluded/disoccluded between frames, the optical flow method should be able to correctly estimate the motion of that pixel between the frames.",
      "The forward and backward flows then should be roughly equal, just in opposing directions.",
      "If a pixel does get occluded/disoccluded between frames, it will not be visible in one the two frames and therefore the optical flow method cannot reliably estimate the motion for that pixel.",
      "It is then expected that the forward and backward flow are unequal.",
      "To measure that effect they roughly use (per pixel) a formula matching length(fw + bw)^2 > length(fw)^2 + length(bw)^2.",
      "Mask c  They create a mask c with the size of the frame.",
      "For every pixel they estimate whether the boundary-equation or the disocclusion-equation is true.",
      "If either of them is true, they add a 0 to the mask, otherwise a 1.",
      "So the mask is 1 wherever there is no disocclusion or motion boundary.",
      "Combination  The final temporal loss is the mean (over all pixels) of c*(x-w)^2.",
      "x is the frame to stylize.",
      "w is the previous stylized frame (frame i-1), warped according to the optical flow between frame i-1 and i.  c is the mask value at the pixel.",
      "By using the difference x-w they ensure that the difference in styles between two frames is low.",
      "By adding c they ensure the style-consistency only at pixels that probably should have a consistent style.",
      "Long-term loss (new)  This loss enforces consistency in style between pairs of frames that are longer apart from each other.",
      "It is a simple extension of the temporal (short-term) loss.",
      "The temporal loss was computed for frames (i-1, i).",
      "The long-term loss is the sum of the temporal losses for the frame pairs {(i-4,i), (i-2,i), (i-1,i)}.",
      "The c mask is recomputed for every pair and 1 if there are no boundaries/disocclusions detected, but only if there is not a 1 for the same pixel in a later mask.",
      "The additional condition is intended to associate pixels with their closest neighbours in time to minimize possible errors.",
      "Note that the long-term loss can completely replace the temporal loss as the latter one is contained in the former one.",
      "Multi-pass approach (new)  They had problems with contrast around the boundaries of the frames.",
      "To combat that, they use a multi-pass method in which they seem to calculate the optical flow in multiple forward and backward passes?",
      "(Not very clear here what they do and why it would help.)",
      "Initialization with previous frame (new)  Instead of starting at a gaussian noise image every time, they instead use the previous stylized frame.",
      "That immediately leads to more similarity between the frames.",
      "Results  Video"
    ],
    "author_id": "ALEJU",
    "pdf_url": "http://arxiv.org/pdf/1604.08610",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 51116334
  },
  {
    "blog_id": "congestion-avoidance-and-control",
    "summary": [
      "Congestion Avoidance and Control \u2013 Jacobson & Karels, 1988  (** corrected spelling of Jacobs_o_n **)  It\u2019s October 1986 and there\u2019s trouble on the internet.",
      "A congestion collapse has reduced the bandwidth between LBL and UC Berkeley by a factor of a thousand.",
      "These two sites happened to be 400 yds apart.",
      "And that drop in capacity?",
      "From 32 Kbps to 40  bps!",
      "This was just the first in a series of such collapses.",
      "Van Jacobsen and Michael Karels set out to investigate.",
      "Beyond just TCP/IP, the lessons they learned have relevance for any situation where traffic management & flow control are important.",
      "For example, they can be applied when managing back-offs and retries with circuit breakers and dimmer switches , and when deciding how quickly to let traffic through again on restoring flow (with a little imagination to consider a window of outstanding requests).",
      "To avoid congestion collapse, the authors propose the principle of the conservation of packets.",
      "By \u2018conservation of packets\u2019 we mean that for a connection \u2018in equilibrium\u2019,  i.e., running stably with a full window of data in transit, the packet flow is what a physicist would call \u2018conservative\u2019: A new packet isn\u2019 t put into the network until an old packet leaves.",
      "The physics of flow predicts that systems with this property should be robust in the face of congestion.",
      "Note that QJump which we looked at last week uses exactly this principle.",
      "If this principle were obeyed, congestion collapse would become the exception rather than the rule.",
      "Thus congestion control involves finding places that violate conservation and fixing them.",
      "There are only three ways that packets can fail to be conserved:  The connection doesn\u2019t get to equilibrium (typically, the sender floods the receiver on start or restart)  A sender injects a new packet before an old packet has exited  The equilibrium can\u2019t be reached because of resource limits along the path  These can be addressed by, respectively: starting slowly to bring a connection up to equilibrium; conserving equilibrium through management of RTT variability to avoid too many retransmissions; and adapting the path by throttling back sources under in-network congestion.",
      "Slow Start  Once a connection is up and running acks can regulate a \u2018clock\u2019 that stops packets being sent too quickly for the receiver, since acks can\u2019t be generated any faster than packets can get through the network.",
      "But how do you get the data flowing in the first place?",
      "To start the `clock\u2019, we developed a slow-start algorithm to gradually increase the amount of data in-transit.",
      "Although we flatter ourselves that the design of this algorithm is rather subtle, the implementation is trivial\u2014one new state variable and three lines of code in the sender:  Add a congestion window, cwnd, to the per-connection state.",
      "When starting or restarting after a loss, set cwnd to one packet.",
      "On each ack for newdata, increase cwnd by one packet.",
      "When sending, send the minimum of receiver\u2019s advertised window and cwnd.",
      "Actually, the slow-start window increase isn\u2019 t that slow: it takes time R log2W where R is the round-trip-time and W is the window size in packets.",
      "This means the window opens quickly enough to have a negligible effect on performance, even on links with a large bandwidth\u2013delay product.",
      "Retransmissions (retries)  A good round trip time estimator, the core of the retransmit timer, is the single most important feature of any protocol implementation that expects to survive heavy load.",
      "And it is frequently botched\u2026One mistake is not estimating the variation of the round trip time, R .",
      "From queuing theory we know that R and the variation in R increase quickly with load.",
      "See the details in appendix A of the paper for a cheap to compute variation estimator (prior versions of the TCP specification recommended a constant value to use), which can then be used to update the retransmit timeout operator for the next packet sent.",
      "Another timer mistake is in the backoff after a retransmit: If a packet has to be retransmitted more than once, how should the retransmits be spaced?",
      "For a transport endpoint embedded in a network of unknown topology and with an unknown, unknowable and constantly changing population of competing conversations, only one scheme has any hope of working\u2014exponential backoff\u2014but a proof of this is beyond the scope of this paper.",
      "It\u2019s all to do with linear system theory apparently:  Linear system theory says that if a system is stable, the stability is exponential.",
      "This suggests that an unstable system (a network subject to random load shocks and prone to congestive collapse can be stabilized by adding some exponential damping (exponential timer backoff) to its primary excitation (senders, traffic sources).",
      "Congestion management  If timeouts are set appropriately as above, then timeouts will be due to lost packets.",
      "Packets could be damaged in transit (comparatively rare), or dropped due to congestion in the network.",
      "Therefore we can assume that timeouts are a good proxy sign for network congestion.",
      "The network must be able to signal the transport endpoints that congestion is occurring (or about to occur).",
      "And the endpoints must have a policy that decreases utilization if this signal is received and increases utilization if the signal isn\u2019 t received.",
      "We can use timeouts as the signal, but what shoud the policy be?",
      "When the network is congested\u2026 the queue lengths will start increasing exponentially.",
      "The system will stabilize only if the traffic sources throttle back at least as quickly as the queues are growing.",
      "Load is controlled by adjusting the size of the window.",
      "So when congestion is signalled the window  is resized to d.W, where d is a constant   The first thought is to use a symmetric, multiplicative increase, possibly with a longer time constant,  Wi = bWi-1 where 1 < b < 1/d .",
      "This is a mistake.",
      "The result will oscillate wildly and, on the average, deliver poor throughput.",
      "The best increase policy is to make small, constant changes to the window size: Wi = Wi-1 + u.",
      "We end up with the following simple implementation:  On any timeout, set cwnd to half the current window size (this is the multiplicative decrease).",
      "On each ack for new data, increase cwnd by 1/cwnd (this is the additive increase).",
      "When sending, send the minimum of the receiver\u2019 s advertised window and cwnd."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://ee.lbl.gov/papers/congavoid.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 8151447
  },
  {
    "blog_id": "markovian_gans",
    "summary": [
      "See also  Video explanation by authors  What  They describe a method that can be used for two problems:  (1) Choose a style image and apply that style to other images.",
      "(2) Choose an example texture image and create new texture images that look similar.",
      "In contrast to previous methods their method can be applied very fast to images (style transfer) or noise (texture creation).",
      "However, per style/texture a single (expensive) initial training session is still necessary.",
      "Their method builds upon their previous paper \" Combining Markov Random Fields and Convolutional Neural Networks for Image Synthesis \".",
      "How  Rough overview of their previous method:  Transfer styles using three losses:  Content loss: MSE between VGG representations.",
      "Regularization loss: Sum of x-gradient and y-gradients (encouraging smooth areas).",
      "MRF-based style loss: Sample k x k patches from VGG representations of content image and style image.",
      "For each patch from content image find the nearest neighbor (based on normalized cross correlation) from style patches.",
      "Loss is then the sum of squared errors of euclidean distances between content patches and their nearest neighbors.",
      "Generation of new images is done by starting with noise and then iteratively applying changes that minimize the loss function.",
      "They introduce mostly two major changes:  (a) Get rid of the costly nearest neighbor search for the MRF loss.",
      "Instead, use a discriminator-network that receives a patch and rates how real that patch looks.",
      "This discriminator-network is costly to train, but that only has to be done once (per style/texture).",
      "(b) Get rid of the slow, iterative generation of images.",
      "Instead, start with the content image (style transfer) or noise image (texture generation) and feed that through a single generator-network to create the output image (with transfered style or generated texture).",
      "This generator-network is costly to train, but that only has to be done once (per style/texture).",
      "MDANs  They implement change (a) to the standard architecture and call that an \"MDAN\" (Markovian Deconvolutional Adversarial Networks).",
      "So the architecture of the MDAN is:  Input: Image (RGB pixels)  Branch 1: Markovian Patch Quality Rater (aka Discriminator)  Starts by feeding the image through VGG19 until layer relu3_1.",
      "(Note: VGG weights are fixed/not trained.)",
      "Then extracts k x k patches from the generated representations.",
      "Feeds each patch through a shallow ConvNet (convolution with BN then fully connected layer).",
      "Training loss is a hinge loss, i.e. max margin between classes +1 (real looking patch) and -1 (fake looking patch).",
      "(Could also take a single sigmoid output, but they argue that hinge loss isn't as likely to saturate.)",
      "This branch will be trained continuously while synthesizing a new image.",
      "Branch 2: Content Estimation/Guidance  Note: This branch is only used for style transfer, i.e if using an content image and not for texture generation.",
      "Starts by feeding the currently synthesized image through VGG19 until layer relu5_1.",
      "(Note: VGG weights are fixed/not trained.)",
      "Also feeds the content image through VGG19 until layer relu5_1.",
      "Then uses a MSE loss between both representations (so similar to a MSE on RGB pixels that is often used in autoencoders).",
      "Nothing in this branch needs to trained, the loss only affects the synthesizing of the image.",
      "MGANs  The MGAN is like the MDAN, but additionally implements change (b), i.e. they add a generator that takes an image and stylizes it.",
      "The generator's architecture is:  Input: Image (RGB pixels) or noise (for texture synthesis)  Output: Image (RGB pixels) (stylized input image or generated texture)  The generator takes the image (pixels) and feeds that through VGG19 until layer relu4_1.",
      "Similar to the DCGAN generator, they then apply a few fractionally strided convolutions (with BN and LeakyReLUs) to that, ending in a Tanh output.",
      "(Fractionally strided convolutions increase the height/width of the images, here to compensate the VGG pooling layers.)",
      "The output after the Tanh is the output image (RGB pixels).",
      "They train the generator with pairs of (input image, stylized image or texture).",
      "These pairs can be gathered by first running the MDAN alone on several images.",
      "(With significant augmentation a few dozen pairs already seem to be enough.)",
      "One of two possible loss functions can then be used:  Simple standard choice: MSE on the euclidean distance between expected output pixels and generated output pixels.",
      "Can cause blurriness.",
      "Better choice: MSE on a higher VGG representation.",
      "Simply feed the generated output pixels through VGG19 until relu4_1 and the reuse the already generated (see above) VGG-representation of the input image.",
      "This is very similar to the pixel-wise comparison, but tends to cause less blurriness.",
      "Note: For some reason the authors call their generator a VAE, but don't mention any typical VAE technique, so it's not described like one here.",
      "They use Adam to train their networks.",
      "For texture generation they use Perlin Noise instead of simple white noise.",
      "In Perlin Noise, lower frequency components dominate more than higher frequency components.",
      "White noise didn't work well with the VGG representations in the generator (activations were close to zero).",
      "Results  Similar quality like previous methods, but much faster (compared to most methods).",
      "For the Markovian Patch Quality Rater (MDAN branch 1):  They found that the weights of this branch can be used as initialization for other training sessions (e.g. other texture styles), leading to a decrease in required iterations/epochs.",
      "Using VGG for feature extraction seems to be crucial.",
      "Training from scratch generated in worse results.",
      "Using larger patch sizes preserves more structure of the structure of the style image/texture.",
      "Smaller patches leads to more flexibility in generated patterns.",
      "They found that using more than 3 convolutional layers or more than 64 filters per layer provided no visible benefit in quality.",
      "Result of their method, compared to other methods.",
      "Architecture of their model."
    ],
    "author_id": "ALEJU",
    "pdf_url": "http://arxiv.org/pdf/1604.04382",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 37010180
  },
  {
    "blog_id": "towards-deploying-decommissioned-mobile-devices-as-cheap-energy-efficient-compute-nodes",
    "summary": [
      "Towards deploying decommissioned mobile devices as cheap energy-efficient compute nodes Shahrad & Wentzlaff, HotCloud\u201917  I have one simple rule when it comes to selecting papers for The Morning Paper: I only cover papers that I like and find interesting.",
      "There are some papers though, that manage to generate in me a genuine feeling of excitement, as in \u201cthis is so cool, I can\u2019t wait to share it.\u201d This is one of those papers!",
      "You may remember back in 2010 when the US Air Force had the breakthrough idea to create a supercomputer out of 1,760 Sony Playstations .",
      "Well, Shahrad & Wentzlaff want us to pack our data centers with racks of decommissioned mobile phones!",
      "The case they put forward is both fascinating and compelling.",
      "Firstly, there are a lot of old mobile phones, and the mobile system on chips (SoCs) inside them have been gaining in power while having a low TCO.",
      "Secondly, it\u2019s possible to pack a number of them in a 2U unit.",
      "Thirdly, there are a number of use cases for which such a collection of wimpy nodes seem well suited.",
      "And finally of course, it\u2019s a wonderfully green way of recycling older devices that may e.g., have cracked screens, sluggish software etc..",
      "Deploying decommissioned mobile devices can be a major move towards green computing.",
      "This is mostly due to the fact that most of the carbon footprint of those devices comes from their production.",
      "Such deployment extends effective lifetime of mobile devices and decreases their average global warming potential (GWP), benefiting the environment.",
      "Are mobile phones really powerful enough to be useful in a data center?",
      "Both industry and academia already have their eye on mobile SoCs as the next most cost-effective platform in HPC \u2013 the gap between mobile SoCs and commodity server processors is shrinking and their TCO is much lower.",
      "If you look at mobile SoC performance for the last five years, something very interesting shows up:  (What a lovely s-curve example btw.).",
      "Moore\u2019s law is kicking in, and the performance gap between a new and 3-year old device will shrink  The relative performance gap between high-end and low-end SoCs is shrinking, leading to similar performance on a cheaper device.",
      "Mobile CPU single core thermal design point has saturated at around 1.5W, so the performance power budget should stay steady as devices scale.",
      "Meanwhile, newer devices actually have slightly lower energy efficiency as they push for the last reserves of power.",
      "So decommissioned devices will actually have better overall energy efficiency.",
      "What applications could you run on a bunch of old phones?",
      "Due to their energy efficiency and improved performance, ARM-based architectures have recently gained substantial attention for HPC and cloud infrastructure deployment.",
      "ARM multicores deliver good energy proportionality for server workloads.",
      "Here are some promising use cases:  I/O intensive applications that are unable to saturate their CPU.",
      "Modern mobile SoCs support high bandwidth I/O and ample RAM size so I/O intensive applications can run on them with less I/O-CPU mismatch.",
      "Will your next VM be running on a decommissioned mobile phone?",
      "Low-end VMs on Amazon EC2 burstable t2.nano and t2.micro have 0.5GB and 1GB of memory respectively.",
      "Common hypervisors (KVM, Xen, \u2026) support virtualizing ARM and an average mobile device has more than 2GB of memory \u2013 so a cloud provider could assign multiple such instances to each device!",
      "Applications requiring low-end GPU acceleration for platforms such as OpenCL.",
      "A SoC\u2019s GPU can be shared between multiple tenants.",
      "Increasing the heterogeneity of cloud infrastructure to diversify reliability.",
      "How do you efficiently install mobile phone arrays inside a data center?",
      "The authors\u2019 proposed design shows that decommissioned mobile devices can be housed in standard server racks.",
      "With three rows of fans, a network router, and a power supply, there is room for 84 cages (smartphones) of a size that fits more than 75% of models (notably excluding tablets!).",
      "With an average of 5.6 CPU cores per device, that adds up to about 470 cores in a 2U server box.",
      "Networking can be achieved either with a USB tree and shared master node, or USB on-the-go to each device.",
      "The latter will give much higher network performance, but requires more network switches.",
      "The phones come with another advantage that we get for free \u2013 batteries!",
      "Researchers have proposed using distributed UPSs or batteries to shave peak power in data centers.",
      "This allows installing more servers using the same power infrastructure and decreases the TCO\u2026 Distributed batteries effectively dampen temporal power demand variations; shaving the peak power under high utilization, while storing energy under low utilization.",
      "The high energy storage density enables more aggressive power capping of servers that are filled with used mobile devices.",
      "Even assuming 15% battery degradation per year, the capacity will be 4-8x denser than purpose designed distributed UPS solutions.",
      "Is it cost effective?",
      "We\u2019ve seen that in theory racks of decommissioned mobile devices can be done, and we\u2019ve seen that there are some potential use cases for such systems.",
      "But does it make financial sense??",
      "The authors choose the Samsung Galaxy Note 4 as a representative three-year old device, and match it against a Lenovo Flex System x880 X6 which has similar performance as 84 Note 4s.",
      "CAPEX and OPEX work out as follows (the authors assumed that the monitoring engineering, and installation cost of the mobile array is twice that of a standard server):  A TCO analysis shows that the mobile array beats the traditional server on TCO by some margin.",
      "(In the figures below \u2018A\u2019 is the traditional server, \u2018B\u2019 is the mobile array, and &delta; is the depreciation rate).",
      "The right sub-figures in Figure 5 (above) compare TCO when those two servers have different lifetimes.",
      "This analysis is essential for a fair comparison because we anticipate our proposed server to have a shorter lifetime compared to a new high-end server.",
      "It can be seen that with much shorter lifetimes, our proposed server can deliver better TCO values.",
      "It also shows how the equal-TCO margin (the line between light and dark areas) varies for different depreciation rates."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://www.usenix.org/system/files/conference/hotcloud17/hotcloud17-paper-shahrad.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 31459000
  },
  {
    "blog_id": "achieving-human-parity-in-conversational-speech-recognition",
    "summary": [
      "Achieving Human Parity in Conversational Speech Recognition Xiong et al. Microsoft Technical Report, 2016  The headline story here is that for the first time a system has been developed that exceeds human performance in one of the most difficult of all human speech recognition tasks: natural conversations held over the telephone.",
      "This is known as conversational telephone speech, or CTS.",
      "[CTS] is especially difficult due to the spontaneous (neither read nor planned) nature of the speech, its informality, and the self-corrections, hesitations, and other disfluencies that are pervasive.",
      "The reference datasets for this task are the Switchboard and Fisher data collections from the 1990s and early 2000s.",
      "The apocryphal story here is that human performance on the task is about 4% error rate.",
      "But no-one can quite pin down where that 4% number comes from.",
      "So the Microsoft team took advantage of an existing professional transcription service used by Microsoft:  To measure human performance, we leveraged an existing pipeline in which Microsoft data is transcribed on a weekly basis.",
      "This pipeline uses a large commercial vendor to perform a two-pass transcription.",
      "In the first pass, a transcriber works from scratch to transcribe the data.",
      "In the second pass, a second listener monitors the data to do error correction.",
      "Dozens of hours of test data are processed in each batch.",
      "One week, we added the NIST 2000 CTS evaluation data to the work-list, without further comment\u2026  For the switchboard portion of the dataset, the professional human transcribers achieved a 5.9% error rate, and for the \u2018call-home\u2019 portion of the test set 11.3%.",
      "\u201cThe same informality, multiple speakers per channel, and recording conditions that make CallHome hard for computers make it difficult for people as well.\u201d  Notably, the performance of our artificial system aligns almost exactly with the performance of people on both sets.",
      "And so one can\u2019t help but wonder how much longer those professional transcribers will continue to be needed!",
      "Did they know they were gathering the evidence that one day might help lead to the elimination of their jobs???",
      "Here\u2019s a table full of cryptic acronyms that show the performance of the system that Microsoft put together, using various acoustic models.",
      "The bottom-line (strictly, bottom two lines) is what matters here: parity on the switchboard dataset, and a slight advantage for the ASR (Automatic Speech Recognition) system of the CallHome dataset.",
      "How did the Microsoft team manage to pull this off?",
      "Our system\u2019s performance can be attributed to the systematic use of LTSMs for both acoustic and language modeling, as well as CNNs in the acoustic model, and extensive combination of complementary models.",
      "Training was made feasible (reducing times from month to 1-3 weeks) by using Microsoft\u2019s CNTK Cognitive Toolkit to parallelize SGD training, coupled with the use of the 1-bit SGD parallelization techique from prior work:  In [65], we showed that gradient values can be quantized to just a single bit, if one carries over the quantization error from one minibatch to the next.",
      "Each time a sub-gradient is quantized, the quantization error is computed and remembered, and then added to the next minibatch\u2019s sub-gradient.",
      "This reduces the required bandwidth 32-fold with minimal loss in accuracy.",
      "How it works under the covers  The model details are concisely explained, targeting an audience of speech recognition experts (i.e. not me!).",
      "It is still possible for a lay-reader to gain some appreciation of what\u2019s involved though.",
      "It\u2019s also another reminder that we\u2019re rapidly assembling a powerful collection of building blocks that through good systems engineering can be combined into very effective systems.",
      "Expect to see an explosion of applied AI/ML/whatever-your-preferred-phrase-is applications as this trend continues.",
      "Our progress is a result of the careful engineering and optimization of convolutional and recurrent neural networks.",
      "While the basic structures have been well known for a long period, it is only recently that they have emerged as the best models for speech recognition.",
      "Surprisingly, this is the case for both acoustic modeling and language modeling.",
      "The CNN and RNN based acoustic models can model a large amount of acoustic context with temporal invariance, and in the case of CNNs, with frequency invariance as well.",
      "In language modeling, RNNs improve on classical N-gram models through the use of an unbounded word history and the generalization ability of continuous word representations .",
      "The paper describes a whole family of systems that were explored in order to find the best performing combination.",
      "The best acoustic model was formed by combining independently trained ResNet and VGG models using a score fusion weight.",
      "\u2018VGG\u2019 stands for the University of Oxford Visual Geometry Group and the architecture they developed in \u201c Very deep convolutional networks for large-scale visual recognition \u201d.",
      "Their networks use 16-19 layers with small 3\u00d73 filters in all convolutional layers.",
      "The ResNet architecture is also borrowed from the field of image recognition.",
      "Speaker adaptive modeling is then applied by conditioning the network on an i-vector characterization of each speaker using 100-dimensional i-vectors.",
      "The i-vectors are added to the activation of each CNN layer via a learnable weight matrix.",
      "After initial training, model parameters are optimized using a maximum mutual information (MMI) objective function:  where w is a word sequence, and a is an acoustic realization of a word sequence.",
      "The performance improvements obtained from this lattice-free MMI (LFMMI) training phase, as well as i-vectors, can be seen in the following table:  An initial decoding of acoustic model outputs is done with a WFST ( Weighted Finite State Transducer ) decoder.",
      "We use an N-gram language model trained and pruned with the SRILM toolkit.",
      "The first-pass LM has approximately 15.9 million bigrams, trigrams, and 4-grams, and a vocabulary of 30500 words, and give a perplexity of 54 on RT-03 speech transcripts.",
      "The N-best performing hypotheses from the WFST decoding are then rescored using a combination of a large N-gram language model and neural net language models.",
      "The best performing language model used LTSMs with three hidden layers, and 1000 hidden units in each layer.",
      "\u201cFor the final system, we interpolated two LSTM-LMs with an N-gram LM for the forward direction LM, and similarly for the backward direction LM.\u201d"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://arxiv.org/pdf/1610.05256",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 12625177
  },
  {
    "blog_id": "includeos",
    "summary": [
      "IncludeOS: A minimal, resource efficient unikernel for cloud systems \u2013 Bratterud et al. 2015  There has been lots of excitement around unikernels over the last year, and especially with the recent acquisition of the Unikernel Systems team by Docker ( MirageOS , Mergeable Persistent Data Structures , Jitsu: Just-in time summoning of Unikernels ).",
      "Whereas MirageOS is built around an OCaml stack, in today\u2019s paper choice we get a look at IncludeOS, which is built on a C++ stack.",
      "In true unikernel style, you just include the parts of the operating system you need, directly linked with your application.",
      "What makes me smile every time is that in IncludeOS this is literally achieved via \u2018#include <os>\u2019 !",
      "In this paper we present IncludeOS, a single-tasking operating system designed for virtualizedenvironments.",
      "IncludeOS provides a novel way for developers to build their C++-based code directlyinto a virtual machine at compile-time\u2026  A fully virtualized \u201cHello World\u201d service in IncludeOS (which of course includes the necessary components of the OS) uses only 8.45MB of memory.",
      "A Ubuntu 14.04 OS image (the default guest OS for OpenStack) is around 300MB by comparison.",
      "Even running a regular Java Hello World program (ignoring the OS), just the Java process itself takes about 28MB.",
      "If you\u2019re spinning up lots of instances in your cluster, this reduction in memory overhead can result in significant savings \u2013 memory being one of the most expensive resources.",
      "A minimal IncludeOS VM can also boot in about 0.3s.",
      "A DNS service built with IncludeOS results in a 158K disk image (for comparison, the MirageOS DNS server image came in at 200K).",
      "Finally, IncludeOS is designed to be very efficient at runtime, when idle it uses no CPU at all.",
      "The designers of IncludeOS were guided by the \u2018Zero Overhead Principle\u2018 :  IncludeOS aims for true minimality in the sense that nothing should be included by default that the service does not explicitly need.",
      "This corresponds to the zero overhead principle of e.g. C++; \u201dwhat you don\u2019t use you don\u2019t pay for.\u201d \u2026 While many other projects are related, IncludeOS is different: where systems such as Mirage and OSv aims to provide a platform for a high-level language-runtimes, which impose significant resource penalties in themselves, IncludeOS aims to represent absolute minimality.",
      "Including only what is needed in an OS image is a job that can be delegated to the GCC tool chain:  The mechanism used for extracting only what is needed from the operating system, is the one provided by default by modern linkers.",
      "Each part of the OS is compiled into an object-file, such as ip4.o, udp.o, pci_device.o etc., which are then combined using ar to form a static library os.a.",
      "When a program links with this library, only what\u2019s necessary will automatically be extracted by the linker and end up in the final binary.",
      "To facilitate this build process a custom GCC-toolchain has been created.",
      "For the C standard library, IncludeOS uses RedHat\u2019s standard library implementation due to its small size, reliance on only a handful of system calls, and ability to be compiled into a statically linked library.",
      "\u201cThe C++ standard library is larger and trickier\u2026\u201d Currently IncludeOS uses Electronic Art\u2019s EASTL exception-free implementation, future work will include a port of a full-featured implementation.",
      "IncludeOS currently has only one device driver, namely a VirtioNet Device driver.",
      "The key benefit of virtio is that the hypervisor does not need to emulate a certain physical device, but instead can insert data directly into a queue in memory shared by the guest.",
      "While Virtio 1.0 has recently emerged as an OASIS standard, none of the hypervisors used during development supported any of the new features.",
      "Therefore the driver currently only implements Virtio Legacy functionality, but development has been done with future support for Virtio 1.0 in mind.",
      "The network stack was a more complex challenge, since existing network stacks are often entangled with the operating system and not designed with the zero overhead principle in mind.",
      "The IncludeOS project is working on a completely modularized networking stack \u2013 the current implementation is sufficiently advanced to support e.g. the DNS server implementation previously mentioned, and work is underway to complete a full TCP and IPv6 stack that will also be running standalone in Linux user space.",
      "Currently, all IRQ handlers in IncludeOS will simply (atomically) update a counter, and defer further handling to the main event-loop, whenever there is time.",
      "This eliminates the need for a context switch, while also eliminating concurrency-related issues such as race conditions.",
      "The CPU is kept busy by having all I/O be asynchronous, so that no blocking occurs.",
      "This encourages a callback-based programming model, such as is common in modern Javascript applications.",
      "This is one of a number of factors that contribute to IncludeOS\u2019s excellent runtime performance:  There is no system call overhead as the OS and the service are one binary, eliminating the need for memory protection barriers.",
      "There is no unnecessary overhead from timer interrupts.",
      "There is no I/O waiting, since IncludeOS uses an asynchronous event-based I/O model.",
      "There is no overhead from emulating the Programmable Interrupt Timer (i.e. no periodic timer interrupts, and no pre-emptive scheduling).",
      "The number of protected instructions has been kept very low reduce VM exits.",
      "That being so, IncludeOS in its current form is not fit for every task.",
      "In particular, deferring all IRQ\u2019s will cause the VM to seem unresponsive (i.e. not answer ping) under workloads requiring a lot of CPU activity per request (this is not the case for DNS)\u2026 For services requiring several seconds of CPU-processing for each request, ICMP-packets would simply be queued until the virtio-queue was full, at which point they would be dropped, giving the impression of an unresponsive service.",
      "A really interesting future development for IncludeOS is the design of a Node.js style framework for supporting high performance web-applications:  A near-future use case for IncludeOS will be running high-performance web-applications, written in an asynchronous programming style similar to Node.js, but in a maximally efficient and minimal-overhead C++ language framework.",
      "These services will have no host kernel dependencies, running directly on top of virtual hardware, in any IaaS cloud."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://github.com/hioa-cs/IncludeOS/blob/master/doc/papers/IncludeOS_IEEE_CloudCom2015_PREPRINT.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 76635514
  },
  {
    "blog_id": "fractional_max_pooling",
    "summary": [
      "What and why  Traditionally neural nets use max pooling with 2x2 grids (2MP).",
      "2MP reduces the image dimensions by a factor of 2.",
      "An alternative would be to use pooling schemes that reduce by factors other than two, e.g. 1 < factor < 2.",
      "Pooling by a factor of sqrt(2) would allow twice as many pooling layers as 2MP, resulting in \"softer\" image size reduction throughout the network.",
      "Fractional Max Pooling (FMP) is such a method to perform max pooling by factors other than 2.",
      "How  In 2MP you move a 2x2 grid always by 2 pixels.",
      "Imagine that these step sizes follow a sequence, i.e. for 2MP: 2222222...",
      "If you mix in just a single 1 you get a pooling factor of <2.",
      "By chosing the right amount of 1s vs. 2s you can pool by any factor between 1 and 2.",
      "The sequences of 1s and 2s can be generated in fully random order or in pseudorandom order, where pseudorandom basically means \"predictable sub patterns\" (e.g. 211211211211211...).",
      "FMP can happen disjoint or overlapping.",
      "Disjoint means 2x2 grids, overlapping means 3x3.",
      "Results  FMP seems to perform generally better than 2MP.",
      "Better results on various tests, including CIFAR-10 and CIFAR-100 (often quite significant improvement).",
      "Best configuration seems to be random sequences with overlapping regions.",
      "Results are especially better if each test is repeated multiple times per image (as the random sequence generation creates randomness, similar to dropout).",
      "First 5-10 repetitions seem to be most valuable, but even 100+ give some improvement.",
      "An FMP-factor of sqrt(2) was usually used.",
      "Random FMP with a factor of sqrt(2) applied five times to the same input image (results upscaled back to original size).",
      "Rough chapter-wise notes  (1) Convolutional neural networks  Advantages of 2x2 max pooling (2MP): fast; a bit invariant to translations and distortions; quick reduction of image sizes  Disadvantages: \"disjoint nature of pooling regions\" can limit generalization (i.e. that they don't overlap?",
      "); reduction of image sizes can be too quick  Alternatives to 2MP: 3x3 pooling with stride 2, stochastic 2x2 pooling  All suggested alternatives to 2MP also reduce sizes by a factor of 2  Author wants to have reduction by sqrt(2) as that would enable to use twice as many pooling layers  Fractional Max Pooling = Pooling that reduces image sizes by a factor of 1 < alpha < 2  FMP introduces randomness into pooling (by the choice of pooling regions)  Settings of FMP:  Pooling Factor alpha in range [1, 2] (1 = no change in image sizes, 2 = image sizes get halfed)  Choice of Pooling-Regions: Random or pseudorandom.",
      "Random is stronger (?).",
      "Random+Dropout can result in underfitting.",
      "Disjoint or overlapping pooling regions.",
      "Results for overlapping are better.",
      "(2) Fractional max-pooling  For traditional 2MP, every grid's top left coordinate is at (2i-1, 2j-1) and it's bottom right coordinate at (2i, 2j) (i=col, j=row).",
      "It will reduce the original size N to 1/2N, i.e. 2N_in = N_out.",
      "Paper analyzes 1 < alpha < 2, but alpha > 2 is also possible.",
      "Grid top left positions can be described by sequences of integers, e.g. (only column): 1, 3, 5, ...  Disjoint 2x2 pooling might be 1, 3, 5, ... while overlapping would have the same sequence with a larger 3x3 grid.",
      "The increment of the sequences can be random or pseudorandom for alphas < 2.",
      "For 2x2 FMP you can represent any alpha with a \"good\" sequence of increments that all have values 1 or 2, e.g. 2111121122111121...",
      "In the case of random FMP, the optimal fraction of 1s and 2s is calculated.",
      "Then a random permutation of a sequence of 1s and 2s is generated.",
      "In the case of pseudorandom FMP, the 1s and 2s follow a pattern that leads to the correct alpha, e.g. 112112121121211212...  Random FMP creates varying distortions of the input image.",
      "Pseudorandom FMP is a faithful downscaling.",
      "(3) Implementation  In their tests they use a convnet starting with 10 convolutions, then 20, then 30, ...",
      "They add FMP with an alpha of sqrt(2) after every conv layer.",
      "They calculate the desired output size, then go backwards through their network to the input.",
      "They multiply the size of the image by sqrt(2) with every FMP layer and add a flat 1 for every conv layer.",
      "The result is the required image size.",
      "They pad the images to that size.",
      "They use dropout, with increasing strength from 0% to 50% towards the output.",
      "They use LeakyReLUs.",
      "Every time they apply an FMP layer, they generate a new sequence of 1s and 2s.",
      "That indirectly makes the network an ensemble of similar networks.",
      "The output of the network can be averaged over several forward passes (for the same image).",
      "The result then becomes more accurate (especially up to >=6 forward passes).",
      "(4) Results  Tested on MNIST and CIFAR-100  Architectures (somehow different from (3)?",
      "):  MNIST: 36x36 img -> 6 times (32 conv (3x3?)",
      "-> FMP alpha=sqrt(2)) -> ?",
      "-> ?",
      "-> output  CIFAR-100: 94x94 img -> 12 times (64 conv (3x3?)",
      "-> FMP alpha=2^(1/3)) -> ?",
      "-> ?",
      "-> output  Overlapping pooling regions seemed to perform better than disjoint regions.",
      "Random FMP seemed to perform better than pseudorandom FMP.",
      "Other tests:  \"The Online Handwritten Assamese Characters Dataset\": FMP performed better than 2MP (though their network architecture seemed to have significantly more parameters  \"CASIA-OLHWDB1.1 database\": FMP performed better than 2MP (again, seemed to have more parameters)  CIFAR-10: FMP performed better than current best network (especially with many tests per image)"
    ],
    "author_id": "ALEJU",
    "pdf_url": "http://arxiv.org/pdf/1412.6071",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 32533290
  },
  {
    "blog_id": "churchs-thesis-and-functional-programming",
    "summary": [
      "Church\u2019s Thesis and Functional Programming \u2013 Turner 2006  One of a collection of papers celebrating the 70th anniversary of Church\u2019s thesis in 2006, as recently recommended by Erik Meijer on twitter.",
      "Both the thesis and the lambda calculus have been of seminal influence on the development of Computing Science.",
      "There were three independently developed definitions for the computable functions of the natural numbers in the early -mid 1930s.",
      "These were proved to be equivalent, and then\u2026  A few months later Turing (1936) introduced his concept of a logical computing machine \u2013 a finite automation with an unbounded tape divided into squares\u2026.",
      "In reviewing the Turing paper in 1937, Church wrote:  there is involved here the equivalence of three different notions: computability by a Turing machine, general recursiveness .",
      ".",
      ".",
      "and lambda-definability .",
      ".",
      ".The first has the advantage of making the identification with effectiveness in the ordinary sense evident immediately \u2026 The second and third have the advantage of suitability for embodiment in a system of symbolic logic  The Turing machine ultimately led to the development of the Turing/von-Neumann computer.",
      "The second and third notions led to the development of functional programming.",
      "Of the various convergent notions of computability Church\u2019s lambda calculus is distinguished by its combination of simplicity with remarkable expressive power.",
      "The lambda calculus has three productions: a variable, function application, and function abstraction (a lambda expression, for example \u03bbx.x ).",
      "The essential rule of substitution, called beta reduction, tells us how to supply a value for a function parameter and substitute all occurences of that parameter in the expression body for the provided value.",
      "In other words, what it means to provide an argument to a function.",
      "Also note that it is from the lamda calculus we get the term \u2018combinator\u2019.",
      "A combinator is simply a lambda expression with no unbound variables.",
      "(called a \u2018closed term\u2019).",
      "Famous combinators are named by single letters.",
      "For example, the identity combinator is called \u2018I\u2019:  I = \u03bbx.x  And the constant combinator is called \u2018K\u2019:  K = \u03bbx.\u03bby.x  (Constant because K z returns a function that always returns z, whatever argument you give it\u2026).",
      "Combinators can also be used to support recursive functions:  The master-stroke, which shows every recursive function to be \u03bb-definable is to find a universal fixpoint operator, that is a term Y with the property that for any term F, Y F \u21d4 F(Y F) There are many such terms, of which the simplest is due to H.B.Curry.",
      "Y = \u03bbf.",
      "(\u03bbx.f(xx))(\u03bbx.f(xx))  By convention, this combinator is represented by the letter Y.",
      "Yes HN, that\u2019s the \u2018y-combinator.\u2019  Church showed how to represent numbers as function iterators (a, f a, f f a, f f f a , \u2026).",
      "The scheme also works for any data type.",
      "Moreover we are not limited to arithmetic.",
      "The idea behind the Church numerals is very general and allows any data type \u2014 pairs, lists, trees and so on \u2014 to be represented in a purely functional way.",
      "Each datum is encoded as a function that captures its elimination operation, that is the way in which information is extracted from it during computation.",
      "It is also possible to represent codata, such as infinite lists, infinitary trees and so on.",
      "There follows a wonderful section on functional programming, contrasted with imperative programming:  Imperative programming languages, from the earliest such as FORTRAN and COBOL which emerged in the 1950\u2019s to current \u201dobject-oriented\u201d ones such as C++ and Java have certain features in common.",
      "Their basic action is the assignment command, which changes the content of a location in memory and they have an explicit flow of control by which these state changes are ordered.",
      "This reflects more or less directly the structure of the Turing/von Neumann computer, as a central processing unit operating on a passive store.",
      "Backus (1978) calls them \u201dvon Neumann languages\u201d.",
      "Functional programming languages offer a radical alternative \u2014 they are descriptive rather than imperative, have no assignment command and no explicit flow of control \u2014 sub-computations are ordered only partially, by data dependency.",
      "(We\u2019ve encountered this idea earlier in the morning paper series too, at #themorningpaper no.",
      "20 ).",
      "Now comes an important point about what happens when you try to add functional support to imperative languages, echoed by Erik Meijer recently The Curse of the Excluded Middle , #themorningpaper no.",
      "41. :  The disadvantages of functional programming within a language that includes imperative features are two.",
      "First, you are not forced to explore the limits of the functional style, since you can escape at will into an imperative idiom.",
      "Second, the presence of side effects, exceptions etc., even if they are rarely used, invalidate important theorems on which the benefits of the style rest.",
      "There are many other good passages within the paper which we do not have space here to enumerate.",
      "I leave you with an extended quotation from the conclusions:  Church\u2019s Thesis played a founding role in computing theory by providing a single notion of effective computability.",
      "Without this foundation we might have been stuck with a plethora of notions of computability depending on computer architecture, programming language etc.",
      ": we might have Motorola-computable versus Intel-computable, Java-computable versus C-computable and so on.",
      "The \u03bb-calculus, which Church developed during the period of convergence from which the Thesis emerged, has influenced almost every aspect of the development of programming and programming languages.",
      "It is the basis of functional programming, which after a long infancy is entering adulthood as a practical alternative to traditional ad-hoc imperative programming languages.",
      "Many important ideas in mainstream programming languages\u2014recursion, procedures as parameters, linked lists and trees, garbage collectors \u2014 came by cross fertilization from functional programming.",
      "Moreover the main schools of both operational and denotational semantics are \u03bb-calculus based and amount to using functional programming to explain other programming systems."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://www.cs.kent.ac.uk/people/staff/dat/miranda/ctfp.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 37263635
  },
  {
    "blog_id": "swapout",
    "summary": [
      "What  They describe a regularization method similar to dropout and stochastic depth.",
      "The method could be viewed as a merge of the two techniques (dropout, stochastic depth).",
      "The method seems to regularize better than any of the two alone.",
      "How  Let x be the input to a layer.",
      "That layer produces an output.",
      "The output can be:  Feed forward (\"classic\") network: F(x).",
      "Residual network: x + F(x).",
      "The standard dropout-like methods do the following:  Dropout in feed forward networks: Sometimes 0, sometimes F(x).",
      "Decided per unit.",
      "Dropout in residual networks (rarely used): Sometimes 0, sometimes x + F(x).",
      "Decided per unit.",
      "Stochastic depth (only in residual networks): Sometimes x, sometimes x + F(x).",
      "Decided per layer.",
      "Skip forward (only in residual networks): Sometimes x, sometimes x + F(x).",
      "Decided per unit.",
      "Swapout (any network): Sometimes 0, sometimes F(x), sometimes x, sometimes x + F(x).",
      "Decided per unit.",
      "Swapout can be represented using the formula y = theta_1 * x + theta_2 * F(x).",
      "* is the element-wise product.",
      "theta_1 and theta_2 are tensors following bernoulli distributions, i.e. their values are all exactly 0 or exactly 1.",
      "Setting the values of theta_1 and theta_2 per unit in the right way leads to the values 0 (both 0), x (1, 0), F(x) (0, 1) or x + F(x) (1, 1).",
      "Deterministic and Stochastic Inference  Ideally, when using a dropout-like technique you would like to get rid of its stochastic effects during prediction, so that you can predict values with exactly one forward pass through the network (instead of having to average over many passes).",
      "For Swapout it can be mathematically shown that you can't calculate a deterministic version of it that performs equally to the stochastic one (averaging over many forward passes).",
      "This is even more the case when using Batch Normalization in a network.",
      "(Actually also when not using Swapout, but instead Dropout + BN.)",
      "So for best results you should use the stochastic method (averaging over many forward passes).",
      "Results  They compare various dropout-like methods, including Swapout, applied to residual networks.",
      "(On CIFAR-10 and CIFAR-100.)",
      "General performance:  Results with Swapout are better than with the other methods.",
      "According to their results, the ranking of methods is roughly: Swapout > Dropout > Stochastic Depth > Skip Forward > None.",
      "Stochastic vs deterministic method:  The stochastic method of swapout (average over N forward passes) performs significantly better than the deterministic one.",
      "Using about 15-30 forward passes seems to yield good results.",
      "Optimal parameter choice:  Previously the Swapout-formula y = theta_1 * x + theta_2 * F(x) was mentioned.",
      "theta_1 and theta_2 are generated via Bernoulli distributions which have parameters p_1 and p_2.",
      "If using fixed values for p_1 and p_2 throughout the network, it seems to be best to either set both of them to 0.5 or to set p_1 to >0.5 and p_2 to <0.5 (preference towards y = x).",
      "It's best however to start both at 1.0 (always y = x + F(x)) and to then linearly decay them to both 0.5 towards the end of the network, i.e. to apply less noise to the early layers.",
      "(This is similar to the results in the Stochastic Depth paper.)",
      "Thin vs. wide residual networks:  The standard residual networks that they compared to used a (16, 32, 64) pattern for their layers, i.e. they started with layers of each having 16 convolutional filters, followed by some layers with each having 32 filters, followed by some layers with 64 filters.",
      "They tried instead a (32, 64, 128) pattern, i.e. they doubled the amount of filters.",
      "Then they reduced the number of layers from 100 down to 20.",
      "Their wider residual network performed significantly better than the deep and thin counterpart.",
      "However, their parameter count also increased by about 4 times.",
      "Increasing the pattern again to (64, 128, 256) and increasing the number of layers from 20 to 32 leads to another performance improvement, beating a 1000-layer network of pattern (16, 32, 64).",
      "(Parameter count is then 27 times the original value.)",
      "Comments  Stochastic depth works layer-wise, while Swapout works unit-wise.",
      "When a layer in Stochastic Depth is dropped, its whole forward- and backward-pass don't have to be calculated.",
      "That saves time.",
      "Swapout is not going to save time.",
      "They argue that dropout+BN would also profit from using stochastic inference instead of deterministic inference, just like Swapout does.",
      "However, they don't mention using it for dropout in their comparison, only for Swapout.",
      "They show that linear decay for their parameters (less dropping on early layers, more on later ones) significantly improves the results of Swapout.",
      "However, they don't mention testing the same thing for dropout.",
      "Maybe dropout would also profit from it?",
      "For the above two points: Dropout's test error is at 5.87, Swapout's test error is at 5.68.",
      "So the difference is already quite small, making any disadvantage for dropout significant.",
      "Visualization of how Swapout works.",
      "From left to right: An input x; a standard layer is applied to the input F(x); a residual layer is applied to the input x + F(x); Skip Forward is applied to the layer; Swapout is applied to the layer.",
      "Stochastic Depth would be all units being orange (x) or blue (x + F(x))."
    ],
    "author_id": "ALEJU",
    "pdf_url": "http://arxiv.org/pdf/1605.06465v1",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 69013017
  },
  {
    "blog_id": "when-and-why-your-code-starts-to-smell-bad",
    "summary": [
      "When and Why Your Code Starts to Smell Bad \u2013 Tufano et al. 2015  Yesterday we saw that maintaining project quality is a key issue for integrators (maintainers).",
      "So it seems appropriate that my third choice from the recent ICSE \u201915 conference papers examines the question of when quality starts to slip at the code level, and what causes it.",
      "Bad code smells (shortly \u201ccode smells\u201d or \u201csmells\u201d), i.e., symptoms of poor design and implementation choices, represent one important factor contributing to technical debt, and possibly affecting the maintainability of a software system\u2026 to the best of our knowledge, there is no comprehensive empirical investigation into when and why code smells are introduced in software projects.",
      "Common wisdom suggests that urgent maintenance activities and pressure to deliver features while prioritizing time-to-market over code quality are often the causes of such smells.",
      "Generally speaking, software evolution has always been considered as one of the reasons behind \u201csoftware ageing\u201d or \u201cincreasing complexity.\u201d  Tufano et al. studied 200 open source projects from the Apache, Eclipse, and Android ecosystems to understand when and why smells were introduced.",
      "Over half a million commits were analysed.",
      "The study was based on a mix of smells related to large and complex components as well as smells related to lack of adoption of good practices, so as to be representative of the different categories of smells.",
      "We focus our study on the following types of smells: 1) Blob Class: a large class with different responsibilities that monopolizes most of the system\u2019s processing;  2) Class Data Should be Private: a class exposing its attributes, violating the information hiding principle;  3) Complex Class: a class having a high cyclomatic complexity;  4) Functional Decomposition: a class where inheritance and polymorphism are poorly used, declaring many private fields and implementing few methods; 5) Spaghetti Code: a class without structure that declares long methods without parameters.",
      "I\u2019m going to skip over the methodology and jump straight to the findings \u2013 see the full paper for more details on how the analysis was conducted.",
      "When are code smells introduced?",
      "The surprising finding in the light of the software ageing theory is that most of the smell instances are introduced when a code entity is first added to the versioning system.",
      "When a smell does appear at a later point, \u201cits symptoms (metric value increases) occur very fast, and not gradually.\u201d In the case of Blobs for example:  For the overall dataset, the slope for classes that will become Blobs is 849.90 as compared to the 0.25 of clean classes.",
      "Thus, while the cohesion of classes generally decreases over time, classes destined to become Blobs exhibit cohesion metric loss orders of magnitude faster than clean classes.",
      "In general, the results in Table V show strong differences in the metrics\u2019 slope between clean and smelly files, indicating that it could be possible to create recommenders warning developers when the changes performed on a specific code component show a dangerous trend that could lead to the introduction of a bad smell.",
      "Why are code smells introduced?",
      "Among the three different ecosystems analyzed, results show that smell instances are mainly introduced when developers perform enhancement operations on the system.",
      "When considering the three ecosystems altogether, for all the considered types of smells the percentage of smell-introducing commits tagged as enhancement ranges between 60% and 66%.",
      "Note that by enhancement we mean changes applied by developers on existing features aimed at improving them.",
      "If you consider both enhancements and new features the percentage rises to over 80%.",
      "Another endorsement for the theory that most smells are introduced on day one.",
      "Bug-fixes do of course introduce smells as well (between 6-16% of smells are introduced during bug fixing).",
      "Finally, refactoring \u2013 which is supposed to clean up the code and reduce smells, is also a source of  smell introduction!",
      "While refactoring is the principal treatment to remove smells, we found 394 cases in which developers introduced new smells when performing refactoring operations.",
      "What I can\u2019t easily see in the figures is a comparison to baseline activity.",
      "For example, 6-16% of smells may be introduced during bug fixing, but does that make bug fixing more or less smelly than other activities?",
      "We don\u2019t know unless we also know what overall % of activity is devoted to bug fixing\u2026  More smells are also introduced in the run-up to a release (but again, we don\u2019t know if more work overall is also done in the run-up to a release \u2013 often yes in my experience):  As expected, most of the smells are introduced the last month before issuing a release.",
      "Indeed, the percentage of smells introduced more than one month prior to issuing a release is really low (ranging between 0% and 11%).",
      "This consideration holds for all the ecosystems and for all the bad smells analyzed, thus confirming the common wisdom that the deadline pressure on developers can be one of the main causes for smell introduction.",
      "And those smells are often introduced by the files\u2019 owners, not by newcomers:  We can also observe that generally the developers who introduce a smell are not newcomers while often they are owners of the files.",
      "At the first glance, this could look like an unexpected result.",
      "The owner of the file\u2014one of the most experienced developers of the file\u2014is the one that has the higher likelihood of introducing a smell.",
      "However, as also discussed by Zeller in his book Why programs fail, more experienced developers tend to perform more complex and critical tasks.",
      "Thus, it is likely that their commits are more prone to introducing design problems.",
      "(It\u2019s also the case that the owner of the file is just more likely to do more work in the file, and hence have more chance of introducing smells.",
      "Plus, we\u2019ve already been told that many smells are introduced when an entity is first created, and by definition that is by the owner of the file)."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://dibt.unimol.it/fpalomba/documents/C4.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 72744034
  },
  {
    "blog_id": "same-stats-different-graphs-generating-datasets-with-varied-appearance-and-identical-statistics-through-simulated-annealing",
    "summary": [
      "Same stats, different graphs: generating datasets with varied appearance and identical statistics through simulated annealing Matejka & Fitzmaurice et al., CHI\u201917  Today\u2019s paper choice is inspired by the keynote that Prof. Miriah Meyer gave at the recent Velocity conference in London, \u2018 Why an interactive picture is worth a thousand numbers .\u2019 She made a wonderful and thought-provoking case for the power of visualisations, and especially visualisations you can interact with, playing a central role in our process of understanding.",
      "Better ways of seeing and exploring data lead to better insights.",
      "Meyer opened her talk by showing us Cairo\u2019s Datasaurus (\u2018 Never trust summary statistics alone; always visualize your data \u2019).",
      "You can calculate all the statistic summaries you like over the datasaurus dataset, run regressions, perform clustering, and so on.",
      "But until you look at it with the right visualisation, you\u2019re never going to reach the same understanding as you get from looking at the data this way:  Since the early 70\u2019s, Anscome\u2019s Quartet has been frequently used to illustrate the importance of graphical representations when exploring data:  The effectiveness of Anscombe\u2019s Quartet is not due to simply having four different data sets which generate the same statistical properties, it is that four clearly different and identifiably distinct datasets are producing the same statistical properties.",
      "In \u2018Same Stats, Different Graphs,\u2019 Matjeka & Fitzmaurice show a method for purposefully creating datasets which are identical over a range of statistical properties (of your choosing), yet produce dissimilar graphics.",
      "In my mind there\u2019s a connection here to the idea of adversarial inputs to deep neural nets, which we might similarly express on some level as \u2018Same Stats, Different Classes.\u2019 Another thing I get from this paper is a very visual reminder of \u2018Same Outcome (in terms of stats), Different Causes.\u2019 There are lots of different hypotheses you could come up with that may produce the effect you\u2019re seeing.",
      "Their method doesn\u2019t just produce datasets that retain the same statistical properties while looking different though, it also allows you to guide the way in which the visualisation looks different (a bit like crafting an adversarial input to produce a specific mis-classification).",
      "For example, in the figure below we have an initial data set (top left) and a set of target shapes for data set generation.",
      "Here are the results produced by the technique when using these target shapes \u2013 every one of these has the same summary statistics as the initial dataset!",
      "These examples are all in 2D, but there\u2019s nothing about the technique that limits it to two dimensions.",
      "Here are some 1-D examples:  How it works  The key insight behind our approach is that while generating a dataset from scratch to have particular statistical properties is relatively difficult, it is relatively easy to take an existing dataset, modify it slightly, and maintain (nearly) the same statistical properties.",
      "With repetition, this process creates a dataset with a different visual appearance from the original, while maintaining the same statistical properties.",
      "Further, if the modifications to the dataset are biased to move the points towards a particular goal, the resulting graph can be directed towards a particular visual appearance.",
      "In pseudo-code, it looks like this:  Where:  Initial_ds is the seed dataset defining the statistical properties which should be maintained  Perturb modifies the current version of the dataset by moving one or more points by a small amount in a random direction.",
      "The small amount is chosen from a normal distribution and calibrated so that 95% or more of movements should result in the overall statistical properties remaining unchanged (to two decimal places).",
      "The temp parameter is a temperature used for simulated annealing.",
      "The Fit function checks if perturbing the points has improved the overall fitness, and accepts it if so.",
      "When coercing the dataset into a target shape, it uses the average distance of all points to the nearest point on the target shape.",
      "To avoid getting stuck in a locally-optimal solution, a perturbation may also be accepted if the current temperature is greater than a random number between 0 and 1.",
      "Temperature starts out at 0.4, and is gradually reduced to 0.01 using a quadratically-smoothed monotonic cooling schedule.",
      "The newly returned perturbation is then tested (isErrorOk) to ensure that overall stats have not changed (within 2 decimal places), and becomes the current dataset if so.",
      "Here\u2019s an example of dataset evolution across 200,000 iterations:  And of course you don\u2019t have to start with a random cloud, you can evolve from any seed dataset.",
      "Here are some transformations from our friend the datasaurus:  Simpson\u2019s paradox  Here\u2019s a fun example where the target shape is used to produce a dataset exhibiting Simpson\u2019s paradox.",
      "Start out with a strongly positively correlated dataset, for example:  Then give the algorithm a target shape that directs the dataset towards a series of negatively sloping lines:  A few iterations later, and we have a dataset with the same overall strongly positive correlation that we started with, but each subset of the data has an individually negative correlation.",
      "I find that a very satisfying way of demonstrating the effect!",
      "Cloning datasets for anonymity  As discussed by Govindaraju and Haslett another use for datasets with the same statistical properties is the creation of \u201ccloned\u201d datasets to anonymise sensitive data.",
      "In this case, it is important that individual data points are changed while the overall structure of the data remains similar.",
      "This can be accomplished by performing a Kolmogorov-Smirnov test within the isErrorOk function\u2026  This is clearly similar to the idea of adding noise to a dataset to enhance privacy.",
      "I guess I need to read Govindaraju and Haslett\u2019s paper though, as it seems to me at first glance that if all you have maintained are the overall statistical properties you might as well provide those alone.",
      "Anything else inferred from the generated data must just be an artificial artefact?",
      "It must depend on how far you move from the original dataset\u2026  The code and datasets presented in this work are available from www.autodeskresearch.com/publications/samestats .",
      "If the thought of finding better insights through better visualisations inspires you, you might want to check out Miriah Meyer\u2019s forthcoming book: \u2018 Making data visual: a practical guide to using visualisation for insight.",
      "\u2019"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/3025453.3025912?download=true",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 13187019
  },
  {
    "blog_id": "unsupervised_representation_learning_with_deep_convolutional_generative_adversarial_networks",
    "summary": [
      "What  DCGANs are just a different architecture of GANs.",
      "In GANs a Generator network (G) generates images.",
      "A discriminator network (D) learns to differentiate between real images from the training set and images generated by G.  DCGANs basically convert the laplacian pyramid technique (many pairs of G and D to progressively upscale an image) to a single pair of G and D.  How  Their D: Convolutional networks.",
      "No linear layers.",
      "No pooling, instead strided layers.",
      "LeakyReLUs.",
      "Their G: Starts with 100d noise vector.",
      "Generates with linear layers 1024x4x4 values.",
      "Then uses fractionally strided convolutions (move by 0.5 per step) to upscale to 512x8x8.",
      "This is continued till Cx32x32 or Cx64x64.",
      "The last layer is a convolution to 3x32x32/3x64x64 (Tanh activation).",
      "The fractionally strided convolutions do basically the same as the progressive upscaling in the laplacian pyramid.",
      "So it's basically one laplacian pyramid in a single network and all upscalers are trained jointly leading to higher quality images.",
      "They use Adam as their optimizer.",
      "To decrease instability issues they decreased the learning rate to 0.0002 (from 0.001) and the momentum/beta1 to 0.5 (from 0.9).",
      "Architecture of G using fractionally strided convolutions to progressively upscale the image.",
      "Results  High quality images.",
      "Still with distortions and errors, but at first glance they look realistic.",
      "Smooth interpolations between generated images are possible (by interpolating between the noise vectors and feeding these interpolations into G).",
      "The features extracted by D seem to have some potential for unsupervised learning.",
      "There seems to be some potential for vector arithmetics (using the initial noise vectors) similar to the vector arithmetics with wordvectors.",
      "E.g. to generate mean with sunglasses via vector(men) + vector(sunglasses).",
      "Generated images, bedrooms.",
      "Generated images, faces.",
      "Rough chapter-wise notes  Introduction  For unsupervised learning, they propose to use to train a GAN and then reuse the weights of D.  GANs have traditionally been hard to train.",
      "Approach and model architecture  They use for D an convnet without linear layers, withput pooling layers (only strides), LeakyReLUs and Batch Normalization.",
      "They use for G ReLUs (hidden layers) and Tanh (output).",
      "Details of adversarial training  They trained on LSUN, Imagenet-1k and a custom dataset of faces.",
      "Minibatch size was 128.",
      "LeakyReLU alpha 0.2.",
      "They used Adam with a learning rate of 0.0002 and momentum of 0.5.",
      "They note that a higher momentum lead to oscillations.",
      "LSUN  3M images of bedrooms.",
      "They use an autoencoder based technique to filter out 0.25M near duplicate images.",
      "Faces  They downloaded 3M images of 10k people.",
      "They extracted 350k faces with OpenCV.",
      "Empirical validation of DCGANs capabilities  Classifying CIFAR-10 GANs as a feature extractor  They train a pair of G and D on Imagenet-1k.",
      "D's top layer has 512*4*4 features.",
      "They train an SVM on these features to classify the images of CIFAR-10.",
      "They achieve a score of 82.8%, better than unsupervised K-Means based methods, but worse than Exemplar CNNs.",
      "Classifying SVHN digits using GANs as a feature extractor  They reuse the same pipeline (D trained on CIFAR-10, SVM) for the StreetView House Numbers dataset.",
      "They use 1000 SVHN images (with the features from D) to train the SVM.",
      "They achieve 22.48% test error.",
      "Investigating and visualizing the internals of the networks  Walking in the latent space  The performs walks in the latent space (= interpolate between input noise vectors and generate several images for the interpolation).",
      "They argue that this might be a good way to detect overfitting/memorizations as those might lead to very sudden (not smooth) transitions.",
      "Visualizing the discriminator features  They use guided backpropagation to visualize what the feature maps in D have learned (i.e. to which images they react).",
      "They can show that their LSUN-bedroom GAN seems to have learned in an unsupervised way what beds and windows look like.",
      "Forgetting to draw certain objects  They manually annotated the locations of objects in some generated bedroom images.",
      "Based on these annotations they estimated which feature maps were mostly responsible for generating the objects.",
      "They deactivated these feature maps and regenerated the images.",
      "That decreased the appearance of these objects.",
      "It's however not as easy as one feature map deactivation leading to one object disappearing.",
      "They deactivated quite a lot of feature maps (200) and they objects were often still quite visible or replaced by artefacts/errors.",
      "Vector arithmetic on face samples  Wordvectors can be used to perform semantic arithmetic (e.g. king - man + woman = queen).",
      "The unsupervised representations seem to be useable in a similar fashion.",
      "E.g. they generated images via G. They then picked several images that showed men with glasses and averaged these image's noise vectors.",
      "They did with same with men without glasses and women without glasses.",
      "Then they performed on these vectors men with glasses - mean without glasses + women without glasses to get `womean with glasses"
    ],
    "author_id": "ALEJU",
    "pdf_url": "http://arxiv.org/pdf/1511.06434",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 26105266
  },
  {
    "blog_id": "measuring-abstract-reasoning-in-neural-networks",
    "summary": [
      "The paper proposes a dataset to diagnose the abstract reasoning capabilities of learning systems.",
      "The paper shows that a variant of the relational networks, explicitly designed for abstract reasoning, outperforms models like ResNets.",
      "Idea  Visual reasoning tasks, that are inspired by the human IQ test, are used to evaluate the models in terms of generalization.",
      "Let\u2019s say that we want to test if the model understands the abstract notion of \u201cincreasing\u201d.",
      "We could train the model on data that captures the notion of \u201cincreasing\u201d, in terms of say increasing size (or quantities) of objects and then test it on a dataset where the notion is expressed in terms of increasing intensity of color.",
      "The dataset is then used to evaluate if the models can find any solution to such abstract reasoning tasks and how well they generalize when the abstract content is specifically controlled.",
      "Dataset  Raven\u2019s Progressive Matrics (RPMs):  Consists of an incomplete 3x3 matrix of images where the missing image needs to be filled in, typically by choosing from a set of candidate images.",
      "As such, it is possible to justify multiple answers to be correct though, in practice, the right answer is the one with the simplest explanation.",
      "Procedurally Generated Matrices (PGMs)  Generating RPM like matrices procedurally by building an abstract structure for matrices.",
      "The abstract structure S consists of 3 components: (i) Relation types (R), (ii) Object types (O) and (iii) Attribute types (A).",
      "ie *S = {(r, o, a)  r in R, o in O and a in A}*.",
      "This can be read as: \u201cStructure S is instantiated on attribute a of object o and exhibits the relation r\u201d.",
      "For example, S is instantiated on \u201ccolor\u201d of object \u201cshape\u201d and exhibits the relation \u201cincreasing\u201d.",
      "In general, the structure could be made of more than one such tuple and more are the tuples, harder is the task.",
      "Given the structure, sample values v for each attribute a while conforming with the relation r. For example, if the attribute is \u201ccolor\u201d and the relation is \u201cincreasing\u201d, the intensity of color must increase.",
      "The resulting structure is rendered as pixels.",
      "Test for Generalization  The paper tests for the following generalization scenarios:  Neutral: The structure of the training and test data can contain any tuple.",
      "Interpolation: The training data contains even-indexed members of the attribute values while the test data contains odd-indexed members of the attribute values.",
      "Extrapolation: The training data contains first-half of the attribute values while the test data contains the second-half of the attribute values.",
      "Heldout attribute: Training data contains no tuples with (o = shape, a = color) or (o = line, a = type).",
      "Heldout triples: Out of 29 possible triples, 7 are held out from training and only used during testing.",
      "Heldout pair-of-triples: Out of 400 possible sets of pair of triples, 40 were held out and used only during testing.",
      "Heldout pair-of-triples: Out of 400 possible sets of pair of triples, 40 were held out and used only during testing.",
      "Heldout attribute pair: Out of 20 (unordered) variable attribute pairs, 4 were held out and used only during testing.",
      "Models  Input: 8 context panels (from the 3x3) matrix where the last panel needs to be filled.",
      "CNN-MLP - 4 layer CNN with batchnorm and ReLU.",
      "ResNet - ResNet-50 (as it perfomed better than ResNet-101 and ResNet-152).",
      "LSTM  Wild Relation Network (WReN) - A CNN model encodes the 8 panels and the candidate answers and feeds them as input to a relational network.",
      "Context-blind ResNet - ResNet network without the context (or the 8 input panels).",
      "Results  WReN model outperforms the other models on the Neutral setup.",
      "Models have a harder time differentiating between size than quantity.",
      "WRen is the best performing models in all the setups and rest of the discussion only applies to that model.",
      "Generalisation is easy in the context of interpolation while worst in case of extrapolation, hinting at the limited generalization capability of the models.",
      "Auxiliary Training  The model is also trained to predict the relevant relation, object and attribute types using the meta-targets that encode this information.",
      "The auxiliary training helps in all the cases.",
      "Further, the model\u2019s accuracy on the main task is where in the cases where it solves the auxiliary tasks well.",
      "Key Takeaway  For abstract visual reasoning tasks, the choice of models can make a large difference, the case in consideration of ResNets vs Relational Networks.",
      "Using auxiliary loss that encourages the model to \u201cexplain\u201d its reasoning (in this case by predicting the attributes, relations, etc) helps to improve the performance on the main task as well.",
      "Given that the challenge is motivated by tasks used to measure human IQ, it would have been interesting to get an estimate of human performance on at least a subset of this dataset."
    ],
    "author_id": "shugan",
    "pdf_url": "http://proceedings.mlr.press/v80/santoro18a/santoro18a.pdf",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 15757806
  },
  {
    "blog_id": "detecting-discontinuities-in-large-scale-systems",
    "summary": [
      "Detecting Discontinuities in Large-Scale Systems \u2013 Malik et al 2014.",
      "The 7th IEEE/ACM International Conference on Utility and Cloud Computing is coming to London in a couple of weeks time.",
      "Many of the papers don\u2019t seem to be online yet, but here\u2019s one that is.",
      "Malik et al. tackle the problem of long-term forecasting for infrastructure provisioning, and in particular identifying discontinuities in performance data so that models are trained on the most relevant data.",
      "One of the fundamental problems faced by analysts in preparing data for use in forecasting is the timely identification of data discontinuities.",
      "A discontinuity is an abrupt change in a time-series pattern of a performance counter that persists but does not recur.",
      "Analysts need to identify discontinuities in performance data so that they can a) remove the discontinuities from the data before building a forecast model and b) retrain an existing forecast model on the performance data from the point in time where a discontinuity occurred.",
      "We\u2019re also treated to a good overview of the forecasting process in general.",
      "Practitioners and data scientists spend considerable time (e.g. up to 80%) in preparing data for their forecast algorithms!",
      "Where does all this time go?",
      "The accuracy of forecasting results depends on the quality of the performance data (i.e., performance counters; such as CPU utilization, bandwidth consumption, network traffic and Disk IOPS) fed to the forecasting algorithms, i.e., missing value imputation, calculating and adjusting times stamp drifts of logged performance data across hundreds of VMs, identification and removal of outliers and anomalies and in cases, scaling and standardizing the data to remove bias among performance counters.",
      "One of the fundamental problems faced by analysts in preparing data for long-term forecast is the identification and removal of data discontinuities.",
      "Discontinuities, like anomalies, are abrupt changes in time-series patterns.",
      "Unlike anomalies, which are temporary, discontinuities persist.",
      "They do not appear instantaneousy, but over a brief period called a transition period.",
      "detecting a discontinuity provides analysts a reference point to retrain their forecasting models and make necessary adjustments.",
      "After cleaning the logs (e.g. dealing with missing or empty counter values), Principle Component Analysis is used to \u201cselect the least correlated subset of performance counters that can still explain the maximum variations in the data.\u201d The performance data needs to be normalized for PCA to work well\u2026  To eliminate PCA bias towards those variables with a larger variance, we standardized the performance counters via Unit Variance scaling, i.e., by dividing the observations of each counter variable by the variable\u2019s standard deviation.",
      "Scaled performance counter data are then further mean centered to reduce the risk of collinearity.",
      "PCA was chosen due to its \u201csuperior performance in identifying performance counters that are sensitive to minute changes in both workload and environment as compared to many other supervised and unsupervised machine learning techniques.\u201d  The determined principle component performance counters are then fed into an anomaly detector.",
      "This phase works by finding changes in the data that cannot be easily represented (approximated) by a quadratic function:  When working with training data, we discover (potential) discontinuities by presuming that discontinuities cannot be well modelled by a low order polynomial function.",
      "Given a performance counter time series data {v[t]}, we approximate the series by the quadratic function _f(t) = c+bt+at\u00b2 that performance counter time series data {v[t]}, we approximate the series by the quadratic function f(t) = c+bt+at\u00b2 that minimizes the least squared error (LSE).",
      "We presume that series containing sudden dramatic changes, anomalies, or discontinuities will not be fit as well by this approximation and so have a larger LSE.",
      "From this set of discovered anomalies, discontinuities are then identified by looking at the distribution of the performance counter in question before and after the anomaly transition period.",
      "For discontinuities, the change will persist, whereas for ordinary anamolies it will not.",
      "The Wilcoxon rank-sum test is used for this comparison.",
      "A disappointment in the paper (for this reader) is that much of the testing was done based on deliberately injecting anomalies and discontinuities into existing data.",
      "As a side-effect though, that means we are treated to a discussion on the most common causes of anomalies and discontinuities IRL:  Causes of temporary anomalies:  80% of the performance anomalies in large software systems are due to software inconsistencies and human errors  the most common anomaly occuring in the field is related to transient memory issues (memory spikes)  large enterprises report that periodic CPU saturation is one of the fundamental field problems  interfering workloads are a major cause of performance degradation in data centers (resulting from competition for resources).",
      "Causes of discontinuities:  Increase in workload due to promotions, new products, mergers and acquisitions  Change in transaction patterns, where a \u2018transaction\u2019 in this context is a sequence of events.",
      "Most likely caused by a new version of the software deployed in the data center.",
      "Upgrades to infrastructure hardware or software  Finally, the approach was back-tested against 7 years worth of real production logs for which analysts had already identified the discontinuities.",
      "Precision and recall were both at 92% with the optimum algorithm settings for this data set."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://129.97.186.80/~migod/papers/2014/ucc14.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 82043898
  },
  {
    "blog_id": "the-scalable-commutativity-rule-designing-scalable-software-for-multicore-processors",
    "summary": [
      "The Scalable Commutativity Rule: Designing Scalable Software for Multicore Processors \u2013 Clements et al. 2013  The way you design your interface (API) has a significant impact on the scalability you can achieve with any implementation.",
      "Clements et al. define the Scalable Commutativity Rule \u2013 which will look familiar to those who study distributed systems \u2013 and a tool called COMMUTER, which together can guide your API design.",
      "Using these techniques they were able to redesign 18 file-system related POSIX calls and greatly increase scalability.",
      "Two aspects I really like in this work are (a) the focus on the impact of your API design on ultimate scalability (vs. implementation optimisations and techniques), and (b) the notion that while not all operations may commute all of the time, there can be significant regions in the execution history where they can.",
      "This is useful information we can exploit\u2026  If all this sounds interesting, I highly recommend reading the full paper.",
      "My copy is extensively highlighted and I can only give you a flavour here.",
      "At the core of our approach is this scalable commutativity rule: In any situation where several operations commute\u2014meaning there\u2019s no way to distinguish their execution order using the interface\u2014they have an implementation whose memory accesses are conflict-free during those operations.",
      "Or, more concisely, whenever interface operations commute, they can be implemented in a way that scales.",
      "Clements et al. study SIM commutativity: State dependent, Interface-based, and monotonic.",
      "When operations commute in the context of a specific system state, specific operation arguments, and specific concurrent operations, we show that an implementation exists that is conflict-free for that state and those arguments and concurrent operations.",
      "This exposes many more opportunities to apply the rule to real interfaces\u2014and thus discover scalable implementations\u2014than a more conventional notion of commutativity would.",
      "Consider Unix system calls.",
      "Very few unconditionally commute in every state and history \u2013 getpid() is one such example.",
      "But many more can conditionally commute based on state and arguments etc.",
      "For example, calls to open(\"a\", O_CREATE|O_EXCL) when called from processes with different working directories.",
      "The COMMUTER tool takes an interface model, and determines the precise conditions under which sets of operations commute.",
      "\u201cThe tool can be integrated into the development process to drive initial design and implementation, to incrementally improve existing implementations, or to help developers understand the commutativity of an interface.\u201d  To reason about implementation scalability, we need to model implementations in enough detail to tell whether different threads\u2019 \u201cmemory accesses\u201d are conflict-free\u2026conflict freedom is our proxy for scalability.",
      "This requires just enough modelling of the state behind the interface to check for access conflicts.",
      "An important result of the formally defined scalable commutativity rule (see section 3.4) is that in a region of history which SIM commutes, there exists a correct implementation in that region whose steps are conflict free.",
      "You just have to find that implementation!",
      "The authors show one way of constructing it as a proof that it is always possible, but this is not recommended as an actual implementation technique.",
      "We believe it is easier to create practical scalable implementations for operations that commute in more situations.",
      "The arguments and system states for which a set of operations commutes often collapse into fairly well-defined classes (e.g., file creation might commute whenever the containing directories are different).",
      "In practice, implementations scale for whole classes of states and arguments, not just for specific histories\u2026 In the end, a system designer must decide which situations involving commutative operations are most important, and find practical implementation strategies that scale in those situations.",
      "COMMUTER  The user expresses a symbolic model of the interface in Python, and the ANALYZER component of COMMUTER generates expressions in terms of arguments and state that indicate exactly when sets of operations commute.",
      "These expressions can be directly inspected, and also passed to TESTGEN which can convert these conditions into real test cases.",
      "To capture different conflict conditions as well as path conditions, we introduce a new notion called conflict coverage.",
      "Conflict coverage exercises all possible access patterns on shared data structures: looking up two distinct items from different operations, looking up the same item, etc.",
      "TESTGEN approximates conflict coverage\u2026  For a model of 18 POSIX system calls, TESTGEN produces a total of 13,664 test cases, and these find scalability issues in the Linux ramfs file system and virtual memory system.",
      "MTRACE uses a modified version of qemu and guest Linux kernel to run the test cases generated by TESTGEN against a real implementation, and checks that the implementation is conflict-free in each case.",
      "Applying COMMUTER in the design of a file system interface  The authors used COMMUTER to guide the design of a new file system called ScaleFS:  Given that Linux does not scale in many cases, how hard is it to implement scalable file systems and virtual memory systems?",
      "To answer this question, we designed and implemented a ramfs-like in-memory file system called ScaleFS and a virtual memory system called RadixVM for sv6, our research kernel based on xv6\u2026  Figure 6 below shows the scalability of system call pairs as achieved by ramfs (left) and ScaleFS (right).",
      "A big reduction in the number of conflicting cases.",
      "COMMUTER, sv6, and a browser for the data in this  paper are available at  [url]"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://people.csail.mit.edu/nickolai/papers/clements-sc.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 11542688
  },
  {
    "blog_id": "making-smart-contracts-smarter",
    "summary": [
      "Making smart contracts smarter Luu et al., CCS 2016  This is the fourth in a series of papers from the ACM Queue Research for Practice \u2018 Cryptocurrencies, Blockchains and Smart Contracts \u2018 selections, in which Luu at al.",
      "look at smart contracts in Ethereum.",
      "Smart contracts are a really intriguing idea and have generated a lot of interest/excitement, but they also have a number of properties which make them both likely targets for attackers and also hard to get right.",
      "Regular readers of The Morning Paper will not be surprised to see our old friend error and exception handling popping up as one of the chief causes of problems again!",
      "After scanning 19,366 Ethereum contracts using the OYENTE tool described in the paper, the authors found vulnerabilities in 8,833 of them.",
      "Here\u2019s the plan: after a brief introduction to smart contracts, we\u2019ll discuss what it is that makes them especially attractive targets, followed by a look at typical vulnerabilities.",
      "We\u2019ll then finish up by seeing what we can do about the situation to make contracts more secure in the future.",
      "What exactly is a smart contract?",
      "A smart contract is identified by an address (a 160-bit identifier) and its code resides on the blockchain.",
      "Users invoke a smart contract in present cryptocurrencies by sending transactions to the contract address.",
      "Specifically, if a new transaction is accepted by the blockchain and has a contract address as the recipient, then all participants on the mining network execute the contract code with the current state of the blockchain and the transaction payloads as inputs.",
      "The network then agrees on the output and the next state of the contract by participating in a consensus protocol.",
      "In Ethereum, contracts are introduce to the blockchain via special creation transactions.",
      "Contracts are essentially functions whose Ethereum Virtual Machine (EVM) bytecode is incorporated in the blockchain as part of the creation transaction.",
      "The contracts themselves can be written in higher-level languages and compiled to EVM bytecode.",
      "Contract functions are stateful: they have private storage on the blockchain, and can also hold some amount of virtual Ether coins.",
      "The private storage is allocated and initialized by running a constructor, subsequent transactions sent to the contract address invoke the anonymous function.",
      "Here\u2019s an example Puzzle contract:  Note the contract state declared on lines 2-6, constructor on lines 8-13, and anonymous transaction function on lines 15-29.",
      "A default input variable msg holds the sender, amount of Ether sent to the contract, and any included data as part of the invocation.",
      "In this particular contract, if the owner initiates the transaction (line 16) they can extract the current reward value and replace it with some other amount (lines 17-21).",
      "Anyone else invoking the transaction can submit a potential solution, and will receive the reward if the solution is accepted (lines 23-29).",
      "All miners execute the transaction, which will incur some computation cost:  Ethereum pays miners some fees proportional to the required computation.",
      "Specifically, each instruction in the Ethereum bytecode has a pre-specified amount of gas.",
      "When a user sends a transaction to invoke a contract, she has to specify how much gas she is willing to provide for the execution (called gasLimit) as well as the price for each gas unit (called gasPrice).",
      "A miner who includes the transaction in his proposed block subsequently receives the transaction fee corresponding to the amount of gas the execution actually burns multiplied by gasPrice.",
      "If the execution costs more than the gasLimit then execution is terminated and the state is restored to the initial state at the start of the function execution.",
      "The miner still receives gasLimit compensation though.",
      "Why are smart contracts attractive targets?",
      "Smart contracts have associated value \u2013 potentially handling large numbers of coins worth hundreds of dollars apiece.",
      "The 8,833 contracts in the first 1,460,000 blocks in the Ethereum network had a total balance of over 3 million Ether (about $30M USD) at the time the paper was written.",
      "The infamous attack on \u2018TheDAO\u2019 caused a loss of about $60M to TheDAO\u2019s investors.",
      "Smart contract vulnerabilities  So we know that smart contracts have value as attack targets.",
      "They also have a combination of features that should make any experienced software developer raise an eyebrow:  They execute in permissionless networks which arbitrary participants can join (i.e., under byzantine conditions)  Miners and/or callers have meaningful control over the environment in which the transactions execute (which transactions to accept, transaction ordering, setting of block timestamp, manipulation of call stack)  All of the above must be reasoned about in an environment which punishes anyone who doesn\u2019t get it right first time \u2013 there is no patching mechanism:  There is no way to patch a buggy smart contract, regardless of its popularity or how much money it has, without reversing the blockchain (a formidable task).",
      "Therefore, reasoning about the correctness of smart contracts before deployment is critical, as is designing a safe smart contract system.",
      "Note: you can explicitly design versioning/upgrade capabilities into your smart contract code, since contracts can call each other.",
      "See e.g.,  [url]"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/2976749.2978309?download=true",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 82387857
  },
  {
    "blog_id": "bengiotppb17",
    "summary": [
      "**Summary**  Representation (or feature) learning with unsupervised learning has yet really to yield the type of results that many believe to be achievable.",
      "For example, we\u2019d like to unleash an unsupervised learning algorithm on all web images and then obtain a representation that captures the various factors of variation we know to be present (e.g. objects and people).",
      "One popular approach for this is to train a model that assumes a high-level vector representation with independent components.",
      "However, despite a large body of literature on such models by now, such so-called disentangling of these factors of variation still seems beyond our reach.",
      "In this short paper, the authors propose an alternative to this approach.",
      "They propose that disentangling might be achievable by learning a representation whose dimensions are each separately **controllable**, i.e. that each have an associated policy which changes the value of that dimension **while letting other dimensions fixed**.",
      "Specifically, the authors propose to minimize the following objective:  $\\mathop{\\mathbb{E}}_s\\left[\\frac{1}{2}||s-g(f(s))||^2_2 \\right] - \\lambda \\sum_k \\mathbb{E}_{a,s}\\left[\\sum_a \\pi_k(a|s) \\log sel(s,a,k)\\right]$  where  - $s$ is an agent\u2019s state (e.g. frame image) which encoder $f$ and decoder $g$ learn to autoencode - $k$ iterates over all dimensions of the representation space (output of encoder) - $a$ iterates over actions that the agent can take - $\\pi_k(a|s)$ is the policy that is meant to control the $k^{\\rm th}$ dimension of the representation space $f(s)_k$ - $sel(s,a,k)$ is the selectivity of $f(s)_k$ relative to other dimensions in the representation, at state $s$:  $sel(s,a,k) = \\mathop{\\mathbb{E}}_{s\u2019\\sim {\\cal P}_{ss\u2019}^a}\\left[\\frac{|f_k(s\u2019)-f_k(s)|}{\\sum_{k\u2019} |f_{k\u2019}(s\u2019)-f_{k\u2019}(s)| }\\right]$  ${\\cal P}_{ss\u2019}^a$ is the conditional distribution over the next step state $s\u2019$ given that you are at state $s$ and are taking action $a$ (i.e.",
      "the environment transition distribution).",
      "One can see that selectivity is higher when the change $|f_k(s\u2019)-f_k(s)|$ in dimension $k$ is much larger than the change  $|f_{k\u2019}(s\u2019)-f_{k\u2019}(s)|$ in the other dimensions $k\u2019$.",
      "A directed version of selectivity is also proposed (and I believe was used in the experiments), where the absolute value function is removed and $\\log sel$ is replaced with $\\log(1+sel)$ in the objective.",
      "The learning objective will thus encourage the discovery of a representation that is informative of the input (in that you can reconstruct it) and for which there exists policies that separately control these dimensions.",
      "Algorithm 1 in the paper describes a learning procedure for optimizing this objective.",
      "In brief, for every update, a state $s$ is sampled from which an update for the autoencoder part of the loss can be made.",
      "Then, iterating over each dimension $k$, REINFORCE is used to get a gradient estimate of the selectivity part of the loss, to update both the policy $\\pi_k$ and the encoder $f$ by using the policy to reach a next state $s\u2019$.",
      "**My two cents**  I find this concept very appealing and thought provoking.",
      "Intuitively, I find the idea that valuable features are features which reflect an aspect of our environment that we can control more sensible and possibly less constraining than an assumption of independent features.",
      "It also has an interesting analogy of an infant learning about the world by interacting with it.",
      "The caveat is that unfortunately, this concept is currently fairly impractical, since it requires an interactive environment where an agent can perform actions, something we can\u2019t easily have short of deploying a robot with sensors.",
      "Moreover, the proposed algorithm seems to assume that each state $s$ is sampled independently for each update, whereas a robot would observe a dependent stream of states.",
      "Accordingly, the experiments in this short paper are mostly \u201cproof of concept\u201d, on simplistic synthetic environments.",
      "Yet they do a good job at illustrating the idea.",
      "To me this means that there\u2019s more interesting work worth doing in what seems to be a promising direction!"
    ],
    "author_id": "hlarochelle",
    "pdf_url": "http://arxiv.org/pdf/1703.07718",
    "author_full_name": "Hugo Larochelle",
    "source_website": "https://www.shortscience.org/user?name=hlarochelle",
    "id": 47698609
  },
  {
    "blog_id": "deepface",
    "summary": [
      "What  They describe a CNN architecture that can be used to identify a person given an image of their face.",
      "How  The expected input is the image of a face (i.e. it does not search for faces in images, the faces already have to be extracted by a different method).",
      "Face alignment / Frontalization  Target of this step: Get rid of variations within the face images, so that every face seems to look straight into the camera (\"frontalized\").",
      "2D alignment  They search for landmarks (fiducial points) on the face.",
      "They use SVRs (features: LBPs) for that.",
      "After every application of the SVR, the localized landmarks are used to transform/normalize the face.",
      "Then the SVR is applied again.",
      "By doing this, the locations of the landmarks are gradually refined.",
      "They use the detected landmarks to normalize the face images (via scaling, rotation and translation).",
      "3D alignment  The 2D alignment allows to normalize variations within the 2D-plane, not out-of-plane variations (e.g. seeing that face from its left/right side).",
      "To normalize out-of-plane variations they need a 3D transformation.",
      "They detect an additional 67 landmarks on the faces (again via SVRs).",
      "They construct a human face mesh from a dataset (USF Human-ID).",
      "They map the 67 landmarks to that mesh.",
      "They then use some more complicated steps to recover the frontalized face image.",
      "CNN architecture  The CNN receives the frontalized face images (152x152, RGB).",
      "It then applies the following steps:  Convolution, 32 filters, 11x11, ReLU (-> 32x142x142, CxHxW)  Max pooling over 3x3, stride 2 (-> 32x71x71)  Convolution, 16 filters, 9x9, ReLU (-> 16x63x63)  Local Convolution, 16 filters, 9x9, ReLU (-> 16x55x55)  Local Convolution, 16 filters, 7x7, ReLU (-> 16x25x25)  Local Convolution, 16 filters, 5x5, ReLU (-> 16x21x21)  Fully Connected, 4096, ReLU  Fully Connected, 4030, Softmax  Local Convolutions use a different set of learned weights at every \"pixel\" (while a normal convolution uses the same set of weights at all locations).",
      "They can afford to use local convolutions because of their frontalization, which roughly forces specific landmarks to be at specific locations.",
      "They use dropout (apparently only after the first fully connected layer).",
      "They normalize \"the features\" (probably the 4096 fully connected layer).",
      "Each component is divided by its maximum value across a training set.",
      "Additionally, the whole vector is L2-normalized.",
      "The goal of this step is to make the network less sensitive to illumination changes.",
      "The whole network has about 120 million parameters.",
      "Visualization of the architecture:  Training  The network receives images, each showing a face, and is trained to classify the identity of the face (e.g. gets image of Obama, has to return \"that's Obama\").",
      "They use cross-entropy as their loss.",
      "Face verification  In order to tell whether two images of faces show the same person they try three different methods.",
      "Each of these relies on the vector extracted by the first fully connected layer in the network (4096d).",
      "Let these vectors be f1 (image 1) and f2 (image 2).",
      "The methods are then:  Inner product between f1 and f2.",
      "The classification (same person/not same person) is then done by a simple threshold.",
      "Weighted X^2 (chi-squared) distance.",
      "Equation, per vector component i: weight_i (f1[i] - f2[i])^2 / (f1[i] + f2[i]).",
      "The vector is then fed into an SVM.",
      "Siamese network.",
      "Means here simply that the absolute distance between f1 and f2 is calculated (|f1-f2|), each component is weighted by a learned weight and then the sum of the components is calculated.",
      "If the result is above a threshold, the faces are considered to show the same person.",
      "Results  They train their network on the Social Face Classification (SFC) dataset.",
      "That seems to be a Facebook-internal dataset (i.e. not public) with 4.4 million faces of 4k people.",
      "When applied to the LFW dataset:  Face recognition (\"which person is shown in the image\") (apparently they retrained the whole model on LFW for this task?",
      "):  Simple SVM with LBP (i.e. not their network): 91.4% mean accuracy.",
      "Their model, with frontalization, with 2d alignment: ???",
      "no value.",
      "Their model, no frontalization (only 2d alignment): 94.3% mean accuracy.",
      "Their model, no frontalization, no 2d alignment: 87.9% mean accuracy.",
      "Face verification (two images -> same/not same person) (apparently also trained on LFW?",
      "unclear):  Method 1 (inner product + threshold): 95.92% mean accuracy.",
      "Method 2 (X^2 vector + SVM): 97.00% mean accurracy.",
      "Method 3 (siamese): Apparently 96.17% accuracy alone, and 97.25% when used in an ensemble with other methods (under special training schedule using SFC dataset).",
      "When applied to the YTF dataset (YouTube video frames):  92.5% accuracy via X^2-method."
    ],
    "author_id": "ALEJU",
    "pdf_url": "https://www.cs.toronto.edu/~ranzato/publications/taigman_cvpr14.pdf",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 63621777
  },
  {
    "blog_id": "bc739e6815ab6217e0cf0a8f706786",
    "summary": [
      "The paper introduces an architecture for end-to-end Reinforcement Learning (RL) optimization for task-oriented dialogue systems and its application to a multimodal task - grounding the dialogue into a visual context.",
      "Encoder Decoder Models vs RL Models  Encoder Decoder models do not account for the planning problems (which are inherent in the dialogue systems) and do not integrate seamlessly with external contexts or knowledge bases.",
      "RL models can handle the planning problem but require online learning and a predefined structure of the task.",
      "Dataset  Corpus of data collected for GuessWhat?!",
      "game  The goal of the game is to locate an unknown object in a rich image scene by asking a series of questions.",
      "The authors use their previous work in GuessWhat?!",
      "paper to build a supervised agent and a neural training environment.",
      "The agent and the environment are used to train a Deep RL agent online which can solve the task.",
      "Link to summary of GuessWhat?!",
      "paper  Training Environment  Oracle  Given an image and a question about the image, the oracle can reply in \"yes\" or \"no\" or \"not applicable\".",
      "Questioner  Given an image, and a list of previous question&answers (if applicable), the questioner generates a question with the aim of locating the object in the image.",
      "Guesser  Once the questioner is confident of having identified the image, the oracle presents a list of objects to the guesser to choose from.",
      "These components are based on the previous work by the authors where they develop nueral baselines for these components.",
      "GuessWhat!?",
      "as a Markov Decision Process  The state of the system is defined as the set of:  list of words/tokens generated so far in the current question  list of questions and answers so far  input image  The action space comprises of set of all the words in the vocabulary (5K in this context).",
      "Transition Graph  if <stop> word is choosen as the action, the full dialogue is terminated.",
      "if <?> word is choosen as the action, the current question is considered completed and the answer is sampled from the oracle.",
      "if any other word is chosen as the action, it is used to update the state of the systema and the process continues.",
      "Reward is defined for every state-action pair.",
      "The dialogue is automatically terminated after Jmax questions while the questions are automatically terminated after Imax words.",
      "The game can be modelled as an episodic RL scenario where generator can be trained using Policy Gradient methods.",
      "The paper defines the reward as 1 if the guesser could identify the correct object and 0 otherwise.",
      "Even though MDP assumption of \"reward being a function of state and action\" does not hold, policy gradient method employed by the paper (called as REINFORCE) is still applicable if the MDP is partially observable.",
      "Training Process  Train the oracle, the questioner and the guesser independently.",
      "Finetune the trained questioner using the proposed RL framework.",
      "Results  On test set, the baseline approach obtained 45% accuracy while the paper reports 53% accuracy.",
      "Beam search baseline (case of supervised encoder-decoder model) tends to repeat questions - an indication of poor generalization.",
      "Beam search basline also tends to generate longer sentences and incoherent sequence of questions (indicating towards the lack of planning component).",
      "RL trained model favours enumerating object categories and spatial information.",
      "RL trained model tends to generate shorter dialogues and uses a smaller set of vocabulary.",
      "One player, called as the oracle, is randomly assigned an object in the given image.",
      "The second player, called as the questioner, tries to locate the object, given just the image.",
      "The questioner can ask a series of questions about the object and the oracle can reply in \"yes\" or \"no\" or \"not applicable\".",
      "Once the questioner is confident of having identified the image, the oracle presents a list of objects to the questioner to choose from.",
      "A small penalty is added, every time a question is asked, so as to encourage informative questions only.",
      "GuessWhat?!",
      "Game  One player, called as the oracle, is randomly assigned an object in the given image.",
      "The second player, called as the questioner, tries to locate the object, given just the image.",
      "The questioner can ask a series of questions about the object and the oracle can reply in \"yes\" or \"no\" or \"not applicable\".",
      "Once the questioner is confident of having identified the image, the oracle presents a list of objects to the questioner to choose from.",
      "A small penalty is added, every time a question is asked, so as to encourage informative questions only.",
      "Dataset  A filtered subset of images from MSCOCO is used as the image set.",
      "Two separate tasks create on Amazon Mechanical Turk (AMT) - for the role of oracle and questioner.",
      "Data was post processed -- both manually and using AMT -- to account for things like spelling mistakes and validation.",
      "Final dataset comprises of 150K thousand human game iterations with 800K question-answer pairs on 60K images.",
      "Dataset is available at  [url]"
    ],
    "author_id": "shugan",
    "pdf_url": "https://www.robots.ox.ac.uk/~vgg/publications/2014/Pickup14/pickup14.pdf",
    "author_full_name": "Shagun Sodhani",
    "source_website": "https://github.com/shagunsodhani/papers-I-read",
    "id": 89853012
  },
  {
    "blog_id": "a-deep-learning-approach-to-improve-emotion-cause-extraction-135bd9ea3899",
    "summary": [
      "This paper aims to develop a deep learning method to extract causes behind emotions in a document.",
      "It\u2019s a relatively new NLP task so the authors mainly aim to show its feasibility using a multi-task learning approach.",
      "The proposed approach aims to separately extract emotion and causes from text using multi-task learning and then conducts emotion-cause pairing and filtering using an improved version of the multi-task learning model.",
      "To understand the task, consider the following statement as an example document:  There are five clauses and the fourth clause is considered the emotion clause conveying the emotion of happiness.",
      "The clauses that contain causes (cause clause) are the second clause and the third clause.",
      "Previously, the task was known as ECE and involved detecting if a clause was a cause given the annotated emotion label of a document.",
      "See the example in the previous figure.",
      "The task is to find the corresponding cause clauses that correspond to the emotion of \u201chappy\u201d.",
      "The main shortcomings of this task are that emotions of a document must first be annotated before cause extraction, which limits its real-world application and ignores the fact that emotions and causes are mutually indicative.",
      "In contrast, the proposed ECPE task (shown on the right of the figure) outputs pairs of emotion-cause clauses and doesn\u2019t need to provide annotations in advance.",
      "As seen in the example, two emotion-cause clauses are extracted without providing the emotion label of \u201chappy\u201d.",
      "Ideally, identifying causes may be able to improve emotion extraction from text and vice-versa, assuming emotion and cause are not mutually independent.",
      "The Approach  The framework consists of two parts: 1) extract sets of emotion and cause clauses from each document via two kinds of multi-task learning networks, and 2) conduct emotion-cause pairing and filtering.",
      "The pairing is done via Cartesian product applied to the set of emotion and cause clauses.",
      "This yields a set of candidate emotion-cause pairs, on which a filter is applied to remove clause pairs that don\u2019t contain a causal relationship.",
      "To perform the first part of the framework (i.e. emotion and cause clause extraction), the Bi-LSTM based multi-task framework shown in the figure below is used.",
      "The model (also referred to as Indep) shows how the prediction for each type of clause is performed via the Bi-LSTM hidden representations which consist of context-aware representations of each clause.",
      "The assumption is that emotion and cause clauses are not mutually independent, therefore, the authors propose an interactive multi-task learning network to capture the correlation between them.",
      "To this end, a more interactive Bi-LSTM based multi-task framework (as shown in the figure below) is proposed.",
      "Compared to the previous multi-task model, the improved model performs a few extra steps in the upper layers which allows the prediction to happen based on the interaction of the two types of representations.",
      "Note that two separate enhanced models based on this framework are used: one for enhancing emotion extraction based on cause extraction (Inter CE), and one for enhancing cause extraction based on emotion extraction (Inter EC).",
      "The output of these models is a set of emotion clauses and a set of cause clauses.",
      "The objective now is to pair the two sets to form emotion-cause pairs that form causal relationships.",
      "A Cartesian product renders the set of all possible pairs which are represented by a feature vector consisting of three components: emotion clause representation, cause clause representation, and the distance between the two clauses.",
      "The cause relationship is determined via a logistic regression model which takes as input the emotion-cause candidate pair features, applied a Sigmoid operation, and outputs a 1 or 0 to indicate if the relationship exists for each pair.",
      "The pairs that output 1 are the final set of emotion-cause pairs and used for evaluation.",
      "Results  The benchmark emotion-cause dataset by Gui et al., 2016 is used for all experiments.",
      "Precision, recall, and F1 scores are used as the evaluation metrics.",
      "The table below shows the experimental results for all the separate tasks using the three proposed models based on multi-task learning.",
      "Inter-EC produced better results, especially as it relates to the ECPE task.",
      "One can also observe that both emotion extraction and cause extraction improve each other in individual tasks.",
      "The results are consistent with the intuition that emotion and cause are mutually indicative.",
      "Other variations of the models that leverage the dataset available annotations are also tested, which unsurprisingly show improvements since they use the additional information to improve the predictions of the different tasks.",
      "You can observe the results of the new models (labeled by the Bound tag) below:  The results below show the effectiveness of the pair filtering phase:  Other results using rule-based and machine learning methods are also presented in the paper.",
      "In summary, the overall results indicate that emotion extraction helps to improve cause extraction and cause extraction also helps to enhance emotion extraction.",
      "Combined, both sub-tasks are used to improve ECPE.",
      "The authors also comment that there is room for improvement architecture-wise and also to improve cause extraction which was observed to be the more difficult task.",
      "Paper: Emotion-Cause Pair Extraction: A New Task to Emotion Analysis in Texts \u2014 Rui Xia and Zixiang Ding  Source Code:  [url]"
    ],
    "author_id": "DAIR-AI",
    "pdf_url": "https://arxiv.org/pdf/1906.01267",
    "author_full_name": "Elvis Saravia",
    "source_website": "https://github.com/dair-ai/nlp_paper_summaries",
    "id": 30621133
  },
  {
    "blog_id": "firefly-algorithms-for-multi-model-optimization",
    "summary": [
      "Firefly Algorithms for Multimodal Optimization \u2013 Xin-She Yang, 2010  This is the third post in a mini-series on nature-inspired optimisation algorithms.",
      "The flashing light of fireflies is an amazing sight in the summer sky in the tropical and temperate regions.",
      "There are about two thousand firefly species, and most fireflies produce short and rhythmic flashes.",
      "The pattern of flashes is often unique for a particular species.",
      "The flashing light is produced by a process of bioluminescence, and the true functions of such signaling systems are still being debated.",
      "However, two fundamental functions of such flashes are to attract mating partners (communication), and to attract potential prey.",
      "In addition, flashing may also serve as a protective warning mechanism.",
      "The rhythmic flash, the rate of flashing and the amount of time form part of the signal system that brings both sexes together.",
      "Females respond to a male\u2019s unique pattern of flashing in the same species, while in some species such as photuris, female fireflies can mimic the mating flashing pattern of other species so as to lure and eat the male fireflies who may mistake the flashes as a potential suitable mate.",
      "The flashing lights of fireflies inspired the Firefly Algorithm.",
      "Start with an initial population of fireflies spread over the solution space, and let the brightness of an individual firefly be affected or determined by the landscape of the objective function (the one we are trying to optimise).",
      "Fireflies are attracted to (move towards) brighter fireflies in each iteration.",
      "At the end of the game (e.g. a specified number of intervals), the brightest firefly is the winner!",
      "There are three simplifying rules for our artificial fireflies:  All fireflies are unisex, one firefly is attracted to another regardless of sex  Attractiveness is proportional to brightness.",
      "Both attractiveness and brightness decrease as the distance between two fireflies increases  The brightness is determined by the objective function.",
      "\u201cFor a maximization problem, the brightness can simply be proportional to the value of the objective function.",
      "Other forms of brightness can be defined in a similar way to the fitness function in genetic algorithms.\u201d  There are two key issues in modelling the firefly algorithm: the variation of light intensity, and the formulation of attractiveness.",
      "Attractiveness is determined by brightness, but as we also know, \u2018beauty is in the eye of the beholder.\u2019 More precisely, attractiveness \u03b2 varies with the distance rij between fireflies i and j.",
      "Light intensity decreases with distance from its source too, and light is absorbed in the media, so attractiveness varies with the degree of absorption.",
      "Light intensity follows an inverse square law (1/r2), and absoption depends on an absorption coefficient \u03b3 and the distance r proportional to e-\u03b3r.",
      "We can combine these effects in a Guassian approximation:  I( r ) = I0e-\u03b3r2  where I0 is the intensity at the origin.",
      "Since attractiveness \u03b2 is proportional to light intensity we can therefore use:  \u03b2( r ) = \u03b20e-\u03b3r2  And to avoid calculating an exponential function, we can replace this with:  \u03b2( r ) = \u03b20/(1 + \u03b3r2)  The distance between fireflies is just the Cartesian Distance, which we can extend into multiple dimensions as needed.",
      "Putting all this together with a randomisation parameter \u03b1 gives us the equation for moving fireflies on each iteration, firefly i moves towards a brighter firefly j according to:  xi = xi + \u03b20e-\u03b3r2ij(xj \u2013 xi) + \u03b1(rand \u2013 1/2)  It is worth pointing out that the distance r defined above is not limited to the Euclidean distance.",
      "We can define many other forms of distance r in the n-dimensional hyperspace, depending on the type of problem of our interest.",
      "For example, for job scheduling problems, r can be defined as the time lag or time interval.",
      "For complicated networks such as the Internet and social networks, the distance r can be defined as the combination of the degree of local clustering and the average proximity of vertices.",
      "In fact, any measure that can effectively characterize the quantities of interest in the optimization problem can be used as the \u2018distance\u2019 r.  When \u03b3 \u2192 0 we have the situation where light intensity does not decrease with distance in an idealized sky, and this corresponds to Particle Swarm Optimisation .",
      "When \u03b3 \u2192 \u221e we have \u2018very shortsighted fireflies\u2019 !",
      "This corresponds to the completely random search method.",
      "As the firefly algorithm is usually in somewhere between these two extremes, it is possible to adjust the parameter \u03b3 and \u03b1 so that it can outperform both the random search and PSO.",
      "In fact, FA can find the global optima as well as all the local optima simultaneously in a very effective manner\u2026.",
      "Our simulation results for finding the global optima of various test functions suggest that particle swarm often outperforms traditional algorithms such as genetic algorithms, while the new firefly algorithm is superior to both PSO and GA in terms of both efficiency and success rate.",
      "This implies that FA is potentially more powerful in solving NP-hard problems which will be investigated further in future studies."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://arxiv.org/pdf/1003.1466v1.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 97703387
  },
  {
    "blog_id": "predicate-logic-as-programming-language",
    "summary": [
      "Predicate Logic as Programming Language \u2013 Kowalski 1974  The purpose of programming languages is to enable the communication from man to machine of problems and their general means of solution.",
      "Kowalski shows us that predicate logic can be used as the basis of a \u201cuseful and practical, high-level, non-deterministic programming language with sound theoretical foundations.\u201d This is the foundation for Prolog and Datalog.",
      "As a programming language, predicate logic is the only language which is entirely user-oriented.",
      "It differs from existing high-level languages in that it possesses no features which are meaningful only in machine terms.",
      "Sentences are expressed in clausal form with a very simple syntax.",
      "B1,...,Bm <- A1,...,An  which can be interpreted as meaning B1 or \u2026 or Bm is implied by A1 and \u2026 and An.",
      "It is our thesis, that clausal form defines a natural and useful language in its own right, that thoughts can be conveniently expressed directly in clausal form, and that literal translation from another language, such as full predicate logic, often distorts the original thought.",
      "The Horn clause subset of predicated logic is that where clauses contain at most one disjunction (B) term.",
      "This is the form used for computation in the paper.",
      "A simple factorial example is given.",
      "Let Fact(x,y) denote the fact that the factorial of x is y, and let s(x) be the successor of x in the natural numbers.",
      "Finally, let Times(x,y,z) denote that x * y is z.",
      "Then:  -- the factorial of 0 is s(0), i.e. 1  Fact(0,s(0)) <-    -- if v is the factorial of x, and s(x) * v is u, then -- the factorial of s(x) is u Fact(s(x),u) <- Fact(x,v), Times(s(x),v,u)  To read these programs, I mentally insert an \u2018if\u2019 ; we can deduce the terms on the left if the terms on the right are true.",
      "Contrast this to a functional declaration where we mentally insert an \u2018is\u2019:  factorial 0     = 1 factorial (n+1) = (n+1) * factorial n  Non-determinism and the potential for parallelism  Predicate logic is an essentially non-deterministic programming language.",
      "Non-determinism is due to the fact that a given program and activating goal statement may admit more than a single legitimate computation.",
      "This is nicely illustrated with a program for sorting lists.",
      "Let Sort(x,y) denote that y is a sorted version of x; Perm(x,y) that y is a permutation of x; Ord(y) that y is ordered; Del(x,y,z) that deleting x from y results in z; and LE(x,y) that x is less than or equal to y; then:  Sort(x,y) <- Perm(x,y), Ord(y)      Perm(nil,nil) <- Perm(z,cons(x,y)) <- Perm(z',y), Del(x,z,z')      Del(x,cons(x,y),y) <- Del(x,cons(y,z),cons(y,z')) <- Del(x,z,z')      Ord(nil) <- Ord(cons(x,nil)) <- Ord(cons(x,cons(y,z))) <- LE(x,y), Ord(cons(y,z))  Consider three difference approaches to computing the sorted version of a list based on these declarations:  Generate a permutation y of x, and then test to see if y is ordered, or  Generate an ordered list y, and then test to see if it is a permutation of x, or  Grow increasingly longer ordered subsets of x, adding one element from x at each stage.",
      "Clearly the difference in efficiency can be enormous, but the meaning, as determined by the input-output relation Sort(x,y), computed by the program, is the same.",
      "It is in this sense that the sequencing of procedure calls can be said to have no semantics.",
      "This is fundamentally what enables highly-parallel executions in languages such as Dedalus , a Datalog derivative:  The use of parallel processes and co-routines is a particular way of sequencing procedure calls.",
      "The possibility of independent parallel processing arises when, for example, different procedure calls in the same body share no variables.",
      "In such a case, the independent procedure calls can be activated simultaneously and, given a single processor, their execution sequences can be interleaved arbitrarily.",
      "40 years later, and we\u2019re seeing a mini-revival in interest in Datalog and its derivatives."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://www-public.it-sudparis.eu/~gibson/Teaching/CSC4504/ReadingMaterial/Kowalski74.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 39155988
  },
  {
    "blog_id": "high_resolution_image_synthesis_with_conditional_gans",
    "summary": [
      "What  They suggest a method to generate (via GANs) high-resolution images that match given segmentation maps.",
      "They introduce a new loss for GANs.",
      "They suggest a method to easily change the appearances of object instances in the images.",
      "How  Architecture  They train two generators (theoretically can be any number of generators).",
      "The first (small-scale) generator gets a small-scale segmentation map as its input (e.g. a quarter of the intended output size).",
      "It generates an image matching that segmentation map -- as in any other GAN.",
      "The small scale generator is then trained for some time.",
      "Then they add a second generator (large-scale).",
      "At each execution, first the small-scale generator is executed.",
      "It receives the small-scale segmentation map as input, transforms that into a feature map and uses that map to generate an output image.",
      "Then the large-scale generator is executed.",
      "It first uses convolutions to downscale the large-scale segmentation map.",
      "Then it adds (in residual fashion) the feature map from the small-scale generator.",
      "Then it uses these features to generate the large-scale output image.",
      "Visualization:  Additionally, they train three different discriminators.",
      "Each discriminator works completely independently and gets a different scale as input (e.g. D1: 1x downscaled, D2: 2x downscaled, D3: 4x downscaled).",
      "Each image produced by the generator is fed through all three discriminators.",
      "This allows to do multi-scale discrimination (training the generator at the coarse and fine level).",
      "Loss  They add a feature matching loss, which incentivizes the generator to produce images that are projected by the discriminator onto features that are similar to real images.",
      "Equation:  They add a perceptual loss, which works similar to the feature matching loss, but the network to generate the features is VGG16 (instead of the discriminator).",
      "Equation:  They use LSGAN (least squares GAN) for everything else.",
      "Instance Segmentation  The generator gets a semantic segmentation map as its input.",
      "Downside: The map does not indicate where each object instance ends.",
      "This makes the image generation needlessly more complicated.",
      "To fix that, they add instance information.",
      "Just adding the instance segmentation map is difficult, as there can be infinitely many instances, resulting in an input with infinitely many channels.",
      "Instead, they just use the segmentation map as one channel and add the boundaries of the object instances as a second channel.",
      "Visualization:  Instance Manipulation  Their aim is to develop a tool in which one can select an object instance and easily change its appearance.",
      "That appearance change could be created by changing the generator's input noise vector.",
      "Downside 1: This would also change everything else in the image.",
      "Downside 2: Potentially many noise vectors might result in practically the same appearance.",
      "So many vectors would have to be tried by the user.",
      "They use an instance-wise feature embedding to fix that.",
      "Step 1:  Use an autoencoder-like network E to transform each input image into a feature map.",
      "E has an encoder that encodes the image into a small (bottleneck) feature map.",
      "E has e decoder that decodes the bottleneck into a large feature map (not an image!)",
      "of the same size as the input image.",
      "E's output feature map could be added to the generator's input, i.e. no special autoencoder loss is needed.",
      "Step 2:  E has transformed the input image to a feature map.",
      "Apply average pooling to that feature map per object instance.",
      "This effectively creates one feature vector per object instance (upscaled so that it cover's the object instance's area in the original image).",
      "The feature vector contains information about the object instance's appearance.",
      "They set each feature vector to have three components (i.e. E's output has three channels).",
      "Visualization:  Step 3:  Now train the generator as before, but additionally to the semantic segmentation maps it gets the pooled feature maps as inputs.",
      "As mentioned, E is trained in parallel.",
      "Step 4:  Generator and E are now trained.",
      "Convert each image in the dataset to a feature map via E. This results in a large number of instance-specific appearance vectors.",
      "Run K-Means clustering on these vectors with K=10 clusters per class.",
      "Result: Per object class 10 different appearance vectors.",
      "By changing the vector of on object instance to a different one (among these 10), the object's appearance in the image can be changed without significant effects on the remainder of the image.",
      "Results  Subjectively more realistic looking images than pix2pix and CRF at 2048x1024.",
      "Tests with Mechanical Turk also show that their images look more realistic to most people.",
      "Tests with Mechanical Turk indicate that both feature loss and perceptual loss (based on VGG16) improve image quality.",
      "(Though numbers for perceptual loss aren't that clear.)",
      "Tests show that the generated images have high matching with the segmentation maps given to the generator.",
      "Adding instance boundaries as an input channel helps with image quality at object boundaries:  Easy changing of object instance appearances, see video.",
      "They also test on other datasets (NYU, ADE20K, Helen Faces) and get good results there too."
    ],
    "author_id": "ALEJU",
    "pdf_url": "https://arxiv.org/pdf/1711.11585",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 86508243
  },
  {
    "blog_id": "flower-pollination-algorithm-for-global-optimization",
    "summary": [
      "Flower Pollination Algorithm for Global Optimization \u2013 Xin-She Yang 2013  The last of the optimisation algorithms we\u2019ll look at for now, today\u2019s paper is the most recent (2013) and also by Xin-She Yang.",
      "Once more, we only get comparisons to genetic algorithms and PSO, and once more the comparison is favourable.",
      "In many design applications in engineering and industry, we have to try to find the optimal solution to a given problem under highly complex constraints.",
      "Such constrained optimization problems are often highly nonlinear, to find the optimal solutions is often a very challenging task if it is not impossible.",
      "Most conventional optimization do not work well for problems with nonlinearity and multimodality.",
      "(The) current trend is to use nature-inspired metaheuristic algorithms to tackle such difficult problems, and it has been shown that metaheuristics are surprisingly very efficient.",
      "For this reason, the literature of metaheuristics has expanded tremendously in the last two decades.",
      "Up to now, researchers have only used a very limited characteristics inspired by nature, and there is room for more algorithm development.",
      "Flower pollination can be viewed as a \u2018survival of the fittest\u2019 optimization process for plant species.",
      "Pollination can be abiotic or biotic.",
      "90% of all plants use biotic pollination, in which pollen is transferred by pollinators such as insects and other animals.",
      "The other 10% use abiotic pollination which uses mechanisms such as wind and water to transfer pollen.",
      "Flower constancy is the tendency of certain pollinators to exclusively visit certain flowering plant species.",
      "Such flower constancy may have evolutionary advantages because this will maximize the transfer of flower pollen to the same or conspecific plants, and thus maximizing the reproduction of the same flower species.",
      "Pollination can be via cross-pollination or self-pollination.",
      "Cross-pollination is the transfer of pollen from a different plant, whereas self-pollination is the transfer of pollen from a flower of the same plant.",
      "Biotic cross-pollination (transfer of pollen across different plants by insects and other animals) can occur at a long distance, and thus can be considered as global pollination.",
      "In addition, bees and birds may behave as L\u00e9vy flight behaviour, with jump or fly distance steps obeying a L\u00e9vy distribution.",
      "Furthermore, flower constancy can be used an increment step using the similarity or difference of two flowers.",
      "The two key steps in the Flower Pollination Algorithm represent global pollination (biotic cross-pollination) and local pollination (abiotic and self-pollination).",
      "Which of the two pollination processes is used on a particular iteration for a particular flower is controlled by a probability p.  Due to the physical proximity and other factors such as wind, local pollination can have a significant fraction p in the overall pollination activities.",
      "For simplicity, it is assumed that each plant has a single flower, and each flower produces only one pollen gamete.",
      "Thus plant, flower, pollen gamete, and problem solution are all identified with each other.",
      "Global pollination is modelled by the following process:  xit+1 = xit + L(xit \u2013 gbest)  xit is the pollen i or solution vector xi at iteration t. gbest is the globally best solution.",
      "The parameter L is a step size, and is drawn from a  L\u00e9vy distribution as we saw with the Cuckoo Search algorithm.",
      "This models insects moving over a long distance with various distance steps.",
      "Local pollination is represented by:  xit+1 = xit + \u03b5(xjt \u2013 xkt)  where xjt and xkt represent pollens from different flowers of the same plant species.",
      "This becomes a local random walk if we draw \u03b5 from [0,1].",
      "The whole algorithm comes together as follows:  Initialize a population with random solutions  For each iteration, for each flower in the population:  Draw a random variable rand in [0,1]  If rand < p, then perform global pollination  Else perform local pollination with j and k chosen randomly from among all solutions  Evaluate the new solutions, and if they are better update the population  The winner is the global best solution at the end of the prescribed number of iterations  p = 0.8 was found to work well in simulations.",
      "Our simulation results have shown that the the proposed flower pollination algorithm is very efficient and can outperform both genetic algorithm and particle swarm optimization.",
      "The convergence rate is essentially exponential as we have seen from the convergence comparison in the previous section.",
      "The reasons that FPA is efficient can be twofold: long-distance pollinators and flower consistency.",
      "Pollinators such as insects can travel long distances, and thus they introduce the ability (into the algorithm) that they can escape any local landscape and subsequently explore a larger search space.",
      "This acts as exploration moves.",
      "On the other hand, flower consistency ensures that the same species of the flowers (thus similar solutions) are chosen more frequently and thus guarantee convergence more quickly.",
      "This step is essentially an exploitation step.",
      "The interplay and interaction of these key components and the selection of the best solution gbest ensures that the algorithm is very efficient."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://arxiv.org/pdf/1312.5673v1.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 91633253
  },
  {
    "blog_id": "group_normalization",
    "summary": [
      "What  They propose Group Normalization (GN), a normalization technique similar to Batch Normalization (BN).",
      "It works examplewise and splits the channels into groups before normalizing similar to BN.",
      "They argue that this is similar to e.g. HOG or SIFT normalizing over groups.",
      "As GN works examplewise it is not negatively affected by small batch sizes or batches with non-iid data.",
      "This allows to train at tiny batch sizes, opening up GPU memory for larger models.",
      "Their normalization is partially motivated from neuroscience, where some models assume normalization across cell responses.",
      "How  There are multiple existing normalization techniques similar to GN, which normalize an (N, C, H, W) (examples, channels, height, width) based on z-transformations, i.e. x'_i = (x_i - mean_i)/std_i.",
      "Each normalization technique works in different ways:  Batch Normalization: Normalizes each channel along the (N, H, W) axes, i.e. computes statistics over all examples in the batch.",
      "Hence it is dependent on the batch size and each example is influenced by all other examples in the batch.",
      "BN is the same as Instance Normalization for batch size 1.",
      "Layer Normalization: Normalizes along the (C, H, W) axes, i.e. computes statistics per example (one mean and one standard deviation value per example).",
      "Instance Normalization: Normalizes long the (H, W) axes, i.e. computes one mean and standard deviation per example and channel.",
      "Other examples or channels do not influence the computation.",
      "Group Normalization (their method): Normalizes roughly along the (C, H, W) axes, similar to Layer Normalization.",
      "However, the channel axis is subdivided into groups of channels and the normalization happens over all channels within the same group (instead of over all channels in the example as in Layer Normalization).",
      "GN becomes Instance Normalization when the number of groups is set to the number of channels.",
      "GN becomes Layer Normalization when the number of groups is set to 1.",
      "Visualization of the relationship:  Results  They test on ImageNet, training on 8 GPUs.",
      "They use ResNet-50.",
      "Their default number of groups in GN is 32.",
      "At batch size 32: They observe 0.5 percentage points higher validation error with GN as opposed to BN (layer normalization: 1.7 points worse, instance normalization: 4.8 points worse).",
      "They observe slightly lower training error of GN compared to BN.",
      "They argue that BN profits from a regularizing effect from the examples in each batch being randomly picked (i.e. mean and variance are a bit stochastic).",
      "At batch size 2: GN's accuracy remains constant (compared to batch size 32), while BN degrades significantly.",
      "GN beats BN by 10.6 points.",
      "They also compare to Batch Renormalization with well chosen hyperparameters and find that it improves BN's performance, but it still performs 2.1 points worse than GN.",
      "Accuracy vs. batch size:  Channels per group  They found on ImageNet (with ResNet-50) that around 8 or more channels per group performs best.",
      "They get similar results when training with ResNet-101.",
      "VGG-16  They trained VGG-16 on ImageNet with no normalization, BN and GN.",
      "They observed that the feature maps before normalization and ReLU had a significantly wider value range (-80 to +20) when using no normalization as opposed to using normalization (BN: -3 to +3, GN: -4 to +3).",
      "Object Detection and Segmentation on COCO  They compare BN and GN for object detection and segmentation on COCO using Mask R-CNN with different backbones.",
      "ResNet-50 backbone  Using GN instead of (frozen) BN improves box AP by 1.1 points and mask AP by 0.8 points.",
      "Feature Pyramid Network (FPN) backbone  They compare using BN and GN only in the head of Mask R-CNN.",
      "BN performs significantly worse (9 points) than GN, even though the batch size is at 512 RoIs.",
      "They argue that this is because the examples are not iid (they all come from the same image).",
      "GN in the backbone improves AP by 0.5 points.",
      "Using GN also allows them to train the whole model from scratch (i.e. no pretraining) without training instabilities.",
      "They reach scores very close to the pretrained versions.",
      "Video classification on Kinetics  They test on the Kinetics dataset, which requires classifying videos and hence leads to large memory consumption per example.",
      "They use a ResNet-50 Inflated 3D (I3D) network as baseline.",
      "Normalization is extended from (H,W) axes to (T,H,W) axes, adding the temporal dimension.",
      "GN performs overall superior to BN by up to 1.2 points accuracy.",
      "GN profits from not sub-sampling frames (e.g. using only every second frame as input).",
      "BN however does not improve its accuracy from deactivating subsampling, because that also required reducing the batch size from 8 to 4 examples.",
      "The positive effect of deactivating subsampling was masked by the negative effect of reducing the batch size, leading to a false conclusion that subsampling does not degrade accuracy."
    ],
    "author_id": "ALEJU",
    "pdf_url": "http://openaccess.thecvf.com/content_ECCV_2018/papers/Yuxin_Wu_Group_Normalization_ECCV_2018_paper.pdf",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 98262950
  },
  {
    "blog_id": "a-light-introduction-to-transformer-xl-be5737feb13",
    "summary": [
      "Background  Language modeling has been recently addressed using unsupervised training methods such as ELMo and BERT .",
      "However, it still remains a challenge to properly equip neural networks with long-term dependency.",
      "Recent models were designed with an attention mechanism to help ease optimization \u2014 by dealing with vanishing gradient \u2014 and enable the learning of long-term dependency.",
      "However, the context is of fixed-length in these cases so the model cannot capture longer-term dependency and suffers from a problem known as context fragmentation.",
      "Context fragmentation refers to when the model lacks the necessary contextual information to predict the first few symbols due to the way the context was selected \u2014 usually without respect to a sentence or semantic boundaries.",
      "Moreover, previous models don\u2019t support information flow across segments during training and employ fixed context length, which means there is no room for the model to capture longer-term dependency.",
      "In the context of language modeling, hidden states can be reused to allow information flow across segments (a kind of memory).",
      "This could help to support longer-term dependency and deal with context fragmentation.",
      "However, for the architecture to support state reuse, temporal coherence must be managed, as we discuss next.",
      "Transformer-XL  During training, vanilla language models don\u2019t make effective use of context information and segments are treated individually.",
      "In addition, semantic boundaries during segmentation are usually not respected since most methods employ standard chunked sequences of fixed lengths.",
      "During the evaluation, fixed-length contexts are used and segments are processed from scratch, which becomes expensive, even though context fragmentation is somewhat addressed.",
      "This paper aims to focus on the problem of efficiency by better modeling longer-term dependency.",
      "In language modeling, Transformer networks are limited by a fixed-length context and thus can be improved through learning longer-term dependency.",
      "The paper proposes a novel method called Transformer-XL (meaning extra long) for language modeling, which enables a Transformer architecture to learn longer-term dependency \u2014 via a recurrence mechanism \u2014 beyond a fixed length without disrupting temporal coherence.",
      "The method is different from other previous approaches that focus on other strategies to support long-term dependency such as additional loss signals and augmented memory structure.",
      "A segment-level recurrent mechanism is introduced which enables the model to reuse previous hidden states at training time, addressing both the issues of fixed-length context and context fragmentation.",
      "In other words, the historical information can be reused and it can be extended up to as much as GPU memory allows.",
      "See the training and evaluation phases in the figure below.",
      "Transformer-XL \u2014 training and evaluation phase ( figure source )  To properly reuse hidden states, the authors propose a mechanism called relative positional encodings which helps to avoid temporal confusion.",
      "Current models can\u2019t distinguish the positional difference between inputs in different segments at different layers.",
      "Relative position encoding addresses this problem by encoding positional information bias in the hidden states, which differs from other approaches that perform this as the input level.",
      "Since a Transformer architecture is involved, the process above is achieved by computing the relative distance between each key vector and query vector and injecting it into the attention score.",
      "With some new parameterization trick of the terms used to derive the attention score between query and vector, the relative position information can be incorporated.",
      "The recurrence component is now equipped with the proposed relative positional embedding and this whole procedure represents the proposed Transformer-XL architecture.",
      "Results  Transformer-XL obtains strong results for both word-level and character-level language modeling applied to a variety of datasets such as WikiText-103, text8, and One Billion Word.",
      "The proposed model is compared with a vanilla model that was recently used for character-level language modeling ( Al-Rfou et al., 2018 ), which also leverages deeper self-attention.",
      "Note that the vanilla model cannot support dependency lengths larger than the upper bound segment length.",
      "Transformer-XL reduces previous SoTA perplexity score on several datasets such as text8, enwiki8, One Billion Word, and WikiText-103.",
      "Besides the SoTA performances, the authors claim that the method is more flexible, faster during evaluation (1874 times speedup), generalizes well on small datasets, and is effective at modeling short and long sequences.",
      "See a summary of some of the results obtained on the different datasets in the Tables below.",
      "You can check the rest of the results in the full paper linked below.",
      "Other Benefits  An ablation study to examine the effects of both the recurrence mechanism and the proposed positional encoding scheme is provided in the paper as well.",
      "The authors also propose a new metric called Relative Effective Context Length that provides a fair way to compare models that are tested with increased context lengths.",
      "Further Readings  Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context  The Annotated Transformer by Harvard NLP Group  Attention Guide by Lilian Weng  Attention Is All You Need  Code repository associated with the paper (TensorFlow and PyTorch)  Character-Level Language Modeling with Deeper Self-Attention  If enough interest is expressed, I may feel tempted to prepare a code walkthrough for this work.",
      "It contains many different components that could be interesting and useful for NLP practitioners and researchers."
    ],
    "author_id": "DAIR-AI",
    "pdf_url": "https://arxiv.org/pdf/1901.02860",
    "author_full_name": "Elvis Saravia",
    "source_website": "https://github.com/dair-ai/nlp_paper_summaries",
    "id": 97923725
  },
  {
    "blog_id": "addressing-challenges-in-deep-rl",
    "summary": [
      "I discuss two recent related papers in the Deep RL literature in this post.",
      "The first paper, by Fujimoto et al., introduces techniques for reducing bias and variance in a popular actor-critic method, Deep Deterministic Policy Gradient (DDPG).",
      "The second paper, by Kostrikov et al., makes a similar contribution by evaluating and addressing bias and variance in inverse RL.",
      "Both of these papers take widely used Deep RL algorithms, empirically and theoretically demonstrate specific weaknesses, and suggest reasonable improvements.",
      "These are valuable studies that help develop a better understanding of Deep RL.",
      "Addressing Function Approx.",
      "Error in AC Methods  If you are unfamiliar with DDPG, you can check out my blog post on the algorithm.",
      "The most important thing to know is that the success of the whole algorithm relies on having a critic network that can accurately estimate $Q$-values.",
      "The only signal the actor network gets in its gradient to help it achieve higher rewards comes from the gradient of the critic wrt the actions selected by the actor.",
      "If the critic gradient is biased, then the actor will fail to learn anything!",
      "In Section 4, the authors begin by empirically demonstrating the overestimation bias present in the critic network (action-value estimator) in DDPG.",
      "They show that the overestimation bias essentially stems from the fact that DPG algorithms have to approximate both the policy and the value functions, and the approximate policy is maximized in the gradient direction provided by the approximate value function (rather than the true value function).",
      "Then, inspired by Double Q-Learning, they introduce a technique they call \u201cclipped Double Q-Learning in AC\u201d for achieving the same idea.",
      "Basically, the critic target becomes  This requires introducing a second critic.",
      "The min makes it so that it is possible to underestimate Q-values, but this is preferable to overestimation.",
      "Then, to help with variance reduction, they suggest:  Delay updating the actor network until the critic network has almost converged  Add some noise to the actions selected by the actor network when updating the critic to help regularize the critic, reminiscent of Expected SARSA  Their experimental results on MuJoco (they call their algorithm TD3) suggest these improvements are very effective, outperforming PPO, TRPO, ACKTR, and others.",
      "Discriminator-Actor-Critic: Addressing Sample Inefficiency and Reward Bias in Adversarial Imitation Learning  EDIT: The title of the paper was previously \u201cAddressing Sample Inefficiency and Reward Bias in Inverse RL\u201d  Seemingly inspired by the former, this paper recently came out exploring inverse RL\u2014specifically, generative adversarial imitation learning (GAIL) and behavioral cloning.",
      "In GAIL, the discriminator learns to distinguish between transitions sampled from an expert and those from a trained policy.",
      "The policy is rewarded for confusing the discrminiator.",
      "However, GAIL is typically quite sample inefficient.",
      "One way the authors suggest to help with the sample inefficiency of GAIL is by using off-policy RL instead of on-policy RL.",
      "They modify the GAIL objective to be  Basically, $\\pi_E$ is the expert policy, from which trajectories are sampled, and $\\mathscr{R}$ is the replay buffer, from which trajectories are sampled from ~all previous policies.",
      "They ignore the importance sampling term in practice.",
      "Since TD3 is technically a deterministic policy gradient algorithm, I\u2019m assuming one way to implement this importance sampling would be to have the actor output the mean of a multivariate Gaussian\u2014this Gaussian could then be used to define the entropy term of the policy and the importance sampling ratio.",
      "This is fairly common for continuous control tasks like MuJoco\u2026the authors note that the importance sampling wasn\u2019t used in practice, however.",
      "They further analyzed different reward functions for GAIL, and show that certain GAIL reward functions can actually inhibit learning depending on the particular MDP (e.g., if the environment has a survival bonus or penalty).",
      "To create a more robust reward function that will learn the expert policies, they suggest explicitly learning rewards for absorbing states of the MDP.",
      "They implement this by adding an indicator to these particular states so that the GAIL discriminator can identify whether reaching an absorbing state is desirable from the perspective of the expert.",
      "In the OpenReview thread , one reviewer makes sure to point out that the problems with inverse RL algorithms highlighted in the paper are due to incorrect implementations of the MDP, rather than shortcomings of the algorithms themselves ( see this comment in particular ).",
      "Very interestingly, they used VR to generate expert trajectories of gripping blocks with a Kuka arm.",
      "This environment has a per-step penalty, and the normal GAIL reward fails to learn the expert policy due to the the learning inhibition caused by the reward function bias.",
      "The proposed method learns to imitate the expert quickly due to its added reward for absorbing states.",
      "It would be great to investigate the effects of using off-policy samples in the objective more carefully (why exactly does importance sampling not matter?",
      "The absorbing state reward stuff being so useful is surprising, and should be helpful in future applications where GAIL is used for inverse RL."
    ],
    "author_id": "pemami",
    "pdf_url": "https://arxiv.org/pdf/1809.02925v2",
    "author_full_name": "Patrick Emami",
    "source_website": "https://pemami4911.github.io/index.html",
    "id": 43650136
  },
  {
    "blog_id": "lopez-pazncsb16",
    "summary": [
      "This paper tests the following hypothesis, about features learned by a deep network trained on the ImageNet dataset:   *Object features and anticausal features are closely related.",
      "Context features and causal features are not necessarily related.",
      "*  First, some definitions.",
      "Let $X$ be a visual feature (i.e. value of a hidden unit) and $Y$ be information about a label (e.g.",
      "the log-odds of probability of different object appearing in the image).",
      "A causal feature would be one for which the causal direction is $X \\rightarrow Y$.",
      "An anticausal feature would be the opposite case, $X \\leftarrow Y$.",
      "As for object features, in this paper they are features whose value tends to change a lot when computed on a complete original image versus when computed on an image whose regions *falling inside* object bounding boxes have been blacked out (see Figure 4).",
      "Contextual features are the opposite, i.e. values change a lot when blacking out the regions *outside* object bounding boxes.",
      "See section 4.2.1 for how \"object scores\" and \"context scores\" are computed following this description, to quantitatively measure to what extent a feature is an \"object feature\" or a \"context feature\".",
      "Thus, the paper investigates whether 1) for object features, their relationship with object appearance information is anticausal (i.e. whether the object feature's value seems to be caused by the presence of the object) and whether 2) context features are not clearly causal or anticausal.",
      "To perform this investigation, the paper first proposes a generic neural network model (dubbed the Neural Causation Coefficient architecture or NCC) to predict a score of whether the relationship between an input variable $X$ and target variable $Y$ is causal.",
      "This model is trained by taking as input datasets of $X$ and $Y$ pairs synthetically generated in such a way that we know whether $X$ caused $Y$ or the opposite.",
      "The NCC architecture first embeds each individual $X$,$Y$ instance pair into some hidden representation, performs mean pooling of these representations and then feeds the result to fully connected layers (see Figure 3).",
      "The paper shows that the proposed NCC model actually achieves SOTA performance on the T\u00fcbingen dataset, a collection of real-world cause-effect observational samples.",
      "Then, the proposed NCC model is used to measure the average object score of features of a deep residual CNN identified as being most causal and most anticausal by NCC.",
      "The same is done with the context score.",
      "What is found is that indeed, the object score is always higher for the top anticausal features than for the top causal features.",
      "However, for the context score, no such clear trend is observed (see Figure 5).",
      "**My two cents**  I haven't been following the growing literature on machine learning for causal inference, so it was a real pleasure to read this paper and catch up a little bit on that.",
      "Just for that I would recommend the reading of this paper.",
      "The paper does a really good job at explaining the notion of *observational causal inference*, which in short builds on the observation that if we assume IID noise on top of a causal (or anticausal) phenomenon, then causation can possibly be inferred by verifying in which direction of causation the IID assumption on the noise seems to hold best (see Figure 2 for a nice illustration, where in (a) the noise is clearly IID, but isn't in (b)).",
      "Also, irrespective of the study of causal phenomenon in images, the NCC architecture, which achieves SOTA causal prediction performance, is in itself a nice contribution.",
      "Regarding the application to image features, one thing that is hard to wrap your head around is that, for the $Y$ variable, instead of using the true image label, the log-odds at the output layer are used instead in the study.",
      "The paper justifies this choice by highlighting that the NCC network was trained on examples where $Y$ is continuous, not discrete.",
      "On one hand, that justification makes sense.",
      "On the other, this is odd since the log-odds were in fact computed directly from the visual features, meaning that technically the value of the log-odds are directly caused by all the features (which goes against the hypothesis being tested).",
      "My best guess is that this isn't an issue only because NCC makes a causal prediction between *a single feature* and $Y$, not *from all features* to $Y$.",
      "I'd be curious to read the authors' perspective on this.",
      "Still, this paper at this point is certainly just scratching the surface on this topic.",
      "For instance, the paper mentions that NCC could be used to encourage the learning of causal or anticausal features, providing a new and intriguing type of regularization.",
      "This sounds like a very interesting future direction for research, which I'm looking forward to."
    ],
    "author_id": "hlarochelle",
    "pdf_url": "http://arxiv.org/pdf/1605.08179",
    "author_full_name": "Hugo Larochelle",
    "source_website": "https://www.shortscience.org/user?name=hlarochelle",
    "id": 16124640
  },
  {
    "blog_id": "scaling-concurrent-log-structured-data-stores",
    "summary": [
      "Scaling Concurrent Log-Structured Data Stores \u2013 Golan-Gueta et al. 2015  Key-value stores based on log-structured merge trees are everywhere.",
      "The original design was intended to mitigate slow disk I/O.",
      "Once this is achieved, as we scale to more and more cores the authors find that in-memory contention now becomes the bottleneck (see yesterday\u2019s piece on the Universal Scalability Law ).",
      "By replacing the in-memory component of the LSM-tree with a concurrent hash map data structure it is possible to get much better scalability.",
      "Disk access is a principal bottleneck in storage systems, and remains a bottleneck even with today\u2019s SSDs.",
      "Since reads are often effectively masked by caching, significant emphasis is placed on improving write throughput and latency.",
      "It is therefore not surprising that log-structured merge solutions, which batch writes in memory and merge them with on-disk storage in the background, have become the de facto choice for today\u2019s leading key-value stores\u2026  (Side note: \u2013 will storage access still be the principal bottleneck when we have NVMM ?)",
      "Google\u2019s LevelDB is the state-of-the-art implementation of a single machine LSM that serves as the backbone in many of such key-value stores.",
      "It applies coarse-grained synchronization that forces all puts to be executed sequentially, and a single threaded merge process.",
      "These two design choices significantly reduce the system throughput in multicore environment.",
      "This effect is mitigated by HyperLevelDB, the data storage engine that powers HyperDex.",
      "It improves on LevelDB in two key ways: (1) by using fine-grained locking to increase concurrency, and (2) by using a different merging strategy\u2026 Facebook\u2019s key-value store RocksDB also builds on LevelDB.",
      "Links: LevelDB , HyperLevelDB , HyperDex , RocksDB .",
      "One way of dealing with the challenge of multiple cores is to further partition the data and run multiple Log Structured Merge Data Stores (LSM-DS) on the same machine.",
      "A fine-grained partitioning mechanism is recommended by Dean et al. in The Tail at Scale .",
      "Golan-Gueta et al. put forward two counter-arguments to this approach: (a) consistent snapshot scans do not span multiple partitions, instead requiring transactions across shards, and (b) you need a system-level mechanism for managing partitions, which can itself become a bottleneck.",
      "The following chart shows the performance of the authors\u2019 concurrent-LSM (cLSM) implementation with one large partition, vs LevelDB and HyperLevelDB handling the same amount of overall data but divided into four partitions.",
      "cLSM achieves much better throughput as the number of concurrent threads increases, but\u2026  A first glance at the RHS of the chart makes cLSM look impressive.",
      "Note a couple of things though: the performance at 16 threads is quite close for all systems (oh, and the hardware used for the test can support 16 hardware threads ;) ); and above 16 threads although cLSM performs much better than the others, its absolute gains in throughput are relatively modest.",
      "You can also just start to see a nice curve as predicted but the USL in the cLSM throughput numbers.",
      "More convincing is the data from the evaluation of workloads logged in production by a key-value store supporting \u2018some of the major personalized content and advertising systems on the web.\u2019 See figure 10 from the paper, reproduced below:  cLSM also manages to maintain lower latency numbers than the competition as throughput scales.",
      "How is it done?",
      "First off, the in-memory component of the LSM tree is replaced by a \u2018thread-safe map data structure.\u2019  We assume a thread-safe map data structure for the in- memory component, i.e., its operations can be executed by multiple threads concurrently.",
      "Numerous data structure implementations, provide this functionality in a non-blocking and atomic manner.",
      "This is then integrated into the normal LSM merge operation.",
      "Snapshot scans are implemented on top of the in-memory map on the assumption that the map provides iterators with weak consistency (if an element is included in the map for the duration of the scan, it will be included in the scan results).",
      "Finally, an atomic read-modify-write operation is introduced:  We now introduce a general read-modify-write operation, RMW(k,f), which atomically applies an arbitrary function f to the current value v associated with key k and stores f(v) in its place.",
      "Such operations are useful for many applications, ranging from simple vector clock update and validation to implementing full-scale transactions.",
      "The implementation assumes a linked-list or derivative implementation type for the map structure.",
      "On this basis it uses optimistic concurrency control, and the contents of the prev and succ nodes in the chain to detect conflicts.",
      "At its heart though, the improvement seems to boil down to the simple idea of \u2018why don\u2019t we use an existing efficient in-memory structure for the in-memory part of the LSM-tree\u2019.",
      "That seems so simple and obvious (the benefit of hindsight perhaps?)",
      "that I\u2019m left with the nagging feeling maybe I\u2019m missing something here\u2026?"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://dl.acm.org/doi/pdf/10.1145/2741948.2741973?download=true",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 82821959
  },
  {
    "blog_id": "xlnet-outperforms-bert-on-several-nlp-tasks-9ec867bb563b",
    "summary": [
      "Two pretraining objectives that have been successful for pretraining neural networks used in transfer learning NLP are autoregressive (AR) language modeling and autoencoding (AE).",
      "Autoregressive language modeling is not able to model deep bidirectional context which has recently been found to be effective in several downstream NLP tasks such as sentiment analysis and question answering.",
      "On the other hand, autoencoding based pretraining aims to reconstruct original data from corrupted data.",
      "A popular example of such modeling is used in BERT, an effective state-of-the-art technique used to address several NLP tasks.",
      "One advantage of models like BERT is that bidirectional contexts can be used in the reconstruction process, something that AR language modeling lacks.",
      "However, BERT partially masks the input (i.e. tokens) during pretraining which results in a pre-training-finetune discrepancy.",
      "In addition, BERT assumes independence on predicted tokens, something which AR models allow for via the product rule which is used to factorize the joint probability of predicted tokens.",
      "This could potentially help with the pretrain-finetune discrepancy found in BERT.",
      "The proposed model (XLNet) borrows ideas from the two types of language pretraining objectives (AR and AE) while avoiding their limitations.",
      "The XLNet model  XLNet makes use of a permutation operation during training time that allows context to consists of tokens from both left and right, capturing the bidirectional context, making it a generalized order-aware AR language model.",
      "During pretraining, XLNet adopts the segment recurrent mechanism and relative encoding scheme proposed in Transformer-XL .",
      "Essentially, the novel permutation language modeling objective (see paper for extra details) allows sharing of the model parameters across all the permuted factorization orders.",
      "This enables the AR model to properly and effectively capture bidirectional context while avoiding the independence assumption and pretrain-finetune discrepancy that BERT is subject to.",
      "Simply put it, XLNet keeps the original sequence order, uses positional encodings, and relies on a special attention mask in Transformers to achieve the said permutation of the factorization order.",
      "In other words, the original Transformer architecture is modified and re-parameterized to avoid issues such as target ambiguity and pretrain-finetune discrepancy.",
      "The core change happens in the hidden representation layers (see paper for details).",
      "XLNet is based on the Transformer-XL which it uses as the main pretraining framework.",
      "Obviously, for the proposed permutation operation to work, a few modifications are proposed, which enforce the proper reuse of the hidden states from previous segments.",
      "Some design ideas from BERT are also used to perform partial prediction and support certain tasks that consist of multiple segments like question and context paragraph in question answering.",
      "From the following examples below, we can observe that both BERT and XLNet compute the objective differently.",
      "In general, XLNet captures more important dependencies between prediction targets, such as (New, York), which BERT omits.",
      "XLNet also proves to cover more dependencies as compared to GPT and ELMo .",
      "Overall, XLNet makes a compelling case for bridging the gap between language modeling and pretraining, all achieved by leveraging AR modeling and borrowing techniques from previous methods like BERT and Transformer-XL.",
      "More importantly, XLNet aims to address the pretrain-finetune discrepancy, which means language models can potentially improve downstream tasks through this useful generalization.",
      "Experiments  Several sources like BooksCorpus, English Wikipedia, Giga5, and Common Crawl are combined and used for pretraining.",
      "Tokenization is achieved with SentencePiece.",
      "The same architecture hyperparameters as BERT-Large are used in XLNet-Large and trained on 512 TPU v3 chips for 500K epochs with an Adam optimizer.",
      "A linear learning rate decay and a batch size of 2048 are used, all leading to roughly 2.5 days of training.",
      "XLNet-Large was not able to leverage the additional data scale, so XLNet-Base (analogous to BERT-Base) was used to conduct a fair comparison with BERT.",
      "This also means that only BooksCorpus and English Wikipedia were used for pretraining.",
      "Results  RACE involves a reading comprehension dataset that is used to test the question answering and long text understanding capabilities of the model.",
      "As shown in the table below, XLNet outperforms (in terms of accuracy) both GPT and BERT pretraining models.",
      "SQuAD and NewsQA are also popular reading comprehension datasets which consist of two tasks.",
      "Specifically, XLNet jointly trains on SQuAD 2.0 and NewsQA and obtains state-of-the-art performance on this task, outperforming BERT even on the dev set (see results below).",
      "XLNet now holds state-of-the-art (SoTA) results on several text classification benchmarks such as IMDB and DBpedia (see results below).",
      "GLUE consists of 9 natural language understanding tasks.",
      "Using XLNet, multiple settings such as single-task and multi-task, as well as single models and ensembles are tested on GLUE.",
      "In the end, a multi-task ensemble XLNet achieves SoTA results on 7 out of 9 tasks.",
      "XLNet outperforms BERT on the different datasets as seen in the table below.",
      "An interesting ablation study and extended results are provided in the paper to justify some of the design choices made in XLNet and how it compares with other models such as BERT and GPT.",
      "XLNet: Generalized Autoregressive Pretraining for Language Understanding \u2014 (Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le)"
    ],
    "author_id": "DAIR-AI",
    "pdf_url": "https://arxiv.org/pdf/1906.08237",
    "author_full_name": "Elvis Saravia",
    "source_website": "https://github.com/dair-ai/nlp_paper_summaries",
    "id": 52208833
  },
  {
    "blog_id": "optimizing-search-engines-using-clickthrough-data",
    "summary": [
      "Optimizing Search Engines using Clickthrough Data \u2013 Joachims, 2002  Today\u2019s choice is another KDD \u2018test-of-time\u2019 winner .",
      "The paper introduced the problem of ranking documents w.r.t.",
      "a query using not explicit user feedback but implicit user feedback in the form of clickthrough data.",
      "The author presented the Ranking SVM Algorithm to solve the proposed ranking problem.",
      "The paper has stimulated much follow-up research, and the Ranking SVM Algorithm has been widely used for many applications, as evidenced by the very large number of citations.",
      "The work has also been included in various textbooks.",
      "It\u2019s hard to believe that it was as recently as 2002 that the idea of using clickthrough data to improve search engine rankings was ground-breaking research!",
      "It seems so obvious to us now, but it must have been a very exciting breakthrough at the time.",
      "(And of course, there\u2019s the tricky question of exactly how to do it, once you\u2019ve had the insight).",
      "Which WWW page(s) does a user actually want to retrieve when he types some keywords into a search engine?",
      "\u2026 Unfortunately, experience shows that users are only rarely willing to give explicit feedback.",
      "However, this paper argues that sufficient information is already hidden in the logfiles of WWW search engines.",
      "Since major search engines receive millions of queries per day, such data is available in abundance.",
      "Compared to explicit feedback data, which is typically elicited in laborious user studies, any information that can be extracted from logfiles is virtually free and substantially more timely.",
      "We can model clickthrough data as a triplet (q,r,c) : query, ranked list of results, clicks.",
      "The query q and the returned ranking r can easily be recorded whenever the resulting ranking is displayed to the user.",
      "For recording the clicks, a simple proxy system can keep a logfile.",
      "Each query is assigned a unique ID which is stored in the query-log along with the query words and the presented ranking.",
      "The links on the results-page presented to the user do not lead directly to the suggested document, but point to a proxy server.",
      "These links encode the query-ID and the URL of the suggested document.",
      "When the user clicks on the link, the proxy-server records the URL and the query-ID in the click-log.",
      "The proxy then uses the HTTP Location command to forward the user to the target URL.",
      "This process can be made transparent to the user and does not influence system performance.",
      "We still need to be a little bit careful interpreting (q,r,c) triplets \u2013 users are naturally biased to click on links towards the top of the search rankings so we can\u2019t treat a click as an absolute indication of search result relevance.",
      "But we can interpret it as a relative ranking of the results.",
      "Suppose a search returns a set of links link1, link2, \u2026 linkn.",
      "A user that clicks on link3 but not link1 or link2 must have scanned past  links 1 and 2 before clicking on 3.",
      "This strategy is captured by the following algorithm:  For a ranking (link1,link2,link3, \u2026) and a set C containing the ranks of the clicked-on links, extract a preference example linki <r* linkj, for all pairs 1 &leq; j < i, with i \u2208 C and j \u2209 C.  \u201cUnfortunately, this type of feedback is not suitable for standard machine learning algorithms.\u201d  Consider a search engine with an operational retrieval function f, and let rf(q) be the ranking returned by f for the query q.",
      "We want to evaluate how closely its ranking rf(q) approximates the optimal ordering r*.",
      "Kendall\u2019s \u03c4 is used for this, being the most frequently used measure in statistics for comparing the ordinal correlation of two random variables.",
      "Take two finite strict orderings over some set of documents D, ra and rb.",
      "Examine all pairs of documents d1 and d2 in D \u2013 if ra and rb agree on the relative ranking call (d1,d2) a concordant pair, otherwise if they disagree they are a discordant pair.",
      "Let P be the number of concordant pairs, and Q be the number of discordant pairs.",
      "Kendall\u2019s \u03c4 is simply the fraction of the total number of pairs that are concordant:  \u03c4(ra,rb) = (P \u2013 Q) / (P + Q)  We are now in a position to define the problem of learning a ranking function.",
      "For a fixed but unknown distribution Pr(q, r\u2217) of queries and target rankings on a document collection D with m documents, the goal is to learn a retrieval function f(q) for which the expected Kendall\u2019s \u03c4 is maximal.",
      "This turns out to be equivalent to minimizing the number of discordant pairs (Q).",
      "Unfortunately\u2026 this problem is NP-hard, but it is possible to approximate the solution.",
      "Joachims shows how to do this with a Support Vector Machine.",
      "My SVM-fu isn\u2019t up to giving any meaningful analysis of the way this is done, so I\u2019ll just show you where Joachims ends up:  \u03b5i,j,k are the introduced approximation variables that makes the problem practical, and C is a parameter that enables the trading off of margin size against training error."
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://www.cs.cornell.edu/People/tj/publications/joachims_02c.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 82220044
  },
  {
    "blog_id": "began",
    "summary": [
      "Tensorflow implementation  What  They suggest a GAN algorithm that is based on an autoencoder with Wasserstein distance.",
      "Their method generates highly realistic human faces.",
      "Their method has a convergence measure, which reflects the quality of the generates images.",
      "Their method has a diversity hyperparameter, which can be used to set the tradeoff between image diversity and image quality.",
      "How  Like other GANs, their method uses a generator G and a discriminator D.  Generator  The generator is fairly standard.",
      "It gets a noise vector z as input and uses upsampling+convolutions to generate images.",
      "It uses ELUs and no BN.",
      "Discriminator  The discriminator is a full autoencoder (i.e. it converts input images to 8x8x3 tensors, then reconstructs them back to images).",
      "It has skip-connections from the 8x8x3 layer to each upsampling layer.",
      "It also uses ELUs and no BN.",
      "Their method now has the following steps:  Collect real images x_real.",
      "Generate fake images x_fake = G(z).",
      "Reconstruct the real images r_real = D(x_real).",
      "Reconstruct the fake images r_fake = D(x_fake).",
      "Using an Lp-Norm (e.g. L1-Norm), compute the reconstruction loss of real images d_real = Lp(x_real, r_real).",
      "Using an Lp-Norm (e.g. L1-Norm), compute the reconstruction loss of fake images d_fake = Lp(x_fake, r_fake).",
      "The loss of D is now L_D = d_real - d_fake.",
      "The loss of G is now L_G = -L_D.",
      "About the loss  r_real and r_fake are really losses (e.g. L1-loss or L2-loss).",
      "In the paper they use L(...) for that.",
      "Here they are referenced as d_* in order to avoid confusion.",
      "The loss L_D is based on the Wasserstein distance, as in WGAN.",
      "L_D assumes, that the losses d_real and d_fake are normally distributed and tries to move their mean values.",
      "Ideally, the discriminator produces very different means for real/fake images, while the generator leads to very similar means.",
      "Their formulation of the Wasserstein distance does not require K-Lipschitz functions, which is why they don't have the weight clipping from WGAN.",
      "Equilibrium  The generator and discriminator are at equilibrium, if E[r_fake] = E[r_real].",
      "(That's undesirable, because it means that D can't differentiate between fake and real images, i.e. G doesn't get a proper gradient any more.)",
      "Let g = E[r_fake] / E[r_real], then:  Low g means that E[r_fake] is low and/or E[r_real] is high, which means that real images are not as well reconstructed as fake images.",
      "This means, that the discriminator will be more heavily trained towards reconstructing real images correctly (as that is the main source of error).",
      "High g conversely means that real images are well reconstructed (compared to fake ones) and that the discriminator will be trained more towards fake ones.",
      "g gives information about how much G and D should be trained each (so that none of the two overwhelms the other).",
      "They introduce a hyperparameter gamma (from interval [0,1]), which reflects the target value of the balance g.  Using gamma, they change their losses L_D and L_G slightly:  L_D = d_real - k_t d_fake  L_G = r_fake  k_t+1 = k_t + lambda_k (gamma d_real - d_fake).",
      "k_t is a control term that controls how much D is supposed to focus on the fake images.",
      "It changes with every batch.",
      "k_t is clipped to [0,1] and initialized at 0 (max focus on reconstructing real images).",
      "lambda_k is like the learning rate of the control term, set to 0.001.",
      "Note that gamma d_real - d_fake = 0 <=> gamma d_real = d_fake <=> gamma = d_fake / d_real.",
      "Convergence measure  They measure the convergence of their model using M:  M = d_real + |gamma d_real - d_fake|  M goes down, if d_real goes down (D becomes better at autoencoding real images).",
      "M goes down, if the difference in reconstruction error between real and fake images goes down, i.e. if G becomes better at generating fake images.",
      "Other  They use Adam with learning rate 0.0001.",
      "They decrease it by a factor of 2 whenever M stalls.",
      "Higher initial learning rate could lead to model collapse or visual artifacs.",
      "They generate images of max size 128x128.",
      "They don't use more than 128 filters per conv layer.",
      "Results  NOTES:  Below example images are NOT from generators trained on CelebA.",
      "They used a custom dataset of celebrity images.",
      "They don't show any example images from the dataset.",
      "The generated images look like there is less background around the faces, making the task easier.",
      "Few example images.",
      "Unclear how much cherry picking was involved.",
      "Though the results from the tensorflow example (see like at top) make it look like the examples are representative (aside from speckle-artifacts).",
      "No LSUN Bedrooms examples.",
      "Human faces are comparatively easy to generate.",
      "Example images at 128x128:  Effect of changing the target balance gamma:  High gamma leads to more diversity at lower quality.",
      "Interpolations:  Convergence measure M and associated image quality during the training:"
    ],
    "author_id": "ALEJU",
    "pdf_url": "https://arxiv.org/pdf/1703.10717",
    "author_full_name": "Alexander Jung",
    "source_website": "https://github.com/aleju/papers",
    "id": 17350626
  },
  {
    "blog_id": "brownout-building-more-robust-cloud-applications",
    "summary": [
      "Brownout: building more robust cloud applications \u2013 Klein et al. 2014  How can we design cloud applications to be resilient in the face of varying resources and user load, and always deliver the best possible user experience?",
      "That\u2019s a pretty important question these days, and Klein et al. report on a very interesting new development combining control theory and adaptive application behaviour with impressive results.",
      "Our work borrows from the concept of brownout in electrical grids.",
      "Brownouts are an intentional voltage drop often used to prevent blackouts through load reduction in case of emergency.",
      "In such a situation, incandescent light bulbs dim, hence originating the term.",
      "Applications can saturate \u2013 i.e. become unable to serve users in a timely manner.",
      "Some users may experience high latencies, while others may not receive any service at all.",
      "The authors argue that it is better to downgrade the user experience and continue serving a larger number of clients with reasonable latency.",
      "We define a cloud application as brownout compliant if it can gradually downgrade user experience to avoid saturation.",
      "This is actually very reminiscent of circuit breakers, as described in Nygard\u2019s \u2018Release It!\u2019 and popularized by Netflix.",
      "If you\u2019re already designing with circuit breakers, you\u2019ve probably got all the pieces you need to add brownout support to your application relatively easily.",
      "To lower the maintenance effort, brownouts should be automatically triggered.",
      "This enables cloud applications to rapidly and robustly avoid saturation due to unexpected environmental changes, lowering the burden on human operators.",
      "Of course, the other thing we might do is provide the application with more resources.",
      "Studies later on in the paper look at what happens when brownout controls are applied as resources are added and removed.",
      "The results indicate that brownout control should be able to smooth out the application response and maximise user experience during such transitions.",
      "How does the brownout model work?",
      "Application designers need to identify the parts of the response that may be considered optional (for example, return product information but not recommendations, or showing a post but not comments), and make it possible to activate the optional computations on a per-request basis.",
      "The application needs to export a dynamically changeable runtime parameter called the \u2018dimmer\u2019.",
      "The setting of the dimmer controls the probability that the optional computations will be performed when generating a given response.",
      "A new application component called the controller is added, its goal is to adjust the dimmer as a function of the current performance.",
      "So whereas a circuit breaker is triggered on failure & timeouts, the dimmer switch acts more like a flow control valve determining how many requests get to execute the optional components.",
      "we synthesize a control-theoretical solution to automatically decide when to activate those optional features  I\u2019ve long said that adapting an application to changing demands is a control-theory problem (and implementing a RabbitMQ-based autoscaler for a SpringOne keynote a couple of years ago made that abundantly clear) so it\u2019s great to see this approach being used here.",
      "It\u2019s also why I have a copy of \u2018 Feedback Control \u2018 on my Kindle waiting to be read.",
      "\u2026control theory allows us to provide some formal guarantees on the system.",
      "Our main aim is to close a loop around a cloud application, constraining the application to have a behaviour that is as predictable as possible.",
      "If your knowledge of control theory is better than mine, you might be able to follow along with the derivation of the controller algorithm!",
      "The end result (after a bit of time spent decoding on my part) actually seems pretty straightforward.",
      "It\u2019s a little bit like the wikipedia page on PID Controllers that I had to refer to: scroll past lots of theory and math, till you get to the \u2018pseudocode\u2019 section at the bottom and you\u2019ll see what I mean!",
      "The question everyone wants answered of course is \u2018does it work?\u2019 Experiments suggest a very strong yes.",
      "Tests were performed first with a constant load, and varying resources (e.g. to simulate failure or loss of nodes and subsequent recovery); then with constant resources and varying load (e.g.",
      "to simulate usage spikes); and finally varying both load and resources.",
      "The time-series results show that the self-adaptive application behaves as intended.",
      "The controller adapts the dimmer both to the available capacity and number of users as expected, and keeps the perceived latencies close to the setpoint.",
      "Moreover, the advantages that the brownout paradigm brings to previously non-adaptive applications can clearly be observed from the results.",
      "The paper includes a number of charts that show very significant improvements in the ability to continue serving user requests within the desired latency targets when the system is under stress.",
      "A word of caution though, they\u2019re not the easiest to interpret.",
      "\u2026self-adaptation through brownout can allow applications to support more users or run on fewer resources than their non-adaptive counterparts.",
      "Hence our proposition enables cloud applications to more robustly deal with unexpected peaks or unexpected failures, without requiring spare capacity.",
      "The work in this paper only considers a single server!",
      "There\u2019s an important extension for multiple servers, some much easier to follow charts, and a discussion on the implications for load balancing that we\u2019ll look at next time\u2026"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "http://www.martinamaggio.com/preprints/icse2014-preprint.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 41161677
  },
  {
    "blog_id": "aspects-of-language-captured-by-bert-32bc3c54016f",
    "summary": [
      "Credit: Arxiv  Recent progress in NLP has mostly been due to the effective pretraining of large neural networks such as BERT and XLNet .",
      "While the performances of these models \u2014 in a transfer learning setting \u2014 are impressive, there is very little understanding of why and what aspects of language they learn.",
      "This paper aims to investigate what aspects of language (i.e. linguistic features) BERT learns from unlabeled data.",
      "The main approach is to observe the attention component of the pre-trained model and obtain interesting insights about what type syntactical information the attention heads capture.",
      "The proposed analysis method probes the attention heads at a broader level (analysis of surface-level patterns) and at the individual level (analysis of specific linguistic phenomena).",
      "Broad Attention  For the surface-level pattern analysis, the attention maps of a BERT-based model are extracted when applied to 1000 random Wikipedia segments.",
      "Note that no masking was used as in the original BERT training.",
      "The model has 12 layers containing 12 attention heads each.",
      "This broader analysis shows that BERT\u2019s attention heads pay little attention to the current token but rather specialize to attend heavily on the next or previous token, especially in the earlier layers.",
      "A substantial amount of BERT\u2019 attention focuses on a few special tokens such as the deliminator token [SEP] which means that such tokens play a vital role in BERT\u2019s performance.",
      "The figure below shows the average attention behavior in each layer for some special tokens such as [CLS] and [SEP].",
      "At first, the authors suspected that the [SEP] token is used as an indicator to collect valuable segment-level information which is passed on to other attention heads.",
      "However, the attention heads processing [SEP] almost entirely keep fixated on themselves and the other [SEP] tokens.",
      "Further theoretical analysis shows that attention over [SEP] tokens could serve as a way for the BERT model to control when an attention head\u2019s function is applicable or not (also referred to as a no-op).",
      "By measuring the entropy of each head\u2019s attention distribution it is possible to observe that some of these heads, particularly in the lower layers, display very broad attention.",
      "The authors state that the output of these heads translates to a bag-of-vectors representation of the sentences.",
      "(See figure below)  Interestingly, entropies for all attention heads on the [CLS] token also have similar behavior with the last layer having huge entropy.",
      "This translates to a representation that attends broadly, aggregating a representation used as input for the next sentence prediction.",
      "Individual Attention Heads  The previous analysis focused on broad attention, so this part focuses on probing the individual attention heads to find out what aspects of language they have learned.",
      "Two tasks were used in this analysis: dependency parsing and coreference resolution.",
      "The first analysis aims to evaluate the attention heads on dependency relation prediction.",
      "The authors found that no single attention head does well at syntax overall.",
      "The main finding in this analysis was that in almost all the relations, the dependent attends to the head word, which makes sense because dependents only have one head.",
      "The figure below shows a relation example where the attention head 8\u201311 perform well.",
      "You can see that the determiners expectedly attend to their corresponding noun.",
      "(See more interesting examples in the paper)  The second analysis aims to evaluate the attention heads on the semantic task of coreference resolution.",
      "Keep in mind that coreference is more difficult as links are usually longer than in syntactic dependencies.",
      "The evaluation is done via asking the following: \u201cwhat percent of the time does the head word of a coreferent mention most attend to the head of one of that mention\u2019s antecedents.\u201d  The figure below shows an example where the attention head 5\u20134 achieves satisfactory accuracy on the antecedent selection task.",
      "With a few more creative tests (see paper for full details), the authors found that BERT\u2019s attention maps have a fairly thorough representation of English syntax.",
      "In addition, it was found that BERT\u2019s vector representations do not capture more syntactic information as compared to its attention maps.",
      "Via the proposed tests above, the authors argue that BERT learns some aspects of syntax purely as a by-product of self-supervised training.",
      "Upon further investigation of the individual attention heads behavior for a given layer, the authors found that some heads behave similarly, possible due to some attention weights being zeroed-out via dropout.",
      "A surprising result, given that other researchers found that encouraging different behavior in attention heads improves a Transformer\u2019s performance.",
      "There is more opportunity to conduct extended analysis to help further understand these behaviors in the attention layer.",
      "Overall, probing attention maps offer a feasible technique to understand what neural networks learn about language.",
      "Paper: What Does BERT Look At?",
      "An Analysis of BERT\u2019s Attention \u2014 Kevin Clark, Urvashi Khandelwal, Omer Levy, Christopher D. Manning  Further Readings:  Attention mechanisms  BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding  XLNet outperforms BERT on several NLP Tasks  An Overview of Modern NLP"
    ],
    "author_id": "DAIR-AI",
    "pdf_url": "https://arxiv.org/pdf/1906.04341v1",
    "author_full_name": "Elvis Saravia",
    "source_website": "https://github.com/dair-ai/nlp_paper_summaries",
    "id": 87885377
  },
  {
    "blog_id": "vime",
    "summary": [
      "The authors present a solution to the problem of exploring the state-action space of a continuous control task efficiently.",
      "Their solution stems from prior work on curiousity-driven exploration, which makes use of key ideas taken from information theory.",
      "The general idea is to have the agent select actions that maximize the information gain about the agent\u2019s internal belief of the dynamics of the model.",
      "They use variational inference (VI) to measure information gain and Bayesian neural networks to represent the agent\u2019s belief of the environment\u2019s dynamics.",
      "The algorithm presented in Section 2.5 is referred to as VIME.",
      "VIME  VIME uses VI to compute the posterior probability of the parameters of the environment dynamics.",
      "That is, for the model $ p(s_{t+1}|s_{t}, a_{t}; \\theta) $, parameterized by the random variable $\\Theta$ with values $\\theta \\in \\Theta$, $p(\\theta | \\mathcal{D}) \\approx q(\\theta; \\phi)$.",
      "In this setting, $q(\\theta; \\phi)$ is represented as factorized distribution, as is common in VI.",
      "In particular, they use a Bayesian Neural Network parameterized by a fully factorized Gaussian distrubtion.",
      "Key point: The agent is encouraged to take actions that lead to states that are maximally informative about the dynamics model.",
      "Letting the history of the agent up to time step $t$ be $\\xi_{t} = \\{s_{0}, a_{0}, s_{1}, a_{1}, \u2026, s_{t}\\} $, we can derive the mutual information of the dynamics model before and after taking action $a_{t}$ as:  Using the variational distribution $q$, we can approximate our posterior distribution by minimizing $ D_{KL} [q(\\theta|\\phi) \\hspace{2pt} || \\hspace{2pt} p(\\theta| \\mathcal{D})] $.",
      "This is done through the maximization of the variational lower bound $L[q]$:  A clear derivation of the variational lower bound is available on wikipedia .",
      "Note that in this case, the log evidence term $ \\log p(\\mathcal{D}) $ is computed as an expectation over the model parameters $\\theta$ w.r.t.",
      "the variational distribution $q$.",
      "The variational distribution is then used to compute a \u201cbonus\u201d for the external reward function as follows:  The KL-divergence between the new approximate posterior and the old approximate posterior thereby represents the information gained by having taken $a_{t}$, ended up in state $s_{t+1}$.",
      "The hyperparameter $\\eta$ controls the amount of incentive to explore (the curiosity).",
      "See Section 2.4 of the paper for an analogy drawn to model compression (the intuition is that the most informative state-action sequence up to time $t$ is the one with the shortest description length).",
      "Section 2.5 contains implementation details and an outline of the overall algorithm.",
      "One of the key points of Section 2.5 is that the use of a fully factorized Gaussian allows the KL-divergence in Eq.",
      "3 to be computedly simply.",
      "Broader Impact on the RL Community  The authors conducted experiments with VIME to show that, when augmenting an RL algorithm, there are significant improvements in the face of sparse reward signals.",
      "They specifically tested it with Trust-Region Policy Optimization (TRPO), REINFORCE, and ERWR.",
      "As a baseline, they used these algorithms sans VIME.",
      "They included a neat figure of the state space exploration in MountainCar of TRPO with and without VIME.",
      "With VIME, the exploration is much more diffused, whereas with naive Gaussian exploration it\u2019s condensed and ball-shaped around the origin.",
      "In my opinion, this is one of the major avenues for further research in RL.",
      "We should be focused on developing strategies for learning with sparse or nonexistant reward signals.",
      "This is extremely important for bringing RL into new problem domains.",
      "As can be seen so far, information theory offers a promising starting point.",
      "Similar work by Mohamed and Rezende in 2015 presented an algorithm inspired by intrinsically motivated RL that focused on the notion of empowerment.",
      "This is a broad term that attempts to formalize the notion of having \u201cinternal drives\u201d that agents can experience to learn about their environment in an unsupervised fashion.",
      "They developed a stochastic variational information maximization algorithm.",
      "The formulation as presented in this paper is useful when explicit external reward functions are not available to an agent.",
      "The computed empowerment can be used in a closed-loop planner such as Q-learning; agents can then learn information-maximizing behaviors this way.",
      "The paper contains some cool examples of this.",
      "A major distinction with VIME, however, is that empowerment doesn\u2019t necessarily favor exploration- as stated by Mohamed and Rezende, agents are only \u2018curious\u2019 about parts of its environment that can be reached within its internal planning horizon.",
      "Despite the significance of the recent work in the intersection of VI and intrinsically motivated RL, these methods are non-trivial and hence will most likely catch on slower.",
      "Notes  Bayesian RL and PAC-MDP  Boltzmann exploration requires a training time exponential in the number of states in order to solve an n-chain MDP..!"
    ],
    "author_id": "pemami",
    "pdf_url": "http://arxiv.org/pdf/1605.09674v2.pdf",
    "author_full_name": "Patrick Emami",
    "source_website": "https://pemami4911.github.io/index.html",
    "id": 74193008
  },
  {
    "blog_id": "detecting-sarcasm-with-deep-convolutional-neural-networks-4a0657f79e80",
    "summary": [
      "Overview  This paper addresses a key NLP problem known as sarcasm detection using a combination of models based on convolutional neural networks (CNNs).",
      "Detection of sarcasm is important in other areas such as affective computing and sentiment analysis because such expressions can flip the polarity of a sentence.",
      "Example  Sarcasm can be considered as expressing a bitter gibe or taunt.",
      "Examples include statements such as \u201cIs it time for your medication or mine?\u201d and \u201cI work 40 hours a week to be this poor\u201d.",
      "(Find more fun examples here )  Challenges  To understand and detect sarcasm it is important to understand the facts related to an event.",
      "This allows for detection of contradiction between the objective polarity (usually negative) and the sarcastic characteristics conveyed by the author (usually positive).",
      "Consider the example, \u201cI love the pain of breakup\u201d, it is difficult to extract the knowledge needed to detect if there is sarcasm in this statement.",
      "In the example, \u201cI love the pain\u201d provides knowledge of the sentiment expressed by the author (in this case positive), and \u201cbreakup\u201d describes a contradicting sentiment (that of negative).",
      "Other challenges that exist in understanding sarcastic statements is the reference to multiple events and the need to extract a large amount of facts, commonsense knowledge, anaphora resolution, and logical reasoning.",
      "The authors avoid automatic feature extraction and rely on CNNs to automatically learn features from a sarcasm dataset.",
      "Contributions  Apply deep learning to sarcasm detection  Leverage user profiling, emotion, and sentiment features for sarcasm detection  Apply pre-trained models for automatic feature extraction  Model  Sentiment shifting is prevalent in sarcasm-related communication; thus, the authors propose to first train a sentiment model (based on a CNN) for learning sentiment-specific feature extraction.",
      "The model learns local features in lower layers which are then converted into global features in the higher layers.",
      "The authors observe that sarcastic expressions are user-specific \u2014 some users post more sarcasm than others.",
      "In the proposed framework, personality-based features, sentiment features, and emotion-based features are incorporated into the sarcasm detection framework.",
      "Each set of features are learned by separate models, becoming pre-trained models used to extract sarcasm-related features from a dataset.",
      "CNN Framework  CNNs are effective at modeling hierarchy of local features to learn more global features, which is essential to learn context.",
      "Sentences are represented using word vectors (embeddings) and provided as input.",
      "Google\u2019s word2vec vectors are employed as input.",
      "Non-static representations are used, therefore, parameters for these word vectors are learned during the training phase.",
      "Max pooling is then applied to the feature maps to generate features.",
      "A fully connected layer is applied followed by a softmax layer for outputting the final prediction.",
      "(See diagram of the CNN-based architecture below)  To obtain the other features \u2014 sentiment (S), emotion (E), and personality (P) \u2014 CNN models are pre-trained and used to extract features from the sarcasm datasets.",
      "Different training datasets were used to train each model.",
      "(Refer to paper for more details)  Two classifiers are tested \u2014 a pure CNN classifier (CNN) and CNN-extracted features fed to an SVM classifier (CNN-SVM).",
      "A separate baseline classifier (B) \u2014 consisting of only the CNN model without the incorporation of the other models (e.g., emotion and sentiment) \u2014 is trained as well.",
      "Experiments  Data \u2014 Balanced and imbalanced sarcastic tweets datasets were obtained from ( Ptacek et al., 2014 ) and The Sarcasm Detector .",
      "Usernames, URLs, and hashtags are removed, and the NLTK Twitter Tokenizer was used for tokenization.",
      "(See paper for more details)  The performances of both the CNN and CNN-SVM classifier, when applied to all datasets, are shown in the table below.",
      "We can observe that when the models (specifically CNN-SVM) combine sarcasm features, emotion features, sentiment features, and personality traits features, it outperforms all the other models with the exception of the baseline model (B).",
      "The table below shows comparison results of the the state-of-the-art model (method 1), other well-known sarcasm detection research (method 2), and the proposed model (method 3).",
      "The proposed model consistently outperforms all the other models.",
      "Generalizability capabilities of the models were tested and the main finding was that if the datasets differed in nature, this significantly impacted the results.",
      "(See visualization of the datasets rendered via PCA below).",
      "For instance, training was done on Dataset 1 and tested on Dataset 2; the F1-score of the model was 33.05%, significantly dropping in accuracy.",
      "Conclusion and Future Work  Overall, the authors found that sarcasm is very topic-dependent and highly contextual, therefore, sentiment and other contextual clues help to detect sarcasm from text.",
      "Pre-trained sentiment, emotion, and personality models are used to capture contextualized information from text.",
      "Hand-crafted features (e.g., n-grams), though somewhat useful for sarcasm detection, will produce very sparse feature vector representations.",
      "For those reasons, word embeddings are used as input features.",
      "References  Ref:  [url]"
    ],
    "author_id": "DAIR-AI",
    "pdf_url": "https://arxiv.org/pdf/1610.08815",
    "author_full_name": "Elvis Saravia",
    "source_website": "https://github.com/dair-ai/nlp_paper_summaries",
    "id": 8264189
  },
  {
    "blog_id": "1709.01507",
    "summary": [
      "\"The SE module can learn some nonlinear global interactions already known to be useful, such as spatial normalization.",
      "The channel wise weights make it somewhat more powerful than divisive normalization as it can learn feature-specific inhibitions (ie: if we see a lot of flower parts, the probability of boat features should be diminished).",
      "It also has some similarity to bio inhibitory circuits.\"",
      "By jcannell on reddit  Slides:  [url]"
    ],
    "author_id": "joecohen",
    "pdf_url": "http://arxiv.org/pdf/1709.01507v1",
    "author_full_name": "Joseph Cohen",
    "source_website": "https://www.shortscience.org/user?name=joecohen",
    "id": 81623492
  },
  {
    "blog_id": "cohenl016",
    "summary": [
      "Basically they observe a pattern they call The Filter Lottery (TFL) where the random seed causes a high variance  in the training accuracy:  !",
      "[]( [url]"
    ],
    "author_id": "joecohen",
    "pdf_url": "http://arxiv.org/pdf/1602.05931",
    "author_full_name": "Joseph Cohen",
    "source_website": "https://www.shortscience.org/user?name=joecohen",
    "id": 4741660
  },
  {
    "blog_id": "1605.09410",
    "summary": [
      "This combines the ideas of recurrent attention to perform object detection in an image  [ref]  for multiple objects  [ref]  with semantic segmentation  [ref] .",
      "Segmenting subregions is to avoid a global resolution bias (the object would take up the majority of pixels) and to allow multiple scales of objects to be segmented.",
      "Here is a video that demos the method described in the paper:   [url]"
    ],
    "author_id": "joecohen",
    "pdf_url": "http://arxiv.org/pdf/1605.09410v1",
    "author_full_name": "Joseph Cohen",
    "source_website": "https://www.shortscience.org/user?name=joecohen",
    "id": 60641106
  },
  {
    "blog_id": "1611.07004",
    "summary": [
      "Summary by [brannondorsey]( [url]"
    ],
    "author_id": "joecohen",
    "pdf_url": "http://arxiv.org/pdf/1611.07004v1",
    "author_full_name": "Joseph Cohen",
    "source_website": "https://www.shortscience.org/user?name=joecohen",
    "id": 63993374
  },
  {
    "blog_id": "graphit-a-high-performance-graph-dsl",
    "summary": [
      "GraphIt: a high-performance graph DSL Zhang et al., OOPSLA\u201918  See also:  [url]"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://arxiv.org/pdf/1805.00923.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 13237217
  },
  {
    "blog_id": "1612.09346",
    "summary": [
      "This work deals with rotation equivariant convolutional filters.",
      "The idea is that when you rotate an image you should not need to relearn new filters to deal with this rotation.",
      "First we can look at how convolutions typically handle rotation and how we would expect a rotation invariant solution to perform below:  | | | | - | - | |  [url]"
    ],
    "author_id": "joecohen",
    "pdf_url": "http://arxiv.org/pdf/1612.09346v1",
    "author_full_name": "Joseph Cohen",
    "source_website": "https://www.shortscience.org/user?name=joecohen",
    "id": 58600020
  },
  {
    "blog_id": "kingmab14",
    "summary": [
      "Adam is like RMSProp with momentum.",
      "The (simplified) update [[Stanford CS231n]]( [url]"
    ],
    "author_id": "joecohen",
    "pdf_url": "http://arxiv.org/pdf/1412.6980",
    "author_full_name": "Joseph Cohen",
    "source_website": "https://www.shortscience.org/user?name=joecohen",
    "id": 27264933
  },
  {
    "blog_id": "1702.00071",
    "summary": [
      "Here is a video overview:  [url]"
    ],
    "author_id": "joecohen",
    "pdf_url": "http://arxiv.org/pdf/1702.00071v1",
    "author_full_name": "Joseph Cohen",
    "source_website": "https://www.shortscience.org/user?name=joecohen",
    "id": 70176908
  },
  {
    "blog_id": "reunanen03",
    "summary": [
      "This paper discusses an important bias in evaluation of methods using cross-validation.",
      "A method that makes decisions based of cross validation can appear to increase overall performance by simply dealing with the bias of cross-validation and not the real problem."
    ],
    "author_id": "joecohen",
    "pdf_url": "http://www.jmlr.org/papers/volume3/reunanen03a/reunanen03a.pdf",
    "author_full_name": "Joseph Cohen",
    "source_website": "https://www.shortscience.org/user?name=joecohen",
    "id": 59963771
  },
  {
    "blog_id": "playing-fps-games-with-deep-reinforcement-learning",
    "summary": [
      "Playing FPS games with deep reinforcement learning Lample et al. arXiv preprint, 2016  When I wrote up \u2018 Asynchronous methods for deep learning \u2019 last month, I made a throwaway remark that after Go the next challenge for deep learning systems would be to win an esports competition against the best human teams.",
      "Can you imagine the theatre!",
      "Source: \u2018League of Legends\u2019 video game championship is like the World Cup, Super Bowl combined \u2013 Fortune:  [url]"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://arxiv.org/pdf/1609.05521.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 43514018
  },
  {
    "blog_id": "ioffes15",
    "summary": [
      "A *Batch Normalization* applied immediately after fully connected layers and adjusts the values of the feedforward output so that they are centered to a zero mean and have unit variance.",
      "It has been used by famous Convolutional Neural Networks such as GoogLeNet  [ref]  and ResNet  [ref]"
    ],
    "author_id": "joecohen",
    "pdf_url": "http://proceedings.mlr.press/v37/ioffe15.pdf",
    "author_full_name": "Joseph Cohen",
    "source_website": "https://www.shortscience.org/user?name=joecohen",
    "id": 1050101
  },
  {
    "blog_id": "jagannathay16",
    "summary": [
      "The basic approach is an RNN applied to text to predict a medical event such as an ICD code.",
      "It is unclear if the complicated Bi-RNN model is required.",
      "This has some useful applications such as  - Adapt old databases - Correct errors - Upgrade ICD versions  A simple diagram of an RNN applied to medical next is shown below:    [url]"
    ],
    "author_id": "joecohen",
    "pdf_url": "http://aclweb.org/anthology/N/N16/N16-1056.pdf",
    "author_full_name": "Joseph Cohen",
    "source_website": "https://www.shortscience.org/user?name=joecohen",
    "id": 43813190
  },
  {
    "blog_id": "1811.11347",
    "summary": [
      "The paper looks at approaches to predicting individual survival time distributions (isd).",
      "The motivation is shown in the figure below.",
      "Between two patients the survival time varies greatly so we should be able to predict a distribution like the red curve.",
      "[url]"
    ],
    "author_id": "joecohen",
    "pdf_url": "http://arxiv.org/pdf/1811.11347v1",
    "author_full_name": "Joseph Cohen",
    "source_website": "https://www.shortscience.org/user?name=joecohen",
    "id": 53851426
  },
  {
    "blog_id": "zhaose17",
    "summary": [
      "A Critical Paper Review by Alex Lamb:   [url]"
    ],
    "author_id": "joecohen",
    "pdf_url": "http://arxiv.org/pdf/1702.08396",
    "author_full_name": "Joseph Cohen",
    "source_website": "https://www.shortscience.org/user?name=joecohen",
    "id": 93106503
  },
  {
    "blog_id": "szegedyljsraevr14",
    "summary": [
      "This paper introduces the GoogLeNet Inception Architecture The major part of this paper is the *Inception Module* which takes convolutions at multiple layers and provides a good receptive field as well as reducing the overall number of parameters.",
      "!",
      "[Inception Module]( [url]"
    ],
    "author_id": "joecohen",
    "pdf_url": "http://arxiv.org/pdf/1409.4842",
    "author_full_name": "Joseph Cohen",
    "source_website": "https://www.shortscience.org/user?name=joecohen",
    "id": 74971632
  },
  {
    "blog_id": "mathe_2016_cvpr",
    "summary": [
      "!",
      "[]( [url]"
    ],
    "author_id": "joecohen",
    "pdf_url": "http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Mathe_Reinforcement_Learning_for_CVPR_2016_paper.pdf",
    "author_full_name": "Joseph Cohen",
    "source_website": "https://www.shortscience.org/user?name=joecohen",
    "id": 76854692
  },
  {
    "blog_id": "1605.08803",
    "summary": [
      "This paper presents a novel neural network approach (though see [here]( [url]"
    ],
    "author_id": "hlarochelle",
    "pdf_url": "http://arxiv.org/pdf/1605.08803v1",
    "author_full_name": "Hugo Larochelle",
    "source_website": "https://www.shortscience.org/user?name=hlarochelle",
    "id": 65847395
  },
  {
    "blog_id": "1802.00400",
    "summary": [
      "This paper demonstrates that Word2Vec  [ref]  can extract relationships between words and produce latent representations useful for medical data.",
      "They explore this model on different datasets which yield different relationships between words.",
      "[url]"
    ],
    "author_id": "joecohen",
    "pdf_url": "http://arxiv.org/pdf/1802.00400v2",
    "author_full_name": "Joseph Cohen",
    "source_website": "https://www.shortscience.org/user?name=joecohen",
    "id": 18000122
  },
  {
    "blog_id": "cohen13a",
    "summary": [
      "This paper proposes a method to send messages between cell phones over Bluetooth by using the device name field.",
      "This allows devices to communicate directly with each other without pairing."
    ],
    "author_id": "joecohen",
    "pdf_url": "http://arxiv.org/pdf/1307.7814",
    "author_full_name": "Joseph Cohen",
    "source_website": "https://www.shortscience.org/user?name=joecohen",
    "id": 28171009
  },
  {
    "blog_id": "guyone03",
    "summary": [
      "\"The objective of variable selection is three-fold: improving the prediction performance of the predictors, providing faster and more cost-effective predictors, and providing a better understanding of the underlying process that generated the data.\""
    ],
    "author_id": "joecohen",
    "pdf_url": "http://www.jmlr.org/papers/volume3/guyon03a/guyon03a.pdf",
    "author_full_name": "Joseph Cohen",
    "source_website": "https://www.shortscience.org/user?name=joecohen",
    "id": 70719243
  },
  {
    "blog_id": "generalized-data-structure-synthesis",
    "summary": [
      "Generalized data structure synthesis Loncaric et al., ICSE\u201918  Many systems have a few key data structures at their heart.",
      "Finding correct and efficient implementations for these data structures is not always easy.",
      "Today\u2019s paper introduces Cozy (  [url]"
    ],
    "author_id": "ACOLYER",
    "pdf_url": "https://homes.cs.washington.edu/~mernst/pubs/generalized-synthesis-icse2018.pdf",
    "author_full_name": "Adrian Colyer",
    "source_website": "https://blog.acolyer.org/about/",
    "id": 81034618
  },
  {
    "blog_id": "1610.06258",
    "summary": [
      "This paper presents a recurrent neural network architecture in which some of the recurrent weights dynamically change during the forward pass, using a hebbian-like rule.",
      "They correspond to the matrices $A(t)$ in the figure below:  !",
      "[Fast weights RNN figure]( [url]"
    ],
    "author_id": "hlarochelle",
    "pdf_url": "http://arxiv.org/pdf/1610.06258v1",
    "author_full_name": "Hugo Larochelle",
    "source_website": "https://www.shortscience.org/user?name=hlarochelle",
    "id": 72992946
  },
  {
    "blog_id": "cohenl14",
    "summary": [
      "Academic Torrents is a BitTorrent service that aims to make it easy for academics to share data via BitTorrent.",
      "Specific use cases are during competitions where everyone needs access to data quickly.",
      "Also, when a dataset is not available anymore the data can be shared from simple desktop computers and become available globally."
    ],
    "author_id": "joecohen",
    "pdf_url": "https://dl.acm.org/doi/pdf/abs/10.1145/2616498.2616528.pdf",
    "author_full_name": "Joseph Cohen",
    "source_website": "https://www.shortscience.org/user?name=joecohen",
    "id": 1077676
  },
  {
    "blog_id": "cohendb15",
    "summary": [
      "This idea is so badass!",
      "It uses Simple Tree Matching  [ref]  and extends it to work with HTML and then recursively searches an unseen document to align it with previously seen examples.",
      "An overview of the problem of *shift* can be seen on the left of the figure below and  the alignment is shown on the right.",
      "[url]"
    ],
    "author_id": "joecohen",
    "pdf_url": "http://arxiv.org/pdf/1505.01303",
    "author_full_name": "Joseph Cohen",
    "source_website": "https://www.shortscience.org/user?name=joecohen",
    "id": 95858167
  },
  {
    "blog_id": "hezrs15",
    "summary": [
      "This summary is as ridiculous as this network is long.",
      "A good implementation of the network is here:  [url]"
    ],
    "author_id": "joecohen",
    "pdf_url": "http://arxiv.org/pdf/1512.03385",
    "author_full_name": "Joseph Cohen",
    "source_website": "https://www.shortscience.org/user?name=joecohen",
    "id": 74977893
  },
  {
    "blog_id": "duchihs10",
    "summary": [
      "This is Adagrad.",
      "Adagrad is an adaptive learning rate method.",
      "Some sample code from  [[Stanford CS231n]]( [url]"
    ],
    "author_id": "joecohen",
    "pdf_url": "http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf",
    "author_full_name": "Joseph Cohen",
    "source_website": "https://www.shortscience.org/user?name=joecohen",
    "id": 79091650
  },
  {
    "blog_id": "bahdanaucb14",
    "summary": [
      "One core aspect of this attention approach is that it provides the ability to debug the learned representation by visualizing the softmax output (later called $\\alpha_{ij}$) over the input words for each output word as shown below.",
      "[url]"
    ],
    "author_id": "joecohen",
    "pdf_url": "http://arxiv.org/pdf/1409.0473",
    "author_full_name": "Joseph Cohen",
    "source_website": "https://www.shortscience.org/user?name=joecohen",
    "id": 24431610
  },
  {
    "blog_id": "1602.05568",
    "summary": [
      "This model called Med2Vec is inspired by Word2Vec.",
      "It is Word2Vec for time series patient visits with ICD codes.",
      "The model learns embeddings for medical codes as well as the demographics of patients.",
      "[url]"
    ],
    "author_id": "joecohen",
    "pdf_url": "http://arxiv.org/pdf/1602.05568v1",
    "author_full_name": "Joseph Cohen",
    "source_website": "https://www.shortscience.org/user?name=joecohen",
    "id": 73705154
  }
]