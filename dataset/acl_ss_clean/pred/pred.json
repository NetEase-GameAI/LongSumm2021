[{"document": "in this article i describe a research agenda for securing machine learning models against adversarial inputs at test time. this article does not present results but instead shares some of my thoughts about where i think that the field needs to go. modern machine learning works very well on i.i.d. data: data for which each example is drawn independently and for which the distribution generating each example is identical. when these assumptions are relaxed, modern machine learning can perform very poorly. when machine learning is used in contexts where security is a concern, it is desirable to design models that perform well even when the input is designed by a malicious adversary. so far most research in this direction has focused on an adversary who violates the identical assumption, and imposes some kind of restricted worstcase distribution shift. i argue that machine learning security researchers should also address the problem of relaxing the independence assumption and that current strategies designed for robustness to distribution shift will not do so. i recommend dynamic models that change each time they are run as a potential solution path to this problem, and show an example of a simple attack using correlated data that can be mitigated by a simple dynamic defense. this is not intended as a realworld security measure, but as a recommendation to explore this research direction and develop more realistic defenses. notation and definitions x a train or test example or a sample from the model y the true label for an example the parameter vector of the model mtrain number of training examples mtest number of test examples k the number of classes r the error rate of the classifier on the naturally occurring test set r the error rate under the test set attack pmodel(x; ) the probability distribution over output classes learned by the model x an adversarial example f(x,) a transformation of x controlled transformation parameters s the set of allowable values of l(x, y) the loss incurred by the model while classifying input x with label y. the constraint on adversarial perturbation size machine learning is now a working technology and produces predictions that are correct most of the time for many different tasks (taigman et al., 2014; he et al., 2015; wu et al., 2016; amodei et al., 2016). in general, these tasks use naturally occurring data as opposed to data produced by an. adversary who intentionally tries to fool the model. when an adversary tries to fool the model, the adversary generally succeeds (see carlini et al. (2019) for a recent evaluation guidelines explaining how to implement attacks that almost always suceed). the adversary can not only cause the model to make an incorrect prediction (an untargeted attack, e.g. fail to recognize that a photo of a dog is dog) but can also cause the model to make a specific mistake (a targeted attack, e.g. cause the model to think the photo of a dog is specifically an airplane, or some other class chosen by the attacker in advance).. in general, the reason that machine learning performs so much worse under attack is that modern machine learning mostly relies on the i.i.d. we can think of most machine learning algorithms as using a training set to learn the model with the goal of maximizing performance on a test set of examples. assumptions, the train and test examples are all generated independently from an identical distribution. that is, each example is drawn independently from some distribution pdata that remains the same throughout the entire train and test generation process.. when a machine learning system is used in a setting where security is a concern, the i.i.d. assumptions are usually no longer valid. in this article i will focus on testtime attacks against the input of the model. in this setting, the attacker supplies some or all of the test examples. the attacker can draw test samples from a new distribution padv rather than from pdata. this makes the challenge for the model much greater: instead of statistically generalizing from training examples to new examples from the same distribution, the model must also generalize to a different distribution. in most work on adversarial examples so far, the test examples are still generated indepedently from one another, and padv remains similar to pdata, in the sense that a sample from padv is generated by generating a sample from pdata and then modifying it slightly. in this article, i argue that machine learning security researchers should additionally study the setting where test set examples are not independent. one attack strategy in this setting is to use early examples sampled randomly from some simple distribution (such as pdata) until a mistake is found, and then late examples are all copies of a known mistaken point. for modern models, these attacks can drive the error rate after the first mistake is found to modern machine learning models typically have two distinct phases of existence: first, they go through a training stage, in which the parameters are adapted to fit then training set, then the parameters are frozen and the model is deployed to the test / production / inference / serving stage where the same parameters are used to make predictions on new data indefinitely. this split between the training stage and inference stage has been useful because training updates are not very reliable and can often ruin the model. currently it is possible to rigorously evaluate a model and determine that its predictions are reliable, then deploy the model to run in inference mode, but it is not yet feasible to verify ahead of time that a particular training algorithm will produce reliable updates indefinitely. unfortunately, i believe that this separation of dynamic training and static inference must end in order to defend against correlated testtime attacks. to avoid the attacker, i believe that the model must become a moving target that continually changes, even after it has been deployed.. attacks that violate the identical assumption. most adversarial examples attacks so far work by violating the identical assumption. models are designed assuming that all the test data is drawn from pdata (i.e., all the train and test data is drawn from an identical distribution) but instead the adversary supplies examples sampled from padv. on top of this, most work on adversarial examples constrains padv to be highly similar to pdata, in the sense that samples v from padv are generated by slightly modifying samples x from pdata. a common attack strategy is to modify a sample x from pdata by replacing it with f(x,), where f is some transformation of x that preserves semantics when is constrained to some set s of allowable values. a common choice is f(x,) x for for some small value of within these similarity constraints, adversarial examples are often chosen to maximize the loss l(x, y) incurred by the model. the generation process is thus:. the maximization operation is generally approximated with some kind of reasonably cheap optimization algorithm and is often stochastic (goodfellow et al., 2014b; kurakin et al., 2016;. wardefarley goodfellow, 2016; madry et al., 2017). altogether this generation process implicitly defines a distribution padv.. defenses against adversarial examples are often evaluated in terms of their error rate across an adversarially perturbed version of the naturally occurring test set. this metric was introduced by (goodfellow et al., 2014b) and is still recommended, e.g. this metric measures expected error in terms of starting points sampled independently from pdata and worst case error in terms of the transformation parameters applied at each starting point. the metric is thus not truly worst case, because the choice of starting point is treated as random and naturally occuring rather than adversarial.. attacks that violate the independent assumption. in threat models where the attacker could realistically choose the starting point, the true worst case metric.. this setting has previously been described by a research agenda talk (goodfellow, 2018), a review paper (gilmer et al., 2018), and is the basis for a contest (brown et al., 2018). compared to goodfellow (2018), this article focuses much more on the problem of nonindependent test time inputs. (2018) (sec 4.8) describe correlated input attacks but do not advocate researching defenses against them but instead advocate reducing the error rate on naturally occurring test sets or reducing the total volume of errors made by the model. this article argues that such approaches do not provide a defense against a correlated input attacker and that defense mechanisms other than reduced error rate are necessary. (2018) introduce a contest whose eventual winner will most likely need to develop defenses against correlated input attacks, but the report introducing the contest does not include suggestions for possible solutions. this article focuses primarily on speculation about future directions that may lead to a solution. security research should generally include a threat model: what are the attackers goals and capabilities? in this case, we should consider which kinds of attackers are capable of mounting a test time input attack.. at the same time, much of the point of machine learning research is that machine learning has the potential to be fairly general, and applied to many application areas. in this article, i don\u2019t attempt to exhaustively threat model all application areas, but just give some brief examples to show how generic machine learning principles interact with threat modeling in specific application areas.. consider one potential attack vector: evading facial recognition by wearing glasses with patterns on the frames designed to fool the face recognition system (sharif et al., 2016). suppose that when a person physically approaches the entrance to the facility, a guard photographs them and the face recognition system returns an estimate of that person\u2019s specific identity. suppose the attacker wishes to gain access to a secure facility controlled by a whitelist of people who have access.. in this scenario, we may conclude that it is very easy or very difficult for an attacker to mount a correlated input attack depending on a few different factors. first, if we believe that the attacker must probe the system for errors by physically arriving at the facility and attempting to enter wearing different glasses, it seems difficult to mount a correlated input attack. the attacker may be arrested by the guard if the face recognition system correctly rejects them as not belonging to the whitelist. an attacker who arrives repeatedly wearing different glasses each time may be even more likely to be arrested. this means that there is little opportunity to probe the system for errors and also a high cost to mounting failed attacks. under such a scenario, we may actually be interested in the error rate on adversarial modifications of naturally occurring data (i.e., we may be interested in the percentage of people who can appear to be on the whitelist by wearing adversarial glasses). if this error rate is low, attackers will be discouraged from mounting exploratory attacks. if, on the other hand, we believe that it is easy to transfer adversarial examples from other models, then attackers may be able to build their own model or ensemble of models (szegedy et al., 2013; liu et al., 2016),. and find a person for whom some set of glasses will reliably get them into the facility. or, if the face recognition system used by the facility is commercially available, the attackers could buy their own copy, test it for vulnerabilities offline, and then mount a live attack after finding a reliable vulnerability. in either of these cases, the attacker now has a high ability to mount a correlated input attack. however, we may still find some use in studying the error rate on adversarial modifications of naturally sampled data. this error rate is essentially the percentasge of the general population who can be used to mount a reliable attack, given the correct glasses. if the error rate of the system is small enough, it may be difficult for the attacking organization to find and recruit an individual who has both the right face and the skills to carry out the adversarial mission. this is overall merely a mitigating factor and not a complete defense though. we have seen that in this example, the ability of the attacker to perform offline screening determines whether defenses that reduce the error rate on adversarial perturbations of naturally occurring data are of some limited uses (when the attacker can perform screening) or are of relatively high value as a deterrent (when the attacker must perform exploratory attacks live, and faces a high risk of arrest if the error rate is low).. as another example, consider a detector of synthetic media (deepfakes as discussed by (chesney citron, 2018)). a game theoretic analysis of the fakevsfake detectors competition (goodfellow et al., 2014a) suggests that, given enough computation and enough data, the nash equilibrium is for the fakes to come from the same distribution as real data, forcing the fake detector to perform no better than chance. this problem can be avoided in the short term while the fakes and fake detectors are in an arms race approaching the nash equilibrium. the long run is beyond the scope of this article (fake detectors could perform better if given access to signals other than the content of the media itself, tools such as cryptographic signing of real media may be more useful than fake detectors, etc.). as an exercise in threat modeling, consider how to evaluate a short term fake detector. even a fake detector that performs well on randomly sampled data (accuracy of on a collection of real images and a collection of fake images from some generative model) could perform quite poorly in practice. the attacker only needs to find one fake image that bypasses the detector, and then this image can be deployed widely. in fact, with the fake detector\u2019s stamp of approval, the image would be even more credible than if no fake detector existed. in this scenario, correlated data attacks are much more likely to be feasible because the attacker can upload multiple candidate fake images from multiple anonymous accounts and observe which are flagged as fake with impunity. moreover, it is not particularly important for the attacker to be able to choose exactly which image results in a mistake. if the motive of the attacker is to cause political damage to a particular political cause, the set of images damaging to that cause is often quite large. it is very different from the previous face recognition example, where it must be possible to cause a mistake using the face of one of the infiltrators as the starting point for the attack. in this hypothetical scenario, it is particularly important to build models that are robust to correlated data attacks. most current defenses are intended to mitigate problems caused by adversarial distribution shift, but not problems caused by adversarial correlation of testtime examples.. most strategies for mitigating distribution shift can still be highly exploited by an adversary who can impose correlation of test time examples.. one popular family of strategies for mitigating distribution shift is adversarial training (szegedy et al., 2013; goodfellow et al., 2014b; madry et al., 2017). current state of the art adversarially trained models are all still deterministic, so if they have nonzero error rate on naturally occurring data, an attacker can find a single mistake and then repeat it in order to obtain essentially a attack success on correlated attack data. while adversarial training has thus been state of the art on many expectimax research benchmarks (expectation over i.i.d. test examples, max over error induced by adversarial perturbations), it is not useful for resisting true worst case attacks that occur in practice. a similar criticism applies to all other current defense techniques designed to find a fixed decision boundary that reduces an expectimax metric, including certified defenses (e.g. wong kolter (2017); raghunathan et al. this is not to criticize adversarial training or certified defenses against perturbations, because they are useful for making algorithms that perform well despite a change in the datagenerating distribu. the eventual solution will need to address both changes in the datagenerating distribution and relaxation of the assumption that testtime examples are generated independently. as a brief example, let\u2019s walk through a few hypothetical attacks that use correlated data, and a few hypothetical defenses. all of the attacks will be variations of the test set attack described by gilmer et al. the defenses will be variations on three themesimproved supervised learning performance, generic classifiers using a fixed distribution, and test set memorization. all of these are hello world examples of very weak attacks and very weak defenses. my intention in writing this article is to encourage others to develop better defenses in this space (and insofar as better attacks are necessary to study better defenses, also to develop better attacks).. the test set attack basically consists of presenting examples from the test set i.i.d. to the classifier until the classifier makes a mistake. that mistake is then repeated indefinitely. there are obviously many ways to formalize this in practice, allowing us to pay attention to factors like ratelimiting access to the classifier, the classifier accepting batches of data vs individual examples, how the classifier performs on data other than the data provided by a particular adversary, how long the classifier will stay live before being replaced with a different one, etc. here i will focus on a few particularly simple cases.. suppose that the adversary is given a finite number of opportunities to attack the classifier, and thus essentially just presents a test set containing mtest examples.. consider an untargeted attack in which the goal of the adversary is simply to cause misclassification. if the classifier has error rate r, then the expected number of trials to find a mistake is r remaining test examples, the attacker simply repeats this mistake. for example, on cifar10, an attacker allowed to present the same number of examples as in the cifar10 test set (10,000), when attacking a classifier with r .02, would need on average examples to find a mistake, and then would cause mistakes on the remaining examples, for a total error rate of this is thus a fairly strong attack against an undefended model. the attack could be made stronger by using black box adversarial examples to increase r beyond the rate on naturally occurring data.. how do the defenses fare?. adversarial training, etc: existing adversarial robustness approaches such as adversarial training generally do not reduce r on naturally occurring data, so they do not help at all. in fact, many of them increase r and would slightly hurt performance.. better supervised learning: gilmer et al. (2018) suggest focusing on traditional supervised learning metrics such as the error rate on naturally occurring data, and also suggest reducing the total volume of input space that results in an error. overall the error rate under the test set attack obtained by a classifier with error rate r on naturally occurring data is. r. mtest for any r greater than zero, this error rate approaches as mtest approaches infinity. in other words, if the classifier is deployed for long enough, an attacker can exploit it arbitrarily badly. or if we consider a relatively short finite deployment, for our hypothetical cifar10 classifier to achieve an r of .05, it would need to reach an r of roughly on cifar10 i do not know of a good way to estimate the bayes error rate, but it seems reasonable to believe that for many tasks the bayes error rate is well above 104, so for these tasks no deterministic classifier could be reasonably secure.. stochastic models if a deterministic model performs badly, what about a stochastic model? stochastic models have been proposed as defenses against perturbationbased expectimax adversarial examples (feinman et al., 2017). while the specific defenses proposed so far are known to be mostly broken in most cases (carlini wagner, 2017) we can still think about how well stochasticity in general can perform as a defense against the test set attack. suppose that rather than learning a deterministic classification function mapping example x to class y, the model uses a fixed classification distribution, pmodel(y x). these models are still quite vulnerable. suppose the attacker manages to find a point x such that, when sampling class estimates from the model, the model has. then, asymptotically as mtest approaches infinity, the model under attack using x has error rate rx. if there exists even a single point where the true class is not argmaxy pmodel(y x), then rx overall, the stochasticity can make the attack more complicated: the attacker should not just repeat the first x that results in a mistake because the mistake may not be repeatable, the attacker has to maintain an estimate of rx for each x the attacker has tried, and the attacker faces an explorationvsexploitation tradeoff (if the attacker has found a x with rx 0.5, should they repeat it indefinitely, or search for another input with an even higher error rate?). in other words, the attacker must upgrade from the simplest version of the test set attack to some kind of ratetracking, rateoptimizing test set attack. asymptotically though, the stochastic defense can bring the error rate down only as low as 50, unless the baseline r has literally reached abstention: suppose that a model refuses to classify some examples, either determinstically, or by sometimes sampling an abstain class from a fixed pmodel(y x). (depending on the application / threat model, it may or may not be feasible to allow the model to abstain) this does not change the above analysis very much. the model will presumably abstain on many mistakes and thus reduce r, if we define r to be the rate at which the model makes an incorrect prediction rather than abstaining or making a correct prediction. however as long as we do not reduce r to literally the above analysis still applies.. dynamic modeling: i argue that the problem is using a fixed model pmodel(y x), regardless of whether this model is deterministic, makes use of probabilities, or is able to abstain on some inputs. instead, i believe that the model must become a moving target that cannot be repeatedly exploited using the same input. the concept of a moving target defense has precedent in traditional computer security (?). as a hello world example showing that improvements are possible using a dynamic strategy, consider the memorization defense: the classifier memorizes all previously seen x inputs and abstains every time a repeated x is presented. the test set attack thus obtains r r, depending on whether the attacker runs unique test examples trying to explore for a new vulnerability, or whether the attacker runs multiple memorized points that are abstained on. note that the abstention rate on adversarial inputs approaches asymptotically if the attacker prioritizes exploitation over exploitation. in many domains, a high abstention rate on adversarial data is acceptable (e.g., a spam detector that abstains in the presence of an adversary can be acceptable if messages resulting in abstention are treated as spam). the main downside of this approach is that it can have high abstention rates on nonadversarial data. test benchmark datasets containing high dimensional data, the abstention rate will usually be zero, but in realworld applications where many users send the same query (e.g., one legitimate email is sent to two users, who then each query the server to find out whether it is spam) the abstention rate of this approach could be significant. the test set attack and memorization defense could also clearly be extended in an arms race, with the attack being extended to add noise or other minor variations to avoid the detection of duplicates, and the defense being extended to reject approximate matches rather than exact matches. the purpose of this article is not to analyze how that arms race would play out, but just to show that there exists a simple attack for which there exists a dynamic defense better than all fixed defenses.. finally, let\u2019s consider what happens with targeted, rather than untargeted attacks. for a targeted attack, the attacker wishes to hit a specific target class, rather than merely causing any mistake. for most of the analysis above, this just means we use a different value of r. when calculating r, we now need to measure the rate at which inputs are misclassified specifically as the attacker\u2019s target class, not just the rate at which they are misclassified. one rough way to model what happens in this case is just to divide our earlier value for r by k (the number of classes). in domains where k is very large (e.g. speech recognition, where k is combinatorially large because the model outputs a complete sentence containing many words / characters) the test set attack is no longer very effective even against undefended models. in domains where k is medium (e.g. cifar10, where k is 10, or imagenet, where k is 1000) it is still feasible to find targeted attacks within standard benchmark test sets. the analysis of a stochastic model pmodel(y x) is also different in the targeted case, because the existence of a point x that is argmaxclassified as the target class does not imply an attack success rate of anymore, but only an attack success rate of k overall, this analysis suggests that. for targeted attacks, our hello world test set attack is not nearly as interesting as in the untargeted setting. finally, for the memorization defense, one interesting observation is that the model does not need to be able to abstain in order to reduce the attack success rate. rather than abstaining on memorized examples, the model can return a class uniformly at random, reducing the error rate to k the memorization defense without abstention thus reduces r of an arbitrary stochastic. or deterministic model to match the r of the best possible stochastic model for a fixed argmaxclassification r.. so far the defenses here have been discussed primarily in a black box setting, where the attacker is able to send inputs and observe outputs, but does not have a full specification of the model. in the white box setting, the attacker could perform exploratory screening on their own copy of the model. when mounting the real attack, they could thus obtain the asymptotic error rate immediately, rather than ramping up to it after an initial exploratory phase. for deterministic or fixed stochastic models, this asymptotic error rate can be obtained using a single adversarial example. for a dynamic model using the memorization defense, a white box threat model is enough to break the defense and obtain worse results than the asymptotic black box setting. during the offline screening phase, if the attacker is able to find mtest unique mistaken points, then during the online attack phase the attacker can present each of these points to obtain an error rate of in the future, i hope that a more sophisticated dynamic defense might be able to succeeded in a white fading to black box case where the attacker has a full description of the model at some point in time, but the model\u2019s dynamic updates are unpredictable enough that the attacker rapidly loses knowledge of the model as the model is updated.. both the variations on the test set attack presented here and the memorization defense presented here are intended as hello world examples to show that the test set attack, though trivial, can seriously compromise existing defenses, and that the memorization defense, though impractical for realworld use, can reach levels of robustness not reachable by a fixed model. i hope that these hello world examples will pave the way for more practical defenses against correlated data attacks in the future. while i am fairly confident that dynamic model behavior will be a necessary component of an eventual successful defense strategy, i doubt that dynamic model behavior alone will be sufficient. i think it is most likely that the performance of a model under attack is something like an and function: many factors need to be handled correctly for the defense to work, and the performance under attack will be near zero if any one of those factors is absent. thus, introducing a single component, such as dynamic model behavior, may result in no measurable benefit until some other required synergistic component is introduced.. i speculate that the following may also be important mechanisms to combine with dynamic model behavior. in the hello world memorization defense above, i incorporated an abstention mechanism in order to achieve a good defense in the untargeted setting. this was not necessary in the targeted setting.. i suspect that in many practical settings, abstention will be an important mechanism to include in dynamic defenses. in the hello world defense, the model abstained on memorized examples that were previously presented. i suspect that other defenses will need to abstain using other criteria.. suppose that we make a dynamic model that constantly moves its decision boundary, so that attackers cannot predict exactly what it will do. its decision boundary needs to remain somewhat near the true decision boundary or it will not perform well on naturally occurring data. for points near the decision boundary, the dynamic model is thus forced to behave more deterministically. one solution is to abstain on such points. this is just one example of an abstention criterion different from the memorization criterion. other researchers are also exploring abstention mechanisms in other contexts (carlini wagner, 2017; ? in general, i think abstention using a variety of criteria may be necessary to combine with dynamic modeling. another strong candidate is active learning. when a model detects potentially troubling activity at test time it could request ground truth labels for some of these examples. memorization defense is that, while it can detect repeated examples as potentially troubling activity, it does not have a mechanism for determining the correct way to label them. for many applications, legitimate users do not need to run many queries. users mounting the test set attack or similar black box attacks need to run relatively large numbers of queries (e.g. dynamic models that change during their deployment phase may have some limit on how quickly they can change without compromising performance on naturally occurring data. limiting access to the model, for example, ratelimiting the number of queries a user can make, may be an important mechanism to make sure that attackers cannot find vulnerabilities as quickly as dynamic updates to the model remove them. based on the above line of reasoning, i believe it is important for more of the machine learning research community to study defenses against attackers who use correlated attacks at test time, such as finding a single mistake and repeatedly exploiting it. i do not have a complete solution in mind, but i believe that any fixed model, even a fixed model that incorporates stochasticity, will always be a sitting duck that can be reliably broken as soon the attacker has found a weak point. because of this, i believe it will be necessary to develop dynamic models that change their decision function continually during deployment. i suspect it will also be necessary to combine such behavior with other defense mechanisms, such as confidence thresholding, active learning, and limiting access to the model. i call on the machine learning research community to help make such effective defenses a reality. many thanks to nicolas papernot for helpful discussion of drafts of this article.", "summary": "goodfellow motivates the use of dynamical models as defense against adversarial attacks that violate both the identical and independent assumptions in machine learning. specifically, he argues that machine learning is mostly based on the assumption that the data is samples identically and independently from a data distribution. evasion attacks, meaning adversarial examples, mainly violate the assumption that they come from the same distribution. adversarial examples computed within an \\epsilonball around test examples basically correspond to an adversarial distribution the is larger (but entails) the original data distribution. in this article, goodfellow argues that we should also consider attacks violating the independence assumption. this means, as a simple example, that the attacker can also use the same attack over and over again. this yields the idea of correlated attacks as mentioned in the paper\u2019s title. against this more general threat model, goodfellow argues that dynamic models are required; meaning the model needs to change (or evolve) be a moving target that is harder to attack.. also find this summary at [davidstutz.de]([url]/)."}, {"document": "we introduce imaginationaugmented agents (i2as), a novel architecture for deep reinforcement learning combining modelfree and modelbased aspects. in contrast to most existing modelbased reinforcement learning and planning methods, which prescribe how a model should be used to arrive at a policy, i2as learn to interpret predictions from a learned environment model to construct implicit plans in arbitrary ways, by using the predictions as additional context in deep policy networks. i2as show improved data efficiency, performance, and robustness to model misspecification compared to several baselines.a hallmark of an intelligent agent is its ability to rapidly adapt to new circumstances and achieve goals in a wide range of environments [1]. progress has been made in developing capable agents for numerous domains using deep neural networks in conjunction with modelfree reinforcement learning (rl) [2][3][4], where raw observations directly map to values or actions. however, this approach usually requires large amounts of training data and the resulting policies do not readily generalize to novel tasks in the same environment, as it lacks the behavioral flexibility constitutive of general intelligence. modelbased rl aims to address these shortcomings by endowing agents with a model of the world, synthesized from past experience. by using an internal model to reason about the future, here also referred to as imagining, the agent can seek positive outcomes while avoiding the adverse consequences of trialanderror in the real environment including making irreversible, poor decisions. even if the model needs to be learned first, it can enable better generalization across states, remain valid across tasks in the same environment, and exploit additional unsupervised learning signals, thus ultimately leading to greater data efficiency. another appeal of modelbased methods is their ability to scale performance with more computation by increasing the amount of internal simulation. the neural basis for imagination, modelbased reasoning and decision making has generated a lot of interest in neuroscience [5][6][7]; at the cognitive level, model learning and mental simulation have been hypothesized and demonstrated in animal and human learning [8][9][10][11]. its successful deployment in artificial modelbased agents however has hitherto been limited to settings where an exact transition model is available [12] or in domains where models are easy to learn e.g. symbolic environments or lowdimensional systems [13][14][15][16]. in complex domains for which a simulator is not available to the agent, recent successes are dominated by modelfree methods [2,17]. in such domains, the performance of modelbased agents employing standard planning methods usually suffers from model errors resulting from function approximation [18,19]. these errors compound during planning, causing overoptimism and poor agent performance. there are currently no planning or modelbased methods that are robust against model imperfections which are inevitable in complex domains, thereby preventing them from matching the success of their modelfree counterparts. we seek to address this shortcoming by proposing imaginationaugmented agents, which use approximate environment models by learning to interpret their imperfect predictions. our algorithm can be trained directly on lowlevel observations with little domain knowledge, similarly to recent modelfree successes. without making any assumptions about the structure of the environment model and its possible imperfections, our approach learns in an endtoend way to extract useful knowledge gathered from model simulations in particular not relying exclusively on simulated returns. this allows the agent to benefit from modelbased imagination without the pitfalls of conventional modelbased planning. we demonstrate that our approach performs better than modelfree baselines in various domains including sokoban. it achieves better performance with less data, even with imperfect models, a significant step towards delivering the promises of modelbased rl. the i2a architecture in order to augment modelfree agents with imagination, we rely on environment models models that, given information from the present, can be queried to make predictions about the future. we use these environment models to simulate imagined trajectories, which are interpreted by a neural network and provided as additional context to a policy network. in general, an environment model is any recurrent architecture which can be trained in an unsupervised fashion from agent trajectories: given a past state and current action, the environment model predicts the next state and any number of signals from the environment. in this work, we will consider in particular environment models that build on recent successes of actionconditional nextstep predictors [20][21][22], which receive as input the current observation (or history of observations) and current action, and predict the next observation, and potentially the next reward. we roll out the environment model over multiple time steps into the future, by initializing the imagined trajectory with the present time real observation, and subsequently feeding simulated observations into the model. the actions chosen in each rollout result from a rollout policy (explained in section 3.1). the environment model together with constitute the imagination core module, which predicts next time steps (fig 1a). the imagination core is used to produce n trajectoriest , each imagined trajectoryt is a sequence of features (f t1 , ,f t ), where t is the current time, the length of the rollout, andf ti the output of the environment model (i.e. the predicted observation and/or reward). despite recent progress in training better environment models, a key issue addressed by i2as is that a learned model cannot be assumed to be perfect; it might sometimes make erroneous or nonsensical predictions. we therefore do not want to rely solely on predicted rewards (or values predicted figure 2: environment model. the input action is broadcast and concatenated to the observation. a convolutional network transforms this into a pixelwise probability distribution for the output image, and a distribution for the reward. from predicted states), as is often done in classical planning. additionally, trajectories may contain information beyond the reward sequence (a trajectory could contain an informative subsequence for instance solving a subproblem which did not result in higher reward). for these reasons, we use a rollout encoder e that processes the imagined rollout as a whole and learns to interpret it, i.e. by extracting any information useful for the agents decision, or even ignoring it when necessary (fig 1b). each trajectory is encoded separately as a rollout embedding e i e(t i ). finally, an aggregator a converts the different rollout embeddings into a single imagination code c ia a(e , the final component of the i2a is the policy module, which is a network that takes the information c ia from modelbased predictions, as well as the output c mf of a modelfree path (a network which only takes the real observation as input; see fig 1c, right), and outputs the imaginationaugmented policy vector and estimated value v the i2a therefore learns to combine information from its modelfree and imaginationaugmented paths; note that without the modelbased path, i2as reduce to a standard modelfree network [3]. i2as can thus be thought of as augmenting modelfree agents by providing additional information from modelbased planning, and as having strictly more expressive power than the underlying modelfree agent. architectural choices and experimental setup for our experiments, we perform one rollout for each possible action in the environment. the first action in the i th rollout is the i th action of the action set a, and subsequent actions for all rollouts are produced by a shared rollout policy. we investigated several types of rollout policies (random, pretrained) and found that a particularly efficient strategy was to distill the imaginationaugmented policy into a modelfree policy. this distillation strategy consists in creating a small modelfree network (o t ), and adding to the total loss a cross entropy auxiliary loss between the imaginationaugmented policy (o t ) as computed on the current observation, and the policy(o t ) as computed on the same observation. by imitating the imaginationaugmented policy, the internal rollouts will be similar to the trajectories of the agent in the real environment; this also ensures that the rollout corresponds to trajectories with high reward. at the same time, the imperfect approximation results in a rollout policy with higher entropy, potentially striking a balance between exploration and exploitation. in our experiments, the encoder is an lstm with convolutional encoder which sequentially processes a trajectory t the featuresf t are fed to the lstm in reverse order, fromf t tof t1 , to mimic bellman type backup operations. the aggregator simply concatenates the summaries. for the modelfree path of the i2a, we chose a standard network of convolutional layers plus one fully connected one [e.g. we also use this architecture on its own as a baseline agent. our environment model (fig. 2) defines a distribution which is optimized by using a negative loglikelihood loss l model we can either pretrain the environment model before embedding it (with frozen weights) within the i2a architecture, or jointly train it with the agent by adding l model to the total loss as an auxiliary loss. in practice we found that pretraining the environment model led to faster runtime of the i2a architecture, so we adopted this strategy. for all environments, training data for our environment model was generated from trajectories of a partially trained standard modelfree agent (defined below). we use partially pretrained agents because random agents see few rewards in some of our domains. however, this means we have to account for the budget (in terms of real environment steps) required to pretrain the datagenerating agent, as well as to then generate the data. in the experiments, we address this concern in two ways: by explicitly accounting for the number of steps used in pretraining (for sokoban), or by demonstrating how the same pretrained model can be reused for many tasks (for minipacman). using a fixed pretrained environment model, we trained the remaining i2a parameters with asynchronous advantage actorcritic (a3c) [3]. we added an entropy regularizer on the policy to encourage exploration and the auxiliary loss to distill into the rollout policy as explained above. we distributed asynchronous training over to workers; we used the rmsprop optimizer [23]. we report results after an initial round of hyperparameter exploration (details in appendix a). learning curves are averaged over the top three agents unless noted otherwise. a separate hyperparameter search was carried out for each agent architecture in order to ensure optimal performance. in addition to the i2a, we ran the following baseline agents (see appendix b for architecture details for all agents). standard modelfree agent. for our main baseline agent, we chose a modelfree standard architecture similar to [3], consisting of convolutional layers (2 for minipacman, and for sokoban) followed by a fully connected layer. the final layer, again fully connected, outputs the policy logits and the value function. for sokoban, we also tested a large standard architecture, where we double the number of all feature maps (for convolutional layers) and hidden units (for fully connected layers). the resulting architecture has a slightly larger number of parameters than i2a. aside from having an internal environment model, the i2a architecture is very different from the one of the standard agent. to verify that the information contained in the environment model rollouts contributed to an increase in performance, we implemented a baseline where we replaced the environment model in the i2a with a copy model that simply returns the input observation. lacking a model, this agent does not use imagination, but uses the same architecture, has the same number of learnable parameters (the environment model is kept constant in the i2a), and benefits from the same amount of computation (which in both cases increases linearly with the length of the rollouts). this model effectively corresponds to an architecture where policy logits and value are the final output of an lstm network with skip connections. we now demonstrate the performance of i2a over baselines in a puzzle environment, sokoban. we address the issue of dealing with imperfect models, highlighting the strengths of our approach over planning baselines. we also analyze the importance of the various components of the i2a. sokoban is a classic planning problem, where the agent has to push a number of boxes onto given target locations. because boxes can only be pushed (as opposed to pulled), many moves are irreversible, and mistakes can render the puzzle unsolvable. a human player is thus forced to plan moves ahead of time. we expect that artificial agents will similarly benefit from internal simulation. our implementation of sokoban procedurally generates a new level each episode (see appendix d.4 for details, fig. this means an agent cannot memorize specific puzzles. together with the planning aspect, this makes for a very challenging environment for our modelfree baseline agents, which solve less than of the levels after a billion steps of training (details below). we provide videos of agents playing our version of sokoban online [24]. while the underlying game logic operates in a grid world, our agents were trained directly on rgb sprite graphics as shown in fig. there are no aspects of i2as that make them specific to grid world games. since using imagined rollouts is helpful for this task, we investigate how the length of individual rollouts affects performance. the latter was one of the hyperparameters we searched over. a breakdown by number of unrolling/imagination steps in fig. (right) shows that using longer rollouts, while not increasing the number of parameters, increases performance: unrolling steps improves speed of learning and top performance significantly over unrolling step, outperforms 3, and as a test for significantly longer rollouts, outperforms 5, reaching above of levels solved. however, in general we found diminishing returns with using i2a with longer rollouts. it is noteworthy that steps is relatively small compared to the number of steps taken to solve a level, for which our best agents need about steps on average. this implies that even such short rollouts can be highly informative. for example, they allow the agent to learn about moves it cannot recover from (such as pushing boxes against walls, in certain contexts). because i2a with rollouts of length are significantly slower, in the rest of this section, we choose rollouts of length to be our canonical i2a architecture. it terms of data efficiency, it should be noted that the environment model in the i2a was pretrained (see section 3.2). we conservatively measured the total number of frames needed for pretraining to be lower than 1e8. thus, even taking pretraining into account, i2a outperforms the baselines after seeing about 3e8 frames in total (compare again fig. of course, data efficiency is even better if the environment model can be reused to solve multiple tasks in the same environment (section 5). one of the key strengths of i2as is being able to handle learned and thus potentially imperfect environment models. however, for the sokoban task, our learned environment models actually perform quite well when rolling out imagined trajectories. to demonstrate that i2as can deal with less reliable predictions, we ran another experiment where the i2a used an environment model that had shown much worse performance (due to a smaller number of parameters), with strong artifacts accumulating over iterated rollout predictions (fig. (right) shows, even with such a clearly flawed environment model, i2a performs similarly well. this implies that i2as can learn to ignore the latter parts of the rollout as errors accumulate, but still use initial predictions when errors are less severe. finally, note that in our experiments, surprisingly, the i2a agent with poor model ended outperforming the i2a agent with good model. we posit this was due to random initialization, though we cannot exclude the noisy model providing some form of regularization more work will be required to investigate this effect. learning a rollout encoder is what enables i2as to deal with imperfect model predictions. we can further demonstrate this point by comparing them to a setup without a rollout encoder: as in the classic montecarlo search algorithm of tesauro and galperin [25], we now explicitly estimate the value of each action from rollouts, rather than learning an arbitrary encoding of the rollouts, as in i2a. we then select actions according to those values. specifically, we learn a value function v from states, and, using a rollout policy, sample a trajectory rollout for each initial action, and compute the corresponding estimated monte carlo return tt t r a t v (x a t ) where ((x a t , r a t )) t0..t comes from a trajectory initialized with action a. action a is chosen with probability proportional to exp(( t0..t t r a t v (x a t ))/), where is a learned temperature. this can be thought of as a form of i2a with a fixed summarizer (which computes returns), no modelfree path, and very simple policy head. in this architecture, only v, and are learned. we ran this rollout encoderfree agent on sokoban with both the accurate and the noisy environment model. we chose the length of the rollout to be optimal for each environment model (from the same range as for i2a, i.e. (right), when using the high accuracy environment model, the performance of the encoderfree agent is similar to that of the baseline standard agent. however, unlike i2a, its performance degrades catastrophically when using the poor model, showcasing the susceptibility to model misspecification. so far, we have studied the role of the rollout encoder. to show the importance of various other components of the i2a, we performed additional control experiments. results are plotted in fig. first, i2a with the copy model (section 3.3) performs far worse, demonstrating that the environment model is indeed crucial. second, we trained an i2a where the environment model was predicting no rewards, only observations. this also performed worse. however, after much longer training (3e9 steps), these agents did recover performance close to that of the original i2a (see appendix d.2), which was never the case for the baseline agent even with that many steps. hence, reward prediction is helpful but not absolutely necessary in this task, and imagined observations alone are informative enough to obtain high performance on sokoban. note this is in contrast to many classical planning and modelbased reinforcement learning methods, which often rely on reward prediction. the rollout policy is still learned by distillation from the output policy note: the mc curves in fig. only used a single agent rather than averages. i2a mc search mcts87 mcts95 random search millions in previous sections, we illustrated that i2as can be used to efficiently solve planning problems and can be robust in the face of model misspecification. here, we ask a different question if we do assume a nearly perfect model, how does i2a compare to competitive planning methods? beyond raw performance we focus particularly on the efficiency of planning, i.e. the number of imagination steps required to solve a fixed ratio of levels. we compare our regular i2a agent to a variant of monte carlo tree search (mcts), which is a modern guided tree search algorithm [12,26]. for our mcts implementation, we aimed to have a strong baseline by using recent ideas: we include transposition tables [27], and evaluate the returns of leaf nodes by using a value network (in this case, a deep residual value network trained with the same total amount of data as i2a; see appendix d.3 for further details). running mcts on sokoban, we find that it can achieve high performance, but at a cost of a much higher number of necessary environment model simulation steps: mcts reaches the i2a performance of of levels solved when using 25k model simulation steps on average to solve a level, compared to 1.4k environment model calls for i2a. using even more simulation steps, mcts performance increases further, e.g. if we assume access to a highaccuracy environment model (including the reward prediction), we can also push i2a performance further, by performing basic montecarlo search with a trained i2a for the rollout policy: we let the agent play whole episodes in simulation (where i2a itself uses the environment model for shortterm rollouts, hence corresponding to using a modelwithinamodel), and execute a successful action sequence if found, up to a maximum number of retries; this is reminiscent of nested rollouts [28]. with a fixed maximum of retries, we obtain a score of (up from for the i2a itself). the total average number of model simulation steps needed to solve a level, including running the model in the outer loop, is now 4k, again much lower than the corresponding mcts run with 100k steps. note again, this approach requires a nearly perfect model; we dont expect i2a with mc search to perform well with approximate models. see table for a summary of the imagination efficiency for the different methods. lastly, we probe the generalization capabilities of i2as, beyond handling random level layouts in sokoban. our agents were trained on levels with boxes. table shows the performance of i2a when such an agent was tested on levels with different numbers of boxes, and that of the standard modelfree agent for comparison. we found that i2as generalizes well; at boxes, the i2a agent is still able to solve more than half of the levels, nearly as many as the standard agent on boxes. in our final set of experiments, we demonstrate how a single model, which provides the i2a with a general understanding of the dynamics governing an environment, can be used to solve a collection of different tasks. we designed a simple, lightweight domain called minipacman, which allows us to easily define multiple tasks in an environment with shared state transitions and which enables us to do rapid experimentation. 6, left), the player explores a maze that contains food while being chased by ghosts. the maze also contains power pills; when eaten, for a fixed number of steps, the player moves faster, and the ghosts run away and can be eaten. these dynamics are common to all tasks. each task is defined by a vector w rew r , associating a reward to each of the following five events: moving, eating food, eating a power pill, eating a ghost, and being eaten by a ghost. we consider five different reward vectors inducing five different tasks. empirically we found that the reward schemes were sufficiently different to lead to very different highperforming policies (for more details on the game and tasks, see appendix c. to illustrate the benefits of modelbased methods in this multitask setting, we train a single environment model to predict both observations (frames) and events (as defined above, e.g. note that the environment model is effectively shared across all tasks, so that the marginal cost of learning the model is nil. during training and testing, the i2as have access to the frame and reward predictions generated by the model; the latter was computed from model event predictions and the task reward vector w rew as such, the reward vector w rew can be interpreted as an instruction about which task to solve in the same environment [cf. the frostbite challenge of 11]. for a fair comparison, we also provide all baseline agents with the event variable as input. we trained baseline agents and i2as separately on each task. (right) indicate the benefit of the i2a architecture, outperforming the standard agent in all tasks, and the copymodel baseline in all but one task. moreover, we found that the performance gap between i2as and baselines is particularly high for tasks 5, where rewards are particularly sparse, and where the anticipation of ghost dynamics is especially important. we posit that the i2a agent can leverage its environment and reward model to explore the environment much more effectively. some recent work has focused on applying deep learning to modelbased rl. a common approach is to learn a neural model of the environment, including from raw observations, and use it in classical planning algorithms such as trajectory optimization [29][30][31]. these studies however do not address a possible mismatch between the learned model and the true environment. model imperfection has attracted particular attention in robotics, when transferring policies from simulation to real environments [32][33][34]. there, the environment model is given, not learned, and used for pretraining, not planning at test time. [35] also learn to extract information from trajectories, but in the context of imitation learning. [36] take a bayesian approach to model imperfection, by selecting environment models on the basis of their actual control performance. the problem of making use of imperfect models was also approached in simplified environment in talvitie [18,19] by using techniques similar to scheduled sampling [37]; however these techniques break down in stochastic environments; they mostly address the compounding error issue but do not address fundamental model imperfections. a principled way to deal with imperfect models is to capture model uncertainty, e.g. by using gaussian process models of the environment, see deisenroth and rasmussen [15]. the disadvantage of this method is its high computational cost; it also assumes that the model uncertainty is well calibrated and lacks a mechanism that can learn to compensate for possible miscalibration of uncertainty. [38] consider rl with a hierarchy of models of increasing (known) fidelity. a recent multitask for example, in the avoid game, any event is negatively rewarded, and the optimal strategy is for the agent to clear a small space from food and use it to continuously escape the ghosts. it is not necessary to provide the reward vector wrew to the baseline agents, as it is equivalent a constant bias. gp extension of this study can further help to mitigate the impact of model misspecification, but again suffers from high computational burden in large domains, see marco et al. a number of approaches use models to create additional synthetic training data, starting from dyna [40], to more recent work e.g. [42]; these models increase data efficiency, but are not used by the agent at test time. [45] all present neural networks whose architectures mimic classical iterative planning algorithms, and which are trained by reinforcement learning or to predict userdefined, highlevel features; in these, there is no explicit environment model. in our case, we use explicit environment models that are trained to predict lowlevel observations, which allows us to exploit additional unsupervised learning signals for training. this procedure is expected to be beneficial in environments with sparse rewards, where unsupervised modelling losses can complement return maximization as learning target as recently explored in jaderberg et al. internal models can also be used to improve the credit assignment problem in reinforcement learning: henaff et al. [48] learn models of discrete actions environments, and exploit the effective differentiability of the model with respect to the actions by applying continuous control planning algorithms to derive a plan; schmidhuber [49] uses an environment model to turn environment cost minimization into a network activity minimization. [50] learn symbolic networks models of the environment and use them for planning, but are given the relevant abstractions from a handcrafted vision system. close to our work is a study by hamrick et al. [51]: they present a neural architecture that queries learned expert models, but focus on metacontrol for continuous contextual bandit problems. [52] extend this work by focusing on explicit planning in sequential environments, and learn how to construct a plan iteratively. the general idea of learning to leverage an internal model in arbitrary ways was also discussed by schmidhuber [53]. we presented i2a, an approach combining modelfree and modelbased ideas to implement imaginationaugmented rl: learning to interpret environment models to augment modelfree decisions. i2a outperforms modelfree baselines on minipacman and on the challenging, combinatorial domain of sokoban. we demonstrated that, unlike classical modelbased rl and planning methods, i2a is able to successfully use imperfect models (including models without reward predictions), hence significantly broadening the applicability of modelbased rl concepts and ideas. as all modelbased rl methods, i2as tradeoff environment interactions for computation by pondering before acting. this is essential in irreversible domains, where actions can have catastrophic outcomes, such as in sokoban. in our experiments, the i2a was always less than an order of magnitude slower per interaction than the modelfree baselines. the amount of computation can be varied (it grows linearly with the number and depth of rollouts); we therefore expect i2as to greatly benefit from advances on dynamic compute resource allocation (e.g. another avenue for future research is on abstract environment models: learning predictive models at the right level of complexity and that can be evaluated efficiently at test time will help to scale i2as to richer domains. remarkably, on sokoban i2as compare favourably to a strong planning baseline (mcts) with a perfect environment model: at comparable performance, i2as require far fewer function calls to the model than mcts, because their model rollouts are guided towards relevant parts of the state space by a learned rollout policy. this points to further potential improvement by training rollout policies that learn to query imperfect models in a taskrelevant way. each agent used in the paper defines a stochastic policy, i.e. a categorical distribution (a t o t ; ) over discrete actions a. the logits of (a t o t ; ) are computed by a neural network with parameters , taking observation o t at timestep t as input. during training, to increase the probability of rewarding actions being taken, a3c applies an update to the parameters using policy gradient g(): g() log(a t o t ; )a(o t , a t ) where a(o t , a t ) is an estimate of the advantage function [55]. in practice, we learn a value function v (o t ; v ) and use it to compute the advantage as the difference of the bootstrapped kstep return and and the current value estimate: the value function v (o t ; v ) is also computed as the output of a neural network with parameters v the input to the value function network was chosen to be the second to last layer of the policy network that computes the parameter v are updated with v towards bootstrapped kstep return: in our numerical implementation, we express the above updates as gradients of a corresponding surrogate loss [56]. to this surrogate loss, we add an entropy regularizer of ent at (a t o t ; ) log (a t o t ; ) to encourage exploration, with ent thoughout all experiments. where applicable, we add a loss for policy distillation consisting of the crossentropy between and: with scaling parameter dist here denotes that we do not backpropagate gradients of l dist wrt. to the parameters of the rollout policy through the behavioral policy finally, even though we pretrained our environment models, in principle we can also learn it jointly with the i2a agent by a adding an appropriate loglikelihood term of observations under the model. we will investigate this in future research. we optimize hyperparameters (learning rate and momentum of the rmsprop optimizer, gradient clipping parameter, distillation loss scaling dist where applicable) separately for each agent (i2a and baselines). we used rectified linear units (relus) between all hidden layers of all our agents. for the environment models, we used leaky relus with a slope of the standard modelfree baseline agent, taken from [3], is a multilayer convolutional neural network (cnn), taking the current observation o t as input, followed by a fully connected (fc) hidden layer. this fc layer feeds into two heads: into a fc layer with one output per action computing the policy logits log (a t o t , ); and into another fc layer with a single output that computes the value function v (o t ; v ). the sizes of the layers were chosen as follows: for minipacman: the cnn has two layers, both with 3x3 kernels, output channels and strides and 2; the following fc layer has units for sokoban: the cnn has three layers with kernel sizes 8x8, 4x4, 3x3, strides of 4, 2, and number of output channels 32, 64, 64; the following fc has units the model free path of the i2a consists of a cnn identical to one of the standard modelfree baseline (without the fc layers). the rollout encoder processes each frame generated by the environment model with another identically sized cnn. the output of this cnn is then concatenated with the reward prediction (single scalar broadcast into frame shape). this feature is the input to an lstm with (for sokoban) or (for minipacman) units. the same lstm is used to process all rollouts (one per action); the last output of the lstm for all rollouts are concatenated into a single vector c ia of length for sokoban, and on minipacman. this vector is concatenated with the output c mf of the modelfree cnn path and is fed into the fully connected layers computing policy logits and value function as in the baseline agent described above. the copymodel agent has the exact same architecture as the i2a, with the exception of the environment model being replaced by the identity function (constantly returns the input observation). for the i2a, we pretrain separate autoregressive models of order for the raw pixel observations of the minipacman and sokoban environments (see figures and 8) in both cases, the input to the model consisted of the last observation o t , and a broadcasted, onehot representation of the last action a t following previous studies, the outputs of the models were trained to predict the next frame o t1 by stochastic gradient decent on the bernoulli crossentropy between network outputs and data o t1 the sokoban model is a simplified case of the minipacman model; the sokoban model is nearly entirely local (save for the reward model), while the minipacman model needs to deal with nonlocal interaction (movement of ghosts is affected by position of pacman, which can be arbitrarily far from the ghosts). the input and output frames were of size x x (width x height x rgb). the model is depicted in figure it consisted of a size preserving, multiscale cnn architecture with additional fully connected layers for reward prediction. in order to capture longrange dependencies across pixels, we also make use of a layer we call poolandinject, which applies global maxpooling over each feature map and broadcasts the resulting values as feature maps of the same size and concatenates the result to the input. poolandinject layers are therefore sizepreserving layers which communicate the maxvalue of each layer globally to the next convolutional layer. the sokoban model was chosen to be a residual cnn with an additional cnn / fullyconnected mlp pathway for predicting rewards. the input of size 80x80x3 was first processed with convolutions with a large 8x8 kernel and stride of this reduced representation was further processed with two size preserving cnn layers before outputting a predicted frame by a 8x8 convolutional layer. figure 7: the minipacman environment model. the overview is given in the right panel with blowups of the basic convolutional building block (middle panel) and the poolandinject layer (left panel). the basic build block has three hyperparameters n , n , n determining the number of channels in the convolutions; their numeric values are given in the right panel. ghosts always move by one square at each time step. pacman usually moves by one square, except when it has eaten a power pill, which makes it move by two squares at a time. when moving by squares, if pacman new position ends up inside a wall, then it is moved back by one square to get back to a corridor. we say that pacman and a ghost meet when they either end up at the same location, or when their path crosses (even if they do not end up at the same location). when pacman moves to a square with food or a power pill, it eats it. eating a power pill gives pacman super powers, such as moving at double speed and being able to eat ghosts. the effects of eating a power pill last for time steps. when pacman meets a ghost, either pacman dies eaten by the ghost, or, if pacman has recently eaten a power pill, the ghost dies eaten by pacman. if pacman has eaten a power pill, ghosts try to flee from pacman. they otherwise try to chase pacman. a more precise algorithm for the movement of a ghost is given below in pseudo code: if ghost.canmove(dir) then 8: if len(alloweddirections) then we are in a straight corridor, or at a bend 10: if ghost.currentdirection in alloweddirections then 11: return ghost.currentdirection if opposite(ghost.currentdirection) alloweddirections[0] then 13: return alloweddirections [1] 14: return alloweddirections[0] 15: else we are at an intersection 16: if opposite(ghost.currentdirection) in alloweddirections then alloweddirections.remove(opposite(ghost.currentdirection)) ghosts do not turn around when a level is cleared, a new level starts. tasks also differ in the way a level was cleared. regular: level is cleared when all the food is eaten; avoid: level is cleared after steps; hunt: level is cleared when all ghosts are eaten or after steps. ambush: level is cleared when all ghosts are eaten or after steps. rush: level is cleared when all power pills are eaten. the pink bar appears when pacman eats a power pill, and it decreases in size over the duration of the effect of the pill. there are no lives, and episode ends when pacman is eaten by a ghost. the time left before the effect of the power pill wears off is shown using a pink shrinking bar at the bottom of the screen as in fig. in the game of sokoban, random actions on the levels would solve levels with vanishing probability, leading to extreme exploration issues for solving the problem with reinforcement learning. to alleviate this issue, we use a shaping reward scheme for our version of sokoban: every time step, a penalty of is applied to the agent. whenever the agent pushes a box on target, it receives a reward of whenever the agent pushes a box off target, it receives a penalty of finishing the level gives the agent a reward of and the level terminates. the first reward is to encourage agents to finish levels faster, the second to encourage agents to push boxes onto targets, the third to avoid artificial reward loop that would be induced by repeatedly pushing a box off and on target, the fourth to strongly reward solving a level. levels are interrupted after steps (i.e. agent may bootstrap from a value estimate of the last frame, but the level resets to a new one). identical levels are nearly never encountered during training or testing (out of million levels generated, less than were repeated). note that with this reward scheme, it is always optimal to solve the level (thus our shaping scheme is valid). an alternative strategy would have been to have the agent play through a curriculum of increasingly difficult tasks; we expect both strategies to work similarly. our first additional experiment compared i2a with and without reward prediction, trained over a longer horizon. i2a with reward prediction clearly converged shortly after 1e9 steps and we therefore interrupted training; however, i2a without reward prediction kept increasing performance, and after 3e9 steps, we recover a performance level of close to of levels solved, see fig. next, we investigated the i2a with montecarlo search (using a near perfect environment model of sokoban). we let the agent try to solve the levels up to times within its internal model. the base i2a architecture was solving around of levels; mental retries boosted its performance to around of levels solved. although the agent was allowed up to mental retries, in practice all the performance increase was obtained within the first mental retries. exact percentage gain by each mental retry is shown in fig. 12, only of the levels are solved on the first mental attempt, even though the i2a architecture could solve around of levels. the gap is explained by the use of an environment model: although it looks nearly perfect to the naked eye, the model is not actually equivalent to the environment. we first trained a value network that estimates the value function of a trained modelfree policy; to do this, we trained a modelfree agent for 1e9 environment steps. this agent solved close to of episodes. using this agent, we generated 1e8 (frame, return) pairs, and trained the value network to predict the value (expected return) from the frame; training and test error were comparable, and we dont expect increasing the number of training points would have significantly improved the quality of the the value network. the value network architecture is a residual network which stacks one convolution layer and convolution blocks with a final fullyconnected layer of hidden units. the first convolution is convolution with feature maps. each of the three residual convolution block is composed of two convolutional layers; the first is a convolution with feature maps, the second a convolution with feature maps, and the last a layer with feature maps. to help the value networks, we trained them not on the pixel representation, but on a symbolic representation. the trained value network is then employed during search to evaluate leafnodes similar to [12], replacing the role of traditional random rollouts in mcts. the tree policy uses [57,58] with a finetuned exploration constant of depthwise transposition tables for the tree nodes are used to deal with the symmetries in the sokoban environment. external actions are selected by taking the max q value at the root node. the tree is reused between steps but selecting the appropriate subtree as the root node for the next step. reported results are obtained by averaging the results over episodes. we detail here our procedural generation for sokoban levels we follow closely methods described in [59,60]. the generation of a sokoban level involves three steps: room topology generation, position configuration and room reverseplaying. topology generation: given an initial widthheight room entirely constituted by wall blocks, the topology generation consists in creating the empty spaces (i.e. corridors) where boxes, targets and the player can be placed. for this simple random walk algorithm with a configurable number of steps is applied: a random initial position and direction are chosen. afterwards, for every step, the position is updated and, with a probability p 0.35, a new random direction is selected. every visited position is emptied together with a number of surrounding wall blocks, selected by randomly choosing one of the following patterns indicating the adjacent room blocks to be removed (the darker square represents the reference position, that is, the position being visited). note that the room exterior walls are never emptied, so from a widthheight room only a (width2)(height2) space can actually be converted into corridors. the random walk approach guarantees that all the positions in the room are, in principle, reachable by the player. a relatively small probability of changing the walk direction favours the generation of longer corridors, while the application of a random pattern favours slightly more convoluted spaces. position configuration: once a room topology is generated, the target locations for the desired n boxes and the player initial position are randomly selected. there is the obvious prerequisite of having enough empty spaces in the room to place the targets and the player but no other constraints are imposed in this step. reverse playing: once the topology and targets/player positions are generated the room is reverseplayed. in this case, on each step, the player has eight possible actions to choose from: simply moving or movingpulling from a box in each possible direction (assuming for the latter, that there is a box adjacent to the player position). initially the room is configured with the boxes placed over their corresponding targets. from that position a depthfirst search (with a configurable maximum depth) is carried out over the space of possible moves, by expanding each reached player/boxes position by iteratively applying all the possible actions (which are randomly permuted on each step). an entire tree is not explored as there are different combinations of actions leading to repeated boxes/player configurations which are skipped. statistics are collected for each boxes/player configuration, which is, in turn, scored with a simple heuristic: where boxswaps represents the number of occasions in which the player stopped pulling from a given box and started pulling from a different one, while boxdisplacement represents the manhattan distance between the initial and final position of a given box. also whenever a box or the player are placed on top of one of the targets the roomscore value is set to while this scoring heuristic doesnt guarantee the complexity of the generated rooms its aimed to a) favour room configurations where overall the boxes are further away from their original positions and b) increase the probability of a room requiring a more convoluted combination of box moves to get to a solution (by aiming for solutions with higher boxswaps values). this scoring mechanism has empirically proved to generate levels with a balanced combination of difficulties. the reverse playing ends when there are no more available positions to explore or when a predefined maximum number of possible room configurations is reached. the room with the higher roomscore is then returned. a maximum of room topologies and for each of those boxes/player positioning are retried in case a given combination doesnt produce rooms with a score the room configuration tree is by default limited to a maximum depth of applied actions. the total number of visited positions is by default limited to default randomwalk steps: (room width room height).", "summary": "the paper presents i2a (imagination augmented agent) that combines the modelbased and modelfree approaches leading to data efficiency and robustness even with imperfect models. i2a agent uses the predictions from a learned environment model as an additional context in deep policy networks. this leads to improved data efficiency and robustness to imperfect models. i2a agent has two main modules imagination module and the policy module. imagination module environment model this is a recurrent model, trained in an unsupervised manner using the agent trajectories. it can be used to predict the future state given the current state and action. the environment model can be rolled out multiple times to obtain a simulated trajectory or an imagined trajectory. during each rollout, the actions are chosen using a rollout policy r. rollout encoder a rollout encoder e (lstm) is used to process the entire imagined rollout. the imagination module is used to generate n trajectories. each trajectory is a sequence of outputs of the environment model. these n trajectories are concatenated into a single imagination vector. the training data for the environment model is generated from trajectories of a partially trained modelfree agent. pretraining the environment model (instead of joint training with policy) leads to faster runtime. policy module this module uses the output of both modelbased path and modelfree path as its input. it generates the policy vector and value function. rollout strategy one rollout is performed for each possible action in the environment ie, the first action in the ith rollout is the ith action in the action set. subsequent actions are generated using a shared rollout policy \u2019 an effective strategy was to create a small modelfree network \u2019(ot) and then add a kl loss component that encourages \u2019(ot)to be similar to the imagination augmented policy (ot). baselines modelfree agent copymodel agent same as i2a but the environment model is replaced by a copy model that just returns the input observations. environments sokoban task is to push a number of boxes onto given target locations. i2a outperforms the baselines and gains in performance as the number of unrolling steps increases (though at a diminishing rate). in case of poor environment models, the agent seems to be able to ignore the later part of the rollout when the error starts to accumulate. monte carlo search algorithm (without an explicit rollout encoder) performed poorly as compared to the model using rollout encoder. predicting the reward along with value function and action seems to speed up training. if a nearperfect model is available, i2a agent\u2019s performance can be improved by performing monte carlo search with the trained i2a agent for the rollout policy. the agent plays entire episodes in simulation and tries to find a successful action sequence within retries. minipacman i2a agent is evaluated to see if a single model can be used to solve multiple tasks. a new environment is designed to define multiple tasks in an environment with shared state transitions. each task is specified by a 5dimensional reward vector that associates a reward with moving, eating food, eating a pill, eating a ghost and being eaten by a ghost. a single environment model is trained to predict both observations (frames) and events (eg eating a pill). this way, the environment model is shared across all tasks. baseline agents and i2as are trained on each task separately. i2a architecture outperforms the standard agent in all tasks and the copymodel baseline in all but one task. the improvement in performance is higher for tasks where rewards are sparse and where the anticipation of ghost dynamics is especially important indicating that the i2a agent can use the environment model to explore the environment more effectively."}, {"document": "we study the problem of visualizing largescale and highdimensional data in a lowdimensional (typically 2d or 3d) space. much success has been reported recently by techniques that first compute a similarity structure of the data points and then project them into a lowdimensional space with the structure preserved. these two steps suffer from considerable computational costs, preventing the stateoftheart methods such as the tsne from scaling to largescale and highdimensional data (e.g., millions of data points and hundreds of dimensions). we propose the largevis, a technique that first constructs an accurately approximated knearest neighbor graph from the data and then layouts the graph in the lowdimensional space. comparing to tsne, largevis significantly reduces the computational cost of the graph construction step and employs a principled probabilistic model for the visualization step, the objective of which can be effectively optimized through asynchronous stochastic gradient descent with a linear time complexity. the whole procedure thus easily scales to millions of highdimensional data points. experimental results on realworld data sets demonstrate that the largevis outperforms the stateoftheart methods in both efficiency and effectiveness. the hyperparameters of largevis are also much more stable over different data sets. we now live in the era of the big data. understanding and mining largescale data sets have created big opportunities. this work was done when the second author was an intern at microsoft research asia.. copyright is held by the international world wide web conference committee (iw3c2). iw3c2 reserves the right to provide a hyperlink to the author\u2019s site if the material is used in electronic media. www 2016, april 1115, 2016, montral, qubec, canada. http://dx.doi.org/10.1145/2872427.2883041 for business providers, data scientists, governments, educators, and healthcare practitioners. many computational infrastructures, algorithms, and tools are being constructed for the users to manage, explore, and analyze their data sets. information visualization has been playing a critical role in this pipeline, which facilitates the description, exploration, and sensemaking from both the original data and the analysis results [16]. classical visualization techniques have been proved effective for small or intermediate size data; they however face a big challenge when applied to the big data. for example, visualizations such as scatter plots, heatmaps, and network diagrams all require laying out data points on a 2d or 3d space, which becomes computationally intractable when there are too many data points and when the data have many dimensions. indeed, while there exist numerous network diagrams with thousands of nodes, a visualization of millions of nodes is rare, even if such a visualization would easily reveal node centrality and community structures. in general, the problem is concerned with finding an extremely lowdimensional (e.g., 2d or 3d) representation of largescale and highdimensional data, which has attracted a lot of attentions recently in both the data mining community [25, 2, 27] and the infoviz community [11, 14, 17]. compared to the highdimensional representations, the 2d or 3d layouts not only demonstrate the intrinsic structure of the data intuitively and can also be used as the basis to build many advanced and interactive visualizations.. projecting highdimensional data into spaces with fewer dimensions is a core problem of machine learning and data mining. the essential idea is to preserve the intrinsic structure of the highdimensional data, i.e., keeping similar data points close and dissimilar data points far apart, in the lowdimensional space. in literature, many dimensionality reduction techniques have been proposed, including both linear mapping methods (e.g., principle component analysis [15], multidimensional scaling [25]) and nonlinear mapping methods (e.g., isomap [24], locally linear embedding [20], laplacian eigenmaps [2]). as most highdimensional data usually lie on or near a lowdimensional nonlinear manifold, the performance of linear mapping approaches is usually not satisfactory [27]. for nonlinear methods such as the laplacian eigenmaps, although empirically effective on small, laboratory data sets, they generally do not perform well on highdimensional, real data as they are typically not able to preserve both the local and the global structures of the highdimensional data. maaten and hinton proposed the tsne technique [27], which captures both the local and the global structures. maaten further proposed an accelar x iv :1. eration technique [26] for the tsne by first constructing a knearest neighbor (knn) graph of the data points and then projecting the graph into lowdimensional spaces with treebased algorithms. tsne and its variants, which represent a family of methods that first construct a similarity structure from the data and then project the structure into a 2d/3d space (see figure 1), have been widely adopted recently due to the ability to handle realworld data and the good quality of visualizations.. despite their successes, when applied to data with millions of points and hundreds of dimensions, the tsne techniques are still far from satisfaction. the reasons are threefold: (1) the construction of the knearest neighbor graph is a computational bottleneck for dealing with largescale and highdimensional data. tsne constructs the graph using the technique of vantagepoint trees [28], the performance of which significantly deteriorates when the dimensionality of the data grows high; (2) the efficiency of the graph visualization step significantly deteriorates when the size of the data becomes large; (3) the parameters of the tsne are very sensitive on different data sets. to generate a good visualization, one has to search for the optimal parameters exhaustively, which is very time consuming on large data sets. it is still a long shot of the community to create high quality visualizations that scales to both the size and the dimensionality of the data.. we report a significant progress on this direction through the largevis, a new visualization technique that computes the layout of largescale and highdimensional data. the largevis employs a very efficient algorithm to construct an approximate knearest neighbor graph at a high accuracy, which builds on top of but significantly improves a stateoftheart approach to knn graph construction, the random projection trees [7]. we then propose a principled probabilistic approach to visualizing the knearest neighbor graph, which models both the observed links and the unobserved (i.e., negative) links in the graph. the model preserves the structures of the graph in the lowdimensional space, keeping similar data points close and dissimilar data points far away from each other. the corresponding objective function can be optimized through the asynchronous stochastic gradient descent, which scales linearly to the data size n comparing to the one used by the tsne, the optimization process of largevis is much more efficient and also more effective. besides, on different data sets the parameters of the largevis are much more stable.. we conduct extensive experiments on realworld, largescale and highdimensional data sets, including text (words. and documents), images, and networks. experimental results show that our proposed algorithm for constructing the approximate knearest neighbor graph significantly outperforms the vantagepoint tree algorithm used in the tsne and other stateoftheart methods. largevis generates comparable graph visualizations to the tsne on small data sets and more intuitive visualizations on large data sets; it is much more efficient when data becomes large; the parameters are not sensitive to different data sets. on a set of three million data points with one hundred dimensions, largevis is up to thirty times faster at graph construction and seven times faster at graph visualization. largevis only takes a couple of hours to visualize millions of data points with hundreds of dimensions on a single machine.. to summarize, we make the following contributions:. we propose a new visualization technique which is able to compute the layout of millions of data points with hundreds of dimensions efficiently.. we propose a very efficient algorithm to construct an approximate knearest neighbor graph from largescale, highdimensional data.. we propose a principled probabilistic model for graph visualization. the objective function of the model can be effectively optimized through asynchronous stochastic gradient descent with a time complexity of o(n).. we conduct experiments on realworld, very large data sets and compare the performance of largevis and tsne, both quantitatively and visually. to the best of our knowledge, very few visualization techniques can efficiently layout millions of highdimensional data points meaningfully on a 2d space. instead, most visualizations of large data sets have to first layout a summary or a coarse aggregation of the data and then refine a subset of the data (a region of the visualization) if the user zooms in [5]. admittedly, there are other design factors besides the computational capability, for example the aggregated data may be more intuitive and more robust to noises. however, with a layout of the entire data set as basis, the effectiveness of these aggregated/approximate visualizations will only be improved. many visualization tools are designed to layout geographical data, sensor data, and network data. these tools typically cannot handle highdimensional data.. many recent successes of visualizing highdimensional data come from the machine learning community. the tsne first compute a knearestneighbor graph and then visualizes this graph in a 2d/3d space. our work follows this direction and makes significant progress. constructing knearest neighbor (knn) graphs from highdimensional data is critical to many applications such as similarity search, collaborative filtering, manifold learning, and network analysis. while the exact computation of a knn has a complexity of o(n2d) (with n being the number of data points and d being the number of dimensions) which is too costly, existing approaches use roughly three categories of techniques: spacepartitioning trees [3, 10, 21, 7], locality sensitive hashing techniques [8, 6, 12], and neighbor exploring techniques [9]. the spacepartitioning methods divide the entire space into different regions and organize the regions into different tree structures, e.g., kd trees [3, 10], vptrees [28], cover trees [4], and random projection trees [7]. once the trees are constructed, the nearest neighbors of each data point can be found through traversing the trees. the locality sensitive hashing [8] techniques deploy multiple hashing functions to map the data points into different buckets and data points in the same buckets are likely to be similar to each other. the neighbor exploring techniques, such as the nndescent [9], is built on top of the intuition that my neighbors\u2019 neighbors are likely to be my neighbors. starting from an initial nearestneighbor graph, the algorithm iteratively refines the graph by exploring the neighbors of neighbors defined according to the current graph.. the above approaches work efficiently on different types of data sets. the kd trees, vptrees, or covertrees have been proved to very efficient on data with a small number of dimensions. however, the performance significantly deteriorates when the dimensionality of the data becomes large (e.g., hundreds). the nndescent approach is also usually efficient for data sets with a small number of dimensions [9]. a comparison of these techniques can be found at https://github.com/erikbern/annbenchmarks. the random projection trees have demonstrated stateoftheart performance in constructing very accurate knearest neighbor graphs from highdimensional data. however, the high accuracy is at the expense of efficiency, as to achieve a higher accuracy many more trees have to be created. our proposed technique is built upon random projection trees but significantly improves it using the idea of neighbor exploring. the accuracy of a knn graph quickly improves to almost without investing in many trees. the problem of graph visualization is related to dimensionality reduction, which includes two major types of approaches: linear transformations and nonlinear transformations. when projecting the data to extremely lowdimensional spaces (e.g., 2d), the linear methods such as the principle component analysis [15] and the multidimensional scaling [25] usually do not work as effectively as the nonlinear methods as most highdimensional data usually lies on or near lowdimensional nonlinear manifolds. the nonlinear methods such as isomap [24], local linear embedding (lle) [20], laplacian eigenmaps [2] are very effective on laboratory data sets but do not perform really well on realworld highdimensional data. maaten and hinton proposed the tsne [27], which is very effective on realworld data. methods scales to millions of data points. maaten improved the efficiency of tsne through two tree based algorithms [26], which scale better to large graphs. the optimization of the tsne requires the fully batch gradient descent learning, the time complexity of which w.r.t the data size n is o(n logn). largevis can be naturally optimized through asynchronous stochastic gradient descent, with a complexity of o(n). besides, the parameters of tsne are very sensitive on different sets while the parameters of largevis remain very stable.. there are many algorithms developed in the information visualization community to compute the layout of nodes in a network. they can also be used to visualize the knn graph. the majority of these network layout methods use either the abovementioned dimensionality reduction techniques or forcedirected simulations. among them, forcedirected layouts generates better visualizations, but their high computational complexity (ranging from o(n3) to o(n log2 n) with n being the number of nodes [22]) has prevented them from being applied to millions of nodes.. among them, the classical fruchtermanreingo algorithm [11] and the original forceatlas algorithm provided in gephi [1] have a complexity of o(n2). an improved version of forceatlas called the forceatlas2 [14] and the newly developed openord algorithm [17] reduce the time complexity to o(n logn). these two algorithms have been used to visualize one million data points 1, but the complexity prevents them from scaling up further.. the largevis is also related to our previous work on network/graph embedding, the line model [23]. line and other related methods (e.g., skipgram [18]) are not designed for visualization purposes. using them directly to learn 2/3 dimensional representations of data may yield ineffective visualization results. however, these methods can be used as a preprocessor of the data for the visualization (e.g., use line or skipgram to learn dimensional representations of the data and then use largevis to visualize them). in this section, we introduce the new visualization technique largevis. formally, given a largescale and highdimensional data set x xi rdi1,2,...,n , our goal is to represent each data point xi with a lowdimensional vector yi rs, where s is typically or the basic idea of visualizing highdimensional data is to preserve the intrinsic structure of the data in the lowdimensional space. existing approaches usually first compute the similarities of all pairs of xi, xj and then preserve the similarities in the lowdimensional transformation. as computing the pairwise similarities is too expensive (i.e., o(n2d)), recent approaches like the tsne construct a knearest neighbor graph instead and then project the graph into the 2d space. largevis follows this procedure, but uses a very efficient algorithm for knearest neighbor graph construction and a principled probabilistic model for graph visualization. next, we introduce the two components respectively. a knearest neighbor graph requires a metric of distance. we use the euclidean distance xi xj in the highdimensional space, the same as the one used by tsne. 1http://sebastien.pro/gephiesnam.pdf. highdimensional data points xii1,...,n , in which xi rd, constructing the exact knn graph takes o(n2d) time too costly. various indexing techniques have been proposed to approximate the knn graph (see section 2).. among these techniques, the random projection trees have been proved to be very efficient for nearestneighbor search in highdimensional data. the algorithm starts by partitioning the entire space and building up a tree. specifically, for every nonleaf node of the tree, the algorithm selects a random hyperplane to split the subspace corresponding to the nonleaf node into two, which become the children of that node. the hyperplane is selected through randomly sampling two points from the current subspace and then taking the hyperplane equally distant to the two points. this process continues until the number of nodes in the subspace reaches a threshold. once a random projection tree is constructed, every data point can traverse the tree to find a corresponding leaf node. the points in the subspace of that leaf node will be treated as the candidates of the nearest neighbors of the input data point. in practice multiple trees can be built to improve the accuracy of the nearest neighbors. once the nearest neighbors of all the data points are found, the knearest neighbor graph is built.. however, constructing a very accurate knn graph requires many trees to be built, which significantly hurts the efficiency. this dilemma has been a bottleneck of applying random projection trees to visualization. in this paper we propose a new solution: instead of building a large number of trees to obtain a highly accurate knn graph, we use neighbor exploring techniques to improve the accuracy of a less accurate graph. the basic idea is that a neighbor of my neighbor is also likely to be my neighbor [9]. specifically, we build a few random projection trees to construct an approximate knearest neighbor graph, the accuracy of which may be not so high. then for each node of the graph, we search the neighbors of its neighbors, which are also likely to be candidates of its nearest neighbors. we may repeat this for multiple iterations to improve the accuracy of the graph. in practice, we find that only a few iterations are sufficient to improve the accuracy of the knn graph to almost for the weights of the edges in the knearest neighbor graph, we use the same approach as tsne. the conditional probability from data xi to xj is first calculated as:. where the parameter i is chosen by setting the perplexity of the conditional distribution pi equal to a perplexity u. then the graph is symmetrized through setting the weight between xi and xj as:. the complete procedure is summarized in algo. once the knn graph is constructed, to visualize the data we just need to project the nodes of the graph into a 2d/3d space. we introduce a principled probabilistic model for this purpose. the idea is to preserve the similarities of the vertices in the lowdimensional space. algorithm 1: graph construction. data: xii1,...,n , number of trees nt , number of neighbors k, number of iterations iter. result: approximate knearest neighbor graph g. build nt random projection trees on xii1,...,n ; search nearest neighbors:. for each node i in parallel do search the random projection trees for i\u2019s k nearest neighbors, store the results in knn(i);. while t iter do set old knn() knn(), clear knn(); for each node i in parallel do. create max heap hi; for j old knn(i) do. for l old knn(j) do calculate dist(i, l) xi xl; push l with dist(i, l) into hi; pop if hi has more than k nodes; end. end put nodes in hi into knn(i);. end for each node i and each j knn(i) do. add edge (i, j) into graph g; end. calculate the weights of the edges according to eqn. 1, want to keep similar vertices close to each other and dissimilar vertices far apart in the lowdimensional space. given a pair of vertices (vi, vj), we first define the probability of observing a binary edge eij between vi and vj as follows:. where yi is the embedding of vertex vi in the lowdimensional space, f() is a probabilistic function w.r.t the distance of vertex yi and yj , i.e., yi yj when yi is close to yj in the lowdimensional space (i.e., yi yj is small), there is a large probability of observing a binary edge between the two vertices. in reality, many probabilistic functions can be used such as f(x) different probabilistic functions in section (3) only defines the probability of observing a binary edge between a pair of vertices. to further extend it to general weighted edges, we define the likelihood of observing a weighted edge eij wij as follows:. with the above definition, given a weighted graph g (v,e), the likelihood of the graph can be calculated as:. (5) in which e is the set of vertex pairs that are not observed and is an unified weight assigned to the negative edges. (5) models the likelihood of the observed edges, and by maximizing this part similar data points will keep close together in the lowdimensional space; the second part models the likelihood of all the vertex pairs without edges, i.e., negative edges. by maximizing this part, dissim. ilar data will be far away from each other. by maximizing the objective (5), both goals can be achieved. directly maximizing eqn. (5) is computationally expensive, as the number of negative edges is quadratic to the number of nodes. inspired by the negative sampling techniques [18], instead of using all the negative edges, we randomly sample some negative edges for model optimization. for each vertex i, we randomly sample some vertices j according to a noisy distribution pn(j) and treat (i, j) as the negative edges. we used the noisy distribution in [18]: pn(j) d0.75j , in which dj is the degree of vertex j. letting m be the number of negative samples for each positive edge, the objective function can be redefined as:. a straightforward approach to optimize eqn. (6) is stochastic gradient descent, which is problematic however. this is because when sampling an edge (i, j) for model updating, the weight of the edge wij will be multiplied into the gradient. when the values of the weights diverge (e.g., ranging from to thousands), the norms of the gradient also diverge, in which case it is very difficult to choose an appropriate learning rate. we adopt the approach of edge sampling proposed in our previous paper [23]. we randomly sample the edges with the probability proportional to their weights and then treat the sampled edges as binary edges. with this edge sampling technique, the objective function remains the same and the learning process will not be affected by the variance of the weights of the edges.. to further accelerate the training process, we adopt the asynchronous stochastic gradient descent, which is very efficient and effective on sparse graphs [19]. the reason is that when different threads sample different edges for model updating, as the graph is very sparse, the vertices of the sampled edges in different threads seldom overlap, i.e., the embeddings of the vertices or the model parameters usually do not conflict across different threads.. for the time complexity of the optimization, each stochastic gradient step takes o(sm), where m is the number of negative samples and s is the dimension of lowdimensional space (e.g., or 3). in practice, the number of stochastic gradient steps is typically proportional to the number of vertices n therefore, the overall time complexity is o(smn), which is linear to the number of nodes n we evaluate the efficiency and effectiveness of the largevis both quantitatively and qualitatively. in particular, we separately evaluate the performance of the proposed algorithms for constructing the knn graph and visualizing the graph. we select multiple largescale and highdimensional data sets of various types including text (words and documents), images, and networks including the following:. 20ng: the widely used text mining data set 20newsgroups2. we treat each article as a data point.. 2available at http://qwone.com/jason/20newsgroups/. note that although the original data sets all come with variety numbers of dimensions (e.g., size of the vocabulary for text documents), for comparison purposes we represent them with a fixed number of dimensions (e.g., 100) before applying any visualization techniques. this step is not required for largevis in practice, but learning an intermediate representation of the data can improve (e.g., smooth) the similarity structure of the original data. there are quite a few efficient embedding learning techniques (such as skipgram [18] and line [23]), the computational cost of which will not be a burden of the visualization. specifically, the representations of nodes in network data are learned through the line; the representations of words are learned through the line using a simple cooccurrence network; and the representations of documents are simply taken as the averaged vectors of the words in the documents. the vector representation of the image data is already provided from the source, so we do not further learn a new embedding.. we summarize the statistics of the above data sets in table next, we report the results of knn graph construction and graph visualization respectively. all the following results are executed on a machine with 512gb memory, cores at 2.13ghz. when multiple threads are used, the number of threads is always for visualization purposes, in all the experiments, we learn a 2d layout of the data.. 3available at http://yann.lecun.com/exdb/mnist/ 4https://en.wikipedia.org/wiki/wikipedia:database download 5available at http://dblp.unitrier.de/xml/ 6available at https://snap.stanford.edu/data/ we first compare the performance of different algorithms for knearest neighbor graph construction, including:. random projection trees [7]. we use the implementation of random projection trees in the annoy7 system.. vantagepoint trees [28]. this is the approach used by the tsne.. nndescent [9]. this is a representative neighbor exploring technique.. largevis. our proposed technique by improving random projection trees with neighbor exploration.. compares the performance of different algorithms for knn graph construction. the number of neighbors for each data point is set as for each algorithm, we try different values of its parameters, resulting in a curve of running time over accuracy (i.e., the percentage of data points that are truly knearest neighbors of a node). some results of the vantagepoint trees could not be shown as the values are too large. for largevis, only one iteration of neighbor exploring is conducted. overall, the proposed graph construction algorithm consistently achieves the best performance (the shortest running time at the highest accuracy) on all the four data sets, and the vantagepoint trees perform the worst. on the wikidoc data set, which contains around million data points, our algorithm takes only minutes to achieve accuracy while vantagepoint trees take hours, which is times slower. compared to the original random projection trees, the efficiency gain is also salient. on some data sets, e.g., livejournal and csauthor, it is very costly to construct a knn graph at a accuracy through random projection trees. however, with the neighbor exploring techniques, the accuracy of the graph significantly improves to near perfection.. how many iterations of neighbor exploring are required for largevis to achieve a good accuracy? presents the results of the accuracy of knn graph w.r.t the number of iterations of neighbor exploring. we initialize knn graphs with different levels of accuracy, constructed with different numbers of random projection trees. neighbor exploring is very effective. on wikidoc, the accuracy of the approximate knn graph improves from to almost with only one iteration of neighbor exploring. on livejounal, at most three iterations are needed to achieve a very high accuracy,. 7https://github.com/spotify/annoy. even if starting from a very inaccurate knn graph. similar results are also observed in other data sets.. our proposed algorithm for knn graph construction is very efficient, easily scaling to millions of data points with hundreds of dimensions. this solves the computational bottleneck of many data visualization techniques. next, we compare algorithms that visualize the knn graphs. all visualization algorithms use the same knn graphs constructed by largevis as input, setting the perplexity to and the number of neighbors for each data point to we compare the following graph visualization algorithms:. the approach of symmetric stochastic neighbor embedding. to scale it up for large graphs, the barneshut algorithm [26] is used for acceleration.. tsne [26]. the stateoftheart approach for visualizing highdimensional data, also accelerated through the barneshut algorithm.. line [23]. a largescale network/graph embedding method. although not designed for visualization purposes, we directly learn a 2dimensional embedding. firstorder proximity [23] is used.. largevis. our proposed technique for graph visualization introduced in section model parameters and settings. for the model parameters in sne and tsne, we set and the number of iterations to 1, 000, which are suggested by [26]. for the learning rate of tsne, we find the performance is very sensitive. different values and the optimal values on different data sets vary significantly. we report the results with the default learning rate and the optimal values respectively. for both line and largevis, the size of minibatches is set as 1; the learning rate is set as t (1 t/t ), where t is the total number of edge samples or minibatches. different values of initial learning rate is used by line and largevis: in line and in largevis. the number of negative samples is set as and is set as by default. the number of samples or minibatches t can be set proportional to the number of nodes. in practice, a reasonable number of t for million nodes is 10k million. the line and largevis can be naturally parallelized through asynchronously stochastic gradient descent. we also parallelize symmetric sne and tsne by assigning different nodes into different threads in each full batch gradient descent. the evaluation of data visualization is usually subjective. here we borrow the approach adopted by the tsne to evaluate the visualizations quantitatively [26]. we use a knn classifier to classify the data points based on their lowdimensional representations. the intuition of this evaluation methodology is that a good visualization should be able to preserve the structure of the original data as much as possible and hence yield a high classification accuracy with the lowdimensional representations.. comparing different probabilistic functions. we first compare different probabilistic functions in eq. (3), which define the probability of observing a binary edge between a pair of vertices based on the distance of their lowdimensional representations. we compare functions f(x) 1ax2 and f(x) 1exp(x2) with various values of a. presents the results on the wikidoc and the livejournal data sets. we can see among all probabilistic functions, f(x) 1x2 achieves the best result. tion specifies a longtailed distribution, therefore can also solve the crowding problem according to [27]. in the following experiments, we always use f(x) 1x2 results on different data sets we compare the efficiency and effectiveness of different. visualization algorithms. compares the classification accuracy with the knearest neighbor classifier by using the lowdimensional representations as features. for the knn classifier, different numbers of neighbors are tried. for tsne, both the results with default learning rate and the optimal learning rate tuned thorough exhaustively search are reported. on the small data sets 20ng and mnist, which contain less than 100,000 data points, the default. learning rate of tsne yields optimal performance, which is comparable to largevis. however, on the large data sets wikidoc and livejournal, which contain millions of data points, the largevis is more effective or comparable to the tsne with optimal learning rates, significantly outperforming tsne with default learning rate. however, empirically tuning the learning rate of tsne requires repeatedly training, which is very time consuming on the large data sets. the optimal learning rates of tsne on different data sets vary significantly. on the small data sets 20ng and mnist, the optimal learning rate is around 200, while on the large data sets wikidoc and livejournal, the optimal values become as large as comparing to tsne, the performance of largevis is very stable w.r.t the learning rate, the default value of which can be applied to various data sets with different sizes. we also notice that the performance of the line is very bad, showing that an embedding learning method is not appropriate for data visualization as is.. table compares the running time of tsne and largevis for graph visualization. on the small data sets 20ng and mnist, the two algorithms perform comparable to each other. however, on the large data sets, the largevis is much more efficient than the tsne. specially, on the largest data set livejournal, which contains million data points, the largevis is times faster than the tsne.. performance w.r.t. data size we further compare the performance of the largevis and. tsne w.r.t the size of the data in terms of both effectiveness and efficiency. presents the results on the wikidoc and livejournal data sets. different sizes of data sets are obtained by randomly sampling different percentages of the data. 6(a) and 6(b), we can see that as the size of the data increases, by using the default learning rates, the performance of the largevis increases while the performance of tsne decreases. by exhaustively tuning the learning rates, the performance of tsne will be comparable to largevis. however, this process is very time consuming, especially on largescale data sets. 6(c) and 6(d) show that the largevis becomes more and more efficient than tsne as the size of the data grows. this is because the time complexity of graph visualization in tsne is o(n log(n)) while that of largevis is o(n).. parameter sensitivity finally, we investigate the sensitivity of the parameters. in the largevis including the number of negative samples (m) and training samples (t). 7(a) shows the results w.r.t the number of negative samples. when the number of negative samples becomes large enough (e.g., 5), the performance becomes very stable. for each data point, instead of using all the negative edges, we just need to sample a few negative edges according to a noisy distribution. an interesting future direction is to design a more effective noisy distribution for negative edge sampling. 7(b) presents the results w.r.t the number of training samples. when the number samples becomes large enough, the performance becomes very stable. finally, we show several visualization examples so that we can intuitively evaluate the quality of largevis visualizations and compare the performance of tsne and largevis.. present the visualizations. different colors correspond to different categories (20ng), or clusters computed with kmeans based on highdimensional representations (wikiword, wikidoc, csauthors and livejournal). clusters are used for all the four data sets. we can see that on the smallest data set 20ng, the visualizations generated by the tsne and largevis are both meaning. ful and comparable to each other. on the large data sets such as wikidoc and livejournal, which contain at least million data points, the visualizations generated by the largevis look much more intuitive than the ones by tsne.. shows a region of the visualization of dblp papers generated by largevis. each color corresponds to a computer science conference. the visualization is very intuitive. the papers published at www are connected to the papers of www (companion volume), corresponding to its workshop and poster papers. the closest conference to www is icwsm, right to the north. this web cluster is close to sigir and ecir on the west (the information retrieval community), with three digital library conferences close by. kdd papers locate to the east of www, and the database conferences icde, sigmod, edbt and vldb are clustered to the south of kdd. it is interesting to see that the papers published at cikm are split into three different parts, one between sigir and www, and two between kdd and icde, respectively. this clearly reflects the three different tracks of the cikm conference: information retrieval, knowledge management, and databases.. this paper presented a visualization technique called the largevis which lays out largescale and highdimensional data in a lowdimensional (2d or 3d) space. largevis easily scales up to millions of data points with hundreds of dimensions. it first constructs a knearest neighbor graph of the data points and then projects the graph into the lowdimensional space. we proposed a very efficient algorithm for constructing the approximate knearest neighbor graphs and a principled probabilistic model for graph visualization, the objective of which can be optimized effectively and efficiently. experiments on realworld data sets show that the largevis significantly outperforms the tsne in both the graph construction and the graph visualization steps, in terms of both efficiency, effectiveness, and the quality. in the future, we plan to use the lowdimensional layouts generated by the largevis as the basis for more advanced visualizations and generate many intuitive and meaning visualizations for highdimensional data. another interesting direction is to handle data dynamically changing over time. the coauthor ming zhang is supported by the national natural science foundation of china (nsfc grant no. and 61272343); qiaozhu mei is supported by the national science foundation under grant numbers iis1054199 and ccf1048168. [1] m. bastian, s. heymann, m. jacomy, et al. open source software for exploring and manipulating networks. icwsm, 8:361362, [2] m. belkin and p. niyogi. laplacian eigenmaps and spectral techniques for embedding and clustering. in nips, volume 14, pages 585591, [3] j. l. bentley. multidimensional binary search trees used for associative searching. communications of the acm, 18(9):509517, [4] a. beygelzimer, s. kakade, and j. langford. cover trees for nearest neighbor. in proceedings of the 23rd international conference on machine learning, pages acm, [5] s. k. card, j. d. mackinlay, and b. shneiderman. readings in information visualization: using vision to think. morgan kaufmann, [6] m. s. charikar. similarity estimation techniques from rounding algorithms. in proceedings of the thiryfourth annual acm symposium on theory of computing, pages acm, [7] s. dasgupta and y. freund. random projection trees and low dimensional manifolds. in proceedings of the fortieth annual acm symposium on theory of computing, pages acm, [8] m. datar, n. immorlica, p. indyk, and v. s. mirrokni. localitysensitive hashing scheme based on pstable distributions. in proceedings of the twentieth annual symposium on computational geometry, pages acm, [9] w. dong, c. moses, and k. li. efficient knearest neighbor graph construction for generic similarity measures. in proceedings of the 20th international conference on world wide web, pages acm, [10] j. h. friedman, j. l. bentley, and r. a. finkel. an algorithm for finding best matches in logarithmic expected time. acm transactions on mathematical software (toms), 3(3):209226, [11] t. m. fruchterman and e. m. reingold. graph drawing by forcedirected placement. exper., 21(11):11291164, [12] a. gionis, p. indyk, r. motwani, et al. similarity search in high dimensions via hashing. in vldb, volume 99, pages 518529, [13] g. e. hinton and s. t. roweis. stochastic neighbor embedding. in advances in neural information processing systems, pages 833840, [14] m. jacomy, s. heymann, t. venturini, and m. bastian. forceatlas2, a continuous graph layout algorithm for handy network visualization. medialab center of research, 560, [15] i. jolliffe. principal component analysis. wiley online library, [16] d. keim et al. information visualization and visual data mining. visualization and computer graphics, ieee transactions on, 8(1):18, [17] s. martin, w. m. brown, r. klavans, and k. w. boyack. openord: an opensource toolbox for large graph layout. in ist/spie electronic imaging, pages international society for optics and photonics, [18] t. mikolov, i. sutskever, k. chen, g. s. corrado, and j. distributed representations of words and phrases and their compositionality. in advances in neural information processing systems, pages 31113119, [19] b. recht, c. re, s. wright, and f. niu. hogwild: a lockfree approach to parallelizing stochastic gradient descent. in advances in neural information processing systems, pages 693701, [20] s. t. roweis and l. k. saul. nonlinear dimensionality reduction by locally linear embedding. science, 290(5500):23232326, [21] c. silpaanan and r. hartley. optimised kdtrees for fast image descriptor matching. in computer vision and pattern recognition, ieee conference on, pages handbook of graph drawing and visualization. crc press, [23] j. tang, m. qu, m. wang, m. zhang, j. yan, and q. mei. line: largescale information network embedding. in proceedings of the 24th international conference on world wide web, pages international world wide web conferences steering committee, [24] j. b. tenenbaum, v. de silva, and j. c. langford. a global geometric framework for nonlinear dimensionality reduction. multidimensional scaling: i. theory and method. psychometrika, 17(4):401419, [26] l. van der maaten. accelerating tsne using treebased algorithms. the journal of machine learning research, 15(1):32213245, [27] l. van der maaten and g. hinton. visualizing data using tsne. journal of machine learning research, 9(25792605):85, [28] p. n. yianilos. data structures and algorithms for nearest neighbor search in general metric spaces.", "summary": " largevis a technique to visualize largescale and highdimensional data in lowdimensional space.. problem relates to both information visualization and machine learning (and data mining) domain.. [link to the paper]([url]). state of the art method for visualization problem.. start by constructing a similarity structure from the data and then project the structure to 2/3 dimensional space.. an accelerated version proposed that uses a knearest neighbor (knn) graph as the similarity structure.. limitations. computational cost of constructing knn graph for highdimensional data.. efficiency of graph visualization techniques breaks down for large data (o(nlogn) complexity).. parameters are sensitive to the dataset.. constructs knn graph (in a more efficient manner as compared to tsne).. uses a principled probabilistic model for graph visualization.. let us say the input is in the form of n datapoints in d dimensional space.. knn graph construction. random projection tree used for nearestneighborhood search in the highdimensional space with euclidean distance as metric of distance.. tree is constructed by partitioning the entire space and the nodes in the tree correspond to subspaces.. for every nonleaf node of the tree, select a random hyperplane that splits the subspace (corresponding to the leaf node) into two children.. this is done till the number of nodes in the subspace reaches a threshold.. once the tree is constructed, each data point gets assigned a leaf node and points in the subspace of the leaf node are the candidate nearest neighbors of that datapoint.. for high accuracy, a large number of trees should be constructed (which increases the computational cost).. the paper counters this bottleneck by using the idea a neighbor of my neighbor is also my neighbor to increase the accuracy of the constructed graph.. basically construct an approximate knn graph using random projection trees and then for each node, search its neighbors neighbors as well.. edges are assigned weights just like in tsne.. probabilistic model for graph visualization. given a pair of vertices, the probability of observing an edge between them is given as a function of the distance between the projection of the pair of vertices in the lower dimensional space.. the probability of observing an edge with weight w is given as wth power of probability of observing an edge.. given a weighted graph, g, the likelihood of the graph can be modeled as the likelihood of observed edges plus the likelihood of negative edges (vertex pairs without edges).. to simplify the objective function, some negative edges are sampled corresponding to each vertex using a noisy distribution.. the edges are sampled with probability proportional to their weight and then treated as binary edges.. the resulting objective function can be optimized using asynchronous stochastic gradient descent (very effective on sparse graphs).. the overall complexity is o(smn), s is the dimension of lower dimensional space, m is the number of negative samples and n is the number of vertices.. data is first preprocessed with embedding learning techniques like skipgram and line and brought down to 100dimensional space. one limitation is that the data is preprocessed to reduce the number of dimensions to this seems to go against the claim of scaling for hundreds of dimensions.. knn construction is fastest for largevis followed by random projection trees, nn descent, and vantage point trees (used in tsne).. largevis requires very few iterations to create highly accurate knn graphs.. knn classifier is used to measure the accuracy of graph visualization quantitatively.. compared to tsne, largevis is much more stable with respect to learning rate across datasets.. largevis benefits from its linear complexity (as compared to tsnes log linear complexity) and for default learning rate, outperforms tsne for larger datasets."}, {"document": "the madry lab recently hosted a competition designed to test the robustness of their adversarially trained mnist model. attacks were constrained to perturb each pixel of the input image by a scaled maximal l distortion this decision discourages the use of attacks which are not optimized on the l distortion metric. our experimental results demonstrate that by relaxing the l constraint of the competition, the elasticnet attack to deep neural networks (ead) can generate transferable adversarial examples which, despite their high average l distortion, have minimal visual distortion. these results call into question the use of l as a sole measure for visual distortion, and further demonstrate the power of ead at generating robust adversarial examples. deep neural networks (dnns) achieve stateoftheart performance in various tasks in machine learning and artificial intelligence, such as image classification, speech recognition, machine translation and gameplaying. despite their effectiveness, recent studies have illustrated the vulnerability of dnns to adversarial examples (szegedy et al., 2013; goodfellow et al., 2015). for instance, a carefully designed perturbation to an image can lead a welltrained dnn to misclassify. even worse, effective adversarial examples can also be made virtually indistinguishable to human perception. adversarial examples crafted to evade a specific model can even be used to mislead other models trained for the same task, exhibiting a property known as transferability.. to address this problem, the adversarial robustness of neural networks was studied through the lens of robust optimization in (madry et al., 2017), leading to an adversarially robust model for image classification we term as the madry defense model. an attack challenge1 was proposed for the mnist dataset to test the defense, however, attacks were constrained to perturb each pixel by at most 0.3, a scaled maximal l distortion. this rule greatly reduces the power of attacks which are not optimized for the l distortion metric, and imposes an unrealistic constraint on the attacker.. to justify the limitation of the l constraint, we conduct extensive experiments on the madry defense model to investigate the transferability properties of the stateoftheart adversarial attacks without the l constraint. we find that l1based adversarial examples generated by ead, which is short for elasticnet attack to dnns (chen et al., 2017), readily transfer in both the targeted and nontargeted cases, and despite the high l distortion, the visual distortion on the adversarial examples is minimal. these results call into question the use of l to quantify visual distortion, and further demonstrate the stateoftheart transferability properties of ead. the madry defense model is a sufficiently high capacity network trained against the strongest possible adversary, which they deem to be projected gradient descent (pgd) starting from random perturbations around the natural examples (madry et al., 2017). we evaluate using the mnist model,. 1https://github.com/madrylab/mnistchallenge. as generating visually imperceptible adversarial examples using this dataset is difficult due to its simple, grayscale nature. if this attack objective can be achieved, our methodology can be applied to more realistic, color datasets, like cifar10 and imagenet (deng et al., 2009). for the mnist model, iterations of pgd were run, with a step size of gradient steps were taken in the l norm. the network was trained and evaluated against perturbations of size no greater than we experimentally show in the supplementary material that this is the maximum for which the madry defense model can be successfully adversarially trained.. we test the transferability properties of attacks in both the targeted case and nontargeted case against the adversarially trained model. we test the ability for attacks to generate transferable adversarial examples using a single undefended model as well as an ensemble of undefended models, where the ensemble size is set to we expect that if an adversarial example remains adversarial to multiple models, it is more likely to transfer to other unseen models (liu et al., 2016; papernot et al., 2016). we discuss results transferring from an ensemble in the following sections, and provide results transferring from a single undefended model in the supplementary material. the undefended models we use are naturally trained networks of the same architecture as the defended model; the architecture was provided in the competition. for generating adversarial examples, we compare the optimizationbased approach and the fast gradientbased approach using ead and pgd, respectively.. ead generalizes the stateoftheart cw l2 attack (carlini wagner, 2017) by performing elasticnet regularization, linearly combining the l1 and l2 penalty functions (chen et al., 2017). the hyperparameter controls the tradeoff between l1 and l2 minimization. we test ead in both the general case and the special case where is set to 0, which is equivalent to the cw l2 attack. we tune the confidence hyperparameter, which controls the necessary margin between the predicted probability of the target class and that of the rest, in our experiments. full detail on the implementation is provided in the supplementary material.. for l attacks, which we will consider, fast gradient methods (fgm) use the sign of the gradientj of the training loss j with respect to the input for crafting adversarial examples (goodfellow et al., 2015). iterative fast gradient methods (ifgm) iteratively use fgm with a finer distortion, followed by an ball clipping (kurakin et al., 2016). we note that this is essentially projected gradient descent on the negative loss function. we test pgd both with and without random starts, and as the classic ifgm formulation does not include random starts, pgd without random starts shall be termed ifgm for the remainder of the paper. we perform grid search to tune , which controls the allowable l distortion. full detail on the implementation is provided in the supplementary material. in our experiment, random samples from the mnist test set were used. for the targeted case, a target class that is different from the original one was randomly selected for each input image. the results of the tuning for ead is provided in the supplementary material. it is observed that the highest attack success rate (asr) in both the targeted and nontargeted cases was yielded at this was in fact the largest tested, indicating the importance of minimizing the l1 distortion. for generating transferable adversarial examples. furthermore, as can be seen in the supplementary material, the improvement in asr with increasing is more significant at lower , indicating the importance of minimizing the l1 distortion for generating transferable adversarial examples with minimal visual distortion. for pgd and ifgm, was tuned from to at increments, full results of which are provided in the supplementary material.. in table 1, the results for tuning for cw and ead at are provided, and are presented with the results for pgd and ifgm at the lowest values at which the highest asr was yielded, for comparison. it is observed that in both the targeted case and the nontargeted case, ead outperforms cw at all furthermore, in the targeted case, at the optimal 50, ead\u2019s adversarial examples surprisingly have lower average l2 distortion.. in the targeted case, ead outperforms pgd and ifgm at with much lower l1 and l2 distortion. in the nontargeted case, pgd and ifgm yield similar asr at lower l distortion. however, we argue that in the latter case the drastic increase in induced l1 and l2 distortion of pgd and ifgm to generate said adversarial examples indicates greater visual distortion, and thus that the generated examples are less adversarial in nature. we examine this claim in the following section. adversarial examples generated by ead and pgd were analyzed in the nontargeted case, to understand if the successful examples are visually adversarial. a similar analysis in the targeted case is provided in the supplementary material. we find that adversarial examples generated by pgd even at low such as 0.3, at which the attack performance is weak, have visually apparent noise.. in figure 1, adversarial examples generated by ead are directly compared to those generated by pgd with similar average l distortion. ead tuned to the optimal (0.01) was used to generate adversarial examples with as can be seen in table 1, the average l distortion under this configuration is therefore, adversarial examples were generated by pgd with clearly, performing elasticnet minimization aids in minimizing visual distortion, even when the l distortion is large. furthermore, these results also emphasize the issues in solely using l to measure visual distortion, and how setting such a distortion constraint for adversarial training is not sufficient for characterizing the subspaces of visually similar adversarial examples. the madry lab developed a defense model by focusing on training a sufficiently highcapacity network and using the strongest possible adversary. by their estimations, this adversary was pgd, which they motivate as the strongest attack utilizing the local firstorder information about the network. through extensive experiments, we demonstrate that ead is able to outperform pgd in transferring in the targeted case. we also show that ead is able to generate much less visually distorted transferable adversarial examples than pgd with comparable l distortion, due to the drastic reduction in l1 and l2 distortion. these results demonstrate the power of ead, particularly in its transferability capabilities. furthermore, these results indicate the drawbacks of using l as the sole distortion metric, and suggest incorporating l1 or l2 to complement the subspace analysis of adversarial examples for possible defenses such as adversarial training and adversary detection. the mnist model released by the madry lab is adversarially trained using pgd with therefore, it is expected that the existing madry defense model performs poorly against pgd attacks with in order to rectify this issue, the madry defense model was trained with and evaluated against a pgd adversary with tuned from to under increments. as suggested in (madry et al., 2017), the pgd attack was run for iterations, and to account for varying , the stepsize was set to /40.. the adversarial retraining results are shown in table these results suggest that the madry defense model can not be successfully adversarially trained using a pgd adversary with this is understandable as with such large , the visual distortion is clearly perceptible. the targeted attack formulations are discussed below, nontargeted attacks can be implemented in a similar fashion. we denote by x0 and x the original and adversarial examples, respectively, and denote by t the target class to attack. ead generalizes the stateoftheart cw l2 attack (carlini wagner, 2017) by performing elasticnet regularization, linearly combining the l1 and l2 penalty functions (chen et al., 2017). the formulation is as follows:. minimizex c f(x, t) x x01 x x022 subject to x [0, 1]p, (1). f(x, t) maxmax j 6t [logit(x)]j [logit(x)]t,, (2). by increasing , one trades off l2 minimization for l1 minimization. when is set to 0, ead is equivalent to the cw l2 attack. by increasing , one increases the necessary margin between the predicted probability of the target class and that of the rest. therefore, increasing improves transferability but compromises visual quality.. we implement binary search steps on the regularization parameter c (starting from 0.001) and run i iterations for each step with the initial learning rate for finding successful adversarial examples, we use the adam optimizer for the cw attack and implement the projected fista algorithm with the squareroot decaying learning rate for ead (kingma ba, 2014; beck teboulle, 2009). fast gradient methods (fgm) use the gradient j of the training loss j with respect to x0 for crafting adversarial examples (goodfellow et al., 2015). for l attacks, which we will consider, x is crafted by. x x0 sign(j(x0, t)), (3) where specifies the l distortion between x and x0, and sign(j) takes the sign of the gradient. iterative fast gradient methods (ifgm) were proposed in (kurakin et al., 2016), which iteratively use fgm with a finer distortion, followed by an ball clipping. we note that this is essentially projected gradient descent on the negative loss function.. consistent with the implementation in (madry et al., 2017), steps were used. for pgd without random starts, or ifgm, the step size was set to be /40, which has been shown to be an effective attack setting in (tramr et al., 2017). for pgd with random starts, the step size was set to be /40 in order to allow all pixels to reach all possible values. in figure 2, a selected instance (a handwritten 1\u2019) of adversarial examples crafted by pgd at tuned from to at increments is shown. as pgd simply operates on a l distortion budget, as increases, the amount of noise induced to all of the pixels in the image increases. this noise is visually obvious even at low such as 0.3, where the adversarial example does not even successfully transfer. at higher values, pgd appears to change the 1\u2019 to a 3\u2019. however, under such large visual distortion, it is not entirely clear. in figure 3, adversarial examples generated by ead are directly compared to those generated by pgd with similar average l distortion in the targeted case. adversarial examples generated by ead tuned to the optimal (0.01) at have an average l distortion of therefore, these examples were compared to those generated by pgd with and it can be seen that further distortion is needed to generate transferable adversarial examples in the targeted case, however, the benefit that elasticnet minimization applies for improving visual quality is still clearly evident. in table 3, the results of tuning for cw and ead at the optimal (0.01) are provided, and are presented with the results for pgd and ifgm at the lowest where the highest asr was yielded, for comparison. alike the results obtained when using an ensemble of models, it is observed that ead outperforms cw at all in the targeted case, at 50, both ead and cw outperform pgd and ifgm with much lower l1 and l2 distortion. furthermore, at 50, ead\u2019s adversarial examples unexpectedly have lower average l2 distortion than cw\u2019s. in the nontargeted case, pgd and ifgm outperform ead in terms of asr. however, as can be inferred by the large l1 and l2 distortion, the perturbations are visually perceptible. in table 4, the results of tuning when generating adversarial examples with ead attacking an ensemble of models are presented. in table 5, the results of tuning when generating adversarial examples with ead attacking a single model are presented. in both cases, one can see that at the optimal in terms of asr, tuning plays relatively little effect in improving the asr. however, at lower , which is applicable when visual quality is important, increasing plays a rather significant role in improving the asr. this suggests that for generating transferable adversarial examples with minimal visual distortion, minimizing the l1 distortion is more important than minimizing l2, or, by extension, l. in figure 4, the results of tuning when generating adversarial examples with pgd and ifgm attacking an ensemble of models are shown. in figure 5, the results of tuning when generating adversarial examples with pgd and ifgm attacking a single model are shown. the results simply show that by adding large amounts of visual distortion to the original image, an attacker can cause a target defense model to misclassify. however, when compared to ead, the adversarial examples of pgd and ifgm are less transferable and, when successful, the perturbations are more visually perceptible, as indicated by the drastic increase in the l1 and l2 distortion at high", "summary": "sharma and chen provide an experimental comparison of different stateoftheart attacks against the adversarial training defense by madry et al. they consider several attacks, including the carlini wagner attacks [2], elastic net attacks [3] as well as projected gradient descent [1]. their experimental finding that the defense by madry et al. can be broken by increasing the allowed perturbation size (i.e., epsilon) should not be surprising. every network trained adversarially will only defend reliable against attacks from the attacker used during training.. [1] a. madry, a. makelov, l. schmidt, d. tsipras, and a. vladu. towards deep learning models resistant to adversarial attacks. arxiv, 1706.06083, [2] n. carlini and d. wagner. towards evaluating the robustness of neural networks.inieee symposiumon security and privacy (sp), 3957., [3] p.y. chen, y. sharma, h. zhang, j. yi, and c.j. ead: elasticnet attacks to deep neuralnetworks via adversarial examples. also find this summary at [davidstutz.de]([url]/)."}, {"document": "reproducibility is the ability of recreating identical binaries under prede ned build environments. due to the need of quality assurance and the bene t of be er detecting a acks against build environments, the practice of reproducible builds has gained popularity in many opensource so ware repositories such as debian and bitcoin. however, identifying the unreproducible issues remains a labour intensive and time consuming challenge, because of the lacking of information to guide the search and the diversity of the causes that may lead to the unreproducible binaries. in this paper we propose an automated framework called reploc to localize the problematic les for unreproducible builds. reploc features a query augmentation component that utilizes the information extracted from the build logs, and a heuristic rulebased ltering component that narrows the search scope. by integrating the two components with a weighted le ranking module, reploc is able to automatically produce a ranked list of les that are helpful in locating the problematic les for the unreproducible builds. we have implemented a prototype and conducted extensive experiments over realworld unreproducible debian packages in four di erent categories. by considering the topmost ranked le only, reploc achieves an accuracy rate of if we expand our examination to the top ten ranked les in the list produced by reploc, the accuracy rate becomes considering that there are hundreds of source code, scripts, make les, etc., in a package, reploc signi cantly reduces the scope of localizing problematic les. moreover, with the help of reploc, we successfully identi ed and xed six new unreproducible packages from debian and guix.as an indicator of the ability that the binaries could be recreated consistently from source, recent years have witnessed the emerging idea of reproducible builds. given the source les, the reproducibility is described as the ability of building identical binary under prede ned build environments [15]. in this study, source les include source code, scripts, make les, build con gurations, etc [6]. checking the reproducibility of so ware creates a veri able linkage that bridges the gap between the readable source les and the binary packages, which is important from various perspectives. firstly, reproducibility is very important for the safety of build environments. for so ware ecosystems, a acks against the build environment may lead to serious consequences. by compromising the system to produce packages with backdoors [26,45], malicious behaviors such as trusting trust a ack [41] may be introduced during the build time. for example, in 2015, over 4,000 ios applications were infected by a counterfeit version of apples xcode development environment (known as xcodeghost) [1]. xcodeghost injected malicious code during compiling time so that developers unknowingly distributed malware embedded in their applications [21]. obviously, a solution is to ensure that the same source les always lead to the same binary packages so that an infected different binary immediately raises alarms. unfortunately, a major obstacle of detecting such a acks lies in the transparency gap between the source les and their compiled binary packages. due to nondeterministic issues such as timestamps and locales, it is not uncommon that rebuilding an application yields di erent binaries even within secure build environments. erefore, these kinds of a acks o en elude detection because di erent binaries of the same application is normal. besides detecting a acks against build environments, validating the reproducibility is also helpful in debugging and nding certain releasecritical bugs (e.g., libicaldev 1.01.1) [2]. furthermore, in the context of and continuous integration and so ware upgrade [37], reproducible packages could be helpful in caching, and reducing redundant operations, e.g., by eliminating the necessity of delivering the di erent binaries compiled from the same source les. due to the signi cant bene ts, many opensource so ware repositories have initiated their validation processes. ese repositories include gnu/linux distributions such as debian and guix, as well as so ware systems like bitcoin [19]. for instance, since 2014, the number of debians reproducible packages has been steadily increasing. figure presents the trend of the reproducible builds in debian [14]. as of august 2017, over of debians packages could be reproducibly built. despite the e ort towards reproducibility, many packages remain unreproducible. for example, according to debians bug tracking system (bts), as of august 23, 2017, there are 2,342 packages that are not reproducible [14] for the unstable branch targeting the amd64 architecture. such large number of unreproducible packages implies the challenges in detecting and then xing the unreproducible issues. in particular, the localization task for the problematic les is the activity of identifying the source les that cause unreproducibility, which ranks source les based on their likelihood of containing unreproducible issues. currently, the localization task is mostly manually conducted by developers. since there may be hundreds to thousands of source les for a package, the localization tends to be labor intensive and time consuming. to address this problem, we consider the source les as text corpus, and leverages the di log generated by comparing the di erent binaries to guide the search. as such, the localization of the problematic les can be modeled as a classic information retrieval (ir) problem: given the source les and the di log, determine those problematic les from the source les that are relevant to the unreproducible issues. e ir model has the potential to automate the localization task. however, the localization task is challenging, due to its unique characteristics. first, the information for locating the problematic les within the source les is very limited. e di log generated by comparing the di erent binaries, which is considered as the input of the ir process, may not be su ciently informative. we call this challenge an information barrier. in addition, there are many causes that may lead to unreproducible builds, such as embedding timestamps in les and recording le lists in nondeterministic order. e detailed issues are manually listed in debians documentation [12]. moreover, the diverse types of les in a package also add to the complexity of localizing the problematic les, which may reside in not only the source code, but also other types of les such as scripts, make les and build con gurations. we call this challenge a diversecause barrier. to break through the barriers, we propose a localization framework called reploc that targets the localization task in search of problematic les for unreproducible builds. given an unreproducible package with two di erent built binaries as the input, reploc produces a list of ranked source les. reploc features two components that address the two aforementioned challenges. for the information barrier, we develop a ery augmentation (qa) component that utilizes the information extracted from the build logs to enhance the quality of the queries (represented by the le names extracted from the di logs, see section 2). for the diversecause barrier, we develop a heuristic rulebased filtering (hf) component. more speci cally, we propose heuristic rules that are obtained by summarizing the information presented in debians documents. furthermore, we employ a weighted file ranking (fr) component to combine the qa and hf components, and build an integrated framework to automate the localization of the problematic les for unreproducible builds. to evaluate reploc, we have collected a realworld dataset that consists of unreproducible packages. since these packages were later xed with patches from debians bts, we know exactly which les caused the unreproducibility and thus can use the facts to evaluate the accuracy of reploc. if we consider the topmost ranked le only, reploc achieves an accuracy rate of if we expand the range to include top ten ranked les, the accuracy rate becomes generated by diffoscope, h ps://di oscope.org table 1: snippet of altered environment variations first build second build env tz /usr/share/zoneinfo/etc/gmt12 /usr/share/zoneinfo/etc/gmt14 env lang c fr ch.utf8 env language en us:en fr ch:fr env builddir /build/1st /build/2nd for other metrics such as precision and recall, reploc also outperforms the comparative approaches signi cantly. to further evaluate the e ectiveness of our approach, we use reploc on unreproducible packages that have never been xed before. with the help of reploc, we successfully identi ed the problematic les, then manually xed the unreproducible issues over three debian packages. moreover, the usefulness of reploc is examined over a di erent so ware repository (guix [11] in this study). under the guidance of reploc, problematic les for three unreproducible packages from guix are detected and xed. is paper makes the following main contributions. to the best of our knowledge, this is the rst work to address the localization task for unreproducible builds. we propose an e ective framework reploc that integrates heuristic ltering and query augmentation. a prototype has been implemented based on the approach. we have evaluated reploc on unproducibile packages that were later xed in the debian repository. e experimental results show that reploc is e ective. we have made the benchmarks publicly available at h ps://reploc.bitbucket.io. under the guidance of reploc, we xed six unreproducible packages from debian and guix, and submi ed the patches to the btss of the two repositories. among the submi ed patches, four have been accepted. e rest of this paper is organized as follows. in section 2, we give the background of this work. our approach is presented in section 3, followed by experimental study in section e threats to validity and related work are described in sections finally, section concludes the paper. taking debian as a typical example, figure illustrates the common work ow of validating the reproducibility of packages [17]. first, the source les are compiled under two prede ned build environments (steps 12). more speci cally, the build environments are constructed by se ing up altered environment variables or so ware con gurations. for instance, within debians continuous integration system, altered environment variables include locales, timezones, user privileges, etc. table presents a snippet of the altered environment (see [22] for more detailed information). two versions of binaries can be generated with respect to each environment. e two versions are then compared against each other (step 3). if they are not bittobit identical, the localization of problematic les that lead to unreproducible builds is required, based on the di log and the source les (step 4). e build and the comparison procedures (steps 13) can easily be automated, but the localization (step 4) mainly relies on the developers. unfortunately, manual e ort to identify the les that lead to unreproducible builds is nontrivial. as shown in figure 2, the di logs are the major source of the information to guide the localization of the problematic les, which, unfortunately, are not always su ciently informative. figure gives a snippet of the di log for dietlibc, a libc implementation optimized for small size. in the original version (0.33cvs201203256), a static library le di ers between the two versions during the build time (/usr/lib/diet/lib/libcompat.a). as shown in the di log, diffoscope indicates the di erence via the output of the gnu binary utility readelf. however, since the di content may not be well comprehensible (e.g., lines in figure 3), we do not leverage such information in this study. meanwhile, figure presents a snippet of a problematic le (/makefile) and the patch that xes the issue. in figure 4(b), line indicates that the root cause of the unreproducibility lies in the nonstable order of the object les, which are fed to the ar utility to generate libcompat.a (lines of figure 4(a)). e di culty in this example is that, the di log may fail to provide su cient information. ough it is possible to match the correct le with only the le figure 4(a), chances are that other irrelevant les containing the same le name might be matched as well. e aforementioned example illustrates how problematic les can be detected and xed. in reality there are multiple altered build con gurations and can be many corresponding causes that lead to unreproducible builds. for example, changing the timezone environment variable (env tz) may cause the c/c packages that embed date macro to be unreproducible, and the locale environment variable (env lc ) may trigger unreproducible issues of packages that capture the text generated by programs. ese diverse unreproducible causes make the localization task di cult. in this section, we discuss the details of reploc. e upper part of figure depicts the qa component, which enriches the queried information by matching the les in the di log and the build logs, to tackle the information barrier. first, the di log is generated using diffoscope. en, the query extraction module takes the di log as the input, and generates the basic query. in this study, the basic query consists of the le names in the di log. as mentioned, due to the information barrier, the information that can be utilized to localize the problematic les is limited other than a list of les that are di erent within the two build processes. us, we enhance the quality of the queries with the build command retrieval module. e motivation for this module is that, during the build process, the build information such as the executed commands can be obtained. moreover, based on the cooccurrence relationship between the le names in the di log and the build commands, we can identify the build commands with which the les mentioned in the di log are built. hence, it is rational to augment the query by supplementing the build commands from the build log. figure illustrates a snippet of the build log of the exemplifying package dietlibc. it can be observed that the build log is more informative and provides supplementary information with respect to the di log. more speci cally, we rst split the build log into build command segments, with respect to the entering/leaving directory tags generated by make (e.g., lines and of figure 6). with this operation, the commands invoked under the same directory can be grouped together, as a le of the augmentation corpus (denoted as a command le). note that though there are two versions of build logs with respect to the two build environments, since we are interested in the build command, the choice of either version of build log does not have an impact on the results. en, the relevant les in the corpus are obtained by utilizing an ir model. in essence, any ir model can be adopted. in this study, we employ the vector space model (vsm), due to its simplicity and e ectiveness. to realize the vsm based augmentation, we calculate the cosine similarity between basic query and the command les. erea er, the matched commands from the most relevant command les are obtained. in particular, for the vsm model, we assign weight value for each le with the tfidf (term frequencyinverse document frequency) measurement, which is widely used in ir [32]. in this paper, for a term t in a document d, its tf indicates the number of ts occurrences in d, n t denotes the number of les in which t appears, and n means the number of source les. with tfidf de ned, each le is represented as a vector, and the cosine similarity with the basic query is used to rank the command les. where l s represents the inner product of the basic query and the command le, and l s denotes the product of 2norm of the vectors. a er that, the basic query and the retrieved contents, which are commands executed during the build process, are concatenated together as the enhanced query. running example: for dietlibc, all the le names in the di log, e.g., ./usr/lib/diet/lib/libcompat.a, are extracted as the basic query. en, within the augmentation, ar cru binx86 64/libcompat.a [. (line of figure 6) and the build commands in the same command le are retrieved. finally, the contents of the retrieved command les are appended a er the basic query, as the nal query. e hf component is designed to capture the problematic les by incorporating the domain knowledge, which is represented as frequently observed pa erns. in hf, the heuristic rules are constructed based on the following criteria: (1) e rules are manually constructed based on debians documentation [13]. (2) e rules are summarized for the four major categories of unreproducible issues (see setcion 4.2). we traverse the notes in the documentation, and capture those issues that are described as perl compatible regular expression (pcre). for example, invoking gzip without n argument could be expressed using the negative assertions feature of pcre (rule in table 2). meanwhile, as a counterexample, the timestamps embedded in portable executable (pe) binaries are hard to be identi ed by heuristic rules or even by developers [20]. a er manual inspection based on the criteria, we obtain heuristic rules, which are presented in table 2, and described as follows: (1) time macro: using c time preprocessing macro in source les will embed di erent timestamps when compiled at di erent times. (2) date macro: embedding c date preprocessing macro in source les is similar as the previous case. (3) gzip arg: if applying gzip without n argument, timestamps will be embedded in the header of the nal compressed le. 14unsorted wildcard: using wildcard in make les without sorting, similar with pl unsorted key. by applying the rules over the source les (e.g., with gnu grep r p), we obtain a subset of les that may lead to unreproducible builds. note that these rules equally treat the source les as plain text, rather than consider the le types (e.g., based on le extension). e reason is that the unreproducible issues may reside in snippets or templates that do not follow le extension conventions, which are eventually embedded into unreproducible binaries. based on such consideration, we do not sort the matched les in hf. running example: for dietlibc, there are in total ve problematic les, namely, /libpthread/makefile, /libdl/makefile, /debian/rules, implicit, and /makefile. among these les, /makefile (see figure 4(b)) can be captured by the unsorted wildcard rule, in which sort does not appear before wildcard. however, we should note that there may be false alarms, e.g., for unexecuted commands or text in the comments. consequently, hf may fail to place the matched problematic les at the top of the list. e motivations behind the combination of hf and qa are twofold: (1) e heuristic rules in hf focus on the static aspect of the source les, i.e., treat all the source les in a uni ed way, and capture the suspicious les that match the de ned pa erns. such mechanism can handle various le types. unfortunately, there may be false alarms, especially for those les unused during the build process. (2) e build log based augmentation takes the dynamic aspect of the build process into consideration. with qa, we concentrate on the commands invoked during the build process. hence, by combining the mechanisms, we can strengthen the visibility of the problematic les that lead to unreproducible builds. in the fr component, these goals are realized as follows. first, with the augmented query, the relevant les are obtained with the 1, the vsm model is adopted to calculate the similarity values between the augmented query and each source le. second, since we have acquired both the les retrieved by hf and the similarity values between source les and the augmented query, in the le ranking module, it is natural to combine these two types of information, to be er capture the problematic les. for example, we can modify equation and apply sim to rank the source les: where w s for those source les matched by the hf component, and w s otherwise. [0, 1] is a weight parameter to balance the two terms, e.g., large values make reploc favor the hf component. with equation 2, the source les are ranked according to their modi ed similarity to the augmented query, and the top ranked les are returned as the nal results of reploc. we should note that, in this study, we adopt the lelevel localization paradigm, in that the xing for many unreproducible packages is not unique. for instance, statements declaring missing environmental variables can appear anywhere in the le before it is needed. hence, it is di cult to establish linelevel groundtruth. in algorithm 1, we present the pseudocode of reploc, which combines qa (lines 26), hf (lines 710), and fr (lines 1116) sequentially. running example: in table 3, we present the top ve les retrieved by reploc and its individual components. from the table, we can observe that without augmenting the query, fr is able to retrieve two problematic les. however, the topmost ranked le is a changelog (/changes), in that the le names in the di log appear in this le. in contrast, with the query augmented, fr (with qa) is able to rank the two problematic les at the top of the list. meanwhile, although hf is able to capture /libpthread/makefile, the le is not assigned top rank due to other false alarms, e.g., /t.c. finally, by combining fr, qa, and hf, reploc is able to locate four problematic les. in this study, we intend to systematically analyze reploc, by investigating the following research estions (rqs): rq1: is reploc sensitive to the weighting parameter ? rq2: how e ective is reploc? rq4: is reploc helpful in localizing un xed packages? among these rqs, rq1 concentrates on the impact of the weighting scheme between the components in reploc. rq2 focuses on how well reploc performs in terms of di erent quality metrics. rq3 examines whether reploc is time consuming, and rq4 investigates the replocs generalization. in this study, the dataset is constructed by mining debians bts. to the best of our knowledge, debian is the only repository providing both pastversion packages and reproducibilityrelated patches, which are crucial for generating the corpus and the ground truth. consequently, all the packages within the dataset are extracted from debian bts, which are tagged as unreproducible by bug reporter via debtags, i.e., the command line interface for accessing the bts. according to debians documentation, there are categories of reproducible issues [16]. ere are also two special categories indicating the packages that fail to build from source, and the toolchain issues (nondeterministic issues introduced by other packages, see section 5), which are not considered in this study. we download all the categories of bug reports, and download the packages, with their corresponding patches. en, we apply the validation tool kit, to obtain the corresponding di logs and build logs. in this study, we consider those categories with more than packages. with such criterion, we obtain packages in the dataset, which fall into the four largest categories. figure 7(a) illustrates the statistics of the dataset. in the gure, we present the numbers of the open and closed bugs in debians bts, as well as the number of packages in the dataset. among the four categories of packages, the timestamps category contains the most packages (462), followed by fileordering (118), randomness (50), and locale (41). for all the four categories of packages that are labeled as done, the packages in the dataset take a portion of note that there are less packages in the dataset than closed bug reports, since packages may not be compilable due to the upgrade of their dependencies. in figure 7(b), we illustrate the statistics of the patches in the dataset. from the gure, we could observe that there are many types of les that might be involved in the unreproducible builds. for these les, the debian rules les, which are the main build scripts, take the largest portion of the xed les (29.82). auxiliary les, such as the configure scripts and input les (.in), takes the second largest portion (17.21). a er that, there are the make les (11.68), scripts such as python/perl/php les (14.60), c/c les (5.94), xml les (4.80), implicit build les (2.71). since we classify the les based on their le extensions heuristically, there are also of the les that are not easy to classify, e.g, those without le extensions. is phenomenon conforms with the second barrier mentioned in section 1, i.e., the causes to the unreproducible builds are diverse, which makes the localization task very challenging. reploc is implemented in perl and java all the experiments are conducted on an intel core i7 ghz cpu server with gb memory, running gnu/linux with kernel for the comparative algorithms, we consider four variants of reploc, since there is no prior approach addressing this problem. e rst two variants implement two baseline algorithms, which only consider either the hf or the fr model (denoted as reploc(hf) and reploc(fr)). ese two variants are incorporated to examine the performance of its buildingblock components. moreover, reploc(fr) could be considered the simulation of the manual localization, since in fr, the retrieval is realized by matching source les with di log contents. en, reploc(frqa) considers utilizing the qa component to enhance the basic queries extracted from the di logs. finally, reploc indicates the version discussed in section to evaluate the e ectiveness of reploc, metrics commonly used in the ir literatures are employed to evaluate the performance of reploc, including the accuracy rate, the precision, the recall, and the mean average precision (map). e metrics are computed by examining the ranked list of source les returned by the framework in response to a query. e topn source les in the ranked list is called the retrieved set and is compared with the relevance list to compute the precision and recall metrics (denoted by pn and rn respectively). given an unreproducible package with problematic les, a topn accuracy rate score, e.g. a1, a5, and a10, of a localization tool is the portion of topn lists a tool provides that at least one problematic le contains in it [30,48]. in this study, we also report p1, p5, p10 and r1, r5, r10 [28,48]. pn means the portion of problematic les successfully retrieved in a topn list, while rn measures how many problematic les are retrieved in a topn list among all the problematic les: rn retrieved problematic les in the topn list of problematic les precision and recall usually share an inverse relationship, in that, the precision is higher than recall for lower values of n and vice versa for higher values of n an overall metric of retrieval accuracy is known as mean average precision (map), which is the average of the average precision (ap) values over all the problematic les in unreproducible packages. for an unreproducible package with several problematic les, the ap is computed as of les related in the patch , where m is the size of a ranking list, pos(k) indicates whether the kth le in a ranking list is related to the unreproducible build, and pk is the precision described in equation with ap de ned, map can be calculated by averaging all the ap scores across all the unreproducible packages. in this rq, we intend to investigate whether reploc is sensitive to the weighting parameter as described in section 3, in equation 2, we propose the weighted similarity between queries and source les. hence, in this rq, we are interested in investigating replocs behavior as we alter the weight of the two components. more speci cally, for each category of dataset, we randomly select half of the packages, and a grid search from to with a step of is employed to analyze the impact of varying considering the timestamps and the locale datasets, we visually present the trend of the a10, p10, r10 and the map values against the value in figure from the gure, the following observations can be drawn. first, for the randomly selected packages, the performance of reploc exhibits similar trend, i.e., when is set within the range [0.2, 0.4], reploc obtains the best results. second, we observe that reploc is not very sensitive to , unless is too large, which will make reploc prefer the hf component. hence, for the subsequent experiments, is set with answer to rq1: experimental results show that, reploc is not very sensitive to the parameter, which to some extent demonstrates the robustness of reploc. in this rq, we examine whether reploc locates the problematic les accurately. we present the experimental results, and discuss the phenomena observed. in table 4, we rst give the results over the datasets. e table is organized as follows. e rst column indicates the four categories of datasets we built in this study (see section 4.2). e second column represents the four variants of reploc. en, the rest of the table presents the metrics that evaluate the performance of each variant. note that for the accuracy rate, the precision, and the recall, the metric values are averaged over all the packages. besides, we also present the aggregate performance at the bo om of the table. taking the timestamps dataset as an example, several interesting phenomena can be observed. first, the performance of reploc(hf) is not satisfying. even considering the top10 results, the corresponding accuracy rate is around to examine the representativeness of the heuristic rules, in table we present the results of reploc(hf) with single rule. we report the a10, p10, r10, and map of the ve rules that perform the best. among the rules, the gzip arg rule achieves the highest accuracy rate. however, the a10 value is below 30, which is signi cantly outperformed by reploc(hf) that considers all the rules. similar observations could be drawn for other performance metrics, which to some extent con rms the diversecause barrier. second, by comparing the results of reploc(frqa) against reploc(fr) in table 4, we can con rm the usefulness of qa. as figure 9: trends of precision and recall of reploc mentioned, reploc(fr) could be loosely considered the simulation of manual localization, which tries to match the problematic les with the di log contents. over the timestamps dataset, a10 of reploc(fr) is with the augmentation of the query, a10 improves to moreover, when we combine reploc(frqa) with hf, the performance is further improved, i.e., a10 of reploc achieves 82.90, which implies that for over of the unreproducible packages in the timestamps dataset, at least one problematic le is located in the top10 list. besides, similar results are obtained over the other datasets, i.e., reploc(hf) and reploc(fr) perform the worst, reploc(frqa) outperforms reploc(fr) considering the a10 value, and reploc performs the best. associated with table 4, we also conduct statistical tests, to draw con dent conclusions whether one algorithm outperforms the other. for the statistical test, we employ the wilcoxons signed rank test, with a null hypothesis stating that there exists no difference between the results of the algorithms in comparison. we consider the con dence level (i.e., pvalues below are considered statistically signi cant), and adopt the p10 and r10 as the performance metrics. we do not consider the accuracy rate and the map metrics, in that these are aggregate metrics. over all the instances, when comparing reploc with any of the other three baseline variants, the null hypothesis is rejected (pvalue for both p10 and r10), which implies that reploc outperforms their baseline variants in a statistically signi cant way. to gain more insights into the behavior of reploc, we present the performance of the four variants against the number of retrieved results in figure 9, over typical datasets. in the gure, the xaxis and the yaxis indicate the number of retrieved les, and the performance metrics. from the subgures, we con rm that over both the datasets, reploc outperforms the other variants signi cantly, i.e., the performance curves for reploc lie above those for other variants, which implies that for all the cases of the retrieved results, combining the two components is able to obtain be er results. is phenomenon conforms with our observations in table answer to rq2: by comparing the variants of reploc over real world packages, we con rm that by combining the heuristic in this rq, we evaluate reploc from the e ciency perspectives. since manually localizing the unreproducible issues is a time consuming task, automating such process is pro table only if the proposed approach is time e cient. hence, we present the time statistics of the experiments. figure depicts the statistics of the source les as histograms, in which the xaxis indicates the number of source les ( lenum) and the words (wordnum), and the yaxis represents the associated frequency. in this study, the number of les ranges within [6,19890], and the number of words for the majority of the packages ranges around , which implies that manually inspecting the les would be di cult. since the scale of the packages in this study varies greatly, it is intuitive that the localization process over di erent packages will vary accordingly. to investigate this issue, we present the results related to time e ciency considering the three variants of reploc. in figure 11, we illustrate the distributions of the dataset scalability and the execution time. in the subgures, the xaxis indicates the time in seconds, and the yaxis represents the frequency. from the results, we observe that, the indexing of the documents consumes the largest portion of time, compared with other components. in particular, the median of the execution time for reploc is seconds. answer to rq3: in this rq, we investigate the e ciency perspectives of reploc. in this study, the indexing of the document consume the majority of the time. for rq1rq3, to evaluate the performance of reploc properly, we employ the packages that have been xed, and adopt the patches from the bts as the ground truth. however, in the realworld traversed. such observation to some extent demonstrates the usefulness of reploc in leveraging the knowledge from debian to a di erent repository such as guix. a er localizing the problematic le and manually xing, the submi ed patch has been accepted and pushed into the code base of guix [10]. similarly, the patches for djvulibre [8] and libjpegturbo [9] have also been accepted. answer to rq4: we demonstrate that reploc is helpful in localizing un xed unreproducible packages from both debian and guix. in particular, unreproducible issues of packages from both repositories are xed under the guidance of reploc, which have not been xed before this study. ere are several objections a critical reader might raise to the evaluation presented in this study, among which the following two threats deserve special a ention. first, in this study, the heuristic rules in hf are summarized from debians documentation. also, we leverage the build log gathered from the build process. hence, some may argue that the approach cannot be generalized to other so ware repositories because it relies too much on debians infrastructure. to mitigate this threat, in reploc, a ention is paid so that the components are not specialized for debian. for example, despite knowing that the debian rules les take the largest portion of the problematic les (see figure 7(b)), no extra priority is given to these les during ranking. also, in hf, we avoid using heuristic rules speci c to debian, and intend to make the rules as general as possible. for instance, unsorted wildcard is applicable for make le based build systems, and gzip arg is helpful if gzipbased compression is involved. as a result, the results of this study can be generalized to other repositories. as demonstrated in rq4, we have successfully applied reploc to guix. for other repositories, applying reploc should only require minor adaptation. for example, for the fedora project, the build log can be gathered by parsing the verbose output of the mock build tool, and the di log could be generated by diffoscope as well. second, when constructing the datasets, the unreproducible packages caused by the toolchain issues are not considered. for these packages, the unreproducible issues are introduced by the depended packages rather than the current package. hence, identi cation of the toolchain issues is another challenging task that requires further manual investigation [7]. besides, we should note that xing the toolchain issues may help make more packages reproducible. for example, when reproduciblerelated patches were accepted by gcc from upstream, around unreproducible packages that depended on gcc became reproducible automatically [18]. we plan to explore the toolchain issues in the future. first, this study is closely related to the fault localization studies, especially the irbased approaches. [49] proposed a specialized vsm based approach, and consider the similarities between bug reports to localize buggy les. [44] propose a compositional model that integrates multiple variants of vsm. in particular, they model the composition of di erent vsm variants as a optimization problem, and apply a genetic algorithm to search for the suitable composition pa ern between vsm variants. [42] investigate the usefulness of irbased fault localization techniques, and discover that the quality of the bug reports are crucial to the performance of localization tasks. meanwhile, domain knowledge is utilized to improve the performance of irbased bug localization techniques. [48] nd bugxing frequency and bugxing recency of source code les are helpful for bug localization. [38] nd the structure of bug reports and source code les are also good knowledge for bug localization. ey consider bug reports or source code les as documents with structured elds, e.g., summary and description, or le name, class name, and method name, respectively. stacktrace information in bug report is also analyzed [33,46] to improve the performance of bug localization. besides, version histories [39,40,43] and similar bug reports [24] are proved to be useful. besides, with the development of ir techniques, other text mining methodologies are also incorporated to support locating buggy les. for example, due to its e ectiveness, latent dirichlet allocation (lda) has gained its popularity in the eld of bug localization. [31] propose a static ldabased technique for automatic bug localization. [29] propose a localization framework hyloc that combines deep learning and irbased model. ey integrate deep neural network and a vsm variant, to complement the two standalone components. experimental results over real world projects demonstrate that their proposed model outperforms the individual models. [35] propose an incremental framework to update the model parameters of the latent semantic analysis, which is then applied to localize buggy les. experiments over so ware libraries with ten years of version history validate their framework. however, despite the closeness to these studies, we should note that the problem in this study has its unique features. for example, the counterpart of the bug reports in irbased fault localization, i.e., the di logs, are not su ciently informative to guide the retrieval. to the best of our knowledge, there have not been studies on localizing les that cause unreproducible builds. however, there have been studies that address the importance of reproducible builds. for example, wheeler [45] describes a practical technique named diverse double compiling. by compiling the source les twice with di erent compilers, and verifying the compiled binaries, certain types of malicious a acks can be detected and prevented. according to debians documentation, this work partially motivates the reproducible builds practice [15]. [26] investigate the diverse compilation under embedded system, and experimentally quantify the e ciency of diverse compiling for so ware fault tolerance. carnavalet and mannan [25] conduct an empirical study, focusing on the reproducible builds in the context of securitycritical so ware. based on the experiments on the encryption tool truecrypt, they summarize the challenges of reproducibility in practice. [36] address the reproducibility in cloud computing. ey adopt the term reconstructable so ware, and propose a prototype to simplify the creation of reliable distributed so ware. in this study, we focus on the localization task for unreproducible builds, which has not been addressed in the existing studies. in this study, we investigate the localization task for unreproducible builds. we present components that consider heuristic knowledge, similarity based information, as well as their integration as reploc. for empirical validation, we create four categories of publicly available datasets with unreproducible packages from debian. extensive experiments reveal that reploc is able to e ectively localize the les that lead to unreproducible builds. furthermore, with the help of reploc, we successfully identi ed and xed new unreproducible packages from debian and guix. for the future work, we are interested in the localization of problematic les for the toolchain related issues. also, inspired by the recordandplay techniques [34] from the crash reproduction based debugging research [27,47], it would be interesting to leverage these techniques to detect more accurate correspondence between the build commands executed and the built binaries.", "summary": "automated localization for unreproducible builds ren et al., icse\u201918 reproducible builds are an important component of integrity in the software supply chain. attacks against package repositories and build environments may compromise binaries and produce packages with backdoors (see this report for a recent prominent example of compromised packages on dockerhub). if the same source files always lead to the same binary packages, then an infected binary can be much more easily detected. unfortunately, reproducible builds have not traditionally been the norm. nondeterminism creeping into build processes means that rebuilding an application from the exact same source, even within a secure build environment, can often lead to a different binary. due to the significant benefits, many opensource software repositories have initiated their validation processes. these repositories include gnu/linux distributions such as debian and guix, as well as software systems like bitcoin. if you have a nonreproducible build, finding out why can be nontrivial. it takes time and a lot of effort to hunt down and eradicate the causes. for example, debian unstable for amd64 still had 2,342 packages with nonreproducible builds as of august (the number today as i\u2019m writing this is 2,826). you can see a stubbornly persistent group of unreproducible builds in this screen grab from tests.reproduciblebuilds.org : screenshot reploc is a tool for highlighting the files within a package which are the likely root cause of nonreproducible builds. tested against unreproducible debian packages, it achieves a top1 accuracy rate of 47.09, and at top10 accuracy rate of that\u2019s a significant aid to developers looking at cryptic messages from the builds of packages that may include many hundreds of files. with the help of reploc, the authors also identified and fixed nonreproducibility issues in packages from debian and guix. detecting and diagnosing unreproducible builds the debian workflow for testing build reproducibility looks like this: the source is built in two different environments, deliberately constructed with different environment variables and software configurations. if the two resulting binaries are not identical, the package is flagged for human inspection, a process called localization which seeks to localise the causes of unreproducible builds. one of the major inputs to this process is the diff logs, as generated by diffoscope those logs produce output that looks like this: here we can see diffoscope highlighting a difference in libcompat.a. in this case, the root cause is in the makefile: can you spot it? the root issue is the unstable ordering of files passed to the ar utility to generate libcompat.a (wildcard libcompat/.c). here\u2019s a patch that fixes the issue. in general there are many possible causes of unreproducible builds including timezones (making e.g. anything that uses the date macro to be unreproducible ) and locale settings (making e.g. capture of output text unreproducible). introducing reploc reploc begins with the build logs and diff log created by diffoscope, and seeks to automatically highlight the most likely files in which the root cause can be found. reploc\u2019s query augmentation (qa) component uses information from the logs to refine queries that help to pinpoint likely causes. the heuristic filtering component embodies handcoded heuristic rules that further help to highlight possible causes. the combined outputs are passed to ranking component to produce the list of ranked likelyculprit source files as the final output. query augmentation the files names highlighted in the diff log are used as the initial seed query set. the build logs contain additional information relating to these files, for example: the qa component splits the build logs into directory sections passed on the entering / leaving directory\u2019 messages in the logs. each directory section thus contains a set of commands, which is denoted a command file\u2019 in the paper. tf/idf is used to assign a weight value to each command file to assess it\u2019s relevance to the initial seed queries. the commands from the most relevant command files are then added to the query set, to produce an augmented query set. (in the example above, the ar cru binx8664/libcompat.a... command causes us to add the content of this command file). heuristic filtering the authors extract fourteen heuristic rules from debian\u2019s documentation these rules are encoded as perl regular expressions(! ), as summarised in the table below. the time and date rules look for uses of the time and date macros. the gzip rule (3) looks for uses of gzip without the n argument (in which case gzip embeds timestamps in the resulting archive). the date cmd rule (4) looks for capture of the current date using the date shell command pllocaltime looks for perl scripts capturing date and time dateintex looks for date embedding in tex files sortinpipe captures cases where sort is used without a local set targzippipe looks for tar and gzip executed in a pipeline plunsortedkey catches traversal of unsorted hash keys in perl lswithoutlocale captures cases where ls is used without a locale unsortedwildcard looks for the use of wildcard in makefiles without sorting with the rules in hand, it\u2019s a simple matter of running grep over the source tree. heuristic filtering has good recall, but poor precision (i.e., it can produce a lot of false positives). ranking we can compute the cosine similarity (using tf/idf) between each package file and the augmented queries to produce a ranked list of candidate files from the qa module. these are then combined with the files highlighted by the hf module to give a simple weighted score: where qa(f) is the similarity score produced by the qa module, and hf(f) is if the hf module flagged the file, and otherwise. is a configurable parameter to tune the balance between the two terms. reploc in action at the time of the study, debian had packages with accompanying patches that turned unreproducible builds into reproducible ones. this constitutes the ground truth dataset for reploc. the dataset is further divided into four subsets based on the major causes of nonreproducibility. the table below summarises how well reploc gets on. concentrate on the bottom line (reploc) in each row (the other lines show how reploc behaves with different subsets of its modules). an is a topn accuracy rate score, defined as the percentage of topn ranked file lists produced by reploc that contain at least one problematic file (from the patches). pn is a topn precision score, defined as the percentage of files reported in a topn list that are genuinely problematic rn is at topn recall score, defined as the percentage of all problematic files that are successfully identified in a topn list. overall, reploc achieves an average accuracy score of for top10 files. i.e., if you examine the first ten files reploc highlights, you have a chance of finding an issue causing an unreproducible build in at least one of them. you will also find on average of all the files with reproducibility problems by the time you have worked through that top10 list. the authors then used reploc to see if they could find the root causes of unreproducible builds for packages where no ground truth was available (i.e., there was noknown reproducible build process for them). three packages from debian are fixed (reginarexx, fontsuralic, and manpagestr). the problematic files are right at the top of list produced by reploc. three packages from guix are also fixed (libjpegturbo, djvulibre, and skalibs). once more the problematic files are right at the top of the list produced by reploc. future work for the future work, we are interested in the localization of problematic files for toolchain related issues. also, inspired by recordandplay techniques from crash reproduction based debugging research, it would be interesting to leverage these techniques to detect more accurate correspondence between the build commands executed and the built binaries."}, {"document": "we propose a modular reinforcement learning architecture for nonlinear, nonstationary control tasks, which we call multiple modelbased reinforcement learning (mmrl). the basic idea is to decompose a complex task into multiple domains in space and time based on the predictability of the environmental dynamics. the system is composed of multiple modules, each of which consists of a state prediction model and a reinforcement learning controller. the responsibility signal, which is given by the softmax function of the prediction errors, is used to weight the outputs of multiple modules as well as to gate the learning of the prediction models and the reinforcement learning controllers. we formulate mmrl for both discretetime, finite state case and continuoustime, continuous state case. the performance of mmrl was demonstrated for discrete case in a nonstationary hunting task in a grid world and for continuous case in a nonlinear, nonstationary control task of swinging up a pendulum with variable physical parameters.a big issue in the application of reinforcement learning (rl) to realworld control problems is how to deal with nonlinearity and nonstationarity. for a nonlinear, highdimensional system, the conventional discretizing approach necessitates a huge number of states, which makes learning very slow. standard rl algorithms can perform badly when the environment is nonstationary, or has hidden states. these problems have motivated the introduction of modular or hierarchical rl architectures (singh 1992;dayan and hinton 1993;littman et al. 1995;wiering and schmidhuber 1998;parr and russel 1998;sutton et al. the basic problem in modular or hierarchical rl is how to decompose a complex task into simpler subtasks. this paper presents a new rl architecture based on multiple modules, each of which is composed of a state prediction model and a rl controller. with this architecture, a nonlinear and/or nonstationary control task is decomposed in space and time based on the local predictability of the environmental dynamics. the mixture of experts architecture (jacobs et al. 1991) has previously been applied to nonlinear or nonstationary control tasks (gomi and kawato 1993;cacciatore and nowlan 1994). however, the success of such modular architecture depends strongly on the capability of the gating network to decide which of the given modules should be recruited at any particular moment. an alternative approach is to provide each of the experts with a prediction model of the environment and to utilize the prediction errors for the selection of the controllers. (1995), the model that makes the smallest prediction error among a fixed set of prediction models is selected, and its associated single controller is used for control. however, when the prediction models are to be trained with little prior knowledge, task decomposition is initially far from optimal. thus the use of hard competition can lead to suboptimal task decomposition. based on the bayesian statistical framework, pawelzik et al. (1996) proposed the use of annealing in a soft competition network for time series prediction and segmentation. a similar mechanism is used by tani and nolfi (1999) for hierarchical sequence prediction. the use of the softmax function for module selection and combination was originally proposed for a tracking control paradigm as the multiple paired forwardinverse models (mpfim) haruno et al. it was recently reformulated as modular selection and identification for control (mosaic) (wolpert and ghahramani 2000). in this paper, we apply the idea of a softmax selection of modules to the paradigm of reinforcement learning. the resulting learning architecture, which we call multiple modelbased reinforcement learning (mmrl), learns to decompose a nonlinear and/or nonstationary task through the competition and cooperation of multiple prediction models and reinforcement learning controllers. in the following sections, we first formulate the basic mmrl architecture (section 2) and then describe its implementation in discretetime and continuoustime cases, including multiple linear quadratic controllers (mlqc) (section 3). we first test the performance of the mmrl architecture for the discrete case in a hunting task with multiple preys in a grid world (section 4). we also demonstrate the performance of mmrl for continuous case in a nonlinear, nonstationary control task of swinging up a pendulum with variable physical parameters (section 5). the basic idea of this modular architecture is to decompose a nonlinear and/or nonstationary task into multiple domains in space and time so that within each of the domains the environmental dynamics is well predictable. the action output of the rl controllers as well as the learning rates of both the predictors and the controllers are weighted by the responsibility signal, which is a gaussian softmax function of the errors in the outputs of the prediction models. the advantage of this module selection mechanism is that the areas of specialization of the modules are determined in a bottomup fashion based on the nature of the environment. furthermore, for each area of module specialization, the design of the control strategy is facilitated by the availability of the local model of the environmental dynamics. in the following, we consider a discretetime, finite state environment where x 1, ..., n and u 1, ..., m are discrete states and actions, and a continuoustime, continuousstate environmen where x r n and u r m are state and action vectors, and r n is noise. actions are given by a policy, either a stochastic one or a deterministic one the reward r(t) is given as a function of the state x(t) and the action u(t). the goal of reinforcement learning is to improve the policy so that more rewards are acquired in a long run. the basic strategy of reinforcement learning is to estimate cumulative future reward under the current policy as the value function v (x) for each state and then to improve the policy based on the value function. we define the value function of the state x(t) under the current policy as in discrete case (sutton and barto 1998) and in continuous case (doya 2000), where and are the parameters for discounting future reward. the purpose of the prediction model in each module is to predict the next state (discretetime) or the temporal derivative of the state (continuoustime) based on the observation of the state and the action. the responsibility signal i (t) haruno et al. 1999) is given by the relative goodness of predictions of multiple prediction models. for a unified description, we denote the new state in the discrete case as and the temporal derivative of the state in the continuous case as the basic formula for the responsibility signal is given by the bayes rule where p (i) is the prior probability of selecting module i and p (y(t)i) is the likelihood of model i given the observation y(t). in the discrete case, the prediction model gives the probability distribution of the new statex(t) based on the previous state x(t 1) and the action u(t 1) as if there is no prior knowledge about module selection, we take the priors as uniform (p (i) 1/n) and then the responsibility signal is given by where x(t) is the newly observed state. in the continuous case, the prediction model gives the temporal derivative of the stat by assuming that the prediction error is gaussian with variance , the responsibility signal is given by the gaussian softmax function where(t) is the actually observed state change. in the mmrl architecture, the responsibility signal i (t) is used for four purposes: 1) weighting the state prediction outputs; 2) gating the learning of prediction models; 3) weighting the action outputs; and 4) gating the learning of reinforcement learning controller. the outputs of the prediction models are weighted by the responsibility signal i (t). in the discrete case, the prediction of the next state is given by in the continuous case, the predicted state derivative is given b these predictions are used in modelbased rl algorithms and also for the annealing of as described later. the responsibility signal i (t) is also used for weighting the parameter update of the prediction models. in general, it is realized by scaling the error signal of prediction model learning by i (t). 3) action output: the outputs of reinforcement learning controllers are linearly weighted by i (t) to make the action output. in the discrete case, the probability of taking an action u(t) is given by in the continuous case, the output is given by the interpolation of modular outputs 4) reinforcement learning: i (t) is also used for weighting the learning of the rl controllers. the actual equation for the parameter update varies with the choice of the rl algorithms, which are detailed in the next section. when a temporal difference (td) algorithm (barto et al. 1983;sutton 1988;doya 2000) is used, the td error, in the discrete case and in the continuos case, is weighted by the responsibility signal for learning of the ith rl controller. using the same weighting factor i (t) for training the prediction models and the rl controllers helps each rl controller to learn an appropriate policy and its value function for the context under which its paired prediction model makes valid predictions. when there is some prior knowledge or belief about module selection, we incorporate the responsibility predictors haruno et al. by assuming their outputs i (t) are proportional to the prior probability of module selection, from (9), the responsibility signal is given by in modular decomposition of a task, it is desired that modules do not switch too frequently. this can be enforced by incorporating responsibility priors based on the assumption of temporal continuity and spatial locality of module activation. the continuity of module selection is incorporated by taking the previous responsibility signal as the responsibility prediction signal. in the discrete case, we take the responsibility prediction based on the previous responsibilit where is a parameter that controls the strength of the memory effect. from 21and 22, the responsibility signal at time t is given by the product of likelihoods of past module selection where z(t) denotes the normalizing factor, i.e., in the continuous case, we choose the prior where t is an arbitrarily small time difference (note (24) coincides with (22) with t 1). since the likelihood of the module i is given by the gaussian , from recursion as in (23), the responsibility signal at time t is given by that is, a gaussian softmax function of temporally weighted squared errors. in the limit of t 0, (25) can be represented as where e i (t) is a lowpass filtered prediction erro the use of this lowpass filtered prediction errors for responsibility prediction is helpful in avoiding chattering of the responsibility signal (pawelzik et al. in the continuous case, we consider a gaussian spatial prior where c i is the center of the area of specialization, m i is a covariance matrix that specifies the shape, and denotes transpose. these parameters are updated so that they approximate the distribution of the input state x(t) weighted by the responsibility signal, namely, where c and m are update rates. for the rl controllers, it is generally possible to use modelfree rl algorithms, such as actorcritic and qlearning. however, because the prediction models of the environmental dynamics are intrinsic components of the architecture, it is advantageous to utilize these prediction models not just for module selection but also for designing rl controllers. in the following, we describe the use of modelbased rl algorithms for discretetime and continuoustime cases. one special implementation for continuoustime is the use of multiple linear quadratic controllers derived from linear dynamic models and quadratic reward models. now we consider implementation of the mmrl architecture for discretetime, finite state and action problems. the standard way of utilizing a predictive model in rl is to use it for action selection by the onestep search wherer(x(t), u) is the predicted immediate reward andx(t 1) is the next state predicted from the current state x(t) and a candidate action u. in order to implement this algorithm, we provide each module with a reward , and a dynamic model f i (x, x, u). each candidate action u is then evaluated by for the sake of exploration, we use a stochastic version of the greedy action selection (31), where the action u(t) is selected by a gibbs distribution where controls the stochasticity of action selection. the parameters are updated by the error signals weighted by the responsibility ..., n; c(j, x) if j x and zero otherwise), for the reward model, and i (t)(t) for the value function model. next we consider a continuoustime mmrl architecture. a modelbased rl algorithm for a continuoustime, continuousstate system (2) is derived from the where is the time constant of reward discount (doya 2000). under the assumptions that the system is linear with respect to the action and the action cost is convex, a greedy policy is given by where v (x) x is a vector representing the steepest ascent direction of the value function, f(x,u) u is a matrix representing the input gain of the dynamics, and g is a sigmoid function whose shape is determined by the control cost (doya 2000). to implement the hjb based algorithm, we provide each module with a dynamic model f i (x, u) and a value model v i (x). the outputs of the dynamic models (12) are compared with the actually observed state dynamics(t) to calculate the responsibility signal i (t) according to (13). the model outputs are linearly weighted by i (t) for state predictio and value function estimation the derivatives of the dynamic models f i (x,u) u and value models v i (x) x are used to calculate the action for each module they are then weighted by i (t) according to (17) to make the actual action u(t). learning is based on the weighted prediction errors i (t)(x i (t) (t)) for dynamic models and i (t)(t) for value function models. in a modular architecture like the mmrl, the use of universal nonlinear function approximators with large numbers of degreesoffreedom can be problematic because it can lead to an undesired solution in which a single module tries to handle most of the task domain. the use of linear models for the prediction models and the controllers is a reasonable choice because local linear models have been shown to have good properties of quick learning and good generalization (schaal and atkeson 1996). furthermore, if the reward function is locally approximated by a quadratic function, then we can use a linear quadratic controller (see, e.g., bertsekas 1995) for the rl controller design. we use a local linear dynamic model and a local quadratic reward model for each module, where the matrix p i is given by solving the riccati equation the center x v i and the bias v i of the value function is given by then the optimal feedback control for each module is given by the linear feedback the action output is given by weighting these controller outputs by the responsibility signal i (t): the parameters of the local linear models a i , b i , and x d i and those of the quadratic reward models r i , q i , and r i are updated by the weighted prediction , respectively. when we assume that the update of these models is slow enough, then the riccati equations 42may be recalculated only intermittently. we call this method multiple linear quadratic controllers (mlqc). in order to test the effectiveness of the mmrl architecture, we first applied the discrete mmrl architecture to a nonstationary hunting task in a grid world. the hunter agent tries to catch a prey in a torus grid world. there are states representing the position of the prey relative to the hunter. the hunter chooses one of five possible actions: north (n), east (e), south (s), west (w), stay. a prey moves in a fixed direction during a trial. at the beginning of each trial, one of four movement directions ne, nw, se, sw is randomly selected and a prey is placed at a random position in the grid world. when the hunter catches the prey, by stepping into the same grid as the preys, a reward r(t) is given. each step of movement costs r(t) a trial is terminated when the hunter catches a prey, or it fails to catch it within steps. in order to compare the performance of mmrl with conventional methods, we applied standard qlearning and compositional qlearning (cql) (singh 1992) to the same task. a major difference between cql and mmrl is the criterion for modular decomposition: cql uses the consistency of the modular value functions while mmrl uses the prediction errors of dynamic models. in cql, the gating network as well as component qlearning modules are trained so that the composite qvalue well approximates the action value function of the entire problem. in the original cql (singh 1992), the output of the gating network was based on the augmenting bit that explicitly signaled the change in the context. since our goal now is to let the agent learn appropriate decomposition of the task without an explicit cue, we used a modified cql (see appendix for the details of the algorithm and the parameters). figure shows the performance difference of standard qlearning, cql, and mmrl in the hunting task. the modified cql did not perform significantly better than standard, flat qlearning. investigation of the modular q functions of cql revealed that, in most simulation runs, modules did not appropriately differentiate for four different kinds of preys. on the other hand, the performance of mmrl approached close to theoretical optimum. this was because four modules successfully specialized in one of four kinds of prey movement. figure shows examples of the value functions and the prediction models learned by mmrl. from the output of the prediction models f i , it can be seen that the modules 1,2,3 and were specialized for the prey moving to ne, nw, accordance with these movement directions of the prey. a possible reason for the difference in the performance of cql and mmrl in this task is the difficulty of module selection. in cql, when the prey is far from the hunter, the differences in discounted q values for different kinds of preys are minor. thus it would be difficult to differentiate modules based solely on the q values. in mmrl, on the other hand, module selection based on the state change, in this case prey movement, is relatively easy even when the prey is far from the hunter. in order to test the effectiveness of the mmrl architecture, we first applied the mlqc algorithm described in section to the task of swinging up a pendulum with limited torque (figure 4) (doya 2000). the driving torque t is limited in [t max , t max ] with t max mgl. the pendulum has to be swung back and forth at the bottom to build up enough momentum for a successful swing up. the state space was 2dimensional, i.e. x (,) [, ] r where is the joint angle ( means the pendulum hanging down). the reward was given by the height of the tip and the negative squared torque a trial was started from random joint angle [/4, /4] with no angular velocity, and lasted for seconds. we devised the following automatic annealing process for the parameter of the softmax function for the responsibility signal (26). where k denotes the number of trial and e k is the average state prediction error during the kth trial. the parameters were 0.25, a 2, and the initial value set as we first used two modules, each of which had a linear dynamic model initially, the first prediction model predicts the pendulum motion better than the second one, so the responsibility signal becomes close to thus the output of the first rl controller u , which destabilizes the bottom position, is used for control. as the pendulum is driven away from the bottom, the second prediction model predicts the movement better, so becomes higher and the second rl controller takes over and stabilizes the upright position. figure shows the dynamic and reward models when there were eight modules. two modules were specialized for the bottom position and three modules were specialized near the top position, while two other modules were centered somewhere in between. the result shows that proper modularization is possible even when there are redundant modules. figure compares the time course of learning by mlqc with two, four, and eight modules and a nonmodular actorcritic (doya 2000). learning was fastest with two modules. the addition of redundant modules resulted in more variability in the time course of learning. this is because there were multiple possible ways of modular decomposition and due to the variability of the sample trajectories, it took longer time for modular decomposition to stabilize. nevertheless, learning by the 8module mlqc was still much faster than by the nonmodular architecture. an interesting feature of the mlqc strategy is that qualitatively different controllers are derived by the solutions of the riccati equations (42). the controller at the bottom is a positive feedback controller that destabilizes the equilibrium where the reward is minimal, while the controller at the top is a typical linear quadratic regulator that stabilizes the upright state. another important feature of the mlqc is that the modules were flexibly switched simply based on the prediction errors. successful swing up was achieved without any topdown planning of the complex sequence. we then tested the effectiveness of the mmrl architecture for the both nonlinear and nonstationary control task in which mass m and length l of the pendulum were changed every trial. we used four modules, each of which had a linear dynamic model (39) and a quadratic reward model (40). the centers x i of the local linear prediction models were initially set randomly. each trial was started from a random position with [/4, /4] and lasted for seconds. we implemented responsibility prediction with c 50, m 200, and p the parameters of annealing were 0.1, a 2, and an initial value of in the first trials, the physical parameters were fixed at m 1.0, l figure 8(a) shows the change in the position gain (a ) of the four prediction models. the control performance is shown in figure 8(b). figures 8(c,d,e) show prediction models in the section of 0, t initial position gains are set randomly (figure 8(c)). after trials, module and module both specialized in the bottom region ( 0) and learned similar prediction models. module and module also learned the same prediction model in the top region ( ) (figure 8(d)). accordingly, the rl controllers in module and module learned a reward model with a minimum near (0, 0) and a destabilizing feedback policy was given by (15)(17). module and module also learned a reward model with a peak near (, 0) and implemented a stabilizing feedback controller. in to trials, the parameters of the pendulum were switched between m 1, l and m 0.2, l in each trial. at first, the degenerated modules tried to follow the alternating environment (figure 8(a)), and thus swing up was not successful for the new, longer pendulum. the performance for the shorter pendulum was also disturbed (figure 8(b)). after about trials, the prediction models gradually specialized in either new or learned dynamics (figure 8(e)), and successful swing up was achieved both for the shorter and longer pendulums. we found similar module specialization in six of ten simulation runs. in four other runs, due to the bias in initial module allocation, three modules were aggregated in one domain (top or bottom) and one model covered the other domain during the stationary condition. however, after trials in the nonstationary condition, module specialization as shown in figure 8(e) was achieved. we proposed a multiple modelbased reinforcement learning (mmrl) architecture that decomposes a nonlinear and/or nonstationary task in space and time based on the local predictability of the system dynamics. we tested the performance of the mmrl in both nonlinear and nonstationary control tasks. it was shown in simulations of the pendulum swing up task that multiple prediction models were successfully trained and corresponding modelbased controllers were derived. the modules were specialized for different domains in the state space. it was also confirmed in a nonstationary pendulum swingup task that available modules are flexibly allocated for different domains in space and time based on the task demands. the modular control architecture using multiple prediction models was proposed by wolpert and kawato as a computational model of the cerebellum showed in fmri experiments of novel tool use that a large area of the cerebellum is activated initially and then a smaller area remains to be active after long training. they proposed that such local activation spots are the neural correlates of internal models of tools (imamizu et al. they also suggested that internal models of different tools are represented in separated areas in the cerebellum (imamizu et al. our simulation results in a nonstationary environment can provide a computational account of these fmri data. when a new task is introduced, many modules initially compete to learn it. however, after repetitive learning, only a subset of modules are specialized and recruited for the new task. one might argue whether mlqc is a reinforcement learning architecture since it uses lq controllers that were calculated offline. however, when the linear dynamic models and quadratic reward models are learned online, as in our simulations, the entire system realize reinforcement learning. one limitation of mlqc architecture is that the reward function should have helpful gradients in each modular domain. a method for backpropagating the value function of the successor module as the effective reward for the predecessor module is under development. in order to construct a hierarchical rl system, it appears necessary to combine both topdown and bottomup approaches for task decomposition. the mmrl architecture provides one solution for the bottomup approach. combination of this bottomup mechanism with a topdown mechanism is the subject of our ongoing study. the discount factor was set as and the greediness parameters as for both mmrl and cql. the decay parameter of temporal responsibility predictor was for mmrl. we tried different values of for cql without a success. the value used in figures and was", "summary": "standard unsupervised learning aims to learn transferable features. the paper proposes to learn a transferable learning rule (in an unsupervised manner) that can generalize across tasks and architectures. paper approach consider training the model with supervised learning t1 supervisedupdate(t, xt, yt, ). here t denotes the step, (x, y) denotes the data points, denotes the hyperparameters of the optimizer. extending this formulation for metalearning, one could say that t is the step of the inner loop, are the parameters of the meta learning model. further, the paper proposes to use t1 unsupervisedupdate(t, xt, ) ie yt is not used (or even assumed to be available as this is unsupervised learning). the meta update rule is used to learn the weights of a metamodel by performing sgd on the sum of metaobjective over the distribution of tasks (over the course of inner loop training). model base model: mlp with parameters t to ensure that it generalizes across architectures, the update rule is designed to be neurallocal ie updates are a function of pre and postsynaptic neurons though, in practice, this constraint is relaxed to decorrelate neurons by using cross neural information. each neuron i in every layer l (in the base model) has an update network (mlp) which takes as input the feedforward activations, feedback weights and error signals. ie hbl(i) mlp(xbl(i), zbl(i), vl1, l(i), ) b index of the minibatch xl pre nonlinearity activations zl post nonlinearity activations vl feedback weights l error signal all the update networks share the meta parameters the model is run in a standard feedforward manner and the update network (corresponding to each unit) is used to generate the error signal lb(i) lin(hbl(i)). this loss is backpropogated using the set of learned backward weights vl instead of the forward weights wl. the weight update wl is also generated using a perneuron update network. meta objective the metaobjective is based on fitting a linear regression model to labeled examples with a small number of data points. given the emphasis on learning generalizable features, the weights (of linear regression) are estimated on one batch and evaluated on another batch. the metaobjective is to reduce the cosine distance between yb and vtxbl yb actual lables on the evaluation batch xbl features of the evaluation batch (using the base model) v parameters of the linear regression model (learned on train batch) practical considerations meta gradients are approximated using truncated backdrop through time. increasing variation in the training dataset helps the meta optimization process. data is augmented with shifts, rotations, and noise. predicting these coefficients is an auxiliary (regression) task for training the metaobjective. training the system requires a lot of resources days with workers. results with standard unsupervised learning, the performance (on transfer task) starts declining after some time even though the performance (on the unsupervised task) is improving. this suggests that the objective function for the two tasks starts to mismatch. unsupervisedupdate leads to a better generalization as compared to both vae and supervised learning (followed by transfer). unsupervisedupdate also leads to a positive transfer across domains (vision to language) when trained for a shorter duration of time (to ensure that the metaobjective does not overfit). unsupervisedupdate also generalizes to larger model architectures and different activation functions."}, {"document": "a very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions [3]. unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. caruana and his collaborators [1] have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. we achieve some surprising results on mnist and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. we also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish finegrained classes that the full models confuse. unlike a mixture of experts, these specialist models can be trained rapidly and in parallel. also affiliated with the university of toronto and the canadian institute for advanced research. equal contribution.many insects have a larval form that is optimized for extracting energy and nutrients from the environment and a completely different adult form that is optimized for the very different requirements of traveling and reproduction. in largescale machine learning, we typically use very similar models for the training stage and the deployment stage despite their very different requirements: for tasks like speech and object recognition, training must extract structure from very large, highly redundant datasets but it does not need to operate in real time and it can use a huge amount of computation. deployment to a large number of users, however, has much more stringent requirements on latency and computational resources. the analogy with insects suggests that we should be willing to train very cumbersome models if that makes it easier to extract structure from the data. the cumbersome model could be an ensemble of separately trained models or a single very large model trained with a very strong regularizer such as dropout [9]. once the cumbersome model has been trained, we can then use a different kind of training, which we call distillation to transfer the knowledge from the cumbersome model to a small model that is more suitable for deployment. a version of this strategy has already been pioneered by rich caruana and his collaborators [1]. in their important paper they demonstrate convincingly that the knowledge acquired by a large ensemble of models can be transferred to a single small model. a conceptual block that may have prevented more investigation of this very promising approach is that we tend to identify the knowledge in a trained model with the learned parameter values and this makes it hard to see how we can change the form of the model but keep the same knowledge. a more abstract view of the knowledge, that frees it from any particular instantiation, is that it is a learned mapping from input vectors to output vectors. for cumbersome models that learn to discriminate between a large number of classes, the normal training objective is to maximize the average log probability of the correct answer, but a sideeffect of the learning is that the trained model assigns probabilities to all of the incorrect answers and even when these probabilities are very small, some of them are much larger than others. the relative probabilities of incorrect answers tell us a lot about how the cumbersome model tends to generalize. an image of a bmw, for example, may only have a very small chance of being mistaken for a garbage truck, but that mistake is still many times more probable than mistaking it for a carrot. it is generally accepted that the objective function used for training should reflect the true objective of the user as closely as possible. despite this, models are usually trained to optimize performance on the training data when the real objective is to generalize well to new data. it would clearly be better to train models to generalize well, but this requires information about the correct way to generalize and this information is not normally available. when we are distilling the knowledge from a large model into a small one, however, we can train the small model to generalize in the same way as the large model. if the cumbersome model generalizes well because, for example, it is the average of a large ensemble of different models, a small model trained to generalize in the same way will typically do much better on test data than a small model that is trained in the normal way on the same training set as was used to train the ensemble. an obvious way to transfer the generalization ability of the cumbersome model to a small model is to use the class probabilities produced by the cumbersome model as soft targets for training the small model. for this transfer stage, we could use the same training set or a separate transfer set. when the cumbersome model is a large ensemble of simpler models, we can use an arithmetic or geometric mean of their individual predictive distributions as the soft targets. when the soft targets have high entropy, they provide much more information per training case than hard targets and much less variance in the gradient between training cases, so the small model can often be trained on much less data than the original cumbersome model and using a much higher learning rate. for tasks like mnist in which the cumbersome model almost always produces the correct answer with very high confidence, much of the information about the learned function resides in the ratios of very small probabilities in the soft targets. for example, one version of a may be given a probability of of being a and of being a whereas for another version it may be the other way around. this is valuable information that defines a rich similarity structure over the data (i. e. it says which 2s look like 3s and which look like 7s) but it has very little influence on the crossentropy cost function during the transfer stage because the probabilities are so close to zero. caruana and his collaborators circumvent this problem by using the logits (the inputs to the final softmax) rather than the probabilities produced by the softmax as the targets for learning the small model and they minimize the squared difference between the logits produced by the cumbersome model and the logits produced by the small model. our more general solution, called distillation, is to raise the temperature of the final softmax until the cumbersome model produces a suitably soft set of targets. we then use the same high temperature when training the small model to match these soft targets. we show later that matching the logits of the cumbersome model is actually a special case of distillation. the transfer set that is used to train the small model could consist entirely of unlabeled data [1] or we could use the original training set. we have found that using the original training set works well, especially if we add a small term to the objective function that encourages the small model to predict the true targets as well as matching the soft targets provided by the cumbersome model. typically, the small model cannot exactly match the soft targets and erring in the direction of the correct answer turns out to be helpful. neural networks typically produce class probabilities by using a softmax output layer that converts the logit, z i , computed for each class into a probability, q i , by comparing z i with the other logits. where t is a temperature that is normally set to using a higher value for t produces a softer probability distribution over classes. in the simplest form of distillation, knowledge is transferred to the distilled model by training it on a transfer set and using a soft target distribution for each case in the transfer set that is produced by using the cumbersome model with a high temperature in its softmax. the same high temperature is used when training the distilled model, but after it has been trained it uses a temperature of when the correct labels are known for all or some of the transfer set, this method can be significantly improved by also training the distilled model to produce the correct labels. one way to do this is to use the correct labels to modify the soft targets, but we found that a better way is to simply use a weighted average of two different objective functions. the first objective function is the cross entropy with the soft targets and this cross entropy is computed using the same high temperature in the softmax of the distilled model as was used for generating the soft targets from the cumbersome model. the second objective function is the cross entropy with the correct labels. this is computed using exactly the same logits in softmax of the distilled model but at a temperature of we found that the best results were generally obtained by using a condiderably lower weight on the second objective function. since the magnitudes of the gradients produced by the soft targets scale as 1/t it is important to multiply them by t when using both hard and soft targets. this ensures that the relative contributions of the hard and soft targets remain roughly unchanged if the temperature used for distillation is changed while experimenting with metaparameters. each case in the transfer set contributes a crossentropy gradient, dc/dz i , with respect to each logit, z i of the distilled model. if the cumbersome model has logits v i which produce soft target probabilities p i and the transfer training is done at a temperature of t , this gradient is given by: if the temperature is high compared with the magnitude of the logits, we can approximate: if we now assume that the logits have been zeromeaned separately for each transfer case so that so in the high temperature limit, distillation is equivalent to minimizing , provided the logits are zeromeaned separately for each transfer case. at lower temperatures, distillation pays much less attention to matching logits that are much more negative than the average. this is potentially advantageous because these logits are almost completely unconstrained by the cost function used for training the cumbersome model so they could be very noisy. on the other hand, the very negative logits may convey useful information about the knowledge acquired by the cumbersome model. which of these effects dominates is an empirical question. we show that when the distilled model is much too small to capture all of the knowledege in the cumbersome model, intermediate temperatures work best which strongly suggests that ignoring the large negative logits can be helpful. to see how well distillation works, we trained a single large neural net with two hidden layers of rectified linear hidden units on all 60,000 training cases. the net was strongly regularized using dropout and weightconstraints as described in [5]. dropout can be viewed as a way of training an exponentially large ensemble of models that share weights. in addition, the input images were jittered by up to two pixels in any direction. this net achieved test errors whereas a smaller net with two hidden layers of rectified linear hidden units and no regularization achieved errors. but if the smaller net was regularized solely by adding the additional task of matching the soft targets produced by the large net at a temperature of 20, it achieved test errors. this shows that soft targets can transfer a great deal of knowledge to the distilled model, including the knowledge about how to generalize that is learned from translated training data even though the transfer set does not contain any translations. when the distilled net had or more units in each of its two hidden layers, all temperatures above gave fairly similar results. but when this was radically reduced to units per layer, temperatures in the range to worked significantly better than higher or lower temperatures. we then tried omitting all examples of the digit from the transfer set. so from the perspective of the distilled model, is a mythical digit that it has never seen. in this section, we investigate the effects of ensembling deep neural network (dnn) acoustic models that are used in automatic speech recognition (asr). we show that the distillation strategy that we propose in this paper achieves the desired effect of distilling an ensemble of models into a single model that works significantly better than a model of the same size that is learned directly from the same training data. stateoftheart asr systems currently use dnns to map a (short) temporal context of features derived from the waveform to a probability distribution over the discrete states of a hidden markov model (hmm) [4]. more specifically, the dnn produces a probability distribution over clusters of triphone states at each time and a decoder then finds a path through the hmm states that is the best compromise between using high probability states and producing a transcription that is probable under the language model. although it is possible (and desirable) to train the dnn in such a way that the decoder (and, thus, the language model) is taken into account by marginalizing over all possible paths, it is common to train the dnn to perform framebyframe classification by (locally) minimizing the cross entropy between the predictions made by the net and the labels given by a forced alignment with the ground truth sequence of states for each observation: where are the parameters of our acoustic model p which maps acoustic observations at time t, s t , to a probability, p (h t s t ; ) , of the correct hmm state h t , which is determined by a forced alignment with the correct sequence of words. the model is trained with a distributed stochastic gradient descent approach. we use an architecture with hidden layers each containing rectified linear units and a final softmax layer with 14,000 labels (hmm targets h t ). the input is frames of melscaled filterbank coefficients with a 10ms advance per frame and we predict the hmm state of st frame. the total number of parameters is about 85m. this is a slightly outdated version of the acoustic model used by android voice search, and should be considered as a very strong baseline. table 1: frame classification accuracy and wer showing that the distilled single model performs about as well as the averaged predictions of models that were used to create the soft targets. we trained separate models to predict p (h t s t ; ), using exactly the same architecture and training procedure as the baseline. the models are randomly initialized with different initial parameter values and we find that this creates sufficient diversity in the trained models to allow the averaged predictions of the ensemble to significantly outperform the individual models. we have explored adding diversity to the models by varying the sets of data that each model sees, but we found this to not significantly change our results, so we opted for the simpler approach. for the distillation we tried temperatures of [1, 2, 5, 10] and used a relative weight of on the crossentropy for the hard targets, where bold font indicates the best value that was used for table table shows that, indeed, our distillation approach is able to extract more useful information from the training set than simply using the hard labels to train a single model. more than of the improvement in frame classification accuracy achieved by using an ensemble of models is transferred to the distilled model which is similar to the improvement we observed in our preliminary experiments on mnist. the ensemble gives a smaller improvement on the ultimate objective of wer (on a 23kword test set) due to the mismatch in the objective function, but again, the improvement in wer achieved by the ensemble is transferred to the distilled model. we have recently become aware of related work on learning a small acoustic model by matching the class probabilities of an already trained larger model [8]. however, they do the distillation at a temperature of using a large unlabeled dataset and their best distilled model only reduces the error rate of the small model by of the gap between the error rates of the large and small models when they are both trained with hard labels. training an ensemble of models is a very simple way to take advantage of parallel computation and the usual objection that an ensemble requires too much computation at test time can be dealt with by using distillation. there is, however, another important objection to ensembles: if the individual models are large neural networks and the dataset is very large, the amount of computation required at training time is excessive, even though it is easy to parallelize. in this section we give an example of such a dataset and we show how learning specialist models that each focus on a different confusable subset of the classes can reduce the total amount of computation required to learn an ensemble. the main problem with specialists that focus on making finegrained distinctions is that they overfit very easily and we describe how this overfitting may be prevented by using soft targets. jft is an internal google dataset that has million labeled images with 15,000 labels. when we did this work, googles baseline model for jft was a deep convolutional neural network [7] that had been trained for about six months using asynchronous stochastic gradient descent on a large number of cores. this training used two types of parallelism [2]. first, there were many replicas of the neural net running on different sets of cores and processing different minibatches from the training set. each replica computes the average gradient on its current minibatch and sends this gradient to a sharded parameter server which sends back new values for the parameters. these new values reflect all of the gradients received by the parameter server since the last time it sent parameters to the replica. second, each replica is spread over multiple cores by putting different subsets of the neurons on each core. ensemble training is yet a third type of parallelism that can be wrapped around the other two types, but only if a lot more cores are available. waiting for several years to train an ensemble of models was not an option, so we needed a much faster way to improve the baseline model. when the number of classes is very large, it makes sense for the cumbersome model to be an ensemble that contains one generalist model trained on all the data and many specialist models, each of which is trained on data that is highly enriched in examples from a very confusable subset of the classes (like different types of mushroom). the softmax of this type of specialist can be made much smaller by combining all of the classes it does not care about into a single dustbin class. to reduce overfitting and share the work of learning lower level feature detectors, each specialist model is initialized with the weights of the generalist model. these weights are then slightly modified by training the specialist with half its examples coming from its special subset and half sampled at random from the remainder of the training set. after training, we can correct for the biased training set by incrementing the logit of the dustbin class by the log of the proportion by which the specialist class is oversampled. in order to derive groupings of object categories for the specialists, we decided to focus on categories that our full network often confuses. even though we could have computed the confusion matrix and used it as a way to find such clusters, we opted for a simpler approach that does not require the true labels to construct the clusters. in particular, we apply a clustering algorithm to the covariance matrix of the predictions of our generalist model, so that a set of classes s m that are often predicted together will be used as targets for one of our specialist models, m. we applied an online version of the kmeans algorithm to the columns of the covariance matrix, and obtained reasonable clusters (shown in table 2). we tried several clustering algorithms which produced similar results. before investigating what happens when specialist models are distilled, we wanted to see how well ensembles containing specialists performed. in addition to the specialist models, we always have a generalist model so that we can deal with classes for which we have no specialists and so that we can decide which specialists to use. given an input image x, we do topone classification in two steps: step 1: for each test case, we find the n most probable classes according to the generalist model. call this set of classes k. in our experiments, we used n step 2: we then take all the specialist models, m, whose special subset of confusable classes, s m , has a nonempty intersection with k and call this the active set of specialists a k (note that this set may be empty). we then find the full probability distribution q over all the classes that minimizes: where kl denotes the kl divergence, and p m p g denote the probability distribution of a specialist model or the generalist full model. the distribution p m is a distribution over all the specialist classes of m plus a single dustbin class, so when computing its kl divergence from the full q distribution we sum all of the probabilities that the full q distribution assigns to all the classes in ms dustbin. does not have a general closed form solution, though when all the models produce a single probability for each class the solution is either the arithmetic or geometric mean, depending on whether we use kl(p, q) or kl(q, p)). we parameterize q sof tmax(z) (with t 1) and we use gradient descent to optimize the logits z w.r.t. note that this optimization must be carried out for each image. starting from the trained baseline full network, the specialists train extremely fast (a few days instead of many weeks for jft). also, all the specialists are trained completely independently. table shows the absolute test accuracy for the baseline system and the baseline system combined with the specialist models. with specialist models, there is a relative improvement in test accuracy overall. we also report conditional test accuracy, which is the accuracy by only considering examples belonging to the specialist classes, and restricting our predictions to that subset of classes. for our jft specialist experiments, we trained specialist models, each with classes (plus the dustbin class). because the sets of classes for the specialists are not disjoint, we often had multiple specialists covering a particular image class. table shows the number of test set examples, the change in the number of examples correct at position when using the specialist(s), and the relative percentage improvement in top1 accuracy for the jft dataset broken down by the number of specialists covering the class. we are encouraged by the general trend that accuracy improvements are larger when we have more specialists covering a particular class, since training independent specialist models is very easy to parallelize. one of our main claims about using soft targets instead of hard targets is that a lot of helpful information can be carried in soft targets that could not possibly be encoded with a single hard target. in this section we demonstrate that this is a very large effect by using far less data to fit the 85m parameters of the baseline speech model described earlier. table shows that with only of the data (about 20m examples), training the baseline model with hard targets leads to severe overfitting (we did early stopping, as the accuracy drops sharply after reaching 44.5), whereas the same model trained with soft targets is able to recover almost all the information in the full training set (about shy). it is even more remarkable to note that we did not have to do early stopping: the system with soft targets simply converged to this shows that soft targets are a very effective way of communicating the regularities discovered by a model trained on all of the data to another model. targets (3 of training set) table 5: soft targets allow a new model to generalize well from only of the training set. the soft targets are obtained by training on the full training set. the specialists that we used in our experiments on the jft dataset collapsed all of their nonspecialist classes into a single dustbin class. if we allow specialists to have a full softmax over all classes, there may be a much better way to prevent them overfitting than using early stopping. a specialist is trained on data that is highly enriched in its special classes. this means that the effective size of its training set is much smaller and it has a strong tendency to overfit on its special classes. this problem cannot be solved by making the specialist a lot smaller because then we lose the very helpful transfer effects we get from modeling all of the nonspecialist classes. our experiment using of the speech data strongly suggests that if a specialist is initialized with the weights of the generalist, we can make it retain nearly all of its knowledge about the nonspecial classes by training it with soft targets for the nonspecial classes in addition to training it with hard targets. the soft targets can be provided by the generalist. we are currently exploring this approach. the use of specialists that are trained on subsets of the data has some resemblance to mixtures of experts [6] which use a gating network to compute the probability of assigning each example to each expert. at the same time as the experts are learning to deal with the examples assigned to them, the gating network is learning to choose which experts to assign each example to based on the relative discriminative performance of the experts for that example. using the discriminative performance of the experts to determine the learned assignments is much better than simply clustering the input vectors and assigning an expert to each cluster, but it makes the training hard to parallelize: first, the weighted training set for each expert keeps changing in a way that depends on all the other experts and second, the gating network needs to compare the performance of different experts on the same example to know how to revise its assignment probabilities. these difficulties have meant that mixtures of experts are rarely used in the regime where they might be most beneficial: tasks with huge datasets that contain distinctly different subsets. it is much easier to parallelize the training of multiple specialists. we first train a generalist model and then use the confusion matrix to define the subsets that the specialists are trained on. once these subsets have been defined the specialists can be trained entirely independently. at test time we can use the predictions from the generalist model to decide which specialists are relevant and only these specialists need to be run. we have shown that distilling works very well for transferring knowledge from an ensemble or from a large highly regularized model into a smaller, distilled model. on mnist distillation works remarkably well even when the transfer set that is used to train the distilled model lacks any examples of one or more of the classes. for a deep acoustic model that is version of the one used by android voice search, we have shown that nearly all of the improvement that is achieved by training an ensemble of deep neural nets can be distilled into a single neural net of the same size which is far easier to deploy. for really big neural networks, it can be infeasible even to train a full ensemble, but we have shown that the performance of a single really big net that has been trained for a very long time can be significantly improved by learning a large number of specialist nets, each of which learns to discriminate between the classes in a highly confusable cluster. we have not yet shown that we can distill the knowledge in the specialists back into the single large net.", "summary": "in machine learning, it is common to train a single large model (with a large number of parameters) or ensemble of multiple smaller models using the same dataset. while such large models help to improve the performance of the system, they also make it difficult and computationally expensive to deploy the system. the paper proposes to transfer the knowledge from such cumbersome models into a single, simpler model which is more suitable for deployment. this transfer of knowledge is referred to as distillation. idea train the cumbersome model using the given training data in the usual way. train the simpler, distilled model using the class probabilities (from the cumbersome model) as the soft target. thus, the simpler model is trained to generalise the same way as the cumbersome model. if the soft targets have high entropy, they provide much more information than the hard targets and the gradient (between training examples) would vary lesser. one approach is to minimise the l2 difference between logits produced by the cumbersome model and the simpler model. this approach was pursued by bucilu et al. the paper proposes a more general solution which they name distillation. the temperature of the final softmax is increased till the cumbersome model produces a set of soft targets (from the final softmax layer). these soft targets are then used to train the simpler model. it also shows that the proposed approach is, in fact, a more general case of the first approach. approach in the simplest setting, the cumbersome model is first trained with a high value of temperature and then the same temperature value is used to train the simpler model. the temperature is set to when making predictions using the simpler model. it helps to add an auxiliary objective function which corresponds to the crossentropy loss with the correct labels. the second objective function should be given a much lower weight though. further, the magnitude of the soft targets needs to be scaled by multiplying with the square of temperature. experiment the paper reports favourable results for distillation task for the following domains: image classification (on mnist dataset) an extra experiment is performed where the simpler model is not shown any images of but the model fails for only cases out of cases involving automatic speech recognition (asr) an extra experiment is performed where the baseline model is trained using both hard targets and soft targets alternatively. further, only of the total dataset is used. the model using hard targets overfits and has poor test accuracy while the model using soft targets does not overfit and gets much better test accuracy. this shows the regularizing effect of soft targets. training ensemble specialists for very large datasets (jft dataset an internal dataset at google) the experiment shows that while training a single large model would take a lot of time, the performance of the model can be improved by learning a small number of specialised networks (which are faster to train). though it is yet to be shown that the knowledge of such specialist models can be distilled back into a single model."}, {"document": "we extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. the combined system is analogous to a turing machine or von neumann architecture but is differentiable endtoend, allowing it to be efficiently trained with gradient descent. preliminary results demonstrate that neural turing machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples. computer programs make use of three fundamental mechanisms: elementary operations (e.g., arithmetic operations), logical flow control (branching), and external memory, which can be written to and read from in the course of computation (von neumann, 1945). despite its wideranging success in modelling complicated data, modern machine learning has largely neglected the use of logical flow control and external memory.. recurrent neural networks (rnns) stand out from other machine learning methods for their ability to learn and carry out complicated transformations of data over extended periods of time. moreover, it is known that rnns are turingcomplete (siegelmann and sontag, 1995), and therefore have the capacity to simulate arbitrary procedures, if properly wired. yet what is possible in principle is not always what is simple in practice. we therefore enrich the capabilities of standard recurrent networks to simplify the solution of algorithmic tasks. this enrichment is primarily via a large, addressable memory, so, by analogy to turing\u2019s enrichment of finitestate machines by an infinite memory tape, we. dub our device a neural turing machine (ntm). unlike a turing machine, an ntm is a differentiable computer that can be trained by gradient descent, yielding a practical mechanism for learning programs.. in human cognition, the process that shares the most similarity to algorithmic operation is known as working memory. while the mechanisms of working memory remain somewhat obscure at the level of neurophysiology, the verbal definition is understood to mean a capacity for shortterm storage of information and its rulebased manipulation (baddeley et al., 2009). in computational terms, these rules are simple programs, and the stored information constitutes the arguments of these programs. therefore, an ntm resembles a working memory system, as it is designed to solve tasks that require the application of approximate rules to rapidlycreated variables. rapidlycreated variables (hadley, 2009) are data that are quickly bound to memory slots, in the same way that the number and the number are put inside registers in a conventional computer and added to make (minsky, 1967). an ntm bears another close resemblance to models of working memory since the ntm architecture uses an attentional process to read from and write to memory selectively. in contrast to most models of working memory, our architecture can learn to use its working memory instead of deploying a fixed set of procedures over symbolic data.. the organisation of this report begins with a brief review of germane research on working memory in psychology, linguistics, and neuroscience, along with related research in artificial intelligence and neural networks. we then describe our basic contribution, a memory architecture and attentional controller that we believe is wellsuited to the performance of tasks that require the induction and execution of simple programs. to test this architecture, we have constructed a battery of problems, and we present their precise descriptions along with our results. we conclude by summarising the strengths of the architecture. the concept of working memory has been most heavily developed in psychology to explain the performance of tasks involving the shortterm manipulation of information. the broad picture is that a central executive focuses attention and performs operations on data in a memory buffer (baddeley et al., 2009). psychologists have extensively studied the capacity limitations of working memory, which is often quantified by the number of chunks of information that can be readily recalled (miller, 1956).1 these capacity limitations lead toward an understanding of structural constraints in the human working memory system, but in our own work we are happy to exceed them.. in neuroscience, the working memory process has been ascribed to the functioning of a system composed of the prefrontal cortex and basal ganglia (goldmanrakic, 1995). 1there remains vigorous debate about how best to characterise capacity limitations (barrouillet et al., 2004).. ical experiments involve recording from a single neuron or group of neurons in prefrontal cortex while a monkey is performing a task that involves observing a transient cue, waiting through a delay period, then responding in a manner dependent on the cue. certain tasks elicit persistent firing from individual neurons during the delay period or more complicated neural dynamics. a recent study quantified delay period activity in prefrontal cortex for a complex, contextdependent task based on measures of dimensionality of the population code and showed that it predicted memory performance (rigotti et al., 2013).. modeling studies of working memory range from those that consider how biophysical circuits could implement persistent neuronal firing (wang, 1999) to those that try to solve explicit tasks (hazy et al., 2006) (dayan, 2008) (eliasmith, 2013). of these, hazy et al.\u2019s model is the most relevant to our work, as it is itself analogous to the long shortterm memory architecture, which we have modified ourselves. as in our architecture, hazy et al.\u2019s has mechanisms to gate information into memory slots, which they use to solve a memory task constructed of nested rules. in contrast to our work, the authors include no sophisticated notion of memory addressing, which limits the system to storage and recall of relatively simple, atomic data. addressing, fundamental to our work, is usually left out from computational models in neuroscience, though it deserves to be mentioned that gallistel and king (gallistel and king, 2009) and marcus (marcus, 2003) have argued that addressing must be implicated in the operation of the brain. historically, cognitive science and linguistics emerged as fields at roughly the same time as artificial intelligence, all deeply influenced by the advent of the computer (chomsky, 1956) (miller, 2003). their intentions were to explain human mental behaviour based on information or symbolprocessing metaphors. in the early 1980s, both fields considered recursive or procedural (rulebased) symbolprocessing to be the highest mark of cognition. the parallel distributed processing (pdp) or connectionist revolution cast aside the symbolprocessing metaphor in favour of a socalled subsymbolic description of thought processes (rumelhart et al., 1986).. fodor and pylyshyn (fodor and pylyshyn, 1988) famously made two barbed claims about the limitations of neural networks for cognitive modeling. they first objected that connectionist theories were incapable of variablebinding, or the assignment of a particular datum to a particular slot in a data structure. in language, variablebinding is ubiquitous; for example, when one produces or interprets a sentence of the form, mary spoke to john, one has assigned mary the role of subject, john the role of object, and spoke to the role of the transitive verb. fodor and pylyshyn also argued that neural networks with fixedlength input domains could not reproduce human capabilities in tasks that involve processing variablelength structures. in response to this criticism, neural network researchers including hinton (hinton, 1986), smolensky (smolensky, 1990), touretzky (touretzky, 1990), pollack (pollack, 1990), plate (plate, 2003), and kanerva (kanerva, 2009) investigated specific mechanisms that could support both variablebinding and variablelength. structure within a connectionist framework. our architecture draws on and potentiates this work.. recursive processing of variablelength structures continues to be regarded as a hallmark of human cognition. in the last decade, a firefight in the linguistics community staked several leaders of the field against one another. at issue was whether recursive processing is the uniquely human evolutionary innovation that enables language and is specialized to language, a view supported by fitch, hauser, and chomsky (fitch et al., 2005), or whether multiple new adaptations are responsible for human language evolution and recursive processing predates language (jackendoff and pinker, 2005). regardless of recursive processing\u2019s evolutionary origins, all agreed that it is essential to human cognitive flexibility. recurrent neural networks constitute a broad class of machines with dynamic state; that is, they have state whose evolution depends both on the input to the system and on the current state. in comparison to hidden markov models, which also contain dynamic state, rnns have a distributed state and therefore have significantly larger and richer memory and computational capacity. dynamic state is crucial because it affords the possibility of contextdependent computation; a signal entering at a given moment can alter the behaviour of the network at a much later moment.. a crucial innovation to recurrent networks was the long shortterm memory (lstm) (hochreiter and schmidhuber, 1997). this very general architecture was developed for a specific purpose, to address the vanishing and exploding gradient problem (hochreiter et al., 2001a), which we might relabel the problem of vanishing and exploding sensitivity. lstm ameliorates the problem by embedding perfect integrators (seung, 1998) for memory storage in the network. the simplest example of a perfect integrator is the equation x(t 1) x(t) i(t), where i(t) is an input to the system. the implicit identity matrix ix(t) means that signals do not dynamically vanish or explode. if we attach a mechanism to this integrator that allows an enclosing network to choose when the integrator listens to inputs, namely, a programmable gate depending on context, we have an equation of the form x(t 1) x(t) g(context)i(t). we can now selectively store information for an indefinite length of time.. recurrent networks readily process variablelength structures without modification. in sequential problems, inputs to the network arrive at different times, allowing variablelength or composite structures to be processed over multiple steps. because they natively handle variablelength structures, they have recently been used in a variety of cognitive problems, including speech recognition (graves et al., 2013; graves and jaitly, 2014), text generation (sutskever et al., 2011), handwriting generation (graves, 2013) and machine translation (sutskever et al., 2014). considering this property, we do not feel that it is urgent or even necessarily valuable to build explicit parse trees to merge composite structures greedily (pollack, 1990) (socher et al., 2012) (frasconi et al., 1998).. other important precursors to our work include differentiable models of attention (graves,. 2013) (bahdanau et al., 2014) and program search (hochreiter et al., 2001b) (das et al., 1992), constructed with recurrent neural networks. a neural turing machine (ntm) architecture contains two basic components: a neural network controller and a memory bank. figure presents a highlevel diagram of the ntm architecture. like most neural networks, the controller interacts with the external world via input and output vectors. unlike a standard network, it also interacts with a memory matrix using selective read and write operations. by analogy to the turing machine we refer to the network outputs that parametrise these operations as heads.. crucially, every component of the architecture is differentiable, making it straightforward to train with gradient descent. we achieved this by defining blurry\u2019 read and write operations that interact to a greater or lesser degree with all the elements in memory (rather than addressing a single element, as in a normal turing machine or digital computer). the degree of blurriness is determined by an attentional focus mechanism that constrains each read and write operation to interact with a small portion of the memory, while ignoring the rest. because interaction with the memory is highly sparse, the ntm is biased towards storing data without interference. the memory location brought into attentional focus is determined by specialised outputs emitted by the heads. these outputs define a normalised weighting over the rows in the memory matrix (referred to as memory locations). each weighting, one per read or write head, defines the degree to which the head reads or writes. a head can thereby attend sharply to the memory at a single location or weakly to the memory at many locations. let mt be the contents of the n m memory matrix at time t, where n is the number of memory locations, and m is the vector size at each location. let wt be a vector of weightings over the n locations emitted by a read head at time t. since all weightings are normalised, the n elements wt(i) of wt obey the following constraints:. the length m read vector rt returned by the head is defined as a convex combination of the rowvectors mt(i) in memory:. which is clearly differentiable with respect to both the memory and the weighting. taking inspiration from the input and forget gates in lstm, we decompose each write into two parts: an erase followed by an add.. given a weighting wt emitted by a write head at time t, along with an erase vector et whose m elements all lie in the range (0, 1), the memory vectors mt1(i) from the previous timestep are modified as follows:. where is a rowvector of all 1s, and the multiplication against the memory location acts pointwise. therefore, the elements of a memory location are reset to zero only if both the weighting at the location and the erase element are one; if either the weighting or the erase is zero, the memory is left unchanged. when multiple write heads are present, the erasures can be performed in any order, as multiplication is commutative.. each write head also produces a lengthm add vector at, which is added to the memory after the erase step has been performed:. once again, the order in which the adds are performed by multiple heads is irrelevant. the combined erase and add operations of all the write heads produces the final content of the memory at time t. since both erase and add are differentiable, the composite write operation is differentiable too. note that both the erase and add vectors have m independent components, allowing finegrained control over which elements in each memory location are modified. although we have now shown the equations of reading and writing, we have not described how the weightings are produced. these weightings arise by combining two addressing mechanisms with complementary facilities. the first mechanism, contentbased addressing, focuses attention on locations based on the similarity between their current values and values emitted by the controller. this is related to the contentaddressing of hopfield networks (hopfield, 1982). the advantage of contentbased addressing is that retrieval is simple, merely requiring the controller to produce an approximation to a part of the stored data, which is then compared to memory to yield the exact stored value.. however, not all problems are wellsuited to contentbased addressing. in certain tasks the content of a variable is arbitrary, but the variable still needs a recognisable name or address. arithmetic problems fall into this category: the variable x and the variable y can take on any two values, but the procedure f(x, y) x y should still be defined. a controller for this task could take the values of the variables x and y, store them in different addresses, then retrieve them and perform a multiplication algorithm. in this case, the variables are addressed by location, not by content. we call this form of addressing locationbased addressing. contentbased addressing is strictly more general than locationbased addressing as the content of a memory location could include location information inside it. in our experiments however, providing locationbased addressing as a primitive operation proved essential for some forms of generalisation, so we employ both mechanisms concurrently.. figure presents a flow diagram of the entire addressing system that shows the order of operations for constructing a weighting vector when reading or writing. for contentaddressing, each head (whether employed for reading or writing) first produces a length m key vector kt that is compared to each vector mt(i) by a similarity measure k [ , ] the contentbased system produces a normalised weighting wct based on the similarity and a positive key strength, t, which can amplify or attenuate the precision of the focus:. (5) in our current implementation, the similarity measure is cosine similarity:. (6) the locationbased addressing mechanism is designed to facilitate both simple iteration across the locations of the memory and randomaccess jumps. it does so by implementing a rotational shift of a weighting. for example, if the current weighting focuses entirely on a single location, a rotation of would shift the focus to the next location. a negative shift would move the weighting in the opposite direction.. prior to rotation, each head emits a scalar interpolation gate gt in the range (0, 1). the value of g is used to blend between the weighting wt1 produced by the head at the previous timestep and the weighting wct produced by the content system at the current timestep, yielding the gated weighting wgt :. if the gate is zero, then the content weighting is entirely ignored, and the weighting from the previous time step is used. conversely, if the gate is one, the weighting from the previous iteration is ignored, and the system applies contentbased addressing.. after interpolation, each head emits a shift weighting st that defines a normalised distribution over the allowed integer shifts. for example, if shifts between and are allowed, st has three elements corresponding to the degree to which shifts of 1, and are performed. the simplest way to define the shift weightings is to use a softmax layer of the appropriate size attached to the controller. we also experimented with another technique, where the controller emits a single scalar that is interpreted as the lower bound of a width one uniform distribution over shifts. for example, if the shift scalar is 6.7, then st(6) 0.3, st(7) 0.7, and the rest of st is zero.. if we index the n memory locations from to n 1, the rotation applied to wgt by st can be expressed as the following circular convolution:. where all index arithmetic is computed modulo n the convolution operation in equation (8) can cause leakage or dispersion of weightings over time if the shift weighting is not sharp. for example, if shifts of 1, and are given weights of 0.1, and 0.1, the rotation will transform a weighting focused at a single point into one slightly blurred over three points. to combat this, each head emits one further scalar t whose effect is to sharpen the final weighting as follows:. the combined addressing system of weighting interpolation and content and locationbased addressing can operate in three complementary modes. one, a weighting can be chosen by the content system without any modification by the location system. two, a weighting produced by the content addressing system can be chosen and then shifted. this allows the focus to jump to a location next to, but not on, an address accessed by content; in computational terms this allows a head to find a contiguous block of data, then access a particular element within that block. three, a weighting from the previous time step can be rotated without any input from the contentbased addressing system. this allows the weighting to iterate through a sequence of addresses by advancing the same distance at each timestep. the ntm architecture architecture described above has several free parameters, including the size of the memory, the number of read and write heads, and the range of allowed location shifts. but perhaps the most significant architectural choice is the type of neural network used as the controller. in particular, one has to decide whether to use a recurrent or feedforward network. a recurrent controller such as lstm has its own internal memory that can complement the larger memory in the matrix. if one compares the controller to the central processing unit in a digital computer (albeit with adaptive rather than predefined instructions) and the memory matrix to ram, then the hidden activations of the recurrent controller are akin to the registers in the processor. they allow the controller to mix information across multiple time steps of operation. on the other hand a feedforward controller can mimic a recurrent network by reading and writing at the same location in memory at every step. furthermore, feedforward controllers often confer greater transparency to the network\u2019s operation because the pattern of reading from and writing to the memory matrix is usually easier to interpret than the internal state of an rnn. however, one limitation of. a feedforward controller is that the number of concurrent read and write heads imposes a bottleneck on the type of computation the ntm can perform. with a single read head, it can perform only a unary transform on a single memory vector at each timestep, with two read heads it can perform binary vector transforms, and so on. recurrent controllers can internally store read vectors from previous timesteps, so do not suffer from this limitation. this section presents preliminary experiments on a set of simple algorithmic tasks such as copying and sorting data sequences. the goal was not only to establish that ntm is able to solve the problems, but also that it is able to do so by learning compact internal programs. the hallmark of such solutions is that they generalise well beyond the range of the training data. for example, we were curious to see if a network that had been trained to copy sequences of length up to could copy a sequence of length with no further training.. for all the experiments we compared three architectures: ntm with a feedforward controller, ntm with an lstm controller, and a standard lstm network. because all the tasks were episodic, we reset the dynamic state of the networks at the start of each input sequence. for the lstm networks, this meant setting the previous hidden state equal to a learned bias vector. for ntm the previous state of the controller, the value of the previous read vectors, and the contents of the memory were all reset to bias values. all the tasks were supervised learning problems with binary targets; all networks had logistic sigmoid output layers and were trained with the crossentropy objective function. sequence prediction errors are reported in bitspersequence. for more details about the experimental parameters see section the copy task tests whether ntm can store and recall a long sequence of arbitrary information. the network is presented with an input sequence of random binary vectors followed by a delimiter flag. storage and access of information over long time periods has always been problematic for rnns and other dynamic architectures. we were particularly interested to see if an ntm is able to bridge longer time delays than lstm.. the networks were trained to copy sequences of eight bit random vectors, where the sequence lengths were randomised between and the target sequence was simply a copy of the input sequence (without the delimiter flag). note that no inputs were presented to the network while it receives the targets, to ensure that it recalls the entire sequence with no intermediate assistance.. as can be seen from figure 3, ntm (with either a feedforward or lstm controller) learned much faster than lstm alone, and converged to a lower cost. the disparity between the ntm and lstm learning curves is dramatic enough to suggest a qualitative,. rather than quantitative, difference in the way the two models solve the problem. we also studied the ability of the networks to generalise to longer sequences than seen during training (that they can generalise to novel vectors is clear from the training error). figures and demonstrate that the behaviour of lstm and ntm in this regime is radically different. ntm continues to copy as the length increases2, while lstm rapidly degrades beyond length the preceding analysis suggests that ntm, unlike lstm, has learned some form of copy algorithm. to determine what this algorithm is, we examined the interaction between the controller and the memory (figure 6). we believe that the sequence of operations performed by the network can be summarised by the following pseudocode:. initialise: move head to start location while input delimiter not seen do. receive input vector write input to head location increment head location by end while return head to start location while true do. read output vector from head location emit output increment head location by end while. this is essentially how a human programmer would perform the same task in a low2the limiting factor was the size of the memory (128 locations), after which the cyclical shifts wrapped. around and previous writes were overwritten.. level programming language. in terms of data structures, we could say that ntm has learned how to create and iterate through arrays. note that the algorithm combines both contentbased addressing (to jump to start of the sequence) and locationbased addressing (to move along the sequence). also note that the iteration would not generalise to long sequences without the ability to use relative shifts from the previous read and write weightings (equation 7), and that without the focussharpening mechanism (equation 9) the weightings would probably lose precision over time. the repeat copy task extends copy by requiring the network to output the copied sequence a specified number of times and then emit an endofsequence marker. the main motivation was to see if the ntm could learn a simple nested function. ideally, we would like it to be able to execute a for loop containing any subroutine it has already learned.. the network receives randomlength sequences of random binary vectors, followed by a scalar value indicating the desired number of copies, which appears on a separate input channel. to emit the end marker at the correct time the network must be both able to interpret the extra input and keep count of the number of copies it has performed so far. as with the copy task, no inputs are provided to the network after the initial sequence and repeat number. the networks were trained to reproduce sequences of size eight random binary vectors, where both the sequence length and the number of repetitions were chosen randomly from one to ten. the input representing the repeat number was normalised to have mean zero and variance one.. figure shows that ntm learns the task much faster than lstm, but both were able to solve it perfectly.3 the difference between the two architectures only becomes clear when they are asked to generalise beyond the training data. in this case we were interested in generalisation along two dimensions: sequence length and number of repetitions. figure illustrates the effect of doubling first one, then the other, for both lstm and ntm. whereas lstm fails both tests, ntm succeeds with longer sequences and is able to perform more than ten repetitions; however it is unable to keep count of of how many repeats it has completed, and does not predict the end marker correctly. this is probably a consequence of representing the number of repetitions numerically, which does not easily generalise beyond a fixed range.. figure suggests that ntm learns a simple extension of the copy algorithm in the previous section, where the sequential read is repeated as many times as necessary. the previous tasks show that the ntm can apply algorithms to relatively simple, linear data structures. the next order of complexity in organising data arises from indirectionthat is, when one data item points to another. we test the ntm\u2019s capability for learning an instance of this more interesting class by constructing a list of items so that querying with one of the items demands that the network return the subsequent item. more specifically, we define an item as a sequence of binary vectors that is bounded on the left and right by delimiter symbols. after several items have been propagated to the network, we query by showing a random item, and we ask the network to produce the next item. in our experiments, each item consisted of three sixbit binary vectors (giving a total of bits. 3it surprised us that lstm performed better here than on the copy problem. the likely reasons are that the sequences were shorter (up to length instead of up to 20), and the lstm network was larger and therefore had more memory capacity.. per item). during training, we used a minimum of items and a maximum of items in a single episode.. figure shows that ntm learns this task significantly faster than lstm, terminating at near zero cost within approximately 30, episodes, whereas lstm does not reach zero cost after a million episodes. additionally, ntm with a feedforward controller learns faster than ntm with an lstm controller. these two results suggest that ntm\u2019s external memory is a more effective way of maintaining the data structure than lstm\u2019s internal state. ntm also generalises much better to longer sequences than lstm, as can be seen in figure ntm with a feedforward controller is nearly perfect for sequences of up to items (twice the maximum length used in training), and still has an average cost below bit per sequence for sequences of items.. in figure 12, we show the operation of the ntm memory, controlled by an lstm with one head, on a single test episode. in inputs, we see that the input denotes item delimiters as single bits in row after the sequence of items has been propagated, a. the ntm with either a feedforward or lstm controller generalises to much longer sequences of items than the lstm alone. in particular, the ntm with a feedforward controller is nearly perfect for item sequences of twice the length of sequences in its training set.. delimiter in row prepares the network to receive a query item. in this case, the query item corresponds to the second item in the sequence (contained in the green box). in outputs, we see that the network crisply outputs item in the sequence (from the red box). in read weightings, on the last three time steps, we see that the controller reads from contiguous locations that each store the time slices of item this is curious because it appears that the network has jumped directly to the correct location storing item however we can explain this behaviour by looking at write weightings. here we see that the memory is written to even when the input presents a delimiter symbol between items. one can confirm in adds that data are indeed written to memory when the delimiters are presented (e.g., the data within the black box); furthermore, each time a delimiter is presented, the vector added to memory is different. further analysis of the memory reveals that the network accesses the location it reads after the query by using a contentbased lookup that produces a weighting that is shifted by one. additionally, the key used for contentlookup corresponds to the vector that was added in the black box. this implies the following memoryaccess algorithm: when each item delimiter is presented, the controller writes a compressed representation of the previous three time slices of the item. after the query arrives, the controller recomputes the same compressed representation of the query item, uses a contentbased lookup to find the location where it wrote the first representation, and then shifts by one to produce the subsequent item in the sequence (thereby combining contentbased lookup with locationbased offsetting). the goal of the dynamic ngrams task was to test whether ntm could rapidly adapt to new predictive distributions. in particular we were interested to see if it were able to use its. memory as a rewritable table that it could use to keep count of transition statistics, thereby emulating a conventional ngram model.. we considered the set of all possible 6gram distributions over binary sequences. each 6gram distribution can be expressed as a table of numbers, specifying the probability that the next bit will be one, given all possible length five binary histories. for each training example, we first generated random 6gram probabilities by independently drawing all probabilities from the beta(1. , ) distribution.. we then generated a particular training sequence by drawing successive bits using the current lookup table.4 the network observes the sequence one bit at a time and is then asked to predict the next bit. the optimal estimator for the problem can be determined by. 4the first bits, for which insufficient context exists to sample from the table, are drawn i.i.d. from a bernoulli distribution with p bayesian analysis (murphy, 2012):. where c is the five bit previous context, b is the value of the next bit and n0 and n1 are respectively the number of zeros and ones observed after c so far in the sequence. we can therefore compare ntm to the optimal predictor as well as lstm. to assess performance we used a validation set of length sequences sampled from the same distribution as the training data. as shown in figure 13, ntm achieves a small, but significant performance advantage over lstm, but never quite reaches the optimum cost.. the evolution of the two architecture\u2019s predictions as they observe new inputs is shown in figure 14, along with the optimal predictions. close analysis of ntm\u2019s memory usage (figure 15) suggests that the controller uses the memory to count how many ones and zeros it has observed in different contexts, allowing it to implement an algorithm similar to the optimal estimator. this task tests whether the ntm can sort dataan important elementary algorithm. a sequence of random binary vectors is input to the network along with a scalar priority rating for each vector. the priority is drawn uniformly from the range [1, 1]. the target sequence contains the binary vectors sorted according to their priorities, as depicted in figure each input sequence contained binary vectors with corresponding priorities, and each target sequence was the highestpriority vectors in the input.5 inspection of ntm\u2019s. 5we limited the sort to size because we were interested to see if ntm would solve the task using a binary heap sort of depth memory use led us to hypothesise that it uses the priorities to determine the relative location of each write. to test this hypothesis we fitted a linear function of the priority to the observed write locations. figure shows that the locations returned by the linear function closely match the observed write locations. it also shows that the network reads from the memory locations in increasing order, thereby traversing the sorted sequence.. the learning curves in figure demonstrate that ntm with both feedforward and lstm controllers substantially outperform lstm on this task. note that eight parallel read and write heads were needed for best performance with a feedforward controller on this task; this may reflect the difficulty of sorting vectors using only unary vector operations (see section 3.4). for all experiments, the rmsprop algorithm was used for training in the form described in (graves, 2013) with momentum of tables to give details about the network configurations and learning rates used in the experiments. all lstm networks had three stacked hidden layers. note that the number of lstm parameters grows quadratically with. the number of hidden units (due to the recurrent connections in the hidden layers). this contrasts with ntm, where the number of parameters does not increase with the number of memory locations. during the training backward pass, all gradient components are clipped elementwise to the range (10, 10). we have introduced the neural turing machine, a neural network architecture that takes inspiration from both models of biological working memory and the design of digital computers. like conventional neural networks, the architecture is differentiable endtoend and can be trained with gradient descent. our experiments demonstrate that it is capable of learning simple algorithms from example data and of using these algorithms to generalise well outside its training regime. many have offered thoughtful insights, but we would especially like to thank daan wierstra, peter dayan, ilya sutskever, charles blundell, joel veness, koray kavukcuoglu, dharshan kumaran, georg ostrovski, chris summerfield, jeff dean, geoffrey hinton, and demis hassabis.", "summary": "neural turing machine (ntm) consists of a neural network controller interacting with a working memory bank in a learnable manner. this is analogous to computers controllers cpu (hidden activations as registers) and memory matrix ram. controller (modified rnn) interacts with external world via input and output vectors, and with memory via read and write heads. read vector is a convex combination of rowvectors of mt (memory matrix at time t) r\\t \\sum w\\t(i) m\\t(i) where wt is a vector of weightings over n memory locations. writing is decomposed into 1) erasing and 2) adding. the write head produces the erase vector et and the add vector at along with the vector of weightings over memory locations wt. erase and add vectors control which components of memory are updated, while weightings wt control which locations are updated. weight vectors are produced by an addressing mechanism. contentbased addressing. each head produces length m key kt that is compared to each vector mt(i) by cosine similarity and a temperature parameter. the weightings are normalized (softmax).. locationbased addressing. interpolation: each head produces interpolation gate gt that is used to blend between weighting at previous time step and the content weighting of current tilmestep wg\\t g\\t wc\\t (1g\\t)w\\t1. shift: circular convolution (modulo n) with a shift weighting distribution, for example softmax over integer shift positions (say locations). sharpening: each head emits \\gammat to sharpen the final weighting. experiments on copy, repeatcopy, associative memory, ngram emulator and priority sort. [attention and augmented rnns]([url]/)."}, {"document": "we present an autoencoder that leverages learned representations to better measure similarities in data space. by combining a variational autoencoder with a generative adversarial network we can use learned feature representations in the gan discriminator as basis for the vae reconstruction objective. thereby, we replace elementwise errors with featurewise errors to better capture the data distribution while offering invariance towards e.g. we apply our method to images of faces and show that it outperforms vaes with elementwise similarity measures in terms of visual fidelity. moreover, we show that the method learns an embedding in which highlevel abstract visual features (e.g. wearing glasses) can be modified using simple arithmetic. deep architectures have allowed a wide range of discriminative models to scale to large and diverse datasets. however, generative models still have problems with complex data distributions such as images and sound. in this work, we show that currently used similarity metrics impose a hurdle for learning good generative models and that we can improve a generative model by employing a learned similarity measure.. when learning models such as the variational autoencoder (vae) (kingma welling, 2014; rezende et al., 2014), the choice of similarity metric is central as it provides the main part of the training signal via the reconstruction er. preliminary work submitted to the international conference on machine learning (icml).. x. z. x real / gen. encoder. overview of our network. we combine a vae with a gan by collapsing the decoder and the generator into one.. ror objective. for this task, elementwise measures like the squared error are the default. elementwise metrics are simple but not very suitable for image data, as they do not model the properties of human visual perception. a small image translation might result in a large pixelwise error whereas a human would barely notice the change. therefore, we argue in favor of measuring image similarity using a higherlevel and sufficiently invariant representation of the images. rather than handengineering a suitable measure to accommodate the problems of elementwise metrics, we want to learn a function for the task. the question is how to learn such a similarity measure? we find that by jointly training a vae and a generative adversarial network (gan) (goodfellow et al., 2014) we can use the gan discriminator to measure sample similarity. we achieve this by combining a vae with a gan as shown in fig. we collapse the vae decoder and the gan generator into one by letting them share parameters and training them jointly. for the vae training objective, we replace the typical elementwise reconstruction metric with a featurewise metric expressed in the discriminator. our contributions are as follows:. we combine vaes and gans into an unsupervised generative model that simultaneously learns to encode, generate and compare dataset samples.. we show that generative models trained with learned similarity measures produce better image samples than models trained with elementwise error measures.. we demonstrate that unsupervised training results in a latent image representation with disentangled factors of variation (bengio et al., 2013). this is illustrated in experiments on a dataset of face images labelled with visual attribute vectors, where it is shown that simple arithmetic applied in the learned latent space produces images that reflect changes in these attributes. in this section we provide background on vaes and gans. then, we introduce our method for combining both approaches, which we refer to as vae/gan. as we\u2019ll describe, our proposed hybrid is motivated as a way to improve vae, so that it relies on a more meaningful, featurewise metric for measuring reconstruction quality during training. a vae consists of two networks that encode a data sample x to a latent representation z and decode the latent representation back to data space, respectively:. the vae regularizes the encoder by imposing a prior over the latent distribution p(z). typically z n (0, i) is chosen. the vae loss is minus the sum of the expected log likelihood (the reconstruction error) and a prior regularization term:. p(xz)p(z) q(zx) ] lpixelllike lprior. lpixelllike eq(zx) [log p(xz)] (3) lprior dkl(q(zx)p(z)) , (4). where dkl is the kullbackleibler divergence. a gan consists of two networks: the generator network gen(z) maps latents z to data space while the discriminator network assigns probability y dis(x) [0, 1] that. x is an actual training sample and probability y that x is generated by our model through x gen(z) with z p(z). the gan objective is to find the binary classifier that gives the best possible discrimination between true and generated data and simultaneously encouraging gen to fit the true data distribution. we thus aim to maximize/minimize the binary cross entropy:. lgan log(dis(x)) log(1dis(gen(z))) , (5) with respect to dis /gen with x being a training sample and z p(z). an appealing property of gan is that its discriminator network implicitly has to learn a rich similarity metric for images, so as to discriminate them from nonimages. we thus propose to exploit this observation so as to transfer the properties of images learned by the discriminator into a more abstract reconstruction error for the vae. the end result will be a method that combines the advantage of gan as a high quality generative model and vae as a method that produces an encoder of data into the latent space z.. specifically, since elementwise reconstruction errors are not adequate for images and other signals with invariances, we propose replacing the vae reconstruction (expected log likelihood) error term from eq. with a reconstruction error expressed in the gan discriminator. to achieve this, let disl(x) denote the hidden representation of the lth layer of the discriminator. we introduce a gaussian observation model for disl(x) with mean disl(x) and identity covariance:. p(disl(x)z) n (disl(x)disl(x), i) , (6) where x dec(z) is the sample from the decoder of x. we can now replace the vae error of eq. ldislllike eq(zx) [log p(disl(x)z)] (7) we train our combined model with the triple criterion. l lprior ldislllike lgan (8) notably, we optimize the vae wrt. lgan which we regard as a style error in addition to the reconstruction error which can be interpreted as a content error using the terminology from gatys et al. moreover, since both dec and gen map from z to x, we share the parameters between the two (or in other words, we use dec instead of gen in eq. in practice, we have observed the devil in the details during development and training of this model. we therefore provide a list of practical considerations in this section. for overviews of the training procedure.. x. z. x. flow through the combined vae/gan model during training. gray lines represent terms in the training objective.. limiting error signals to relevant networks using the loss function in eq. 8, we train both a vae and a gan simultaneously. this is possible because we do not update all network parameters wrt. in particular, dis should not try to minimize ldislllike as this would collapse the discriminator to we also observe better results by not backpropagating the error signal from lgan to enc.. weighting vae vs. gan as dec receives an error signal from both ldislllike and lgan, we use a parameter to weight the ability to reconstruct vs. fooling the discriminator. this can also be interpreted as weighting style and content. rather than applying to the entire model (eq. 8), we perform the weighting only when updating the parameters of dec:. dec dec(ldislllike lgan) (9). discriminating based on samples from p(z) and q(zx) we observe better results when using samples from q(zx) (i.e. the encoder enc) in addition to our prior p(z) in the gan objective:. lgan log(dis(x)) log(1dis(dec(z))) log(1dis(dec(enc(x)))) (10). note that the regularization of the latent space lprior should make the set of samples from either p(z) or q(zx) similar. however, for any given example x, the negative sample dec(enc(x)) is much more likely to be similar to x than dec(z). when updating according to lgan, we suspect that having similar positive and negative samples makes for a more useful learning signal. elementwise distance measures are notoriously inadequate for complex data distributions like images. in the computer vision community, preprocessing images is a. algorithm training the vae/gan model enc,dec,dis initialize network parameters repeat x random minibatch from dataset z enc(x) lprior dkl(q(zx)p(z)) x dec(z) ldislllike eq(zx) [p(disl(x)z)] zp samples from prior n (0, i) xp dec(zp) lgan log(dis(x)) log(1dis(x)). log(1dis(xp)) // update parameters according to gradients enc. enc(lprior ldislllike ) dec. dec(ldislllike lgan) dis. dislgan until deadline. prevalent solution to improve robustness to certain perturbations. examples of preprocessing are contrast normalization, working with gradient images or pixel statistics gathered in histograms. we view these operations as a form of metric engineering to account for the shortcomings of simple elementwise distance measures. a more detailed discussion on the subject is provided by wang bovik (2009).. neural networks have been applied to metric learning in form of the siamese architecture (bromley et al., 1993; chopra et al., 2005). the learned distance metric is minimized for similar samples and maximized for dissimilar samples using a max margin cost. however, since siamese networks are trained in a supervised setup, we cannot apply them directly to our problem.. several attempts at improving on elementwise distances for generative models have been proposed within the last year. (2015) apply the structural similarity index as an autoencoder (ae) reconstruction metric for greyscale images. (2015) let a vae output two additional images to learn shape and edge structures more explicitly. (2015) append a ganbased sharpening step to their generative model. (2015) supplement a squared error measure with both a gan and an image gradientbased similarity measure to improve image sharpness of video prediction. while all these extensions yield visibly sharper images, they do not have the same potential for capturing highlevel structure compared to a deep learning approach.. in contrast to aes that model the relationship between a dataset sample and a latent representation directly, gans learn to generate samples indirectly. gan generator to produce samples that imitate the dataset according to the gan discriminator, gans avoid elementwise similarity measures by construction. this is a likely explanation for their ability to produce highquality images as demonstrated by denton et al. lately, convolutional networks with upsampling have shown useful for generating images from a latent representation. this has sparked interest in learning image embeddings where semantic relationships can be expressed using simple arithmetic similar to the suprising results of the word2vec model by mikolov et al. first, dosovitskiy et al. (2015) used supervised training to train convolutional network to generate chairs given highlevel information about the desired chair. (2015) have demonstrated encoderdecoder architectures with disentangled feature representations, but their training schemes rely on supervised information. (2015) inspect the latent space of a gan after training and find directions corresponding to eyeglasses and smiles. as they rely on pure gans, however, they cannot encode images making it challenging to explore the latent space.. our idea of a learned similarity metric is partly motivated by the neural artistic style network of gatys et al. (2015) who demonstrate the representational power of deep convolutional features. they obtain impressive results by optimizing an image to have similar features as a subject image and similar feature correlations as a style image in a pretrained convolutional network. in our vae/gan model, one could view ldislllike as content and lgan as style. our style term, though, is not computed from feature correlations but is the error signal from trying to fool the gan discriminator. measuring the quality of generative models is challenging as current evaluation methods are problematic for larger natural images (theis et al., 2015). in this work, we use images of size 64x64 and focus on more qualitative assessments since traditional log likelihood measures do not capture visual fidelity. indeed, we have tried discarding the gan discriminator after training of the vae/gan model and computing a pixelbased log likelihood using the remaining vae. the results are far from competitive with plain vae models (on the cifar10 dataset).. in this section we investigate the performance of different generative models:. plain vae with an elementwise gaussian observation model.. vae with a learned distance (vaedisl ). we first train a gan and use the discriminator network as a learned similarity measure. we select a single layer l at which we measure the similariy according to disl. l is chosen such that the comparison is performed after downsamplings of each a factor of in the convolutional encoder.. the combined vae/gan model. this model is similar to vaedisl but we also optimize dec wrt. this modes has recently been shown capable of generating highquality images (radford et al., 2015).. all models share the same architectures for enc, dec and dis respectively. for all our experiments, we use convolutional architectures and use backward convolution (aka. fractional striding) with stride to upscale images in dec. backward convolution is achieved by flipping the convolution direction such that striding causes upsampling. our models are trained with rmsprop using a learning rate of and a batch size of in table we list the network architectures. we refer to our implementation available online1. we apply our methods to face images from the celeba dataset2 (liu et al., 2015). this dataset consists of 202,599 images annotated with binary attributes such as eyeglasses, bangs, pale skin etc. we scale and crop the images to pixels and use only the images (not the attributes) for unsupervised training.. after training, we draw samples from p(z) and propagate these through dec to generate new images which are shown in fig. the plain vae is able draw the frontal part of the face sharply, but offcenter the images get blurry. this is because the dataset aligns faces using frontal landmarks. when we move too far away from the aligned parts, the recognition model breaks down because pixel correspondence cannot be assumed. vaedisl produces sharper images even offcenter because the reconstruction error is lifted beyond pixels. however, we see severe noisy artefacts which we believe are caused by the harsh downsampling scheme. in comparison, vae/gan and pure gan produce sharper images with more natural textures and face parts.. additionally, we make the vaes reconstruct images taken from a separate test set. reconstruction is not possible with the gan model as it lacks an encoder network. the results are shown in fig. and our conclusions are similar to what. 1http://github.com/andersbll/ autoencodingbeyondpixels. 2we use the aligned and cropped version of the dataset.. we observed for the random samples. note that vaedisl generates noisy blue patterns in some of the reconstructions. we suspect the ganbased similarity measure can collapse to in certain cases (such as the pattern we observe), which encourages dec to generate such patterns. inspired by attempts at learning embeddings in which semantic concepts can be expressed using simple arithmetic (mikolov et al., 2013), we inspect the latent space of a trained vae/gan model. the idea is to find directions. in the latent space corresponding to specific visual features in image space.. we use the binary attributes of the dataset to extract visual attribute vectors. for all images we use the encoder to calculate latent vector representations. for each attribute, we compute the mean vector for images with the attribute and the mean vector for images without the attribute. we then compute the visual attribute vector as the difference between the two mean vectors. this is a very simple method for calculating visual attribute vectors that will have problems with highly correlated visual attributes such as heavy makeup and wearing lipstick. 5, we show face images as well as the reconstructions after adding different visual attribute vectors to the latent representations. though not perfect, we clearly see that the attribute vectors capture semantic concepts like eyeglasses, bangs, etc. when bangs are added to the faces, both the hair color and the hair texture matches the original face. we also see that being a man is highly correlated with having a mustache, which is caused by attribute correlations in the dataset. inspired by the attribute similarity experiment of yan et al. (2015), we seek a more quantative evaluation of our generated images. the idea is to learn a generative model for face images conditioned on facial attributes. at test time, we generate face images by retrieval from chosen attribute configurations and let a separately trained regressor network predict the attributes from the generated images. a good generative model should be able to produce visual attributes that are correctly recognized by the regression model. to imitate the original experiment, we use labeled faces in the wild (lfw) images (huang et al., 2007) with attributes (kumar et al., 2009). we align the face images according to the landmarks in (zhu et al., 2014). additionally, we crop and resize the images to pixels and augment the dataset with common operations. again, we refer to our implementation online for more details.. we construct conditional vae, gan and vae/gan models by concatenating the attribute vector to the vector repre. sentation of the input in enc, dec and dis similar to (mirza osindero, 2014). for enc and dis, the attribute vector is concatenated to the input of the top fully connected layer. our regression network has almost the same architecture as enc. we train using the lfw training set, and during testing, we condition on the test set attributes and sample faces to be propagated through the regression network. figure shows faces generated by conditioning on attribute vectors from the test set. we report regressor performance numbers in table compared to an ordinary vae, the vae/gan model yields significantly better attributes visually that leads to smaller recognition error. the gan network performs suprisingly poorly and we suspect that this is caused by instabilities during training (gan models are very difficult to train reliably due to the minimax objective function). note that our results are not directly comparable with those of yan et al. (2015) since we do not have access to their preprocessing scheme nor regression model. for completeness, we report that we have tried evaluating vae/gan in a semisupervised setup by unsupervised pretraining followed by finetuning using a small number of labeled examples (for both cifar10 and stl10 datasets). unfortunately, we have not been able to reach results competitive with the stateoftheart (rasmus et al., 2015; zhao et al., 2015). we speculate that the intraclass variation may be too high for the vaegan model to learn good generalizations of the different object classes. the problems with elementwise distance metrics are well known in the literature and many attempts have been made at going beyond pixels typically using handengineered measures. much in the spirit of deep learning, we argue that the similarity measure is yet another component which can be replaced by a learned model capable of capturing highlevel structure relevant to the data distribution. in this work, our main contribution is an unsupervised scheme for learning and applying such a distance measure. with the learned distance measure we are able to train an image encoderdecoder network generating images of unprecedented visual fidelity as shown by our experiments. moreover, we show that our network is able to disentangle factors of variation in the input data distribution and discover visual attributes in the highlevel representation of the latent space. in principle, this lets us employ a large set of unlabeled images for training and use a small set of labeled images to discover features in latent space.. we regard our method as an extension of the vae framework. though, it must be noted that the high quality of our generated images is due to the combined training of dec as a both a vae decoder and a gan generator. this makes our method more of a hybrid between vae and gan, and alternatively, one could view our method more as an extension of gan where p(z) is constrained by an additional network.. it is not obvious that the discriminator network of a gan provides a useful similarity measure as it is trained for a different task, namely being able to tell generated samples from real samples. however, convolutional features are often surprisingly good for transfer learning, and as we show, good enough in our case to improve on elementwise distances for images. it would be interesting to see if better features in the distance measure would improve the model, e.g. by employing a similarity measure provided by a siamese network trained on faces, though in practice siamese networks are not a good fit with our method as they require labeled data. alternatively one could investigate the effect of using a pretrained feedforward network. for measuring similarity.. in summary, we have demonstrated a first attempt at unsupervised learning of encoderdecoder models as well as a similarity measure. our results show that the visual fidelity of our method is competitive with gan, which in that regard is considered stateofthe art. we therefore consider learned similarity measures a promising step towards scaling up generative models to more complex data distributions.", "summary": "variational autoencoders are a type of generative model that seek to learn how to generate new data by incentivizing the model to be able to reconstruct input data, after compressing it to a lowdimensional space. typically, the way that the reconstruction is scored against the original is by comparing the pixel by pixel values: a reconstruction gets a high score if it is able to place pixels of color in the same places that the original did. however, there are compelling reasons why this is a subpar way of scoring images. the central one is: it focuses on and penalizes superficial differences, so if the model accurately reproduces the focal object of the image, but does so, say, pixels to the right of where it was previously, that will incur a penalty we might not actually want to apply. the flip side of this is that a direct pixelcomparison loss doesn\u2019t differentiate between pixel differences that do or don\u2019t change the fundamental substance of the image. for instance, having pixels wrong around the border of a dog, making it seem very slightly larger, would be the same amount of error as having pixels concentrated in a weird bulb that appears to be growing out of a dog\u2019s ear, even though the former does a better job of being recognizable as a dog.. the authors of the vae/gan paper have a clever approach to solving this problem, that involves taking the typical pixel loss, and breaking it up into two conceptual parts. the first focuses on aligning the conceptual features of the reconstructed image with the conceptual features of the input image. it does so by running both the input and the reconstruction through a discriminative convolutional model which in the typical way of deep learning learns ever more abstract features at each layer of the network. these conceptual features abstract out the precise pixel values, and instead capture the higher level features of the image. so, instead of calculating the pixelwise squared loss between the specific input x, and its afterbottleneck reconstruction x, you take the squared loss between the feature maps at some layer for both x and x, and push them to be closer together, so that the reconstruction shares the same features as the original. the second focuses on detaillevel specifics of images, but, cleverly, does so in a general, rather than a observationspecific way. this is done by training a ganstyle discriminator to tell the difference between generated images and original image, and then using that loss to train the decoder part of the vae. the cleverness of this comes from the fact that they are still enforcing that the details and structural features of the reconstructed image are not distinguishable from real images, but doing so in a general sense, rather than requiring the details to be an exact match to the details found in a given input x. the authors freely admit that existing metrics of scoring images (which themselves use pixelwise similarity) rate their method as being worse than existing vaes. however, they argue, that\u2019s inherently a flawed metric, that doesn\u2019t capture the aspects of clean visual quality we want in generated image. a metric they propose instead involves using an dataset where a list of attributes are attached to each image (old, black, blond, etc). they add these as additional input while training the network, so that whatever signals the decoder part of the model needs to turn someone blonde, it gets those from the externallygiven attribute vector, rather than a learned representation. this means that, once the model is trained, we can set some value of the attribute vector, and have the decoder generate samples conditional on that. the metric is constructed by taking the decoded samples conditioned on some attribute set, and then taking a classifier model that is trained on the real images to detect attribute values from the images. the generated images are then scored by how closely the predictions from the classifier model match the true values of the attributes. if the generator model were working perfectly, this error rate would as low as for real data. by this metric (which: grain of salt, since they invented), the vae/gan model is superior to both gans and vanilla vaes."}, {"document": "leveraging large historical data in electronic health record (ehr), we developed doctor ai, a generic predictive model that covers observed medical conditions and medication uses. doctor ai is a temporal model using recurrent neural networks (rnn) and was developed and applied to longitudinal time stamped ehr data from 260k patients over years. diagnosis codes, medication codes or procedure codes) were input to rnn to predict (all) the diagnosis and medication categories for a subsequent visit. doctor ai assesses the history of patients to make multilabel predictions (one label for each diagnosis or medication category). based on separate blind test set evaluation, doctor ai can perform differential diagnosis with up to recall30, significantly higher than several baselines. moreover, we demonstrate great generalizability of doctor ai by adapting the resulting models from one institution to another without losing substantial accuracy. a common challenge in healthcare today is that physicians have access to massive amounts of data on patients, but little time nor tools. intelligent clinical decision support anticipates the information at the point of care that is specific to the patient and provider needs. electronic health records (ehr), now commonplace in u.s. healthcare, represent the longitudinal experience of both patients and doctors. these data are being used with increasing frequency to predict future events. while predictive models have been developed to anticipate needs, most existing work has focused on specialized predictive models that predict a limited set of outcomes. however, daytoday clinical practice involves an unscheduled and heterogeneous mix of scenarios and needs different prediction models in the hundreds to thousands. it is impractical to develop and deploy specialized models one by one.. leveraging large historical data in ehr, we developed doctor ai, a generic predictive model that covers observed medical conditions and medication uses. doctor ai is a temporal model using recurrent neural networks (rnn) and was developed and applied to longitudinal time stamped ehr data. in this work, we are particularly interested in whether historical ehr data may be used to predict future physician diagnoses and medication orders. applications that accurately. .l g. ] forecast could have many uses such as anticipating the patient status at the time of visit and presenting data a physician would want to see at the moment. the primary goal of this study was to use longitudinal patient visit records to predict the physician diagnosis and medication order of the next visit. as a secondary goal we predicted the time to the patients next visit. predicting the visit time facilitates guidance of whether a patient may be delayed in seeking care.. the two tasks addressed in this work are different from sequence labeling tasks often seen in natural language processing applications, e.g., partofspeech tagging. our proposed model, doctor ai, performs multilabel prediction (one for each disease or medication category) over time while sequence labeling task predicts a single label at each step. the key challenge was finding a flexible model that is capable of performing the multilabel prediction problem. the two main classes of techniques have been proposed in dealing with temporal sequences: 1) continuoustime markov chain based models (nodelman et al., 2002; lange et al., 2015; johnson and willsky, 2013), and 2) intensity based point process modeling techniques such as hawkes processes (liniger, 2009; zhu, 2013; choi et al., 2015). however, both classes are expensive to compute, especially for nonlinear settings. furthermore, they often make strong assumptions about the data generation process which might not be valid for ehr data. our modeling strategy was to develop a generalized approach to representing patient temporal healthcare experience to predict all the diagnoses, medication categories and visit time. we used recurrent neural network (rnn), considering that rnns have been particularly successful for representation learning in sequential data, e.g. graves (2013); graves and jaitly (2014); sutskever et al. in particular, we make the following main contributions in this paper:. we demonstrate how rnns can be used to represent the patient status and predict diagnosis, medication order and visit time. the trained rnn is able to achieve above recall10 and recall30 for diagnosis prediction, showing potential to serve as a differential diagnosis assistance.. we propose an initialization scheme for rnns using skipgram embeddings (mikolov et al., 2013) and show that it improves the performance of the rnn in both accuracy and speed.. we empirically confirm that rnn models possess great potential for transfer learning across different medical institutions. this suggests that health systems with insufficient patient data can adopt models learned from larger datasets of other health systems to improve prediction accuracy on their smaller population. in this section, we briefly review the common approaches to modeling multilabel event sequences with special focus on the models that have been applied to medical data. there are two main approaches to modeling multilabel event sequences: with or without discretization (binning) of time.. discretization. when the time axis is discretized, the point process data can be converted to binary time series (or time series of count data if binning is coarse) and analyzed via time series analysis techniques (truccolo et al., 2005; bahadori et al., 2013; ranganath et al., 2015). however, this approach is inefficient as it produces long time series whose elements are mostly zero. furthermore, discretization of time introduces noise in the time stamps of visits. finally, these approaches are often not able to model the duration until next event. thus, it is advantageous not to discretize the data both in terms of modeling and computation.. continuoustime models. among the continuoustime models, there are two main techniques: continuoustime markov chain based models (foucher et al., 2007; johnson and willsky, 2013; lange, 2014; liu et al., 2013) and their extension using baysian networks (nodelman et al., 2002; weiss et al., 2012) and intensity function modeling techniques such as cox and hawkes processes (liniger, 2009; zhou et al., 2013; linderman and adams, 2014; choi et al., 2015).. intensity function modeling techniques have been shown to have computational advantages over the continuoustime markov chain based models. moreover, modeling multilabel marked point processes with continuoustime markov chains expands their statespace and make them even more expensive. however, hawkes processes only depend linearly on the past observation times; while there are limited classes of nonlinear hawkes process (zhu, 2013), the temporal dynamics can be more complex. finally, hawkes processes are known to have a flat loss function near optimal value of the parameters which renders the gradientbased learning algorithms inefficient (veen and schoenberg, 2008). in this paper we address these challenges by designing a recurrent neural network which has been shown to be successful in learning complex sequential patterns.. disease progression models. there have been active research in modeling the temporal progression of diseases (mould, 2012). generally, most works can be divided into two groups: works that focus on a specific disease and works that focus on a broader range of diseases.. specificpurpose progression modeling: there have been many studies that focus on modeling the temporal progression of a specific disease based on either intensive use of domainspecific knowledge (de winter et al., 2006; ito et al., 2010; tangri et al., 2011) or taking advantage of advanced statistical methods (liu et al., 2013; jackson et al., 2003; sukkar et al., 2012; zhou et al., 2012). specifically, studies have been conducted on alzheimer\u2019s disease (ito et al., 2010; zhou et al., 2012; sukkar et al., 2012), glaucoma (liu et al., 2013), chronic kidney disease (tangri et al., 2011), diabetes mellitus (de winter et al., 2006), and abdominal aortic aneurysm (jackson et al., 2003). generalpurpose progression modeling: recently, wang et al. (2015) proposed more general approaches to modeling the progression of wider range of diseases. as discussed earlier, choi et al. (2015) used hawkes process, and ranganath et al. (2015) discretized time in order to model multiple patients and multiple diseases. (2014) proposed a graphical model based on markov jump process to predict the stage progression of chronic obstructive pulmonary disease (copd) and its comorbid diseases.. one of the main challenges in using these algorithms is scalability. the datasets used in previous works typically contain up to a few thousands of patients and a few hundreds of codes. even the largest dataset used by ranganath et al. (2015) contains 13,180 patients and 8,722 codes, which is significantly smaller than our dataset described in table need for domainspecific knowledge is also a big challenge. (2014) not only used a smaller dataset (3,705 patients and codes) but also used comorbidity information to improve the performance of their algorithm. such expert knowledge is difficult to obtain from typical ehr data.. deep learning models for ehr. researchers have recently begun attempting to apply neural network based methods (or deep learning) to ehr to utilize its ability to learn complex patterns from data. previous studies such as phenotype learning (lasko et al., 2013; che et al., 2015; hammerla et al., 2015) or representation learning (choi et al., 2016b,a; miotto et al., 2016), however, have not fully addressed the sequential nature of ehr. (2016) is especially related to our work in that both studies use rnn for sequence prediction. however, while lipton et al. (2016) uses regular times series of realvalued variables collected from icu patients to predict diagnosis codes, we use discrete medical codes (e.g. diagnosis, medication, procedure) extracted. from longitudinal patient visit records. also, in each visit we make a prediction about predict diagnosis, medication order in the next visit and and the time to next visit. population and source of data. the source population for this study was primary care patients from sutter health palo alto medical foundation. sutter health is a large primary care and multispecialty group practice that has used an epic systems corporation ehr for more than a decade. the dataset was extracted from a density sampled casecontrol study for heart failure. the dataset consists of deidentified encounter orders, medication orders, problem list records and procedure orders. as inputs, we use icd9 codes, medication codes, and procedure codes. we extracted icd9 codes from encounter records, medication orders, problem list records and procedure orders. generic product identifier (gpi) medication codes and cpt procedure codes were extracted from medication orders and procedure orders respectively. all codes were timestamped with the patients visit time. if a patient received multiple codes in a single visit, those codes were given the same timestamp. we excluded patients that made less than two visits. the resulting dataset consists of 263,706 patients who made on average visits per person. grouping medical codes. there are more about 11,000 unique icd9 codes and 18,000 gpi medication codes in the dataset, many of which are very granular. for example, pulmonary tuberculosis (icd9 code 011) is divided into subcategories (icd9 code 011.01, 011.02, ..., 011.95, 011.96). simply knowing that a patient is likely to have pulmonary tuberculosis is enough to increase the doctor\u2019s awareness of the severity of the clinical situation. therefore, to predict diagnosis and medication order, we grouped codes into higherorder categories to reduce the feature set and information overload. for the diagnosis codes, we use the 3digit icd9 codes, yielding unique codes. for the medication codes, we use the generic product identifier drug class, which groups the medication codes into unique groups. the label yi we use in the following sections represents the 1,778dimensional vector (i.e., 595) for the grouped diagnosis codes and medication codes. this section describes the rnn model for multilabel point processes. we will also describe how we predict diagnosis, medication order and visit time using the rnn model. for each patient, the observations are drawn from a multilabel point process in the form of (ti,xi) for i 1, , n. each pair represents an event, such as an ambulatory care visit, during which multiple medical codes such as icd9 diagnosis codes, procedure codes, or medication codes are documented in the patient record. the multihot label vector xi 0, 1p represents the medical codes assigned at time ti, where p denotes the number of unique medical codes. at each timestamp, we may extract higherlevel codes for prediction purposes and denote it by yi, see the details in section the number of events for each patient may differ.. figure 1: this diagram shows how we have applied rnns to solve the problem of forecasting of next visits\u2019 time and the codes assigned during each visit. the first layer simply embeds the highdimensional input vectors in a lower dimensional space. the next layers are the recurrent units (here two layers), which learn the status of the patient at each timestamp as a realvalued vector. given the status vector, we use two dense layers to generate the codes observed in. the next timestamp and the duration until next visit. gated recurrent units preliminaries. specifically, we implemented our rnn with gated recurrent units (gru). although long short term memory (lstm) (hochreiter and schmidhuber, 1997; graves et al., 2009) has drawn much attention from many researchers, gru has recently shown to have similar performance as lstm, while employing a simpler architecture (chung et al., 2014). in order to precisely describe the network used in this work, we reiterate the mathematical formulation of gru as follows:. hi tanh(whxi ri uhhi1 bh) hi zi hi1 (1 zi) hi. where zi and ri respectively represent the update gate and the reset gate, hi the intermediate memory unit, hi the hidden layer, all at timestep ti. a detailed description of gru is provided in appendix a. description of neural network architecture. our goal is to learn an effective vector representation for the patient status at each timestamp ti. using effective patient representations, we are interested in predicting diagnosis and medication categories in the next visit yi1 and the time duration until the next visit di1 ti1 ti. finally, we would like to perform all these steps jointly in a single supervised learning scheme. we use rnn to learn such patient representations, treating the hidden layer as the representation for the patient status and use it for the prediction tasks.. the proposed neural network architecture (figure 1) receives input at each timestamp ti as the concatenation of the multihot input vector xi of the multilabel categories and the duration di since the last event. in our datasets, the input dimension is as large as 40, thus, the next layer projects the input to a lower dimensional space. then, we pass the lower dimensional vector through rnn (implemented with gru in our study). we can also stack multiple layers of rnn to increase the representative power of the network. finally, we use a softmax layer to predict the diagnosis codes and the medication codes, and a rectified linear unit (relu) to predict the time duration until next visit.. for predicting the diagnosis codes and the medication codes at each timestep ti, a softmax layer is stacked on top of the gru, using the hidden layer hi as the input: yi1 softmax(wcode. for predicting the time duration until the next visit, a rectified linear unit (relu) is placed on top of the gru, again using the hidden layer hi as the input: di1 max(wtime. the objective of training our model is to learn the weights wz,r,h,code, uz,r,h, bz,r,h,code, wtime and btime. the values of all w \u2019s and u \u2019s were initialized to orthonormal matrices using singular. value decomposition of matrices generated from the normal distribution (saxe et al., 2013). the initial value of wtime was chosen from the uniform distribution between and all b\u2019s and btime were initialized to zeros. the joint loss function consists of the cross entropy for the code prediction and the squared loss for the time duration prediction, as described below for a single patient:. l(w ,u , b,wtime, btime) n1 i1 ( yi1 log(yi1) (1 yi1) log(1 yi1) ) di1 di122 as mentioned above, the multihot vectors xi of almost 40,000 dimensions are first projected to a lower dimensional space, then put into the gru. we employed two different approaches for this: (1) we put an extra layer of a certain size between the multihot input xi and the gru, and call it the embedding layer. we denote the weight matrix between the multihot input vector and the embedding layer as wemb. then we learn the weight wemb as we train the entire model. (2) we initialize the weight wemb with a matrix generated by skipgram algorithm (mikolov et al., 2013), then refine the weight wemb as we train the entire model. this can be seen as using the pretrained skipgram vectors as the input to the rnn and finetuning them with the joint prediction task. the brief description of learning the skipgram vectors from the ehr is provided in appendix b. the first and second approach can be formulated as follows:. h (1) i [tanh(xi wemb bemb), di] (1) h (1) i [xi wemb, di] (2). where [, ] is the concatenation operation used for appending the time duration to the multihot vector h. (1) i to make it an input vector to the gru. we now describe the details of our experiments in the proposed rnn approach to forecasting the future clinical events. the source code of doctor ai is publicly available at https://github.com/ mp2893/doctorai. for training all models including the baselines, we used of the patients as the training set and as the test set. we trained the rnn models for epochs (i.e., iterations over the entire training data) and then evaluated the final performance against the test set. to avoid overfitting, we used dropout between the gru layer and the prediction layer (i.e. code prediction and time duration prediction). dropout was also used between gru layers if we were using a multilayer gru. we also applied norm2 regularization on both wcode and wtime. both regularization coefficients were set to the size of the hidden layer hi of the gru was set to to guarantee a sufficient expressive power. after running sets of preliminary experiments where we tried the size from to 2000, we noticed that the code prediction performance started to saturate around all models were implemented with theano (bastien et al., 2012) and trained on a machine equipped with two nvidia tesla k80 gpus.. we train total four different variation of doctor ai as follows,. rnn1: rnn with a single hidden layer initialized with a random orthogonal matrix for wemb.. rnn2: rnn with two hidden layers initialized with a random orthogonal matrix for wemb.. rnn1ir: rnn using a single hidden layer initialized embedding matrix wemb with the skipgram vectors trained on the entire dataset.. rnn2ir: rnn with two hidden layers initialized embedding matrix wemb with the skipgram vectors trained on the entire dataset. the performance of algorithms in predicting diagnoses and medication codes was evaluated using the topk recall defined as:. topk recall of true positives in the top k predictions. topk recall mimics the behavior of doctors conducting differential diagnosis, where doctors list most probable diagnoses and treat patients accordingly to identify the patient status. therefore, a machine with a high topk recall translates to a doctor with an effective diagnostic skill. this makes topk recall an attractive performance metric for our problem.. we select the maximum k to be to evaluate the performance of the models not only for simple cases but also for complex cases. near of the patients have been assigned with more than diagnosis and medication codes at least once. since it is those complex cases that are of interest to predict and analyze, we choose k to be large enough (i.e., times of the mean).. coefficient of determination (r2) was used to evaluate the predictive performance of regression and forecasting algorithms. it compares the accuracy of the prediction with respect to the simple prediction by mean of the target variable.. r2 i (yi yi) because time to the next visit can be highly skewed, we measure the r2 performance of the algorithms in predicting log(di) to lower the impact of anomalous long durations in the performance metric. in the same spirit, we train all models to predict the logarithm of the time duration between visits. we compare our model against several baselines as described below. some of the existing techniques based on continuoustime markov chain and latent space models were not scalable enough to be trained using the entire dataset in a reasonable amount of time; thus comparison is not feasible. we compare our algorithms against simple baselines that are based on experts\u2019 intuition about the dynamics of events in clinical settings. the first baseline uses a patient\u2019s medical codes in the last visit as the prediction for the current visit. this baseline is competitive when the status of a patient with a chronic condition stabilizes over time. we enhanced this baseline using the topk most frequent labels observed in visits prior to the current visits. in the experiments we observe that the baseline of topk most frequent labels is quite competitive. logistic and neural network time series models. a common way to perform prediction task is to use xi1 to predict the codes in the next visit xi using logistic regression or multilayer perceptron (mlp). to enhance the baseline further, we can use the data from l time lags before. and aggregate them xi1 xi2 ,xil for some duration l to create the features for prediction of xi. similarly, we can have a model that predicts the time until next visit using rectified linear units (relu) as the output activation. we set the lag l so that the logistic regression and mlp can use information from maximum five past visits. the details of mlp design are described in appendix c. table compares the results of different algorithms with rnn based doctor ai. we report the results in three settings: when we are interested in (1) predicting only diagnosis codes (dx), (2) predicting only medication codes (rx), and (3) jointly predicting dx codes, rx codes, and the time duration to next visit. the results confirm that the proposed approach is able to outperform the baseline algorithms by a large margin. note that the recall values for the joint task are lower than those for dx code prediction or rx code prediction because the hypothesis space is larger for the joint prediction task.. the superior performance of rnn based approaches can be attributed to the efficient representation that they learn for patients at each visit (bengio et al., 2013; schmidhuber, 2015). rnns are able to learn succinct feature representations of patients by accumulating the relevant information from their history and the current set of codes, which outperformed handpicked features of frequency baselines.. table confirms that learning patient representation with rnn is easier with the input vectors that are already efficient representations of the medical codes. the rnn trained with the skipgram vectors (denoted by rnnir) consistently outperforms the rnn that learns the weight matrix wemb directly from the data, with only one exception, the medication prediction recall30, although the differences are insignificant. the results also confirm that having multiple layers when using rnn improves its ability to learn more efficient representations. the results also indicate that a single layer rnn might have enough representative power to capture the dynamics of medications, and adding more layers may not improve the performance.. the results also indicate that our approach significantly improves the accuracy of predicting the time duration until the next visit compared to the baselines. however, the absolute value of r2 metric shows that accurate prediction of time intervals remains as a challenge. we believe achieving significantly better time prediction without extra features should be difficult because the timing of a clinical visit can be affected by many personal factors such as financial status, location of residence, means of transportation, and life style, to name a few. thus, without such sensitive personal information, which is rarely included in the ehr, accurate prediction of time intervals should be unlikely. to study the applicability of our model in a realworld setting where patients have varying length of medical records, we conducted an additional experiment to study the relationship between the length of the patient medical history and the prediction performance. to this end, we selected 5,800 patients from the test set who had more than visits. we used the best performing model to predict the diagnosis codes at visits at different times and found the mean and standard error of recall across the selected patients. figure 2a shows the result of the experiment. we believe that the increase in performance can be due to two reasons: (1) rnn is able to learn a better estimate of the patient status as it sees longer patient records and (2) visits are correlated with poor health. those with high visit count are more likely to be severely ill, and therefore their future is easier to predict.. another experiment was conducted to understand the behavior of the network by giving synthetic inputs. we chose hypertension (icd9 code 401.9) as an example of a frequently observed diagnosis, and klinefelter\u2019s syndrome (icd9 code 758.7) as an example of an infrequent diagnosis. we created two synthetic patients who respectively have visits of and then we used the best performing model to predict the diagnosis codes for the next visits. figure 2b shows contrasting patterns: when the input is one of the frequent codes such as hypertension, the network quickly learns a more specific set of output codes as next disease. when we select an infrequent code like klinefelter\u2019s syndrome as the input, the network\u2019s output is more diverse and mostly the frequently observed codes. the top codes after convergence shown in table in appendix d confirm the disparity of the diversity of the predicted codes for the two cases. as we observed from the previous experiments, the dynamics of clinical events are complex, which requires models with a high representative power. however, many institutions have not yet collected large scale datasets, and training such models could easily lead to overfitting. to address this challenge, we resort to the recent advances in domain adaptation techniques for deep neural networks (mesnil et al., 2012; bengio, 2012; yosinski et al., 2014; hoffman et al., 2014).. a different dataset, mimic ii, which is a publicly available clinical dataset collected from icu patients over years of observation, was chosen to conduct the experiment. this dataset differs from the sutter dataset in that it consists of demographically and diagnostically different patients. the number of patients who made at least two visits is 2,695, and the number of unique diagnosis code (3digit icd9 code) is 767, which is a subset of the sutter dataset. from the dataset, we extracted sequences of 3digit icd9 codes. we chose 2,290 patients for training, for testing. we chose the 2layer rnn with dimensional hidden layer, and performed two experiments: 1) we trained the model only on the mimic ii dataset. 2) we initialized the coefficients of the model with the values learned from the 3digit icd9 sequences of the sutter data, then we refined. the coefficients with the mimic ii dataset. figure shows the vast improvement of the prediction performance induced by the knowledge transfer from the sutter data. in this work, we proposed doctor ai system, which is a rnnbased model that can learn efficient patient representation from a large amount of longitidinal patient records and predict future events of patients. we tested doctor ai on a large realworld ehr datasets, which achieved recall30 and significantly outperformed many baselines. we have also shown that the patient\u2019s visit count and the rarity of medical codes highly influence the performance. we have also demonstrated that knowledge learned from one hospital could be adapted to another hospital. the empirical analysis by a medical expert confirmed that doctor ai not only mimics the predictive power of human doctors, but also provides diagnostic results that are clinically meaningful.. one limitation of doctor ai is that, in medical practice, incorrect predictions can sometimes be more important than correct predictions as they can degrade patient health. also, although doctor ai has shown that it can mimic physicians\u2019 average behavior, it would be more useful to learn to perform better than average. we set as our future work to address these issues so that doctor ai can provide practical help to physicians in the future.. learning efficient representations of medical codes (e.g. diagnosis codes, medication codes, and procedure codes) may lead to improved performance of many clinical applications. we specifically used skipgram mikolov et al. (2013) to learn realvalued multidimensional vectors to capture the latent representation of medical codes from the ehr.. we processed the private dataset so that diagnosis codes, medication codes, procedure codes are laid out in a temporal order. if there are multiple codes at a single visit, they were laid out in a random order. then using the context window size of to the left and to the right, and applying skipgram, we were able to project diagnosis codes, medication codes and procedure codes into the same lower dimensional space, where similar or related codes are embedded close to one another. for example, hypertension, obesity, hyperlipidemia all share similar values compared to pneumonia or bronchitis. the trained skipgram vectors are then plugged into rnn so that a multihot vector can be converted to vector representations of medical codes. we use a multilayer perceptron with a hidden layer of width 2,000. we apply l2 regularization to all of the weight matrices. the activation functions in the first and output layers are selected to be tanh and softmax functions respectively. for prediction of time intervals, we used rectified linear units. the detailed results are shown in table to take a closer look at the performance of doctor ai, in table (in appendix d) we list the predicted, true, and historical diagnosis codes for five visits of different patients. the blue items represent the correct predictions. the results are promising and show that, given the history of the patient, the doctor ai can predict the true diagnostic codes. the results highly mimic the way a human doctor will interpret the disease predictions from the history. for all five of the cases shown in table 3, the set of predicted diseases contain most, if not all of the true diseases. for example, in the first case, the top predicted diseases match the true diseases. a human doctor would likely predict similar diseases to the ones predicted with doctor ai, since old myocardial infarction and chronic ischemic heart disease can be associated with infections and diabetes (stevens et al., 1978).. in the fourth case, visual disturbances can be associated with migraines and essential hypertension (keith et al., 1939). further, essential hypertension may be linked to cognitive function (kuusisto et al., 1993), which plays a role in anxiety disorders and dissociative and somatoform disorders. regarding codes that are guessed incorrectly with the fourth case, they can still be plausible given the history. for example, cataracts, and disorders of refraction and accommodation could have been guessed based on a history of visual disturbances, as well as strabismus and disorders of binocular eye movements. allergic rhinitis could have been guessed, because there was a history of allergic rhinitis. in summary, doctor ai is able to very accurately predict the true diagnoses in the sample patients. the results are promising and should motivate future studies involving the application of doctor ai on different datasets exhibiting other populations of patients.. a b. o li sm d ia b et es m el li tu s c h ro n ic k id n ey d is ea se (c k. d ). ed a n. em ia s n ee d fo r p ro p h y la ct ic v a cc in a n. d in. ea se s s p ec ia l sc re en in g fo r m a li g n a n t n eo p la sm s. v es m. el li tu s c a ta ra ct e n co u n te r fo r o th. n u. si ti s d is ea se s o f. es o p. h a g u s o th er a n d u n sp ec ifi. su re s f u n ct io n a l d ig es ti v e d is o rd er s, n o t el se w h er e cl a ss. ro m. b o si s e n c o u n te r fo r o th e r a n d u n sp e c ifi. o r d. e r s. o f. jo in t p e r so n a l h is to r y o f c e r ta in o th e r d is e a se s o rg a n o r ti ss u e re p la ce d b y o th er m ea n s o th er d is o rd er s o f so ft ti ss u es o st e o a r th r o si s a n d a ll ie d d is o r d e r s o th er d is o rd er s o f b o n e a n d ca rt il a g e p er ip h er a l en th es o p a th ie s a n d a ll ie d sy n d ro m es p h le b it is a n d th ro m b o p h le b it is. e a se s o th e r a n d u n sp e c ifi e d d is o r d e r s o f jo in t e n c o u n te r fo r o th e r a n d u n sp e c ifi e d. p r o c e d. u r e s. v ro m. b o si s in ju ry to p er ip h er a l n er v e( s) o f p el v ic. li m b o rg a n o r ti ss u e re p la ce d b y o th er m ea n s. v r a in e c a ta ra ct o rg a n o r. ti ss. er m. ea n s d is o rd er s o f re fr a ct io n a n d a cc o m m o d a ti. ze m a v is u a l d is tu rb a n ce s s tr a b is m u s a n d o th er d is o rd er s o f b in. m ia s d is o r d e r s o f li p o id. b o li sm e ss en ti a l h y p er te n si o n s y m p to m s in v o lv in g re sp ir a to ry sy. it u s o th er fo rm s o f ch ro. rt d. is ea se s y m p to m s in v o lv in g u ri n a ry sy st em o th er d is ea se s o f en d o ca rd iu m. rt d. is ea se h e a r t fa il u r e d is o r d e r s o f li p o id m e ta. b o li sm c a r d ia c d y sr h y th m ia s. li ti s h ea rt fa il u re s y m p to m s in v o lv in g re sp ir a to ry sy. co d. e a ft. er ti m. e st. ep h y p. e r te. sy n. d r o m e ic d d es cr ip ti o n ic d d es cr ip ti o n e ss en ti a l h y p er te n si o n d is o rd er s o f li p o id m et a b o li sm d is o rd er s o f li p o id m et a b o li sm v g en er a l m ed ic a l ex a m in a ti o n s y m p to m s in v o lv in g re sp ir a to ry sy st em a n d o th er ch es t sy m p to m s v n ee d fo r p ro p h y la ct ic v a cc in a ti o n a n d in o cu la ti o n. a g a in. ea se s v n ee d fo r p ro p h y la ct ic v a cc in a ti o n a n d in o cu la ti o n a g a in st co m b in a ti o n s o f d is ea se s o st eo m y el it is , p er io st it is , a n d o th er in fe ct io n s in v o lv in g b o n e n o n sp ec ifi c fi n d in g s o n ex a m in a ti o n o f b lo o d g en er a l sy m p to m s v s p ec ia l sc re en in g fo r m a li g n a n t n eo p la sm s s y m p to m s co n ce rn in g n u tr it io n , m et a b o li sm , a n d d ev el o p m en t v n ee d fo r p ro p h y la ct ic v a cc in a ti o n a n d in o cu la ti o n a g a in st ce rt a in d is ea se s s ch iz o p h re n ic d is o rd er s v g en er a l m ed ic a l ex a m in a ti o n v s p ec ia l sc re en in g fo r m a li g n a n t n eo p la sm s g en er a l sy m p to m s m a li g n a n t n eo p la sm o f to n g u e d is o rd er s o f fl u id , el ec tr o ly te , a n d a ci d b a se b a la n ce v n ee d fo r p ro p h y la ct ic v a cc in a ti o n a n d in o cu la ti o n a g a in st co m b in a ti o n s o f. d is. ea se s s y m p to m s in v o lv in g sk in a n d o th er in te g u m en ta ry ti ss u e d ia b et es m el li tu s v it a m in d d efi ci en cy s y m p to m s in v o lv in g sk in a n d o th er in te g u m en ta ry ti ss u e o th er a n d u n sp ec ifi ed d is o rd er s o f jo in t s y m p to m s in v o lv in g re sp ir a to ry sy st em a n d o th er ch es t sy m p to m s c a rd ia c d y sr h y th m ia s l eu k em ia o f u n sp ec ifi ed ce ll ty p e d is o rd er s o f ex te rn a l ea r e ss en ti a l h y p er te n si o n d ia b et es m el li tu s n o n sp ec ifi c fi n d in g s o n ex a m in a ti o n o f b lo o d o th er d is o rd er s o f u re th ra a n d u ri n a ry tr a ct ir o n d efi ci en cy a n em ia s v s p ec ia l in v es ti g a ti o n s a n d ex a m in a ti o n s d is o rd er s o f p en is o th er sy m p to m s in v o lv in g a b d o m en a n d p el v is o th er d efi ci en cy a n em ia s o th er d is o rd er s o f so ft ti ss u es v n ee d fo r p ro p h y la ct ic v a cc in a ti o n a n d in o cu la ti o n a g a in st b a ct er ia l d is ea se s o th er ce ll u li ti s a n d a b sc es s p a rk in so n \u2019s d is ea se v n ee d fo r p ro p h y la ct ic v a cc in a ti o n a n d in o cu la ti o n a g a in st b a ct er ia l d is ea se s d is o rd er s o f a d re n a l g la n d s o th er a n d u n sp ec ifi ed d is o rd er s o f b a ck o th er il ld efi n ed a n d u n k n o w n ca u se s o f m o rb id it y a n d m o rt a li ty v e n co u n te r fo r o th er a n d u n sp ec ifi ed p ro ce d u re s a n d a ft er ca re a cq u ir ed h y p o th y ro id is m o v er w ei g h t, o b es it y a n d o th er h y p er a li m en ta ti o n v e n co u n te r fo r o th er a n d u n sp ec ifi ed p ro ce d u re s a n d a ft er ca re v s p ec ia l sc re en in g fo r o th er co n d it io n s m a li g n a n t n eo p la sm o f st o m a ch v o th er p er so n s se ek in g co n su lt a ti o n p er si st en t m en ta l d is o rd er s d u e to co n d it io n s cl a ss ifi ed el se w h er e c h ro n ic k id n ey d is ea se (c k d ) v s p ec ia l in v es ti g a ti o n s a n d ex a m in a ti o n s g o u t o th er p a ra ly ti c sy n d ro m es v o th er co n d it io n s in fl u en ci n g h ea lt h st a tu s m a li g n a n t n eo p la sm o f o ro p h a ry n x", "summary": "this paper presents an applications of rnns to predict clinical events, such as disease diagnosis and medication prescription and their timing.. the paper proposes/suggests:. applying an rnn to disease diagnosis, medication prescription and timing prediction.. initializing the neural net with skipgrams instead of onehot vectors. however, it seems from the description that the authors are not initializing, rather just feeding a different feature vector into the rnn.. initializing a model that is to be trained on a small corpus from a model trained on a large corpus works. concludes: information can be transferred between models (read across hospitals)."}, {"document": "convolutional neural networks (cnns) have proven highly effective at image synthesis and style transfer. for most users, however, using them as tools can be a challenging task due to their unpredictable behavior that goes against common intuitions. this paper introduces a novel concept to augment such generative architectures with semantic annotations, either by manually authoring pixel labels or using existing solutions for semantic segmentation. the result is a contentaware generative algorithm that offers meaningful control over the outcome. thus, we increase the quality of images generated by avoiding common glitches, make the results look significantly more plausible, and extend the functional range of these algorithmswhether for portraits or landscapes, etc. applications include semantic style transfer and turning doodles with few colors into masterful paintings! image processing algorithms have improved dramatically thanks to cnns trained on image classification problems to extract underlying patterns from large datasets (simonyan and zisserman 2014). as a result, deep convolution layers in these networks provide a more expressive feature space compared to raw pixel layers, which proves useful not only for classification but also generation (mahendran and vedaldi 2014). for transferring style between two images in particular, results are astonishingespecially with painterly, sketch or abstract styles (gatys, ecker, and bethge 2015).. however, to achieve good results using neural style transfer in practice today, users must pay particular attention to the composition and/or style image selection, or risk seeing unpredictable and incorrect patterns. for portraits, facial features can be ruined by incursions of background colors or clothing texture, and for landscapes pieces of vegetation may be found in the sky or other incoherent places. there\u2019s certainly a place for this kind of glitch art, but many users become discouraged not being able to get results they want.. through our social media bot that first provided these algorithms as a service (champandard 2015), we observe that users have clear expectations how style transfer should. this research was funded out of the marketing budget.. figure 1: synthesizing paintings with deep neural networks via analogy. (a) original painting by renoir, (b) semantic annotations, (c) desired layout, (d) generated output.. occur: most often this matches semantic labels, e.g. hair style and skin tones should transfer respectively regardless of color. unfortunately, while cnns routinely extract semantic information during classification, such information is poorly exploited by generative algorithmsas evidenced by frequent glitches.. we attribute these problems to two underlying causes:. while cnns used for classification can be repurposed to extract style features (e.g. textures, grain, strokes), they were not architected or trained for correct synthesis.. higherlevel layers contain the most meaningful information, but this is not exploited by the lowerlevel layers used in generative architectures: only error backpropagation indirectly connects layers from top to bottom.. to remedy this, we introduce an architecture that bridges the gap between generative algorithms and pixel labeling neural networks. the architecture commonly used for image synthesis (simonyan and zisserman 2014) is augmented with semantic information that can be used during generation. then we explain how existing algorithms can be adapted to include such annotations, and finally we show. case some applications in style transfer as well as image synthesis by analogy (e.g. the image analogy algorithm (hertzmann et al. 2001) is able to transfer artistic style using pixel features and their local neighborhoods. while more recent algorithms using deep neural networks generate better quality results from a stylistic perspective, this technique allows users to synthesize new images based on simple annotations. as for recent work on style transfer, it can be split into two categories: specialized algorithms or more general neural approaches.. the first neural network approach to style transfer is grambased (gatys, ecker, and bethge 2015), using socalled gram matrices to represent global statistics about the image based on output from convolution layers. these statistics are computed by taking the inner product of intermediate activationsa tensor operation that results in a n n matrix for each layer of n channels. during this operation, all local information about pixels is lost, and only correlations between the different channel activations remain. when glitches occur, it\u2019s most often due to these global statistics being imposed onto the target image regardless of its own statistical distribution, and without any understanding of local pixel context.. a more recent alternative involves a patchbased approach (li and wand 2016), which also operates on the output of convolution layers. for layers of n channels, neural patches of are matched between the style and content. image using a nearest neighbor calculation. operating on patches in such a way gives the algorithm local understanding of the patterns in the image, which overall improves the precision of the style transfer since fewer errors are introduced by globally enforcing statistical distributions.. both gram and patchbased approaches struggle to provide reliable user controls to help address glitches. the primary parameter exposed is a weighting factor between style and content; adjusting this results in either an abstractstyled mashup that mostly ignores the input content image, or the content appears clearly but its texture looks washed out (see figure 2, second column). finding a compromise where content is replicated precisely and the style is faithful remains a challengein particular because the algorithm lacks semantic understanding of the input.. thankfully, recent cnn architectures are capable of providing such semantic context, typically by performing pixel labeling and segmentation (thoma 2016). these models rely primarily on convolutional layers to extract highlevel patterns, then use deconvolution to label the individual pixels. however, such insights are not yet used for synthesis despite benefits shown by nonneural approaches.. the stateoftheart specialized approaches to style transfer exploit semantic information to great effect, performing color transfer on photo portraits using specifically crafted image segmentation (yang et al. in particular, facial features are extracted to create masks for the image, then masked segments are processed independently and colors can be transferred between each corresponding part (e.g.. background, clothes, mouth, eyes, etc.) thanks to the additional semantic information, even simpler histogram matching algorithms may be used to transfer colors successfully. our contribution builds on a patchbased approach (li and wand 2016) to style transfer, using optimization to minimize content reconstruction error ec (weighted by ) and style remapping error es (weight ). see (gatys, ecker, and bethge 2015) for details about ec.. e ec es (1). first we introduce an augmented cnn (figure 6) that incorporates semantic information, then we define the input semantic map and its representation, and finally show how the algorithm is able to exploit this additional information. the most commonly used cnns for image synthesis is vgg (2014), which combines pooling and convolution layers l with filters (e.g. the first layer after third pool is named conv4 1). intermediate postactivation results are labeled xl and consist of n channels, which capture patterns from the images for each region of the image: grain, colors, texture, strokes, etc. other architectures tend to skip pixels regularly, compress data, or optimized for classificationresulting in lowquality synthesis (nikulin and novak 2016).. our augmented network concatenates additional semantic channels ml of size m at the same resolution, computed by downsampling a static semantic map specified as input. result is a new output with n m channels, denoted sl and labeled accordingly for each layer (e.g. sem4 1).. before concatenation, the semantic channels are weighted by parameter to provide an additional user control point:. for style images, the activations for the input image and its semantic map are concatenated together as sls. for the output image, the current activations xl and the input content\u2019s semantic map are concatenated as sl. note that the semantic part of this vector is, therefore, static during the optimization process (implemented using lbfgs).. this architecture allows specifying manually authored semantic maps, which proves to be a very convenient tool for user controladdressing the unpredictability of current generative algorithms. it also lets us transparently integrate recent pixel labeling cnns (thoma 2016), and leverage any advances in this field to apply them to image synthesis. the input semantic map can contain an arbitrary number of channels m whether doing image synthesis or style transfer, there are only two requirements: each image has its own semantic map of the same aspect. ratio, though it can be lower resolution (e.g. 4x smaller) since it\u2019ll be downsampled anyway.. the semantic maps may use an arbitrary number of channels and representation, as long as they are consistent for the current style and content (so m must be the same). common representations include single greyscale channels or rgba colorsboth of which are very easy to author. the semantic map can also be a collection of layer. mask per label as output by existing cnns, or even some kind of semantic embedding that compactly describes image pixels (i.e. the representation for hair, beards, and eyebrows in portraits would be in close proximity). patches of k k are extracted from the semantic layers and denoted by the function , respectively (sls) for the input style patches and (sl) for the current image patches. for any patch i in the current image and layer l, its nearest neighbor nn(i) is computed using normalized crosscorrelationtaking into account weighted semantic map:. the style error es between all the patches i of layer l in the current image to the closest style patch is defined as the sum of the euclidean distances:. note that the information from the semantic map in ml is used to compute the best matching patches and contributes to the loss value, but is not part of the derivative of the loss relative to the current pixels; only the differences in activation xl compared to the style patches cause an adjustment of the image itself via the lbfgs algorithm.. by using an augmented cnn that\u2019s compatible with the original, existing patchbased implementations can use the additional semantic information without changes. if the semantic map and ml is zero, the original algorithm (2016) is intact. in fact, the introduction of the parameter from equation provides a convenient way to introduce semantic style transfer incrementally. the following experiments were generated from vgg19 network using augmented layers sem3 and sem4 1, with patches and no additional rotated or scaled versions of the style images. the semantic maps used were manually edited as rgb images, thus channels are in the range [0..255]. the seed for the optimization was random, and rendering completed in multiple increasing resolutionsas usual for patchbased approaches (li and wand 2016). on a gtx970 with 4gb of gpu ram, rendering takes from to minutes depending on quality and resolution. transferring style in faces is arguably the most challenging task to meet expectationsand particularly if the colors in the corresponding segments of the image are opposed. typical results from our solution are shown in portraits from figure 2, which contains both success cases (top row) and suboptimal results (bottom row). the input images were chosen once upfront and not curated to showcase representative results; the only iteration was in using the semantic map as a tool to improve the quality of the output.. in the portraits, the semantic map includes four main labels for background, clothing, skin and hairwith minor color variations for the eyes, mouth, nose and ears. (the semantic maps in this paper are shown as greyscale, but contain three channels.). in practice, using semantic maps as annotations helps alleviate issues with patch or grambased style transfer. often, repeating patches appear when setting style weight too high (figure 2, second row). when style weight is low, pat. terns are not transferred but lightly blended (figure 2, first row). the semantics map prevents these issues by allowing the style weight to vary relative to the content without suffering from such artifacts; note in particular that the skin tones and background colors are transferred more faithfully. given a fixed weight for the content loss 10, the style loss for images in this paper ranges from to depending on image pairs. figure shows a grid with visualizations of results as and vary; we note the following:. the quality and variety of the style degenerates as increases too far, without noticeably improving the precision wrt. annotations.. as decreases, the algorithm reverts to its semantically unaware version that ignores the annotations provided, but also indirectly causes an increase in style weight.. the default value of is chosen to equalize the value range of the semantic channels ml and convolution activations xl, in this case lowering from its default allows style to be reused across semantic borders, which may be useful for certain applications if used carefully.. in general, with the recommended default value of , adjusting style weight now allows meaningful interpolation that does not degenerate into abstract patchworks. here we report observations from working with the algorithm, and provide our interpretations.. semantic map values since the semantic channels ml are integrated into the same patchbased calculation, it affects how the normalized crosscorrelation takes place. if the channel range is large, the values from convolution xl will be scaled very differently depending on the location in the map. this may be desired, but in most cases it seems sensible to make sure values in ml have similar magnitude.. authored representations we noticed that when users are asked to annotate images, after a bit of experience with the system, they implicitly create semantic embeddings that compactly describe pixel classes. for example, the representation of a stubble would be a blend between hair and skin, jewelry is similar but not identical to clothing, etc. such representations seem better suited to semantic style transfer than plain layer masks.. style quality when using semantic maps, only the style patches from the appropriate segment can be used for the target image. when the number of source patches is small, this causes repetitive patterns, as witnessed in parts of figure this can be addressed by loosening the style constraint and lowering , at the cost of precision.. blending segments in the examples shown and others, the algorithm does a great job of painting the borders between image segments, often using appropriate styles from the corresponding image. smoothing the semantic map can help in some cases, however, crisp edges still generate surprising results.. weight sensitivity the algorithm is less fragile to adjustments in style weight; typically as the weight increases, the image degenerates and becomes a patchwork of the style content. the semantic map helps maintain the results more consistent for a wider range of the parameter space.. performance due to the additional channels in the model, our algorithm requires more memory as well as extra computation compared to its predecessor. when using only rgb images this is acceptable: around extra memory for all patches and all convolution output, approximately extra computation. however with pixels labeled using individual classes this grows quickly. this is a concern, although. patchbased solutions in general would benefit from significant optimization that would apply here too. existing style transfer techniques perform well when colors and/or accuracy don\u2019t matter too much for the output image (painterly, abstract or sketch styles, glitch art) or when both image patterns are already similarwhich obviously reduces the appeal and applicability of such algorithms. in this paper, we resolved these issues by annotating input images with a semantic map, either manually authored or from pixel labeling algorithms. we introduced an augmented cnn architecture to leverage this information at runtime, while further tying advances in image segmentation to image synthesis. we showed that existing patchbased algorithms require minor adjustments and perform very well using this additional information.. the examples shown for style transfer show how. this technique helps deal with completely opposite patterns/colors in corresponding image regions, and we analyzed how it helps users control the output of these algorithms better. reducing the unpredictability of neural networks certainly is a step forward towards making them more useful as a tool to enhance creativity and productivity. this work was possible thanks to the nucl.ai conference 2016\u2019s marketing budget that was repurposed for this research. as long as you visit us in vienna on july 1820, it\u2019s better this way! thanks to petra champandardpail, roelof pieters, sander dieleman, jamie blondin, samim winiger, timmy gilbert, kyle mcdonald, mia bergeron, seth johnson.", "summary": " they describe a method to transfer image styles based on semantic classes.. this allows to:. (1) transfer styles between images more accurately than with previous models. so that the background of an image does not receive the style of skin/hair/clothes/... seen in the style image. skin in the synthesized image should receive the style of skin from the style image. same for hair, clothes, etc.. (2) turn simple doodles into artwork by treating the simplified areas in the doodle as semantic classes and annotating an artwork with these same semantic classes. this blob should receive the style from these trees.). their method is based on [combining markov random fields and convolutional neural networks for image synthesis](combiningmrfsandcnnsforimagesynthesis.md).. they use the same content loss and mostly the same mrfbased style loss. (apparently they dont use the regularization loss.). they change the input of the mrfbased style loss.. usually that input would only be the activations of a vgglayer (for the synthesized image or the style source image).. they add a semantic map with weighting gamma to the activation, i.e. representation of image activation of specific layer for that image gamma semantic map.. the semantic map has n channels with 1s in a channel where a specific class is located (e.g. skin).. the semantic map has to be created by the user for both the content image and the style image.. as usually for the mrf loss, patches are then sampled from the representations. the semantic maps then influence the distance measure. patches are more likely to be sampled from the same semantic class.. higher gamma values make it more likely to sample from the same semantic class (because the distance from patches from different classes gets larger).. one can create a small doodle with few colors, then use the colors as the semantic map. then add a semantic map to an artwork and run the algorithm to transform the doodle into an artwork. more control over the transfered styles than previously.. less sensitive to the style weighting, because of the additional gamma hyperparameter.. easy transformation from doodle to artwork.. turning a doodle into an artwork. note that the doodle input image is also used as the semantic map of the input."}, {"document": "deep neural networks (dnns) are one of the most prominent technologies of our time, as they achieve stateoftheart performance in many machine learning tasks, including but not limited to image classification, text mining, and speech processing. however, recent research on dnns has indicated everincreasing concern on the robustness to adversarial examples, especially for securitycritical tasks such as traffic sign identification for autonomous driving. studies have unveiled the vulnerability of a welltrained dnn by demonstrating the ability of generating barely noticeable (to both human and machines) adversarial images that lead to misclassification. furthermore, researchers have shown that these adversarial images are highly transferable by simply training and attacking a substitute model built upon the target model, known as a blackbox attack to dnns. similar to the setting of training substitute models, in this paper we propose an effective blackbox attack that also only has access to the input (images) and the output (confidence scores) of a targeted dnn. however, different from leveraging attack transferability from substitute models, we propose zeroth order optimization (zoo) based attacks to directly estimate the gradients of the targeted dnn for generating adversarial examples. we use zeroth order stochastic coordinate descent along with dimension reduction, hierarchical attack and importance sampling techniques to pinyu chen and huan zhang contribute equally to this work. this work is done during internship at ibm t. j. watson research center permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. copyrights for components of this work owned by others than acm must be honored. abstracting with credit is permitted. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. request permissions from permissionsacm.org. aisec\u201917, november 3, 2017, dallas, tx, usa association for computing machinery. https://doi.org/10.1145/3128572.3140448 efficiently attack blackbox models. by exploiting zeroth order optimization, improved attacks to the targeted dnn can be accomplished, sparing the need for training substitute models and avoiding the loss in attack transferability. experimental results onmnist, cifar10 and imagenet show that the proposed zoo attack is as effective as the stateoftheart whitebox attack (e.g., carlini and wagner\u2019s attack) and significantly outperforms existing blackbox attacks via substitute models. the renaissance of artificial intelligence (ai) in the past few years roots in the advancement of deep neural networks (dnns). recently, dnns have become an essential element and a core technique for existing and emerging ai research, as dnns have achieved stateoftheart performance and demonstrated fundamental breakthroughs in many machine learning tasks that were once believed to be challenging [24]. examples include computer vision, image classification, machine translation, and speech processing, to name a few.. despite the success of dnns, recent studies have identified that dnns can be vulnerable to adversarial examples a slightly modified image can be easily generated and fool a welltrained image classifier based on dnns with high confidence [12, 40]. consequently, the inherent weakness of lacking robustness to adversarial examples for dnns brings out security concerns, especially for missioncritical applications which require strong reliability, including traffic sign identification for autonomous driving and malware prevention [9, 16, 17], among others.. preliminary studies on the robustness of dnns focused on an openbox (whitebox) setting they assume model transparency that allows full control and access to a targeted dnn for sensitivity analysis. by granting the ability of performing back propagation, a technique that enables gradient computation of the output with respect to the input of the targeted dnn, many standard algorithms. n. such as gradient descent can be used to attack the dnn. in image classification, back propagation specifies the effect of changing pixel values on the confidence scores for image label prediction. unfortunately, most real world systems do not release their internal configurations (including network structure and weights), so openbox attacks cannot be used in practice.. throughout this paper, we consider a practical blackbox attack setting where one can access the input and output of a dnn but not the internal configurations. in particular, we focus on the use case where a targeted dnn is an image classifier trained by a convolutional neural network (cnn), which takes an image as an input and produces a confidence score for each class as an output. due to application popularity and security implications, image classification based on cnns is currently a major focus and a critical use case for studying the robustness of dnns.. we consider two types of blackbox attacks in this paper. given a benign example with correct labeling, an untargeted attack refers to crafting an adversarial example leading to misclassification, whereas a targeted attack refers to modifying the example in order to be classified as a desired class. the effectiveness of our proposed blackbox attack (which we call zoo) is illustrated in figure the crafted adversarial examples from our attack not only successfully mislead the targeted dnn but also deceive human perception as the injected adversarial noise is barely noticeable. in an attacker\u2019s foothold, an adversarial example should be made as indistinguishable from the original example as possible in order to deceive a targeted dnn (and sometimes human perception as well). however, the best metric for evaluating the similarity between a benign example and a corresponding adversarial example is still an open question and may vary in different contexts.. in what follows, we summarize recent works on generating and defending adversarial examples for dnns, and specify the blackbox setting of training substitute models for adversarial attacks. we summarize four principal openbox methods developed for attacking image classification trained on dnns as follows. fast gradient sign method (fgsm) [12]: originated from an l constraint on the maximal distortion, fgsm uses the sign of the gradient from the back propagation on a targeted dnn to generate admissible adversarial examples. fgsm has become a popular baseline algorithm for improved adversarial example generation [21, 22], and it can be viewed as an attack framework based on firstorder projected gradient descent [27]. jacobianbased saliency map attack (jsma) [36]: by constructing a jacobianbased saliency map for characterizing the inputoutput relation of a targeted dnn, jsma can be viewed as a greedy attack algorithm that iteratively modifies the most significant pixel based on the saliency map for crafting adversarial examples. each iteration, jsma recomputes the saliency map and uses the derivative of the dnn with respect to the input image as an indicator of modification for adversarial attacks. in addition to image classification, jsma has been applied to other machine learning tasks such as malware classification [14], and other dnn architectures such as recurrent neural networks (rnns) [37]. deepfool [31]: inspired from linear classification models and. the fact that the corresponding separating hyperplanes indicate the decision boundaries of each class, deepfool is an untargeted attack algorithm that aims to find the least distortion (in the sense of euclidean distance) leading to misclassification, by projecting an image to the closest separating hyperplane. in particular, an approximate attack algorithm is proposed for dnns in order to tackle the inherit nonlinearity for classification [40]. carlini wagner (cw) attack [8]: the adversarial attack proposed by carlini and wagner is by far one of the strongest attacks. they formulate targeted adversarial attacks as an optimization problem, take advantage of the internal configurations of a targeted dnn for attack guidance, and use the l2 norm (i.e., euclidean distance) to quantify the difference between the adversarial and the original examples. in particular, the representation in the logit layer (the layer prior to the final fully connected layer as illustrated in figure 2) is used as an indicator of attack effectiveness. consequently, the cw attack can be viewed as a gradientdescent based targeted adversarial attack driven by the representation of the logit layer of a targeted dnn and the l2 distortion. the formulation of the cw attack will be discussed in detail in section furthermore, carlini and wagner also showed that their attack can. successfully bypass different detections methods designed for detecting adversarial examples [7]. transferability: in the context of adversarial attacks, transferability means that the adversarial examples generated from one model are also very likely to be misclassified by another model. in particular, the aforementioned adversarial attacks have demonstrated that their adversarial examples are highly transferable from one dnn at hand to the targeted dnn. one possible explanation of inherent attack transferability for dnns lies in the findings that dnns commonly have overwhelming generalization power and local linearity for feature extraction [40]. notably, the transferability of adversarial attacks brings about security concerns for machine learning applications based on dnns, as malicious examples may be easily crafted even when the exact parameters of a targeted dnn are absent. more interestingly, the authors in [29] have shown that a carefully crafted universal perturbation to a set of natural images can lead to misclassification of all considered images with high probability, suggesting the possibility of attack transferability from one image to another. further analysis and justification of a universal perturbation is given in [30]. while the definition of an openbox (whitebox) attack to dnns is clear and precise having complete knowledge and allowing full access to a targeted dnn, the definition of a blackbox attack to dnns may vary in terms of the capability of an attacker. in an attacker\u2019s perspective, a blackbox attack may refer to the most challenging case where only benign images and their class labels are given, but the targeted dnn is completely unknown, and one is prohibited from querying any information from the targeted classifier for adversarial attacks. this restricted setting, which we call a nobox attack setting, excludes the principal adversarial attacks introduced in section 1.1, as they all require certain knowledge and back propagation from the targeted dnn. consequently, under this nobox setting the research focus is mainly on the attack transferability from one selftrained dnn to a targeted but completely accessprohibited dnn.. on the other hand, in many scenarios an attacker does have the privilege to query a targeted dnn in order to obtain useful information for crafting adversarial examples. for instance, a mobile app or. a computer software featuring image classification (mostly likely trained by dnns) allows an attacker to input any image at will and acquire classification results, such as the confidence scores or ranking for classification. an attacker can then leverage the acquired classification results to design more effective adversarial examples to fool the targeted classifier. in this setting, back propagation for gradient computation of the targeted dnn is still prohibited, as back propagation requires the knowledge of internal configurations of a dnn that are not available in the blackbox setting. however, the adversarial query process can be iterated multiple times until an attacker finds a satisfactory adversarial example. for instance, the authors in [26] have demonstrated a successful blackbox adversarial attack to clarifai.com, which is a blackbox image classification system.. due to its feasibility, the case where an attacker can have free access to the input and output of a targeted dnn while still being prohibited from performing back propagation on the targeted dnn has been called a practical blackbox attack setting for dnns [8, 16, 17, 26, 34, 35]. for the rest of this paper, we also refer a blackbox adversarial attack to this setting. for illustration, the attack settings and their limitations are summarized in figure it is worth noting that under this blackbox setting, existing attacking approaches tend to make use of the power of free query to train a substitute model [17, 34, 35], which is a representative substitute of the targeted dnn. the substitute model can then be attacked using any whitebox attack techniques, and the generated adversarial images are used to attack the target dnn. the primary advantage of training a substitute model is its total transparency to an attacker, and hence essential attack procedures for dnns, such as back propagation for gradient computation, can be implemented on the substitute model for crafting adversarial examples. moreover, since the substitute model is representative of a targeted dnn in terms of its classification rules, adversarial attacks to a substitute model are expected to be similar to attacking the corresponding targeted dnn. in other words, adversarial examples crafted from a substitute model can be highly transferable to the targeted dnn given the ability of querying the targeted dnn at will. one common observation from the development of securityrelated research is that attack and defense often come handinhand, and one\u2019s improvement depends on the other\u2019s progress. similarly, in the context of robustness of dnns, more effective adversarial attacks are often driven by improved defenses, and vice versa. there has been a vast amount of literature on enhancing the robustness of dnns. here we focus on the defense methods that have been shown to be effective in tackling (a subset of) the adversarial attacks introduced in section while maintaining similar classification performance for the benign examples. based on the defense techniques, we categorize the defense methods proposed for enhancing the robustness of dnns to adversarial examples as follows. detectionbased defense: detectionbased approaches aim to differentiate an adversarial example from a set of benign examples using statistical tests or outofsample analysis. interested readers can refer to recent works in [10, 13, 18, 28, 42, 43] and references therein for details. in particular, feature squeezing is. shown to be effective in detecting adversarial examples by projecting an image to a confined subspace (e.g., reducing color depth of a pixel) to alleviate the exploits from adversarial attacks [42, 43]. the success of detectionbased approaches heavily relies on the assumption that the distributions of adversarial and benign examples are fundamentally distinct. however, carlini and wagner recently demonstrated that their attack (cw attack) can bypass different detection algorithms designed for detecting adversarial examples [7], which challenges the fundamental assumption of detectionbased approaches as the results suggest that the distributions of their adversarial examples and the benign examples are nearly indistinguishable. gradient and representation masking: as the use of gradients via back propagation on dnns has been shown to be crucial to crafting adversarial examples, one natural defense mechanism is to hide the gradient information while training a dnn, known as gradient masking. a typical example of gradient masking is the defense distillation proposed in [38]. the authors proposed to retrain a dnn using distillation [15] based on the original confidence scores for classification (also known as soft labels) and introduced the concept of temperature in the softmax step for gradient masking. an extended version has been proposed to enhance its defense performance by incorporating modelagnostic uncertainty into retraining [33]. although the cw attack has shown to be able to break defensive distillation [8], it is still considered as a baseline model for defending adversarial attacks. another defense technique, which we call representation masking, is inspired by the finding that in the cw attack the logit layer representation in dnns is useful for adversarial attacks. as a result, representation masking aims to replace the internal representations in dnns (usually the last few layers) with robust representations to alleviate adversarial attacks. for example, the authors in [6] proposed to integrate dnns with gaussian processes and rbf kernels for enhanced robustness. adversarial training: the rationale behind adversarial training is that dnns are expected to be less sensitive to perturbations (either adversarial or random) to the examples if these adversarial examples are jointly used to stabilize training, known as the data augmentation method. different data augmentation methods have been proposed to improve the robustness of dnns. interested readers can refer to the recent works in [19, 27, 41, 44, 45] and the references therein. notably, the defense model proposed in [27] showed promising results against adversarial attacks, including the fgsm and the cw attack. the authors formulated defense in dnns as a robust optimization problem, where the robustness is improved by iterative adversarial data augmentation and retraining. the results suggest that a dnn can be made robust at the price of increased network capacity (i.e., more model parameters), in order to stabilize training and alleviate the effect of adversarial examples. zeroth order methods are derivativefree optimization methods, where only the zeroth order oracle (the objective function value f (x) at any x) is needed during optimization process. by evaluating the objective function values at two very close points f (x hv) and f (x hv) with a small h, a proper gradient along the direction. vector v can be estimated. then, classical optimization algorithms like gradient descent or coordinate descent can be applied using the estimated gradients. the convergence of these zeroth order methods has been proved in optimization literature [11, 25, 32], and under mild assumptions (smoothness and lipschitzian gradient) they can converge to a stationary point with an extra error term which is related to gradient estimation and vanishes when h our proposed blackbox attack to dnns in section is cast as an optimization problem. it exploits the techniques from zeroth order optimization and therefore spares the need of training a substitute model for deploying adversarial attacks. although it is intuitive to use zeroth order methods to attack a blackbox dnn model, applying it naively can be impractical for large models. for example, the inceptionv3 network [39] takes input images with a size of 3, and thus has p 268, variables (pixels) to optimize. to evaluate the estimated gradient of each pixel, we need to evaluate the model twice. to just obtain the estimated gradients of all pixels, 2p 536, evaluations are needed. for a model as large as inceptionv3, each evaluation can take tens of milliseconds on a single gpu, thus it is very expensive to even evaluate all gradients once. for targeted attacks, sometimes we need to run an iterative gradient descent with hundreds of iterations to generate an adversarial image, and it can be forbiddingly expensive to use zeroth order method in this case.. in the scenario of attacking blackbox dnns, especially when the image size is large (the variable to be optimized has a large number of coordinates), a single step of gradient descent can be very slow and inefficient, because it requires estimating the gradients of all coordinates to make a single update. instead, we propose to use a coordinate descent method to iteratively optimize each coordinate (or a small batch of coordinates). by doing so, we can accelerate the attack process by efficiently updating coordinates after only a few gradient evaluations. this idea is similar to dnn training for large datasets, where we usually apply stochastic gradient descent using only a small subset of training examples for efficient updates, instead of computing the full gradient using all examples to make a single update. using coordinate descent, we update coordinates by small batches, instead of updating all coordinates in a single update as in gradient descent. moreover, this allows us to further improve the efficiency of our algorithm by using carefully designed sampling strategy to optimize important pixels first. we will discuss the detailed algorithm in section we refer to the proposed attack method as blackbox attacks using zeroth order optimization, or zoo for short. below we summarize our main contributions: we show that a coordinate descent based method using only the zeroth order oracle (without gradient information) can effectively attack blackbox dnns. comparing to the substitute model based blackbox attack [35], ourmethod significantly increases the success rate for adversarial attacks, and attains comparable performance to the stateoftheart whitebox attack (cw attack). in order to speed up the computational time and reduce number of queries for our blackbox attacks to largescale dnns, we propose several techniques including attackspace dimension reduction, hierarchical attacks and importance sampling.. in addition to datasets of small image size (mnist and cifar10), we demonstrate the applicability of our blackbox attack model to a large dnn the inceptionv3 model trained on imagenet. our attack is capable of crafting a successful adversarial image within a reasonable time, whereas the substitute model based blackbox attack in [35] only shows success in small networks trained on mnist and is hardly scalable to the case of imagenet. the study of adversarial attacks roots in the need for understanding the robustness of stateoftheart machine learning models [1, 2]. for instance, biggio et el. proposed an effective attack to sabotaging the performance (test accuracy) of support vector machines (svms) by intelligently injecting adversarial examples to the training dataset [5]. gradient based evasion attacks to svms and multilayer perceptrons are discussed in [4]. given the popularity and success of classification models trained by dnns, in recent years there has been a surge in interest toward understanding the robustness of dnns. a comprehensive overview of adversarial attacks and defenses for dnns is given in section here we focus on related work on the blackbox adversarial attack setting for dnns. as illustrated in figure 2, the blackbox setting allows free query from a targeted dnn but prohibits any access to internal configurations (e.g., back propagation), which fits well to the scenario of publicly accessible machine learning services (e.g., mobile apps, image classification service providers, and computer vision packages). under this blackbox setting, the methodology of current attacks concentrates on training a substitute model and using it as a surrogate for adversarial attacks [8, 16, 17, 26, 34, 35]. in other words, a blackbox attack is made possible by deploying a whitebox attack to the substitute model. therefore, the effectiveness of such blackbox adversarial attacks heavily depends on the attack transferability from the substitute model to the target model. different from the existing approaches, we propose a blackbox attack via zeroth order optimization techniques. more importantly, the proposed attack spares the need for training substitute models by enabling a pseudo back propagation on the target model. consequently, our attack can be viewed as if it was a whitebox attack to the target model, and its advantage over current blackbox methods can be explained by the fact that it avoids any potential loss in transferability from a substitute model. the performance comparison between the existing methods and our proposed blackbox attack will be discussed in section in principle, our blackbox attack technique based on zeroth order optimization is a general framework that can be applied to any whitebox attacks requiring back propagation on the targeted dnn. we note that all the effective adversarial attacks discussed in section have such a requirement, as back propagation on a targeted dnn provides invaluable information for an attacker. analogously, one can view an attacker as an optimizer and an adversarial attack as an objective function to be optimized. back propagation provides firstorder evaluation (i.e., gradients) of the objective function for the optimizer for efficient optimization. for the purpose of demonstration, in this paper we proposed a blackbox attack based on the formulation of the cw attack, since the (whitebox) cw attack has significantly outperformed the other attacks discussed in section in terms of the quality (distortion). of the crafted adversarial examples and attack transferability [8]. experimental results in section show that our blackbox version is as effective as the original cw attack but at the cost of longer processing time for implementing pseudo back propagation. we also compare our blackbox attack with the blackbox attack via substitute models in [35], which trains a substitute model as an attack surrogate based on jacobian saliency map [36]. experimental results in section show that our attack significantly outperforms [35], which can be explained by the fact that our attack inherits the effectiveness of the stateoftheart cw attack, and also by the fact that zeroth order optimization allows direct adversarial attacks to a targeted dnn and hence our blackbox attack does not suffer from any loss in attack transferability from a substitute model. as illustrated in figure 2, since we consider the blackbox attack setting where free query from a targeted dnn is allowed while accessing to internal states (e.g., performing back propagation) is prohibited, it suffices to use the notation f (x) to denote a targeted dnn. specifically, the dnn f (x) takes an image x rp (a pdimensional column vector) as an input and outputs a vector f (x) [0, 1]k of confidence scores for each class, where k is the number of classes. the kth entry [f (x)]k [0, 1] specifies the probability of classifying x as class k , and k k1[f (x)]k in principle, our proposed blackbox attack via zeroth order optimization (zoo) can be applied to nondnn classifiers admitting the same input and output relationship. however, since dnns achieved stateoftheart classification accuracy in many image tasks, in this paper we focus on the capability of our blackbox adversarial attack to dnns. our blackbox attack is inspired by the formulation of the cw attack [8], which is one of the strongest whitebox adversarial attacks to dnns at the time of our work. given an image x0, let x denote the adversarial example of x0 with a targeted class label t toward misclassification. the cw attack finds the adversarial example x by solving the following optimization problem:. minimizex x x022 c f (x, t) (1) subject to x [0, 1]p ,. where v2 p i1v i denotes the euclidean norm (l2 norm) of a vector v [v1, ,vp ]t , and c is a regularization parameter. the first term xx022 in (1) is the regularization used to enforce the similarity between the adversarial example x and the image x0 in terms of the euclidean distance, since x x0 is the adversarial image perturbation of x relative to x0. the second term c f (x, t) in (1) is the loss function that reflects the level of unsuccessful adversarial attacks, and t is the target class. carlini and wagner compared several candidates for f (x, t) and suggested the following form for effective targeted attacks [8]:. where z (x) rk is the logit layer representation (logits) in the dnn for x such that [z (x)]k represents the predicted probability that x belongs to class k , and is a tuning parameter for attack transferability. carlini and wagner set for attacking a targeted dnn, and suggested large when performing transfer attacks. the rationale behind the use of the loss function in (2) can be explained by the softmax classification rule based on the logit layer representation; the output (confidence score) of a dnn f (x) is determined by the softmax function:. therefore, based on the softmax decision rule in (3),maxi,t [z (x)]i [z (x)]t implies that the adversarial example x attains the highest confidence score for class t and hence the targeted attack is successful. on the other hand,maxi,t [z (x)]i [z (x)]t implies that the targeted attack using x is unsuccessful. the role of ensures a constant gap between [z (x)]t and maxi,t [z (x)]i , which explains why setting large is effective in transfer attacks.. finally, the box constraint x [0, 1]p implies that the adversarial example has to be generated from the valid image space. in practice, every image can satisfy this box constraint by dividing each pixel value by the maximum attainable pixel value (e.g., 255). carlini and wagner remove the box constraint by replacing x with 1tanhw2 , where w rp by using this changeofvariable, the optimization problem in (1) becomes an unconstrained minimization problem with w as an optimizer, and typical optimization tools for dnns (i.e., back propagation) can be applied for solving the optimal w and obtain the corresponding adversarial example x. the attack formulation using (1) and (2) presumes a whitebox attack because (i): the logit layer representation in (2) is an internal state information of a dnn; and (ii) back propagation on the targeted dnn is required for solving (1). we amend our attack to the blackbox setting by proposing the following approaches: (i) modify the loss function f (x, t) in (1) such that it only depends on the output f of a dnn and the desired class label t ; and (ii) compute an approximate gradient using a finite difference method instead of actual back propagation on the targeted dnn, and solve the optimization problem via zeroth order optimization. we elucidate these two approaches below. loss function f (x, t) based on f : inspired by (2), we propose a new hingelike loss function based on the output f of a dnn, which is defined as. f (x, t) maxmax i,t log[f (x)]i log[f (x)]t ,, (4). where and log is defined as we note that log() is a monotonic function such that for any x ,y 0, logy logx if and only if y x this implies that maxi,t log[f (x)]i log[f (x)]t means x attains the highest confidence score for class t we find that the log operator is essential to our blackbox attack since very often a welltrained dnn yields a skewed probability distribution on its output f (x) such that the confidence score of one class significantly dominates the confidence scores of the other classes. log operator lessens the dominance effect while preserving the order of confidence scores due to monotonicity. similar to (2), in (4) ensures a constant gap between maxi,t log[f (x)]i and log[f (x)]t for untargeted attacks, an adversarial attack is successful when x is classified as any class other than the original class label t0. a similar loss function can be used (we drop the variable t for untargeted attacks):. f (x) maxlog[f (x)]t0 maxi,t0 log[f (x)]i ,, (5). where t0 is the original class label for x, and maxi,t0 log[f (x)]i represents the most probable predicted class other than t0. zeroth order optimization on the loss function: we discuss our optimization techniques for any general function f used for attacks (the regularization term in (1) can be absorbed as a part of f ). we use the symmetric difference quotient [23] to estimate the gradient f (x)xi (defined as i ):. whereh is a small constant (we seth in all our experiments) and ei is a standard basis vector with only the ith component as the estimation error (not including the error introduced by limited numerical precision) is in the order of o(h2). although numerical accuracy is a concern, accurately estimating the gradient is usually not necessary for successful adversarial attacks. one example is fgsm, which only requires the sign (rather than the exact value) of the gradient to find adversarial examples. therefore, even if our zeroth order estimations may not be very accurate, they suffice to achieve very high success rates, as we will show in our experiments.. for any x rp , we need to evaluate the objective function 2p times to estimate gradients of all p coordinates. interestingly, with just one more objective function evaluation, we can also obtain the coordinatewise hessian estimate (defined as hi ):. hi b f (x) x2ii f (x hei ) 2f (x) f (x hei ) h2 remarkably, since f (x) only needs to be evaluated once for all p coordinates, we can obtain the hessian estimates without additional function evaluations.. it is worth noting that stochastic gradient descent and batch gradient descent are two most commonly used algorithms for training dnns, and the cw attack [8] also used gradient descent to attack a dnn in the whitebox setting. unfortunately, in the blackbox setting, the network structure is unknown and the gradient computation via back propagation is prohibited. to tackle this problem, a naive solution is applying (6) to estimate gradient, which requires 2p objective function evaluations. however, this naive solution is too expensive in practice. even for an input image size of 64643, one full gradient descent step requires 24, evaluations, and typically hundreds of iterations may be needed until convergence. to resolve this issue, we propose the following coordinatewise update, which only requires function evaluations for each step.. stochastic coordinate descent: coordinate descentmethods have been extensively studied in optimization literature [3]. at each iteration, one variable (coordinate) is chosen randomly and is. algorithm stochastic coordinate descent. 1: while not converged do 2: randomly pick a coordinate i 1, ,p 3: compute an update by approximately minimizing. updated by approximately minimizing the objective function along that coordinate (see algorithm for details). the most challenging part in algorithm is to compute the best coordinate update in step after estimating the gradient and hessian for xi , we can use any first or second order method to approximately find the best in firstorder methods, we found that adam [20]\u2019s update rule significantly outperforms vanilla gradient descent update and other variants in our experiments, so we propose to use a zerothorder coordinate adam, as described in algorithm we also use newton\u2019s method with both estimated gradient and hessian to update the chosen coordinate, as proposed in algorithm note that when hessian is negative (indicating the objective function is concave along direction xi ), we simply update xi by its gradient. we will show the comparison of these two methods in section experimental results suggest coordinatewise adam is faster than newton\u2019s method.. algorithm zooadam: zeroth order stochastic coordinate descent with coordinatewise adam require: step size , adam states m rp ,v rp ,t zp ,. adam hyperparameters 0.9, 0.999, 1: m 0,v 0,t 2: while not converged do 3: randomly pick a coordinate i 1, ,p 4: estimate i using (6) 5: ti ti 6: mi 1mi (1 1)i , vi 2vi (1 2)2i 7: mi mi/(1 ti1 ), vi vi/(1 ti ) 8: mi vi. note that for algorithmic illustration we only update one coordinate for each iteration. in practice, to achieve the best efficiency of gpu, we usually evaluate the objective in batches, and thus a batch of i and hi can be estimated. in our implementation we estimate b pixels\u2019 gradients and hessians per iteration, and then update b coordinates in a single iteration. we first define x xx0 and x rp to be the adversarial noise added to the original image x0. our optimization procedure starts with x for networks with a large input size p, optimizing over rp (we call it attackspace) using zeroth order methods can be quite slow because we need to estimate a large number of gradients.. algorithm zoonewton: zeroth order stochastic coordinate descent with coordinatewise newton\u2019s method require: step size 1: while not converged do 2: randomly pick a coordinate i 1, ,p 3: estimate i and hi using (6) and (7) 4: if hi then 5: i 6: else 7: i. hi 8: end if 9: update xi xi 10: end while. instead of directly optimizing x rp , we introduce a dimension reduction transformation d(y) where y rm , rane(d) rp , andm p. the transformation can be linear or nonlinear. then, we use d(y) to replace x x x0 in (1):. minimizey d(y)22 c f (x0 d(y), t) (8) subject to x0 d(y) [0, 1]p the use of d(y) effectively reduces the dimension of attackspace from p tom. note that we do not alter the dimension of an input image x but only reduce the permissible dimension of the adversarial noise. a convenient transformation is to defined to be the upscaling operator that resizes y as a sizep image, such as the bilinear interpolation method1. for example, in the inceptionv3 network y can be a small adversarial noise image with dimensionm 3, while the original image dimension is p other transformations like dct (discrete cosine transformation) can also be used. we will show the effectiveness of this method in section when applying attackspace dimension reduction with a smallm, although the attackspace is efficient to optimize using zeroth order methods, a valid attack might not be found due to the limited search space. conversely, if a largem is used, a valid attack can be found in that space, but the optimization process may take a long time. thus, for large images and difficult attacks, we propose to use a hierarchical attack scheme, where we use a series of transformations d1,d2 with dimensionsm1,m2, to gradually increase m during the optimization process. in other words, at a specific iteration j (according to the dimension increasing schedule) we set yj d1i (di1(yj1)) to increase the dimension of y frommi1 to mi (d1 denotes the inverse transformation of d).. for example, when using image scaling as the dimension reduction technique, d1 upscales y fromm1 to 2992993, and d2 upscales y fromm2 to we start withm1 variables to optimize with and use d1 as the transformation, then after a certain number of iterations (when the decrease in the loss function is inapparent, indicating the need of a larger attackspace), we upscale y from to 3, and use d2 for the following iterations.. 1see the details at https://en.wikipedia.org/wiki/bilinearinterpolation. channel r sample prob.. channel g sample prob.. channel b sample prob. one benefit of using coordinate descent is that we can choose which coordinates to update. since estimating gradient and hessian for each pixel is expensive in the blackbox setting, we propose to selectively update pixels by using importance sampling. for example, pixels in the corners or at the edges of an image are usually less important, whereas pixels near the main object can be crucial for a successful attack. therefore, in the attack process we sample more pixels close to the main object indicated by the adversarial noise.. we propose to divide the image into regions, and assign sampling probabilities according to how large the pixel values change in that region. we run a max pooling of the absolute pixel value changes in each region, upsample to the desired dimension, and then normalize all values such that they sum up to every few iterations, we update these sampling probabilities according to the recent changes. in figure 3, we show a practical example of pixel changes and how importance sampling probabilities are generated when attacking the bagel image in figure (a).. when the attackspace is small (for example, 3), we do not use importance sampling to sufficiently search the attackspace. when we gradually increase the dimension of attackspace using hierarchical attack, incorporating importance sampling becomes crucial as the attackspace is increasingly larger. we will show the effectiveness of importance sampling in section we compare our attack (zoo) with carlini wagner\u2019s (cw) whitebox attack [8] and the substitute model based blackbox attack [35]. we would like to show that our blackbox attack can achieve similar success rate and distortion as the whitebox cw attack, and can significantly outperform the substitute model based blackbox attack, while maintaining a reasonable attack time.. our experimental setup is based on carlini wagner\u2019s framework2 with our adam and newton based zeroth order optimizer included. for substitute model based attack, we use the reference implementation (with necessary modifications) in cleverhans3 for comparison. for experiments on mnist and cifar, we use a intel xeon e52690v4 cpu with a single nvidia k80 gpu; for experiments on imagenet, we use a amd ryzen cpu with a single nvidia gtx ti gpu. our experimental code is publicly available4. for implementing zeroth order optimization, we use a batch size of b 128; i.e., we evaluate gradients and update coordinates per iteration. in addition, we set unless specified. for mnist and cifar10, we use the same dnn model as in the cw attack ([8], table 1). for substitute model based attack, we use the same dnnmodel for both the target model. 2https://github.com/carlini/nnrobustattacks 3https://github.com/tensorflow/cleverhans/blob/master/tutorials/mnistblackbox.py 4https://github.com/huanzhang12/zooattack. and the substitute model. if the architecture of a targeted dnn is unknown, blackbox attacks based on substitute models will yield worse performance due to model mismatch. for targeted attacks, we randomly select images from mnist and cifar10 test sets, and skip the original images misclassified by the target model. for each image, we apply targeted attacks to all other classes, and thus there are attacks in total. for untargeted attacks, we randomly select images from mnist and cifar10 test sets. for both ours and the cw attack, we run a binary search up to times to find the best c (starting from 0.01), and terminate the optimization process early if the loss does not decrease for iterations. we use the same step size and adam parameters 0.9, for all methods. for the cw attack, we run 1,000 iterations; for our attack, we run 3,000 iterations for mnist and 1,000 iterations for cifar. note that our algorithm updates far less variables because for each iteration we only update pixels, whereas in the cw attack all pixels are updated based on the full gradient in one iteration due to the whitebox setting. also, since the image size of mnist and cifar10 is small, we do not reduce the dimension of attackspace or use hierarchical attack and importance sampling. for training the substitute model, we use holdout images from the test set and run jacobian augmentation epochs, and set the augmentation parameter we implement fgsm and the cw attack on the substitute model for both targeted and untargeted transfer attacks to the blackbox dnn. for fgsm, the perturbation parameter 0.4, as it is shown to be effective in [35]. for the cw attack, we use the same settings as the whitebox cw, except for setting for attack transferability and using 2,000 iterations. other tricks.when attacking mnist, we found that the changeofvariable via tanh can cause the estimated gradients to vanish due to limited numerical accuracy when pixel values are close to the boundary (0 or 1). alternatively, we simply project the pixel values within the box constraints after each update for mnist. cifar10, we find that using changeofvariable converges faster, as most pixels are not close to the boundary. as shown in table 1, our proposed attack (zoo) achieves nearly success rate. furthermore, the l2 distortions are also close to the cw attack, indicating our blackbox adversarial images have similar quality as the whitebox approach (figures and 5). notably, our success rate is significantly higher than the substitute model based attacks, especially for targeted attacks, while maintaining reasonable average attack time. when transferring attacks from the substitute models to the target dnn, fgsm achieves better success rates in some experiments because it uses a relatively large and introduces much more noise than cw attack. we also find that adam usually works better than the newton\u2019s method in terms of computation time and l2 distortion. attacking a large blackbox network like inceptionv3 [39] can be challenging due to large attackspace and expensive model evaluation. blackbox attacks via substitute models become impractical in this case, as a substitute model with a large enough capacity relative to inceptionv3 is needed, and a tremendous amount of costly jacobian data augmentation is needed to train this model. on the other hand, transfer attacks may suffer from lower success rate comparing to whitebox attacks, especially for targeted attacks. here we apply the techniques proposed in section 3.4, and to efficiently overcome the optimization difficulty toward effective and efficient blackbox attacks. untargeted blackbox attacks to inceptionv3. we use images from the imagenet test set for untargeted attacks. to justify the effectiveness of using attackspace dimension reduction, we exclude small images in the test set and ensure that all the original images are at least in size. we also skip all images that are originally misclassified by inceptionv3. attack techniques and parameters. we use an attackspace of only (the original input space is 2992993) and do not. use hierarchical attack. we also set a hard limit of 1, iterations for each attack, which takes about minutes per attack in our setup. in fact, during 1, iterations, only 192, gradients are evaluated, which is even less than the total number of pixels (299 268, 203) of the input image. we fix c in all inceptionv3 experiments, as it is too costly to do binary search in this case. for both cw and our attacks, we use step size we compare the success rate and average l2 distortion between our zoo attack and the cw whitebox attack in table despite running only 1,500 iterations (within minutes per image) and using a small attackspace (32 3), our blackbox attack achieves about success rate. the average l2 distortion is about times larger than the whitebox attack, but our adversarial images are still visually indistinguishable (figures 1). the success rate and distortion can be further improved if we run more iterations.. targeted blackbox attacks to inceptionv3. for inceptionv3, a targeted attack is much more difficult as there are classes, and a successful attack means one can manipulate the predicted probability of any specified class. however, we report that using our advanced attack techniques, 20, iterations (each with b pixel updates) are sufficient for a hard targeted attack. we select an image (figure (a)) for which our untargeted attack failed, i.e., we cannot even find an untargeted attack in the attackspace, indicating that this image is hard to attack. inceptionv3 classifies it as a bagel with confidence, and other top5 predictions include guillotine, pretzel, granny smith and dough with 1.15, 0.07, and confidence. we deliberately make the attack even harder by choosing the target class as grand piano, with original confidence of only we use attackspace dimension reduction as well as hierarchical attack. we start from an attackspace of 323, and increase it to and at iteration 2,000 and 10,000, respectively. we run the zeroth order adam solver (algorithm 2) with a total of 20,000 iterations, taking about minutes in our setup. also, when the attack space is greater than 32323, we incorporate importance sampling, and keep updating the sampling probability after each iteration. reset adam states.we report an additional trick to reduce the final distortion reset the adam solver\u2019s states when a first valid. attack is found during the optimization process. the reason is as follows. the total loss consists of two parts: l1 b c f (x, t) and l2 b xx022 l1 measures the difference between the original class probability porig and targeted class probability ptarget as defined in (4). when l1 0, porig ptarget, and a valid adversarial example is found. during the optimization process, we observe that before l1 reaches 0, l2 is likely to increase, i.e., adding more distortion and getting closer to the target class. after l1 reaches it cannot go below because it is a hingelike loss, and at this point the optimizer should try to reduce l2 as much as possible while keeping ptarget only slightly larger than porig. however, when we run coordinatewise adam, we found that even after l1 reaches 0, the optimizer still tries to reduce porig and to increase ptarget, and l2 will not be decreased efficiently. we believe the reason is that the historical gradient statistics stored in adam states are quite stale due to the large number of coordinates. therefore, we simply reset the adam states after l1 reaches for the first time in order to make the solver focus on decreasing l2 afterwards. figure shows how the loss decreases versus iterations, with all techniques discussed above applied in red; other curves show the optimization process without a certain technique but all others included. the black curve decreases very slowly, suggesting hierarchical attack is extremely important in accelerating our attack, otherwise the large attackspace makes zeroth order methods infeasible. importance sampling also makes a difference especially after iteration 10,000 when the attackspace is increased to 1281283; it helps us to find the first valid attack over 2,000 iterations earlier,. thus leaving more time for reducing the distortion. the benefit of reseting adam states is clearly shown in table 3, where the final distortion and loss increase noticeably if we do not reset the states.. the proposed zoo attack succeeds in decreasing the probability of the original class by over 160x (from to about 0.6) while increasing the probability of the target class by over 1000x (from to over 0.6, which is top1) to achieve a successful attack. furthermore, as shown in figures 1, the crafted adversarial noise is almost negligible and indistinguishable by human eyes. this paper proposed a new type of blackbox attacks named zoo to dnns without training any substitute model as an attack surrogate. by exploiting zeroth order optimization for deploying pseudo back propagation on a targeted blackbox dnn, experimental results show that our attack attains comparable performance to the stateoftheart whitebox attack (carlini and wagner\u2019s attack). in addition, our blackbox attack significantly outperforms the substitute model based blackbox attack in terms of attack success rate and distortion, as our method does not incur any performance loss in attack transferability. furthermore, we proposed several acceleration techniques for applying our attack to large dnns trained on imagenet, whereas the substitute model based blackbox attack is hardly scalable to a large dnn like inceptionv3.. based on the analysis and findings from the experimental results on mnist, cifar10 and imagenet, we discuss some potential research directions of our blackbox adversarial attacks as follows. accelerated blackbox attack: although our blackbox attack spares the need for training substitution models and attains comparable performance to thewhitebox cwattack, in the optimization process it requires more iterations than a whitebox attack due to additional computation for approximating the gradient of a dnn and for performing pseudo back propagation. in addition to the computation acceleration tricks proposed in section 3, a datadriven approach that take these tricks into consideration while crafting an adversarial example can make our blackbox attack more efficient.. adversarial training using our blackbox attack: adversarial training in dnns is usually implemented in a whitebox setting generating adversarial examples from a dnn to stabilize training and making the retrained model more robust to adversarial attacks. our blackbox attack can serve as an independent indicator of the robustness of a dnn for adversarial training. blackbox attacks in different domains: in this paper we explicitly demonstrated our blackbox attack to image classifiers trained by dnns. a natural extension is adapting our attack to other data types (e.g., speeches, time series, graphs) and different machine learning models and neural network architectures (e.g., recurrent neural networks). in addition, how to incorporate side information of a dataset (e.g., expert knowledge) and existing adversarial examples (e.g., security leaks and exploits) into our blackbox attack is worthy of further investigation. the authors would like to warmly thank florian tramr and tsuiwei weng for their valuable feedbacks and insightful comments. chojui hsieh and huan zhang acknowledge the support of nsf via iis1719097.", "summary": "propose a gradientbased blackbox attack to compute adversarial examples. specifically, they follow the general idea of [1] where the following objective is optimized:. \\minx \\x x0\\2 c \\max\\\\maxi. eq t\\zi\\ zt, \\kappa\\.. here, x is the adversarial example based on training sample x0. the second part expresses that x is supposed to be misclassified, i.e. eq t distinct form the true label t is supposed to be larger that the logit zt corresponding to the true label. this is optimized subject to the constraint that x is a valid image.. the attack proposed in [1] assumes a whitebox setting were we have access to the logits and the gradients (basically requiring access to the full model). chen et al., in contrast want to design a blackbox attacks. therefore, they make the following changes:. instead of using logits zi, the probability distribution fi (i.e. the actual output of the network) is used.. gradients are approximated by finite differences.. personally, i find that the first point does violate a strict blackbox setting. as company, for example, i would prefer not to give away the full probability distribution but just the final decision (or the decision plus a confidence score). then, however, the proposed method is not applicable anymore. anyway, the changed objective looks as follows: \\minx \\x x0\\2 c \\max\\\\maxi. eq t\\\\log fi\\ \\log ft, \\kappa\\ where, according to the authors, the logarithm is essential for optimization. one remaining problem is efficient optimization with finite differences. to this end, they propose a randomized/stochastic coordinate descent algorithm. in particular, in each step, a ranodm pixel is chosen and a local update is performed by calculating the gradient on this pixel using finite differences and performing an adam step.. towards evaluating the robustness of neural networks. ieee symposium of security and privacy, also view this summary at [davidstutz.de]([url]/).."}, {"document": "despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. specifically, our experiments establish that stateoftheart convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. this phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. we corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. we interpret our experimental findings by comparison with traditional models. deep artificial neural networks often have far more trainable model parameters than the number of samples they are trained on. nonetheless, some of these models exhibit remarkably small generalization error, i.e., difference between training error and test error. at the same time, it is certainly easy to come up with natural model architectures that generalize poorly. what is it then that distinguishes neural networks that generalize well from those that don\u2019t? a satisfying answer to this question would not only help to make neural networks more interpretable, but it might also lead to more principled and reliable model architecture design.. to answer such a question, statistical learning theory has proposed a number of different complexity measures that are capable of controlling generalization error. these include vc dimension (vapnik, 1998), rademacher complexity (bartlett mendelson, 2003), and uniform stability (mukherjee et al., 2002; bousquet elisseeff, 2002; poggio et al., 2004). moreover, when the number of parameters is large, theory suggests that some form of regularization is needed to ensure small generalization error. regularization may also be implicit as is the case with early stopping. in this work, we problematize the traditional view of generalization by showing that it is incapable of distinguishing between different neural networks that have radically different generalization performance. work performed while interning at google brain. work performed at google brain.. ar x. iv :1. at the heart of our methodology is a variant of the wellknown randomization test from nonparametric statistics (edgington onghena, 2007). in a first set of experiments, we train several standard architectures on a copy of the data where the true labels were replaced by random labels. our central finding can be summarized as:. deep neural networks easily fit random labels.. more precisely, when trained on a completely random labeling of the true data, neural networks achieve training error. the test error, of course, is no better than random chance as there is no correlation between the training labels and the test labels. in other words, by randomizing labels alone we can force the generalization of a model to jump up considerably without changing the model, its size, hyperparameters, or the optimizer. we establish this fact for several different standard architectures trained on the cifar10 and imagenet classification benchmarks. while simple to state, this observation has profound implications from a statistical learning perspective:. the effective capacity of neural networks is large enough for a bruteforce memorization of the entire data set.. even optimization on random labels remains easy. in fact, training time increases only by a small constant factor compared with training on the true labels.. randomizing labels is solely a data transformation, leaving all other properties of the learning problem unchanged.. extending on this first set of experiments, we also replace the true images by completely random pixels (e.g., gaussian noise) and observe that convolutional neural networks continue to fit the data with zero training error. this shows that despite their structure, convolutional neural nets can fit random noise. we furthermore vary the amount of randomization, interpolating smoothly between the case of no noise and complete noise. this leads to a range of intermediate learning problems where there remains some level of signal in the labels. we observe a steady deterioration of the generalization error as we increase the noise level. this shows that neural networks are able to capture the remaining signal in the data, while at the same time fit the noisy part using bruteforce.. we discuss in further detail below how these observations rule out all of vcdimension, rademacher complexity, and uniform stability as possible explanations for the generalization performance of stateoftheart neural networks.. the role of explicit regularization. if the model architecture itself isn\u2019t a sufficient regularizer, it remains to see how much explicit regularization helps. we show that explicit forms of regularization, such as weight decay, dropout, and data augmentation, do not adequately explain the generalization error of neural networks. explicit regularization may improve generalization performance, but is neither necessary nor by itself sufficient for controlling generalization error.. in contrast with classicial convex empirical risk minimization, where explicit regularization is necessary to rule out trivial solutions, we found that regularization plays a rather different role in deep learning. it appears to be more of a tuning parameter that often helps improve the final test error of a model, but the absence of all regularization does not necessarily imply poor generalization error. as reported by krizhevsky et al. (2012), 2regularization (weight decay) sometimes even helps optimization, illustrating its poorly understood nature in deep learning.. finite sample expressivity. we complement our empirical observations with a theoretical construction showing that generically large neural networks can express any labeling of the training data. more formally, we exhibit a very simple twolayer relu network with p 2nd parameters that can express any labeling of any sample of size n in d dimensions. a previous construction due to livni et al. (2014) achieved a similar result with far more parameters, namely, o(dn). while our depth network inevitably has large width, we can also come up with a depth k network in which each layer has only o(n/k) parameters.. while prior expressivity results focused on what functions neural nets can represent over the entire domain, we focus instead on the expressivity of neural nets with regards to a finite sample. contrast to existing depth separations (delalleau bengio, 2011; eldan shamir, 2016; telgarsky, 2016; cohen shashua, 2016) in function space, our result shows that even depth2 networks of linear size can already represent any labeling of the training data.. the role of implicit regularization. while explicit regularizers like dropout and weightdecay may not be essential for generalization, it is certainly the case that not all models that fit the training data well generalize well. indeed, in neural networks, we almost always choose our model as the output of running stochastic gradient descent. appealing to linear models, we analyze how sgd acts as an implicit regularizer. for linear models, sgd always converges to a solution with small norm. hence, the algorithm itself is implicitly regularizing the solution. indeed, we show on small data sets that even gaussian kernel methods can generalize well with no regularization. though this doesn\u2019t explain why certain architectures generalize better than other architectures, it does suggest that more investigation is needed to understand exactly what the properties are inherited by models that were trained using sgd. (2016) give an upper bound on the generalization error of a model trained with stochastic gradient descent in terms of the number of steps gradient descent took. their analysis goes through the notion of uniform stability (mukherjee et al., 2002; bousquet elisseeff, 2002; poggio et al., 2004). as we point out in this work, uniform stability of a learning algorithm is independent of the labeling of the training data. hence, the concept is not strong enough to distinguish between the models trained on the true labels (small generalization error) and models trained on random labels (high generalization error). this also highlights why the analysis of hardt et al. (2016) for nonconvex optimization was rather pessimistic, allowing only a very few passes over the data. our results show that even empirically training neural networks is not uniformly stable for many passes over the data. consequently, a weaker stability notion is necessary to make further progress along this direction.. there has been much work on the representational power of neural networks, starting from universal approximation theorems for multilayer perceptrons (cybenko, 1989; mhaskar, 1993; delalleau bengio, 2011; mhaskar poggio, 2016; eldan shamir, 2016; telgarsky, 2016; cohen shashua, 2016). all of these results are at the population level characterizing which mathematical functions certain families of neural networks can express over the entire domain. we instead study the representational power of neural networks for a finite sample of size n. this leads to a very simple proof that even o(n)sized twolayer perceptrons have universal finitesample expressivity. our goal is to understand the effective model capacity of feedforward neural networks. toward this goal, we choose a methodology inspired by nonparametric randomization tests. specifically, we take a candidate architecture and train it both on the true data and on a copy of the data in which the true labels were replaced by random labels. in the second case, there is no longer any relationship between the instances and the class labels. as a result, learning is impossible. intuition suggests that this impossibility should manifest itself clearly during training, e.g., by training not converging or slowing down substantially. to our surprise, the training process for several standard achitectures is largely unaffected by this transformation of the labels. this poses a conceptual challenge. whatever justification we had for expecting a small generalization error to begin with must no longer apply to the case of random labels.. to gain further insight into this phenomenon, we experiment with different levels of randomization exploring the continuum between no label noise and completely corrupted labels. we also try out different randomizations of the inputs (rather than labels), arriving at the same general conclusion.. the experiments are run on two image classification datasets, the cifar10 dataset (krizhevsky hinton, 2009) and the imagenet (russakovsky et al., 2015) ilsvrc dataset. we test the inception v3 (szegedy et al., 2015) architecture on imagenet and a smaller version of inception, alexnet (krizhevsky et al., 2012), and mlps on cifar10. please see section a in the appendix for details of the experimental setup. we run our experiments with the following modifications of the labels and input images:. true labels: the original dataset without modification. partially corrupted labels: independently with probability p, the label of each image is. corrupted as a uniform random class. random labels: all the labels are replaced with random ones. shuffled pixels: a random permutation of the pixels is chosen and then the same permuta. tion is applied to all the images in both training and test set. random pixels: a different random permutation is applied to each image independently. gaussian: a gaussian distribution (with matching mean and variance) is used to generate. random pixels for each image.. surprisingly, stochastic gradient descent with unchanged hyperparameter settings can optimize the weights to fit to random labels perfectly, even though the random labels completely destroy the relationship between images and labels. we further break the structure of the images by shuffling the image pixels, and even completely resampling random pixels from a gaussian distribution. but the networks we tested are still able to fit.. figure 1a shows the learning curves of the inception model on the cifar10 dataset under various settings. we expect the objective function to take longer to start decreasing on random labels because initially the label assignments for every training sample is uncorrelated. therefore, large predictions errors are backpropagated to make large gradients for parameter updates. however, since the random labels are fixed and consistent across epochs, the network starts fitting after going through the training set multiple times. we find the following observations for fitting random labels very interesting: a) we do not need to change the learning rate schedule; b) once the fitting starts, it converges quickly; c) it converges to (over)fit the training set perfectly. also note that random pixels and gaussian start converging faster than random labels. this might be because with random pixels, the inputs are more separated from each other than natural images that originally belong to the same category, therefore, easier to build a network for arbitrary label assignments.. on the cifar10 dataset, alexnet and mlps all converge to zero loss on the training set. the shaded rows in table show the exact numbers and experimental setup. we also tested random labels on the imagenet dataset. as shown in the last three rows of table in the appendix, although it does not reach the perfect top1 accuracy, accuracy is still very surprising for a million random labels from categories. note that we did not do any parameter tuning when switching from the true labels to random labels. it is likely that with some modification of the parameters, perfect accuracy could be achieved on random labels. the network also manages to reach top1 accuracy even with explicit regularizers turned on.. partially corrupted labels we further inspect the behavior of neural network training with a varying level of label corruptions from (no corruption) to (complete random labels) on the cifar10 dataset. the networks fit the corrupted training set perfectly for all the cases. figure 1b shows the slowdown of the convergence time with increasing level of label noises. figure 1c depicts the test errors after convergence. since the training errors are always zero, the test errors are the same as generalization errors. as the noise level approaches 1, the generalization errors converge to the performance of random guessing on cifar10. in light of our randomization experiments, we discuss how our findings rule out several traditional approaches to generalization.. rademacher complexity and vcdimension. rademacher complexity is commonly used and flexible complexity measure of a hypothesis class. the empirical rademacher complexity of a hypothesis classh on a dataset x1, uniform random variables. this closely resemble our randomization test. specifically, rn(h) measures on averageh\u2019s capability of fitting all possible1 binary label assignments. generalization guarantees based on rademacher complexity directly characterize the convergence rate by o(rn(h)). however, since the randomization tests suggests the neural networks fit the training set with random labels perfectly, we get rn(h) 1, which is trivial for an upper bound of loss in binary classification. therefore, uniform convergence bounds based on rademacher complexity are not strong enough to characterize the success of deep learning in realistic settings. a similar reasoning rules out vcdimension. deep neural networks with more than n parameters have vcdimension greater than n, which leads to trivial uniform convergence bounds. for neural networks, there is also no direct analog of generalization through margin theory as has been successful for large kernel machines.. stepping away from complexity measures of the hypothesis class, we can instead consider properties of the algorithm used for training. this is commonly done with some notion of stability, such as uniform stability (mukherjee et al., 2002; bousquet elisseeff, 2002; poggio et al., 2004). uniform stability of an algorithm a measures how sensitive the algorithm is to the replacement of a single example. however, it is solely a property of the algorithm, which does not take into account specifics of the data or the distribution of the labels. it is possible to define weaker notions of stability (shalevshwartz et al., 2010). the weakest such stability measure is directly equivalent to bounding generalization error and does take the data into account. however, it has been difficult to utilize this weaker stability notion effectively. most of our randomization tests are performed with explicit regularization turned off. regularizers are the standard tool in theory and practice to mitigate overfitting in the regime when there are more parameters than data points (vapnik, 1998). the basic idea is that although the original hypothesis is too large to generalize well, regularizers help confine learning to a subset of the hypothesis space with manageable complexity. by adding an explicit regularizer, say by penalizing the norm of the optimal solution, the effective rademacher complexity of the possible solutions is dramatically reduced.. as we will see, in deep learning, explicit regularization seems to play a rather different role. as the bottom rows of table in the appendix show, even with dropout and weight decay, inceptionv3 is still able to fit the random training set extremely well if not perfectly. although not shown explicitly, on cifar10, both inception and mlps still fit perfectly the random training set with weight decay turned on. however, alexnet with weight decay turned on fails to converge on random labels. investigate the role of regularization in deep learning, we explicitly compare behavior of deep nets learning with and without regularizers.. instead of doing a full survey of all kinds of regularization techniques introduced for deep learning, we simply take several commonly used network architectures, and compare the behavior when the turning off the equipped regularizers. the following regularizers are covered:. data augmentation: augment the training set via domainspecific transformations. for image data, commonly used transformations include random cropping, random perturbation of brightness, saturation, hue and contrast.. weight decay: equivalent to a regularizer on the weights; also equivalent to a hard constrain of the weights to an euclidean ball, with the radius decided by the amount of weight decay.. dropout (srivastava et al., 2014): mask out each element of a layer output randomly with a given dropout probability. only the inception v3 for imagenet uses dropout in our experiments.. table shows the results of inception, alexnet and mlps on cifar10, toggling the use of data augmentation and weight decay. both regularization techniques help to improve the generalization performance, but even with all of the regularizers turned off, all of the models still generalize very well.. table in the appendix shows a similar experiment on the imagenet dataset. a top1 accuracy drop is observed when we turn off all the regularizers. specifically, the top1 accuracy without regularization is 59.80, while random guessing only achieves top1 accuracy on imagenet. more strikingly, with dataaugmentation on but explicit regularizers off, inception is able to achieve a top1 accuracy of indeed, it seems like the ability to augment the data using known symmetries is significantly more powerful than just tuning weight decay or preventing low training error.. inception achieves top5 accuracy without regularization, while the reported number of the winner of ilsvrc (krizhevsky et al., 2012) achieved so while regularization is important, bigger gains can be achieved by simply changing the model architecture. it is difficult to say that the regularizers count as a fundamental phase change in the generalization capability of deep nets. early stopping was shown to implicitly regularize on some convex learning problems (yao et al., 2007; lin et al., 2016). in table in the appendix, we show in parentheses the best test accuracy along the training process. it confirms that early stopping could potentially1 improve the generalization performance. figure 2a shows the training and testing accuracy on imagenet. the shaded area indicate the accumulative best test accuracy, as a reference of potential performance gain for early stopping. however, on the cifar10 dataset, we do not observe any potential benefit of early stopping.. batch normalization (ioffe szegedy, 2015) is an operator that normalizes the layer responses within each minibatch. it has been widely adopted in many modern neural network architectures such as inception (szegedy et al., 2015) and residual networks (he et al., 2016). although not explicitly designed for regularization, batch normalization is usually found to improve the generalization performance. the inception architecture uses a lot of batch normalization layers. to test the impact of batch normalization, we create a inception w/o batchnorm architecture that is exactly the same as inception in figure 3, except with all the batch normalization layers removed. figure 2b compares the learning curves of the two variants of inception on cifar10, with all the explicit regularizers turned off. the normalization operator helps stablize the learning dynamics, but the impact on the generalization performance is only the exact accuracy is also listed in the section inception w/o batchnorm of table in summary, our observations on both explicit and implicit regularizers are consistently suggesting that regularizers, when properly tuned, could help to improve the generalization performance. however, it is unlikely that the regularizers are the fundamental reason for generalization, as the networks continue to perform well after all the regularizers removed.. 1we say potentially because to make this statement rigorous, we need to have another isolated test set and test the performance there when we choose early stopping point on the first test set (acting like a validation set). much effort has gone into characterizing the expressivity of neural networks, e.g, cybenko (1989); mhaskar (1993); delalleau bengio (2011); mhaskar poggio (2016); eldan shamir (2016); telgarsky (2016); cohen shashua (2016). almost all of these results are at the population level showing what functions of the entire domain can and cannot be represented by certain classes of neural networks with the same number of parameters. for example, it is known that at the population level depth k is generically more powerful than depth k we argue that what is more relevant in practice is the expressive power of neural networks on a finite sample of size n. it is possible to transfer population level results to finite sample results using uniform convergence theorems. however, such uniform convergence bounds would require the sample size to be polynomially large in the dimension of the input and exponential in the depth of the network, posing a clearly unrealistic requirement in practice.. we instead directly analyze the finitesample expressivity of neural networks, noting that this dramatically simplifies the picture. specifically, as soon as the number of parameters p of a networks is greater than n, even simple twolayer neural networks can represent any function of the input sample. we say that a neural network c can represent any function of a sample of size n in d dimensions if for every sample s rd with s n and every function f : s r, there exists a setting of the weights of c such that c(x) f(x) for every x s. theorem there exists a twolayer neural network with relu activations and 2nd weights that can represent any function on a sample of size n in d dimensions.. the proof is given in section c in the appendix, where we also discuss how to achieve widtho(n/k) with depth k. we remark that it\u2019s a simple exercise to give bounds on the weights of the coefficient vectors in our construction. lemma gives a bound on the smallest eigenvalue of the matrix a. this can be used to give reasonable bounds on the weight of the solution w. although deep neural nets remain mysterious for many reasons, we note in this section that it is not necessarily easy to understand the source of generalization for linear models either. indeed, it is useful to appeal to the simple case of linear models to see if there are parallel insights that can help us better understand neural networks.. suppose we collect n distinct data points (xi, yi) where xi are ddimensional feature vectors and yi are labels. letting loss denote a nonnegative loss function with loss(y, y) 0, consider the empirical risk minimization (erm) problem. if d n, then we can fit any labeling. but is it then possible to generalize with such a rich model class and no explicit regularization?. let x denote the n d data matrix whose ith row is xti if x has rank n, then the system of equations xw y has an infinite number of solutions regardless of the right hand side. we can find a global minimum in the erm problem (2) by simply solving this linear system.. but do all global minima generalize equally well? is there a way to determine when one global minimum will generalize whereas another will not? one popular way to understand quality of minima is the curvature of the loss function at the solution. but in the linear case, the curvature of all optimal solutions is the same (choromanska et al., 2015). to see this, note that in the case when yi is a scalar,. 1n n i1 loss(w txi, yi) nx t diag()x,. a similar formula can be found when y is vector valued. in particular, the hessian is not a function of the choice of w. moreover, the hessian is degenerate at all global optimal solutions.. if curvature doesn\u2019t distinguish global minima, what does? a promising direction is to consider the workhorse algorithm, stochastic gradient descent (sgd), and inspect which solution sgd converges. since the sgd update takes the form wt1 wt tetxit where t is the step size and et is the prediction error loss. if w0 0, we must have that the solution has the form w n i1 ixi for some coefficients hence, if we run sgd we have that w xt lies in the span of the data points. if we also perfectly interpolate the labels we have xw y. enforcing both of these identities, this reduces to the single equation. which has a unique solution. note that this equation only depends on the dotproducts between the data points xi. we have thus derived the kernel trick (scholkopf et al., 2001)albeit in a roundabout fashion.. we can therefore perfectly fit any set of labels by forming the gram matrix (aka the kernel matrix) on the data k xxt and solving the linear system k y for this is an n n linear system that can be solved on standard workstations whenever n is less than a hundred thousand, as is the case for small benchmarks like cifar10 and mnist.. quite surprisingly, fitting the training labels exactly yields excellent performance for convex models. on mnist with no preprocessing, we are able to achieve a test error of by simply solving (3). note that this is not exactly simple as the kernel matrix requires 30gb to store in memory. nonetheless, this system can be solved in under minutes in on a commodity workstation with cores and gb of ram with a conventional lapack call. by first applying a gabor wavelet transform to the data and then solving (3), the error on mnist drops to surprisingly, adding regularization does not improve either model\u2019s performance!. similar results follow for cifar10. simply applying a gaussian kernel on pixels and using no regularization achieves test error. by preprocessing with a random convolutional neural net with 32,000 random filters, this test error drops to error2. adding regularization further reduces this number to error. note that this is without any data augmentation.. note that this kernel solution has an appealing interpretation in terms of implicit regularization. simple algebra reveals that it is equivalent to the minimum 2norm solution of xw y. that is, out of all models that exactly fit the data, sgd will often converge to the solution with minimum norm. it is very easy to construct solutions ofxw y that don\u2019t generalize: for example, one could fit a gaussian kernel to data and place the centers at random points. another simple example would be to force the data to fit random labels on the test data. in both cases, the norm of the solution is significantly larger than the minimum norm solution.. unfortunately, this notion of minimum norm is not predictive of generalization performance. for example, returning to the mnist example, the 2norm of the minimum norm solution with no preprocessing is approximately with wavelet preprocessing, the norm jumps to yet the test error drops by a factor of so while this minimumnorm intuition may provide some guidance to new algorithm design, it is only a very small piece of the generalization story. the classical view of machine learning rests on the idea of parsimony. in almost any formulation, learning boils down to extracting lowcomplexity patterns from data. bruteforce memorization is typically not thought of as an effective form of learning. at the same time, it\u2019s possible that sheer memorization can in part be an effective problemsolving strategy for natural tasks.. our results challenge the classical view of learning by showing that many successful neural networks easily have the effective capacity for sheer memorization. this leads us to believe that these models may very well make use of massive memorization when tackling the problems they are trained to solve. it is likely that learning in the traditional sense still occurs in part, but it appears to be deeply intertwined with massive memorization. classical approaches are therefore poorly suited for reasoning about why these models generalize well.. we believe that understanding neural networks requires rethinking generalization. we hope that our paper provides a first stepping stone by problematizing the classical view and pointing towards unresolved conundrums.. 2this convnet is the coates ng (2012) net, but with the filters selected at random instead of with kmeans. we focus on two image classification datasets, the cifar10 dataset (krizhevsky hinton, 2009) and the imagenet (russakovsky et al., 2015) ilsvrc dataset.. the cifar10 dataset contains 50,000 training and 10,000 validation images, split into classes. each image is of size 32x32, with color channels. we divide the pixel values by to scale them into [0, 1], crop from the center to get 28x28 inputs, and then normalize them by subtracting the mean and dividing the adjusted standard deviation independently for each image with the perimagewhitening function in tensorflow (abadi et al., 2015).. for the experiment on cifar10, we test a simplified inception (szegedy et al., 2015) and alexnet (krizhevsky et al., 2012) by adapting the architectures to smaller input image sizes. we also test standard multilayer perceptrons (mlps) with various number of hidden layers.. the small inception model uses a combination of 1x1 and 3x3 convolution pathways. the detailed architecture is illustrated in figure the small alexnet is constructed by two (convolution 5x5 maxpool 3x3 localresponsenormalization) modules followed by two fully connected layers with and hidden units, respectively. finally a 10way linear layer is used for prediction. the mlps use fully connected layers. mlp 1x512 means one hidden layer with hidden units. all of the architectures use standard rectified linear activation functions (relu).. for all experiments on cifar10, we train with sgd with a momentum parameter of an initial learning rate of (for small inception) or (for small alexnet and mlps) are used, with a decay factor of per training epoch. unless otherwise specified, for the experiments with randomized labels or pixels, we train the networks without weight decay, dropout, or other forms of explicit regularization. section discusses the effects of various regularizers on fitting the networks and generalization.. the imagenet dataset contains 1,281,167 training and 50,000 validation images, split into classes. each image is resized to 299x299 with color channels. in the experiment on imagenet, we use the inception v3 (szegedy et al., 2015) architecture and reuse the data preprocessing and experimental setup from the tensorflow package. the data pipeline is extended to allow disabling of data augmentation and feeding random labels that are consistent across epochs. we run the imagenet experiment in a distributed asynchronized sgd system with workers. table shows the performance on imagenet with true labels and random labels, respectively. for any two interleaving sequences of n real numbers b1 x1 b2 x2 bn xn, the nn matrix a [maxxi bj , 0]ij has full rank. its smallest eigenvalue is mini xi bi.. by its definition, the matrix a is lower triangular, that is, all entries with i j vanish. a basic linear algebra fact states that a lowertriangular matrix has full rank if and only if all of the entries on the diagional are nonzero. since, xi bi, we have that maxxi bi, the second claim follows directly from the fact that a lowertriangular matrix has all its eigenvalues on the main diagonal. this in turn follows from the first fact, since a i can have lower rank only if equals one of the diagonal values.. for weight vectors w, b rn and a rd, consider the function c : rn r,. it is easy to see that c can be expressed by a depth network with relu activations.. now, fix a sample s z1, , zn of size n and a target vector y rn. to prove the theorem, we need to find weights a, b, w so that yi c(zi) for all i 1, , n first, choose a and b such that with xi a, zi we have the interleaving property b1 x1 b2 bn xn. this is possible since all zi\u2019s are distinct. next, consider the set of n equations in the n unknowns w,. , n we have c(zi) aw, where a [maxxi bi, 0]ij is the matrix we encountered in lemma we chose a and b so that the lemma applies and hence a has full rank. we can now solve the linear system y aw to find suitable weights w.. while the construction in the previous proof has inevitably high width given that the depth is 2, it is possible to trade width for depth. the construction is as follows. with the notation from the proof and assuming w.l.o.g. , xn [0, 1], partition the interval [0, 1] into b disjoint intervals i1, , ib so that each interval ij contains n/b points. at layer j, apply the construction from the proof to all points in ij this requires o(n/b) nodes at level j. this construction results in a circuit of width o(n/b) and depth b which so far has b outputs (one from each layer). it remains to implement a multiplexer which selects one of the b outputs based on which interval a given input x falls into. this boils down to implementing one (approximate) indicator function fj for each interval ij and outputting b j1 fj(x)oj , where oj is the output of layer j. this results in a single output circuit. implementing a single indicator function requires constant size and depth with relu activiations. hence, the final size of the construction iso(n) and the depth is bc for some constant c. setting k b c gives the next corollary. for every k 2, there exists neural network with relu activations of depth k, width o(n/k) and o(n d) weights that can represent any function on a sample of size n in d dimensions. table list the experiment results of linear models described in section", "summary": "the authors investigate the generalisation properties of several wellknown image recognition networks.. they show that these networks are able to overfit to the training set with accuracy even if the labels on the images are random, or if the pixels are randomly generated. regularisation, such as weight decay and dropout, doesn\u2019t stop overfitting as much as expected, still resulting in accuracy on random training data. they then argue that these models likely make use of massive memorization, in combination with learning lowcomplexity patterns, in order to perform well on these tasks."}, {"document": "several important security issues of deep neural network (dnn) have been raised recently associated with different applications and components. the most widely investigated security concern of dnn is from its malicious input, a.k.a adversarial example. nevertheless, the security challenge of dnn\u2019s parameters is not well explored yet. in this work, we are the first to propose a novel dnn weight attack methodology called bitflip attack (bfa) which can crush a neural network through maliciously flipping extremely small amount of bits within its weight storage memory system (i.e., dram). the bitflip operations could be conducted through wellknown rowhammer attack, while our main contribution is to develop an algorithm to identify the most vulnerable bits of dnn weight parameters (stored in memory as binary bits), that could maximize the accuracy degradation with a minimum number of bitflips. our proposed bfa utilizes a progressive bit search (pbs) method which combines gradient ranking and progressive search to identify the most vulnerable bit to be flipped. with the aid of pbs, we can successfully attack a resnet18 fully malfunction (i.e., top1 accuracy degrade from to 0.1) only through bitflips out of million bits, while randomly flipping bits merely degrades the accuracy by less than recently, deep neural networks (dnns) have demonstrated its great potential of surpassing or close to humanlevel performance in multiple domains, such as object recognition [1], game ai [2], synthetic voice [3], neighborhood voting prediction [4] and etc [5]. it stimulates the demand for deploying the stateoftheart deep learning algorithms in realworld applications to release labors from repetitive work. under such circumstance, the security and robustness of deep neural network is an essential concern which cannot be circumvented.. adversarial example [6] (aka., adversarial attack) is a wellknown security issue of dnn, which can cause the system malfunction with the magnitudeconstrained input equally contributed and cofirst author corresponding author: dfanucf.edu. .c v. ] noise that mankind cannot discern. both attack and defense of adversarial example on the input end of dnn has been heavily investigated in the past couple of years [7, 6, 8] and still be in progress [9, 10, 11]. nevertheless, the security issue of network parameters themselves is not yet well explored. recently, the development of fault injection attack [12] has raised further security concerns on the storage of dnn parameters.. the possible reasons that there was a lack of concerns on the security of network parameters may come in twofold: 1) the neural network is widely recognized as a robust system against parameter variations. 2) the dnns are used to be only deployed on the highperformance computing system (e.g., cpus, gpus, and other accelerators [13, 14]), which normally contains a variety of methods ensuring data integrity. thus, attacking the parameters is more related to a system cybersecurity topic. however, the game has been totally changed during the past few years. first, the robustness of neural network to small perturbation has been put into the spotlight by adversarial examples on dnn input [6, 7]. second, with the aid of dnn compression techniques (e.g., pruning[15] and quantization [16]) and outstanding compact neural network architectures [17, 18], deep neural networks now are friendly to the resourcelimited mobile device as well. such resourcelimited platforms normally lack effective data integrity check mechanism, which makes the deployed dnn vulnerable to popular fault injection techniques, such as row hammer and laser beam [19].. recently, there exist a cohort of works [12, 20] in an attempt to attack dnn network parameters stored in dram using row hammer attack (rha). however, the key limitation to these previous attack methods is that they primarily focused on extremely vulnerable fullprecision dnn model (i.e., parameters in floatingpoint format). our conducted simulation shows that randomly flipping the exponent part of floatingpoint weight could easily overwhelm the functionality of dnn. the explanation behind that is flipping the bits in exponent part of floatingpoint value can increase the weight to extremely large value, thus leading to the exploded output. as a result, attacking the weight constrained dnn (i.e., weights quantized into fixedpoint values) is the primary focus in this work, where the range of weight magnitude relies on the bitwidth of weights.. overview of bitflip attack: in this work, we attempt to perform parameter attack on the weights of quantized dnn, whose weight magnitude is intrinsically constrained owing to the fixedpoint representation. in order to conduct an efficient bitflip attack on weights, for the first time, we propose a bitflip attack (bfa) together with progressive bit search (pbs) technique, that can totally crush a fully functional quantized dnn and convert it to a random output generator with several bitflips. our proposed pbs combines gradient ranking and progressive search to locate the most vulnerable bits, while bfa performs the bitflip operations on the located bits along their gradient ascending directions. in order to identify the vulnerable bits to be flipped within the identical layer and across different layers, we perform the inlayer search and crosslayer search in an iterative way. thus, for each bfa iteration, only the most vulnerable bit elected by the pbs technique will be flip to its opposite binary value. the extensive experiments are conducted regarding various network structure, different datasets and quantization bitwidth, and etc. it is shocking to notice that resnet18 will become. a random output generator (i.e., top1 accuracy) with only bitflips out of million bits by our proposed attacking method, on imagenet dataset. memory bitflip in realworld: flipping a memory cell bit within memory system is a realistic and demonstrated threat model in existing computer systems. recently, kim et al., [21] have demonstrated a method to cause memory bitflip in dram merely through the frequent data accessing, which is now popularly known as rowhammer attack (rha). a malicious user can use rha to modify the data stored in dram memory cell by just flipping one bit at a time. [22] showed that by creating a profile for the bit flips in a dram, row hammer attack can effectively flip a single bit at any address in the software stack. according to the stateoftheart investigations, common error detection and correction techniques, such as errorcorrecting code (ecc) [23] and intel sgx [24], are broken defense mechanism to rha. such existing memory bitflip attack (i.e. rowhammer attack) model brings a huge challenge to the security of dnn powered computing system since its parameters are normally stored in the main memory, i.e. dram, for maximizing the computation throughput, which is directly exposed to the adversarial attacker. moreover, such challenge becomes more severe considering the fact that dnn powered applications are widely deployed in many resourcelimited (e.g. smart iot devices, mobile system, edge devices, etc.) system that lacks necessary data integrity check mechanism.. previous neural network parameter attack. adversarial example attack has been widely explored [25] to evaluate the robustness of dnn. however, we are still at the rudimentary stage towards investigating the effect of network parameter attack on neural network accuracy. neural network parameters have been attacked using different levels of hardware trojans, which require a specific pattern of input to trigger the trojan inside the network [26]. moreover, such trojan attack requires hardware level modifications, which may not be feasible in many practical applications. as a result, fault injection attacks could become a suitable alternative to attack dnn parameters [12]. for example, single bias attack (sba) attacks a certain bias term of a neuron to change the classification of dnn to a different class [12]. other works have injected faults into the activation function of the neural network to miss classify a target input [20].. limitations of previous works. however, these previous attack algorithms are developed based on a fullprecision model (i.e. network parameters are floatingpoint numbers stored in memory in the format of ieee standard for floatingpoint arithmetic [27]), where we believe such attack algorithms may not be efficient. since it is extremely easy to cause dnn malfunction by just flipping the most significant exponent bits of any random floatingpoint weight parameters. through this simple method, it mainly causes dnn malfunction by exponentially increasing the magnitude of particular weight parameters by just several bitflips. we conducted such experiment to prove its efficiency in section based on our simulation results, it shows just bitflip. of the most significant exponent bit of a random floatingpoint number weight could cause resnet18 network totally malfunction on imagenet dataset.. why we need a bit search algorithm. on the other side, most of recent deep neural network applications are performed in quantized platform such as google\u2019s tensor processing unit (tpu) [28], that uses 8bit operations for quantized network. such fixed precision models are more robust to network parameter perturbation. similarly, we conducted another experiment to randomly choose quantized weight for bitflip attack using rha. the simulation results in figure show that bitflip in a quantized resnet18 could only cause accuracy degradation in imagenet, which clearly indicates that random selection of quantized weight parameters to be attacked is not efficient and feasible. thus, an efficient algorithm is required to search for the most vulnerable weights/bits in a quantized dnn. in this section, we present a novel bitflip attack (bfa) method to maliciously cause a dnn system malfunction through flipping extremely small amount of vulnerable bits of weights. our proposed algorithm, called progressive bit search (pbs), is to identify those vulnerable dnn weight parameters (stored in terms of memory bits in dram) that could maximize the accuracy degradation with minimum number of bitflips. it is worth to note that this work focuses on bfa on a more robust dnn with quantized weight parameters instead of floatingpoint number weights as discussed earlier. given a quantized dnn contains l convolutional/fullyconnected layers, the original weights in floatingpoint are symmetrically quantized into 2nq levels with nqbits uniform quantizer. the quantized weights w are arithmetically represented in nqbits signed integer. in the computing memory system, w is stored in the format of twos complement1, which is denoted as b in this work. more details of weights quantization are described in section the goal of this work is to find the optimal combination of vulnerable weight bits to perform bfa, thus maximizing the inference loss of dnn parameterized by the perturbed weights whose twos complement representation is b. such vulnerable bit searching problem can be formulated as an optimization problem as:. 1all the binary weight mentioned hereinafter referred to as the weights in twos complement.. where x and t are the vectorized input and target output2. taken x as the input, the inference computation of network parameterized by blll1 is expressed as f(x; blll1). note that l(, ) calculates the loss between dnn output and target. d(bl,bl) computes the hamming distance between clean and perturbedbinary weight tensor, and nb is maximum hamming distance allowed through the entire dnn. in this work, we adopt a layerwise nqbits uniform quantizer for weight quantization. for lth layer, the quantization process from the floatingpoint base wfpl to its fixedpoint (signed integer) counterpart wl can be described as:. where d is the dimension of weight tensor, wl is the step size of weight quantizer. for training the quantized dnn with nondifferential staircase function (in eq. (3)), we use the straightthrough estimator [29] as other works [16]. note that, since wl r is the coefficient shared by all the weights in lth layer, we only store its fixedpoint part (wl/wl) 2nq1, ..., 2nq1d, rather than wl.. the computing system normally stores the signed integer in two\u2019s complement representation, owing to its efficiency in arithmetic operations (e.g., mul). given one weight element w wl, the conversion from its binary representation (b [bnq1, ..., b0] 0, 1nq ) in two\u2019s complement can be expressed as:. with the conversion relation described by g() in eq. (4), we can inversely obtain the binary representation of weights b from its fixedpoint counterpart as well. in this work, we perform the bfa utilizing the similar mechanism as fgsm [6], which was used to generate adversarial example. the key idea of bfa is to flip the bits along its gradient ascending direction w.r.t the loss of dnn. we take the binary vector b in eq. (4) as an example and attempt to perform bfa upon b. we first calculates the gradients of b w.r.t loss as:. 2note that, all the targets t in this work are not the groundtruth labels, but the outputs of the clean dnn w.r.t the input data.. where l is the inference loss of dnn parametrized by b. the naive operation is to directly perform the bitflip using the gradients obtained in eq. (5) and get perturbed bits as:. however, since the bit value is constrained between and (b 0, 1nq ), flipping the bit as eq. (6) could lead to data overflow. ideally, the bfa is supposed to follow the truth table in table thus, we mathematically redefine the bfa as follows:. where is the bitwise xor operator. m is the mask which indicates whether to perform the bitflip operation. rather than performing the bfa upon each bit throughout the entire network, our goal is to perform bfa in a more precise and effective fashion. in this subsection, we propose a method called progressive bit search (pbs) which combines the gradient ranking and progressive search. the proposed pbs method attempts to identify and flip nb most vulnerable bits per bfa iteration (nb by default), thus progressively degrading the performance of dnn until it reaches the minimum accuracy or the preset number of iteration. as the flowchart of performing pbs depicted in fig. 1, for each attack iteration, the process of bit searching can be generally divided into two successive steps: 1) inlayer search: the inlayer search is performed through electing the nb most vulnerable bits in the selected layer, then record the inference loss if those elected bits are flipped. 2) crosslayer search: with the inlayer search conducted upon each layer of the network independently, the crosslayer search is to evaluate the recorded loss increment caused by bfa with inlayer search, thus identify the top nb vulnerable bits across different layers. the details of each step are described as follows.. convolution layer linear layer scheme iifine modecoarse mode offchip dram onchip sram neural network engine pes fine modecoarse mode offchip dram onchip sram neural network engine pes iot node system. neural network engine pes iot node system. scheme i data communication fine mode inference flow coarse mode inference flow onchip params. for the pbs in kth iteration, inlayer searching of the nb most vulnerable bits from b k. l in lth layer is performed through gradient ranking. with the given vectored input x and target t, the inference and backpropagation are performed successively to calculate the gradients of bits w.r.t the inference loss. then, we descendingly rank the vulnerability of bits by the absolute value of their gradients l/b and elect the bits whose gradients are topnb, such process can be written as:. bk1l top nb bk1l l(f(x; bk1l ll1), t) (9). where topnb function returns the pointer pointing at the storage of those elected nb vulnerable bits. then, we apply the bfa on those elected bits as:. where the mask m is generated following eq. now, with the inlayer search and bfa performed on the lth layer, we have to evaluate the loss increment caused by bfa in eq. where the only difference between b k l ll1 and b k1 l ll1 are the bits flipped in eq. note that, those bits flipped to bkl in eq. (10) will be restored back to b k1 l after the loss evaluation is finished.. crosslayer search. as the aforementioned inlayer search can perform the layerwise vulnerable bits election and bfa evaluation, the crosslayer search evaluates the bfa across the entire network. for the pbs in kth iteration, the crosslayer search first independently conduct the inlayer search on each layer, and generate the loss set as lk1 ,lk2 , ,lkl. then, we could identify the layerj with maximum loss and. reperform the bfa (without restore) on the bits elected in jth layer, which can be expressed as:. after that, pbs is entered into k iteration. datasets: we take two visual datasets: cifar10 [30] and imagenet [31] for object classification task. cifar10 contains 60k rgb images in size of following the standard practice, 50k examples are used for training and the remaining 10k for testing. the images are drawn evenly from classes. imagenet dataset contains 1.2m training images divided into distinct classes. the data augmentation used in this work is identical to methods in [32]. note that, the proposed bfa is performed through randomly draw a sample of input images from the test/validation set, where the default sample size is and for cifar10 and imagenet respectively. then, only the sample input is used to perform bfa, where the rest data and groundtruth labels are isolated from the attacker. moreover, each experimental configuration is run with trials to alleviate error caused by the randomness of sampling input.. network architectures and quantization: for cifar10, experiments are conducted on series of residual network (resnet20/32/44/56)[32], where the weights are quantized into 4/6/8 bitwidth with retraining. for imagenet, we choose a variety of famous network structures, including alexnet, resnet18/34/50. based on our observation, with high bitwidth quantizer (e.g., nq8), directly quantizing the pretrained fullprecision dnn without retraining (i.e., finetuning) only shows negligible accuracy degradation. therefore, for fast evaluation of our proposed bfa on imagenet. dataset and its various network structures, we directly perform the weight quantization without retraining before conducting the bfa.. attack formulation: traditional attack mostly focuses on attacking dnn by feeding perturbed inputs [6] to the network. such adversarial attack can be grouped into two major categories: 1) whitebox attack [6, 7], where the adversary has full access to the network architecture and parameters, and 2) blackbox attack [33, 34], where the adversary can only access the input and output of a dnn without its internal configurations. for our proposed bfa, it demands the full access to the dnn\u2019s weights and gradients. thus bfa can be considered as a white box attack. however, we assume that even under white box attack setup, the attacker has no access to the training dataset, training algorithm and hyper parameters used during the training of network. our bitflip attack is evaluated across different architectures (i.e., resnet20/32/44/56) using varying quantized bitwidths (i.e., nq4/6/8) on cifar10 dataset in table without bfa, the quantized models show negligible accuracy degradation or even higher accuracy in comparison to their fullprecision counterpart. the quantization noise introduced by the weight quantization is considered as a regularization method, which might contribute the accuracy improvement when model training is overfitting.. since cifar10 dataset has different classes of object, degrading the model\u2019s accuracy down to is equivalent to make the model as random output generator. in contrast to adversarial example (e.g., pgd attack [7]), our proposed bfa is unable to degrade the network accuracy to the reason is adversarial example is an inputspecific attack which is designed to misclassify each input separately, while our proposed bfa attempts to misclassify the images from each object category using the identical attacked model. consequently, the successful bfa would be making the dnn to generate output randomly. therefore, we report the number of bitflips nflip required to cause the dnn\u2019s test accuracy to go below as the measurable indicator of bfa performance, for cifar10 dataset.. as the experimental result listed in table 2, for all the resnet architecture with varying quantization bitwidth, the required number of bitflips nflip to make the dnn malfunction is most likely below besides nflip, we take the hamming distance db between clean and perturbedmodel as another measurable indicator. the intuition behind is our proposed bfa attempts to flip the selected bits without considering its original status. thus, it exists the probability that some of the bits might be flipped repeatedly with even times. however, the reality is that such back and forth bitflips rarely happen throughout all the experiments. under varying quantization configurations, there is no obvious relation between the quantization bitwidth and the required number of bitflips (i.e., robustness of dnn against bfa). the summary of evaluation of our attack on imagenet dataset is presented in table we report both baseline and 8bit quantized network accuracy for four popular image. classification architectures on imagenet. we observe roughly reduction in top1 classification accuracy after quantizing the network\u2019s weights to 8bits. since imagenet dataset has different classes of objects, a classification accuracy of can be considered as random output. thus reporting only the number of bit flips nflip required to cause the accuracy to degrade to below would be sufficient to prove the attack\u2019s effectiveness.. for imagenet, bfa with pbs attack requires only (median of trials ) bit flips out of million bits to crush alexnet. however, nflip decreases even more as we perform the attack on resnet architectures. figure shows accuracy degradation for resnet models, which has a much steeper slope than alexnet. as alexnet does not have residual connections, which may result in different response to such gradient based attacks. for resnet networks, as the network parameters keep increasing, it requires lesser number of nflip to attack the network. finally, our attack makes a resnet50 architecture dysfunctional by flipping out of million bits only. the attack achieves such success by modifying roughly of the bits to destroy the fully functional dnn. thus the gravity of dnn parameter\u2019s security concern can be summarized as two identical models with 50m similar weights but only a error in the parameters can generate totally different output values causing a degradation in test accuracy. pbs with various sample size. in our experiment, we randomly sample a set of input images from the test/validation subset to perform the bfa, which we define it as attack sample. then, we evaluate the effectiveness of the attack on the whole test data set which works as a validation. we opted to perform the validation on the whole test. dataset including the random batch that was originally selected for the attack because the sample size is too small compared to the whole test dataset for both imagenet and cifar10. in this section, we perform an ablation study on the attack sample size. in figure 2, we configure the sample size from and plotted top1 validation accuracy, top5 validation accuracy, sample loss and validation loss respectively.. the performance of the attack based on attack sample size can be ranked as: s(128) s(32) s(256) s(64) s(16). even though the effect of sample size does not hinder the attack strength much but with a sample size of 128, our attack requires the fewest bit flips to reach on the other hand, with a sample size of 16, the attack strength slightly degrades. our observation encourages not to select a too large or too small attack sample size. one probable explanation would be if we compute the gradient with respect to large samples, then the attack might fail to properly maximize the loss with respect to every sample. again, if the sample size is too small then the sample loss may not be representative of the whole test data set.. pbs versus random bitflips. in this section, we perform an ablation study on randomly flipping any bits of a random weight in the network. first, we test random bit flip on a fullprecision weight(i.e, floating point) on resnet18 model. point weights represented in standard ieee format, if we change the most significant bits of the exponent section, then the floatingpoint weight value would change by huge amount. as a result, the trained resnet18 network starts malfunctioning even after just one random bit flip.. then, we implement the random bit flip on 8bit quantized resnet18 architecture as shown in figure it shows that by flipping even random bits, the top1 accuracy on imagenet dataset does not degrade more than it demonstrates the need for an efficient bit search algorithm to identify the most vulnerable bits as randomly flipping any bit does not hamper neural network too much. in comparison, our attack algorithm requires just bits out of 93m for resnet18 to totally cause the network to malfunction on imagenet dataset. progressive bit search is the very first attack bit searching algorithm developed to malfunction a quantized neural network through perturbation of stored model parameters using row hammer attack. we already showed in previous section that the previous. attack algorithms [12, 20] on floatingpoint model parameters are not efficient. they do not consider that attacking floating point dnn model is as easy as flipping most significant exponent bits of any random weights. our developed bfa with pbs is the first work that puts emphasis on the need for developing attack algorithms to properly scrutinize the security of dnn model parameters. our attack can crush a dnn model to demonstrate dnn\u2019s vulnerability to intentional malicious bit flips. further, our algorithm would encourage more future work on both attack and defense front in an attempt to make neural network more resilient and robust. why only a few bit flips can cause such destructive phenomena? in the analysis of the existence of adversary in deep neural network, goodfellow et al. [6] concluded that deep neural networks exhibit vulnerability to adversarial examples due to their extreme linearity. the linearity of these models is the reason why they cannot resist adversary. the theory suggests that, with sufficient large input dimension, a network will always be vulnerable to noise injected at any layer. our proposed bfa with pbs attack also introduces noise at different layers of the dnn. any noise injected at the intermediate layer will increase as it is multiplied by the input features for vgg16 network we observed similar phenomena where among the bit flips required to degrade the accuracy to percent, of them are in the first six layers. additionally, we confirm this hypothesis of noise propagation across layers by the experiment shown in table we attack the model by freezing all the layers (making them not accessible to the attacker) except the first layer, then we do the opposite by freezing all the layers except the last one. as expected, attacking the first layer achieves higher attack success. however, this linearity theory may be too simple to explain other complex phenomena inside a dnn and may not hold true across different architectures. for example, resnet architecture which has skip connections, tend to evenly distribute the bit flips across different layers.. bfa with pbs does not suffer from gradient obfuscation. generation of adversarial examples in quantized network using straightthrough estimator introduces gradient obfuscation [36, 37]. attacking a quantized network becomes tricky as such network shows signs of gradient scattering [36]. in this work, we also used a quantized network which implements a uniform quantizer. however, our network directly uses quantized weights to do the inference after training. we calculate the gradient directly with respect to the quantized weights to avoid gradient obfuscation. moreover, the performance of bfa against 4,6,8 bits quantized networks proves that the effectiveness of bfa does not degrade due to the presence of a nondifferentiable function at the forward path.. potential defense methods. in order to defend adversarial examples, most common approach nowadays is to train the network with a mixture of clean and adversarial examples [6, 7]. one of the proposed defense methods against bfa would be to train the network to solve madry\u2019s minmax optimization problem [7]. their approach called adversarial training minimizes two losses: one from real image and other from adversarial image. hence, we perform adversarial training using bfa with pbs to minimize two such losses: one computed from the original network and the other computed from the same network with one bit flip for each batch.. however, unlike adversarial training, such a training method does not help in improving the robustness of the network. our attack can bypass adversarial training scheme primarily because of a large search space of close to 93m bits. even if we train the network to be resilient to several bitflips, there will always remain some bits that will be vulnerable to attack. another potential defense against bfa can be quantized networks. again our observation in table 2, does not show any corelation between number of quantization bits with the number of bitflips required. thus some of the popular adversarial defense methods [7, 37] fail against our bfa attack. the above observations make our attack even more threatening for deep learning applications. our proposed attack is the very first work for vulnerable bit search on quantized neural networks. bfa puts light on why the security analysis for neural network parameters needs more attention. we demonstrate through extensive experiments and analysis that. the vulnerability of dnn parameter to malicious bitflips is extremely severe than anticipated. we would encourage further investigation on both attack and defense front in order to thrive towards developing a more resilient network for deep learning applications.", "summary": "introduce the bitflip attack aimed to degrade a network\u2019s performance by flipping a few weight bits. on cifar10 and imagenet, common architectures such as resnets or alexnet are quantized into bits per weight value (or fewer). then, on a subset of the validation set, gradients with respect to the training loss are computed and in each layer, bits are selected based on their gradient value. afterwards, the layer which incurs the maximum increase in training loss is selected. this way, a network\u2019s performance can be degraded to chance level with as few as flipped bits (on imagenet, using alexnet).. also find this summary at [davidstutz.de]([url]/)."}, {"document": "in this work we propose a structured prediction technique that combines the virtues of gaussian conditional random fields (gcrf) with deep learning: (a) our structured prediction task has a unique global optimum that is obtained exactly from the solution of a linear system (b) the gradients of our model parameters are analytically computed using closed form expressions, in contrast to the memorydemanding contemporary deep structured prediction approaches [1,2] that rely on backpropagationthroughtime, (c) our pairwise terms do not have to be simple handcrafted expressions, as in the line of works building on the densecrf [1,3], but can rather be discovered from data through deep architectures, and (d) out system can trained in an endtoend manner. building on standard tools from numerical analysis we develop very efficient algorithms for inference and learning, as well as a customized technique adapted to the semantic segmentation task. this efficiency allows us to explore more sophisticated architectures for structured prediction in deep learning: we introduce multiresolution architectures to couple information across scales in a joint optimization framework, yielding systematic improvements. we demonstrate the utility of our approach on the challenging voc pascal image segmentation benchmark, showing substantial improvements over strong baselines. we make all of our code and experiments available at https://github.com/ siddharthachandra/gcrf.over the last few years deep learning has resulted in dramatic progress in the task of semantic image segmentation. early works on using cnns as feature extractors [4,5,6] and combining them with standard superpixelbased frontends gave substantial improvements over wellengineered approaches that used handcrafted features. the currently mainstream approach is relying on fully convolutional networks (fcns) [7,8], where cnns are trained to provide fields of outputs used for pixelwise labeling. a dominant research direction for improving semantic segmentation with deep learning is the combination of the powerful classification capabilities of fcns with structured prediction [1,2,3,9,10,11], which aims at improving classification by capturing interactions between predicted labels. one of the first works in the direction of combining deep networks with structured prediction was [3] which advocated the use of denselyconnected conditional random fields (densecrf) [12] to postprocess an fcnn output so as to obtain a sharper segmentation the preserves image boundaries. this was then arxiv:1603.08358v4 [cs.cv] nov used by zheng et al. [1] who combined densecrf with a cnn into a single recurrent neural network (rnn), accommodating the densecrf post processing in an endtoend training procedure. most approaches for semantic segmentation perform structured prediction using approximate inference and learning [9,13]. for instance the techniques of [1,2,3,10] perform meanfield inference for a fixed number of iterations. going for higher accuracy with more iterations could mean longer computation and eventually also memory bottlenecks: backpropagationthroughtime operates on the intermediate unrolled inference results that have to be stored in (limited) gpu memory. furthermore, the nonconvexity of the mean field objective means more iterations would only guarantee convergence to a local minimum. the authors in [14] with a gcrf module. the gcrf module is shown as the box outlined by dotted lines. the factor graph inside the gcrf module shows a 4connected neighbourhood. the white blobs represent pixels, red blobs represent unary factors, the green and blue squares represent vertical and horizontal connectivity factors. the input image is shown in (b). the network populates the unary terms (c), and horizontal and vertical pairwise terms. the gcrf module collects the unary and pairwise terms from the network and proposes an image hypothesis, i.e. scores (d) after inference. these scores are finally converted to probabilities using the softmax function (e), which are then thresholded to obtain the segmentation. it can be seen that while the unary scores in (c) miss part of the torso because it is occluded behind the hand. the flow of information from the neighbouring region in the image, via the pairwise terms, encourages pixels in the occluded region to take the same label as the rest of the torso (d). further it can be seen that the person boundaries are more pronounced in the output (d) due to pairwise constraints between pixels corresponding to the person and background classes. cnnbased pairwise potentials and three iterations of inference, while those in [15] use highlysophisticated modules, effectively learning to approximate meanfield inference. in these two works a more pragmatic approach to inference is taken, considering it as a sequence of operations that need to be learned [1]. these inferningbased approaches of combining learning and inference may be liberating, in the sense that one acknowledges and accommodates the approximations in the inference through endtoend training. we show however here that exact inference and learning is feasible, while not making compromises in the models expressive power. motivated by [16,17], our starting point in this work is the observation that a particular type of graphical model, the gaussian conditional random field (gcrf), allows us to perform exact and efficient maximumaposteriori (map) inference. even though gaussian random fields are unimodal and as such less expressive, gaussian conditional random fields are unimodal conditioned on the data, effectively reflecting the fact that given the image one solution dominates the posterior distribution. the gcrf model thus allows us to construct rich expressive structured prediction models that still lend themselves to efficient inference. in particular, the loglikelihood of the gcrf posterior has the form of a quadratic energy function which captures unary and pairwise interactions between random variables. there are two advantages to using a quadratic function: (a) unlike the energy of general graphical models, a quadratic function has a unique global minimum if the system matrix is positive definite, and (b) this unique minimum can be efficiently found by solving a system of linear equations. we can actually discard the probabilistic underpinning of the gcrf and understand gcrf inference as an energybased model, casting structured prediction as quadratic optimization (qo). gcrfs were exploited for instance in the regression tree fields model of jancsary et al. [17] where decision trees were used to construct gcrfs and address a host of vision tasks, including inpainting, segmentation and pose estimation. in independent work [2] proposed a similar approach for the task of image segmentation with cnns, where as in [14,15,18] fcns are augmented with discriminatively trained convolutional layers that model and enforce pairwise consistencies between neighbouring regions. one major difference to [2], as well as other prior works [1,3,10,14,15], is that we use exact inference and do not use backpropagationthroughtime during training. in particular building on the insights of [16,17], we observe that the map solution, as well as the gradient of our objective with respect to the inputs of our structured prediction module can be obtained through the solution of linear systems. casting the learning and inference tasks in terms of linear systems allows us to exploit the wealth of tools from numerical analysis. 3, for gaussian crfs sequential/parallel meanfield inference amounts to solving a linear system using the classic gaussseidel/jacobi algorithms respectively. instead of these underperforming methods we use conjugate gradients which allow us to perform exact inference and backpropagation in a small number (typically 10) iterations, with a negligible cost (0.02s for the general case in sec. 2, and 0.003s for the simplified formulation in sec. 2.5) when implemented on the gpu. secondly, building further on the connection between map inference and linear system solutions, we propose memoryand timeefficient algorithms for weightsharing (sec. 2.5) and multiscale inference (sec. in particular, in section we show that one can further reduce the memory footprint and computation demands of our method by introducing a pottstype structure in the pairwise term. this results in multifold accelerations, while delivering results that are competitive to the ones obtained with the unconstrained pairwise term. we show that our approach allows us to work with arbitrary neighbourhoods that go beyond the common 4connected neighbourhoods. in particular we explore the merit of using multiscale networks, where variables computed from different image scales interact with each other. this gives rise to a flow of information across differentsized neighborhoods. we show experimentally that this yields substantially improved results over singlescale baselines. we describe our approach in detail, and derive the expressions for weight update rules for parameter learning that are used to train our networks in an endtoend manner. we analyze the efficiency of the linear system solvers and present our multiresolution structured prediction algorithm. we report consistent improvements over wellknown baselines and stateoftheart results on the voc pascal test set. we now describe our approach. consider an image i containing p pixels. although our objective is to assign discrete labels to the pixels, we phrase our problem as a continuous inference task. rather than performing a discrete inference task that delivers one label per variable, we use a continuous function of the form x(p, l) which gives a score for each pairing of a pixel to a label. this score can be intuitively understood as being proportional to the logodds for the pixel p taking the label l, if a softmax unit is used to postprocess x. we denote the pixellevel groundtruth labeling by a discrete valued vector y y p where y 1, , l, and the inferred hypothesis by a real valued vector x r n , where n p l. our formulation is posed as an energy minimization problem. in the following subsections, we describe the form of the energy function, the inference procedure, and the parameter learning approach, followed by some technical details pertinent to using our framework in a fully convolutional neural network. finally, we describe a simpler formulation with pairwise weight sharing which achieves competitive performance while being substantially faster. even though our inspiration was from the probabilistic approach to structured prediction (gcrf), from now on we treat our structured prediction technique as a quadratic optimization (qo) module, and will refer to it as qo henceforth. we define the energy of a hypothesis in terms of a function of the following form: where a denotes the symmetric n n matrix of pairwise terms, and b denotes the n vector of unary terms. in our case, as shown in fig. 1, the pairwise terms a and the unary terms b are learned from the data using a fully convolutional network. in particular and as illustrated in fig. 1, a and b are the outputs of the pairwise and unary streams of our network, computed by a forward pass on the input image. these unary and pairwise terms are then combined by the qo module to give the final perclass scores for each pixel in the image. as we show below, during training we can easily obtain the gradients of the output with respect to the a and b terms, allowing us to train the whole network endtoend. is a standard way of expressing the energy of a system with unary and pairwise interactions among the random variables [17] in a vector labeling task. we chose this function primarily because it has a unique global minimum and allows for exact inference, alleviating the need for approximate inference. note that in order to make the matrix a strictly positive definite, we add to it times the identity matrix i, where is a design parameter set empirically in the experiments. given a and b, inference involves solving for the value of x that minimizes the energy function in eq. if (a i) is symmetric positive definite, then e(x) has a unique global minimum [19] at: as such, inference is exact and efficient, only involving a system of linear equations. our model parameters a and b are learned in an endtoend fashion via the backpropagation method. in the backpropagation training paradigm each module or layer in the network receives the derivative of the final loss l with respect to its output x, denoted by l x , from the layer above. l x is also referred to as the gradient of x. the module then computes the gradients of its inputs and propagates them down through the network to the layer below. to learn the parameters a and b via backpropagation, we require the expressions of gradients of a and b, i.e. we now derive these expressions. to compute the derivative of the loss with respect to b, we use the chain rule of differentiation: l x l b b x application of the chain rule yields the following closed form expression, which is a system of linear equations: when training a deep network, the right hand side l b is delivered by the layer above, and the derivative on the left hand side is sent to the unary layer below. derivative of loss with respect to a the expression for the gradient of a is derived by using the chain rule of differentiation again: l a l x x a using the expression x a a (ai) b, substituting a (ai) (a i) t (a i) , and simplifying the right hand side, we arrive at the following expression: where denotes the kronecker product. thus, the gradient of a is given by the negative of the kronecker product of the output x and the gradient of b. please note that while in this work we use the qo module as the penultimate layer of the network, followed by the softmax crossentropy loss, it can be used at any stage in a network and not only as the final classifier. we now give the expressions for the softmax crossentropy loss and its derivative for sake of completeness. the image hypothesis is a scoring function of the form x(p, l). for brevity, we denote the hypothesis concerning a single pixel by x(l). the softmax probabilities for the labels are then given by p l e x(l) l e x(l) these probabilities are penalized by the crossentropy loss defined as l l y l log p l , where y l is the ground truth indicator function for the ground truth label l , i.e. y l if l l , and y l otherwise. finally the derivative of the softmaxloss with respect to the input is given by: l x(l) p l y l we now describe a simplified qo formulation with shared pairwise terms which is significantly faster in practice than the one described above. we denote by a pi,pj (l i , l j ) the pairwise energy term for pixel p i taking the label l i , and pixel p j taking the label l j in this section, we propose a pottstype pairwise model, described by the following equation: in simpler terms, unlike in the general setting, the pairwise terms here depend on whether the pixels take the same label or not, and not on the particular labels they take. thus, the pairwise terms are shared by different pairs of classes. while in the general setting we learn p l p l pairwise terms, here we learn only p p terms. to derive the inference and gradient equations after this simplification, we rewrite our inference equation where x k , denotes the vector of scores for all the pixels for the class k 1, , l. the perclass unaries are denoted by b k , and the pairwise terms are shared between each pair of classes. the equations that follow are derived by specializing the general inference (eq. 2) and gradient equations (eq. 3,4) to this particular setting. following simple manipulations, the inference procedure becomes a two step process where we first compute the sum of our scores i x i , followed by x k , i.e. the scores for the class k as: derivatives of the unary terms with respect to the loss are obtained by solving: finally, the gradients of are computed as thus, rather than solving a system with a r p lp l , we solve l systems wit a r p p in our case, where l for object classes and background class, this simplification empirically reduces the inference time by a factor of 6, and the overall training time by a factor of we expect even larger acceleration for the mscoco dataset which has semantic classes. despite this simplification, the results are competitive to the general setting as shown in sec. having identified that both the inference problem in eq. and computation of pairwise gradients in eq. require the solution of a linear system of equations, we now discuss methods for accelerated inference that rely on standard numerical analysis techniques for linear systems [20,21]. our main contributions consist in (a) using fast linear system solvers that exhibit fast convergence (sec. 3.1) and (b) performing inference on multiscale graphs by constructing blockstructured linear systems (sec. our contributions in (a) indicate that standard conjugate gradient based linear system solvers can be up to faster than the solutions one could get by a naive application of parallel meanfield when implemented on the gpu. our contribution in (b) aims at accuracy rather than efficiency, and is experimentally validated in sec. the computational cost of solving the linear system of equations in eq. depends on the size of the matrix a, i.e. n n , and its sparsity pattern. in our experiments, while n , the matrix a is quite sparse, since we deal with small 4connected, 8connected and 12connected neighbourhoods. while a number of direct linear system solver methods exist, the sheer size of the system matrix a renders them prohibitive, because of large memory requirements. for large problems, a number of iterative methods exist, which require less memory, come with convergence (to a certain tolerance) guarantees under certain conditions, and can be faster than direct methods. in this work, we considered the jacobi, gaussseidel, conjugate gradient, and generalized minimal residual (gmres) methods [20], as candidates for iterative solvers. (a) shows the average number of iterations required by the aforementioned methods for solving the inference problem in eq. we used images in this analysis, and a tolerance of shows the convergence of these methods for one of these images. conjugate gradients clearly stand out as being the fastest of these methods, so our following results use the conjugate gradient method. our findings are consistent with those of grady in [22]. as we show below, meanfield inference for the gaussian crf can be understood as solving the linear system of eq. 2, namely parallel meanfield amounts to using the jacobi algorithm while sequential meanfield amounts to using the gaussseidel algorithm, which are the two weakest baselines in our comparisons. this indicates that by resorting to tools for solving linear systems we have introduced faster alternatives to those suggested by mean field. in particular the jacobi and gaussseidel methods solve a system of linear equations ax b by generating a sequence of approximate solutions x (k) , where the current solution x (k) determines the next solution x (k1) the update equation for the jacobi method [23] is given by the updates in eq. only use the previous solution x (k) , ignoring the most recently available information. this allows for parallel updates for x. in contrast, the gaussseidel [23] method always uses the most current estimate of x i as given by: as in [24], the gaussian markov random field (gmrf) in its canonical form is expressed as (x) exp x t x t x , where and are called the canonical parameters associated with the multivariate gaussian distribution (x). the update equation corresponding to meanfield inference is given by [25], the expression in eq. is exactly the expression for the jacobi iteration (eq. 12), or the gaussseidel iteration in eq. for solving the linear system , depending on whether we use sequential or parallel updates. one can thus understand sequential and parallel meanfield inference and learning algorithms as relying on weaker system solvers than the conjugate gradientbased ones we propose here. the connection is accurate for gaussian crfs, as in our work and [2], and only intuitive for discrete crfs used in [1,3]. we now turn to incorporating computation from multiple scales in a single system. even though cnns are designed to be largely scaleinvariant, it has been repeatedly reported [26,27] that fusing information from a cnn operating at multiple scales can improve image labeling performance. these results have been obtained for feedforward cnnswe consider how these could be extended to cnns with lateral connections, as in our case. a simple way of achieving this would be to use multiple image resolutions, construct one structured prediction module per resolution, train these as disjoint networks, and average the final results. this amounts to solving three decoupled systems which by itself yields a certain improvement as reported in sec. we advocate however a richer connectivity that couples the scalespecific systems, allowing information to flow across scales. the resulting linear system captures the following multiresolution interactions simultaneously: (a) pairwise constraints between pixels at each resolution, and (b) pairwise constraints between the same image region at two different resolutions. these interresolution pairwise terms connect a pixel in the image at one resolution, to the pixel it would spatially correspond to at another resolution. the interresolution connections help enforce a different kind of pairwise consistency: rather than encouraging pixels in a neighbourhood to have the same/different label, these encourage image regions to have the same/different labels across resolutions. this is experimentally validated in sec. to outperform the simpler multiresolution architecture outlined above. in this example, we have the input image at resolutions. the pairwise matrix a contains two kinds of pairwise interactions: (a) neighbourhood interactions between pixels at the same resolution (these interactions are shown as the blue and green squares), and (b) interactions between the same image region at two resolutions (these interactions are shown as red rectangles). while interactions of type (a) encourage the pixels in a neighbourhood to take the same or different label, the interactions of type (b) encourage the same image region to take the same labels at different resolutions. our implementation is fully gpu based, and implemented using the caffe library. our network processes input images of size 673, and delivers results at a resolution that is times smaller, as in [3]. the input to our qo modules is thus a feature map of size while the testing time per image for our methods is between 0.7s per image, our inference procedure only takes 0.02s for the general setting in sec. 2, and 0.003s for the simplified formulation (sec. this is significantly faster than dense crf postprocessing, which takes 2.4s for a image on a cpu and the 0.24s on a gpu. our implementation uses the highly optimized cublas and cusparse libraries for linear algebra on large sparse matrices. the cusparse library requires the matrices to be in the compressedstoragerow (csr) format in order to fully optimize linear algebra for sparse matrices. our implementation caches the indices of the csr matrices, and as such their computation time is not taken into account in the calculations above, since their computation time is zero for streaming applications, or if the images get warped to a canonical size. in applications where images may be coming at different dimensions, considering that the indexes have been precomputed for the changing dimensions, an additional overhead of 0.1s per image is incurred to read the binary files containing the cached indexes from the hard disk (using an ssd drive could further reduce this). our code and experiments are publicly available at https://github.com/siddharthachandra/gcrf. in this section, we describe our experimental setup, network architecture and results. we evaluate our methods on the voc pascal image segmentation benchmark. this benchmark uses the voc pascal dataset, which consists of training and validation images with manually annotated pixellevel labels for foreground object classes, and background class. in addition, we exploit the additional pixellevel annotations provided by [6], obtaining training images in total. the test set has unannotated images. the evaluation criterion is the pixel intersectionoverunion (iou) metric, averaged across the classes. baseline network (basenet). our basenet is based on the deeplablargefov network from [3]. as in [27], we extend it to get a multiresolution network, which operates at three resolutions with tied weights. more precisely, our network downsamples the input image by factors of and and later fuses the downsampled activations with the original resolution via concatenation followed by convolution. the layers at three resolutions share weights. this acts like a strong baseline for a purely feedforward network. our basenet has convolutional layers, pooling layers, and was pretrained on the mscoco trainval dataset [28]. the initial learning rate was set to and decreased by a factor of at 5k iterations. it was trained for 10k iterations. we extend our basenet to accommodate the binary stream of our network. shows a rough schematic diagram of our network. the basenet forms the unary stream of our qo network, while the pairwise stream is composed by concatenating the rd pooling layers of the three resolutions followed by batch normalization and two convolutional layers. 1, layers c c are shared by the unary and pairwise streams in our experiments. like our basenet, the qo networks were trained for 10k iterations; the initial learning rate was set to which was decreased by a factor of at 5k iterations. we consider three main types of qo networks: plain (qo), shared weights (qo s ) and multiresolution (qo mres ). in this set of experiments we train our methods on the trainaug images, and evaluate them on the val images. all our images were upscaled to an input resolution of the hyperparameter was set to to ensure positive definiteness. we first study the effect of having larger neighbourhoods among image regions, thus allowing richer connectivity. more precisely, we study three kinds of connectivities: (a) 4connected (qo ), where each pixel is connected to its left, right, top, and bottom neighbours, (b) 8connected (qo ), where each pixel is additionally connected to the diagonally adjacent neighbours, and (c) 12connected (qo ), where each pixel is connected to left, right, top, bottom neighbours besides the diagonally adjacent ones. table demonstrates that while there are improvements in performance upon increasing connectivities, these are not substantial. given that we obtain diminishing returns, rather than trying even larger neighbourhoods to improve performance, we focus on increasing the richness of the representation by incorporating information from various scales. 3.2, there are two ways to incorporate information from multiple scales; the simplest is to have one qo unit per resolution (qo res ), thereby enforcing pairwise consistencies individually at each resolution before fusing them, while the more sophisticated one is to have information flow both within and across scales, amounting to a joint multiscale crf inference task, illustrated in fig. it can be seen that our weight sharing simplification, while being significantly faster, also gives better results than qo. finally, the multiresolution framework outperforms the other variants, indicating that having information flow both within and across scales is desirable, and a unified multiresolution framework is better than merely averaging qo scores from different image resolutions. in this set of experiments, we train our methods on the trainaugval images, and evaluate them on the test images. the image resolutions and values are the same as those in sec. in these experiments, we also use the dense crf post processing as in [3,29]. our results are tabulated in tables and we first compare our methods qo, qo s and qo mres with the basenet, where the relative improvements can be most clearly demonstrated. our multiresolution network outperforms the basenet and other qo networks. we achieve a further boost in performance upon using the dense crf post processing strategy, consistently for all methods. we observe that our method yields an improvement that is entirely complementary to the improvement obtained by combining with densecrf. we also compare our results to previously published benchmarks in table when benchmarking against directly comparable techniques, we observe that even though we do not use endtoend training for the crf module stacked on top of our qo network, our method outperforms the previous state of the art crfrnn system of [1] by a margin of we anticipate further improvements by integrating endtoend crf training with our qo. in table 4, we compare our methods to previously published, directly comparable methods, namely those that use a variant of the vgg [30] network, are trained in an endtoend fashion, and use structured prediction in a fullyconvolutional framework. mean iou () deeplabcrossjoint [29] crfrnn [1] in this section we use our pottstype model alongside the deeplabv2 [32] resnet101 network. this network is a branch multiresolution version of the resnet101 network from [31]. it processes the input image at resolutions, with scaling factors of 0.5, 0.75, and 1.0, and then combines the network responses at the different resolutions by upsampling the responses at the lower scales to the original scale, and taking an elementwise maximum of the responses corresponding to each pixel. we learn potts type shared pairwise terms, and these pairwise terms are drawn from a parallel resnet101 network which has layers through conv1 to res5c, and processes the input image at the original scale. table reports quantitative results on the pascal voc test set. we show some qualitative results in fig. it can be seen that our method refines the object boundaries, leading to a better segmentation performance. method mean iou () deeplabv2 crf [32] qo s qo s crf in this work we propose a quadratic optimization method for deep networks which can be used for predicting continuous vectorvalued variables. the inference is efficient and exact and can be solved in seconds on the gpu for each image in the general setting, and seconds for the pottstype pairwise case using the conjugate gradient method. we propose a deeplearning framework which learns features and model parameters simultaneously in an endtoend fcn training algorithm. our implementation is fully gpu based, and implemented using the caffe library. our experimental results indicate that using pairwise terms boosts performance of the network on the task of image segmentation, and our results are competitive with the state of the art methods on the voc benchmark, while being substantially simpler. while in this work we focused on simple connected neighbourhoods, we would like to experiment with fully connected graphical models. secondly, while we empirically verified that setting a constant parameter brought about positivedefiniteness, we are now exploring approaches to ensure this constraint in a general case. we intend to exploit our approach for solving other regression and classification tasks as in [33,34]. the first column shows the colour image, the second column shows the basenet predicted segmentation, the third column shows the basenet output after dense crf post processing. the fourth column shows the qo mres predicted segmentation, and the final column shows the qo mres output after dense crf post processing. it can be seen that our multiresolution network captures the finer details better than the basenet: the tail of the airplane in the first image, the persons body in the second image, the aircraft fan in the third image, the road between the cars tail in the fourth image, and the wings of the aircraft in the final image, all indicate this. while dense crf postprocessing quantitatively improves performance, it tends to miss very fine details.", "summary": "chandra, kokkinos, semantic image segmentation image example put simply, the task is to cluster pixels in an image together and to assign all pixels in a cluster a meaningful label. labels are generally highlevel concepts, such as horse, dog, boat, etc. here is the voc pascal dataset used in this paper. discussion q\u2019s why gcrfs? joint distributions are hard, so exploit factorizable (exponentialfamily) probability distributions and conditional independence. this results in a graphlike structure. if the label assigned to each pixel is a random variable, then an conditional random field suggests that there\u2019s a dependence between a pixel\u2019s label and the label\u2019s of other nearby pixels. author\u2019s argue that continuous gaussian outputs are unimodal conditioned on the data, so given an image, one solution dominates the posterior. the loglikelihood is a quadratic, which allows the approximate inference task to be cast a quadratic optimization in an energyminimization setting. deep gcrf architecture the basic idea is to set the outputs of a convolutional neural network to be the energy of a segmentation hypothesis, in the form of the network predicts what the a (pairwise) and b (unary) terms in the energy function are for an image. \\lambda is set manually to enforce positivedefiniteness. then a quadratic optimization softmax module gives the final perclass scores for each pixel in the image (see fig. originally, a is a (p \\times l) \\times (p \\times l) matrix, where l is the number of class labels and p is number of pixels. shared pairwise terms a is no longer dependent on the number of class labels; only care about pixel interactions independent on what the label is. now, the inference equation (a \\lambda i) x b is reduced to a system of l equations for a of dim p \\times p. conjugate gradient method need to solve x a1b. the a matrix is very sparse, since it only deals with 4, 8, or 12connected neighborhoods. cg is the current recommended approach for solving ax b when a is sparse why? when a is dense, it is recommended to factorize a and then use backsubstitution. see here experiments try out the above fusing information across different resolutions. metric intersection over union baselines multiresolution deeplablargefov, crfrnn qo network baseline extended to have a binary and unary stream. qo module can be shared by all resolutions, or replicated three times for each scale mean iou on voc pascal for this approach. without more details and tests of significance, hard to say whether this method is really more effective than prev sota. it seems to do about iou better than the baselines. also seems to be much faster."}, {"document": "usergenerated passwords tend to be memorable, but not secure. a random, computergenerated 60bit string is much more secure. however, users cannot memorize random 60bit strings. in this paper, we investigate methods for converting arbitrary bit strings into english word sequences (both prose and poetry), and we study their memorability and other properties.passwords chosen by users (e.g., scarlet2) are easy to remember, but not secure (florencio and herley, 2007). a more secure method is to use a systemassigned 60bit random password, such as however, this string is hard to memorize. in this paper, we convert such strings into english phrases, in order to improve their memorability, using natural language processing to select fluent passphrases. our methods are inspired by an xkcd cartoon that proposes to convert a randomlychosen 44bit password into a short, nonsensical sequence of english words. the proposed system divides the 44bit password into four 11bit chunks, and each chunk provides an index into a 2048word english dictionary. xkcds example passphrase is correct horse battery staple: staple the fourword sequence is nonsense, but it is easier to memorize than the 44bit string, and xkcd hypothesizes that users can improve memorability by building an image or story around the four words. in this paper, we investigate other methods for converting a systemgenerated bit string into a memorable sequence of english words. our methods produce whole sentences, e.g. fox news networks are seeking views from downtown streets. as well as short poems, e.g. we also move to 60bit passwords, for better security. one source claims: as of 2011, available commercial products claim the ability to test up to 2,800,000,000 passwords a second on a standard desktop computer using a highend graphics processor. if this is correct, a 44bit password would take one hour to crack, while a 60bit password would take years. our concrete task is as follows: it makes me think of union pacific resource said it looks like most commercial networks some companies keep their windows rolled down so you dont feel connected to any community contains extreme violence and it was a matter of not only its second straight loss parking and utilities have been searching for a third straight road win it was the same girl and now a law professor in the former east german town i know a man who said he was chief of staffs in a real and deep conversation input: a random, systemgenerated 60bit password. output: an english word sequence with two properties: it is memorable. we can deterministically recover the original input 60bit string from it. this implies that we map distinct bit strings into distinct english sequences. if a user memorizes the english word sequence supplied to them, then they have effectively memorized the 60bit string. we now describe our baseline password generation method, followed by four novel methods. in section we experimentally test their memorability. our baseline is a version of xkcd. instead of a 2048word dictionary, we use a 32,7868word dictionary. we assign each word a distinct 15bit code. at runtime, we take a systemassigned 60bit code and split it into four 15bit sequences. we then substitute each 15bit segment with its corresponding word. by doing this, we convert a random 60bit code into a 4word password. the first row of table shows three sample xkcd passwords, along with other information, such as the average number of characters (including spaces). xkcd passwords are short but nonsensical, so we now look into methods that instead create longer but fluent english sentences. we might think to guarantee fluency by selecting sentences from an alreadyexisting text corpus, but no corpus is large enough to contain ( ) distinct sentences. therefore, we must be able to synthesize new english strings. in our first sentence generation method (first letter mnemonic), we store our input 60bit code in the first letters of each word. we divide the 60bit code into 4bit sections, e.g., every 4bit sequence type corresponds to an english letter or two, per table we build a wordconfusion network (or sausage lattice) by replacing each 4bit code with all english words that start with a corresponding letter, e.g. : this yields about paths, some good (is my frog. and some bad (income miner feast. to select the most fluent path, we train a 5gram language model with the srilm toolkit (stolcke, 2002) on the english gigaword corpus. srilm also includes functionality for extracting the best path from a confusion network. table shows sample sentences generated by the method. perhaps surprisingly, even though the sentences are much longer than xkcd (15 words versus words), the ngram language model (lm) score is a bit better. the sentences are locally fluent, but not perfectly grammatical. we can easily reconstruct the original 60bit code by extracting the first letter of each word and applying the table mapping in reverse. most of the characters in the previous methods seem wasted, as only the wordinitial letters bear information relevant to reconstructing the original 60bit sequence table additionally, we nondeterministically introduce a space (or not) between each pair of letters. this yields possible output strings per input, of which consist of legal english words. from those strings, we choose the one that yields the best word 5gram score. it is not immediately clear how to process a letterbased lattice with a wordbased language model. we solve this search problem by casting it as one of machine translation from bitstrings to english. we create a phrase translation table by pairing each english word with a corresponding bit phrase, using table in reverse. sample entries include: din through yields we then use the moses machine translation toolkit (koehn et al., 2007) to search for the 1best translation of our input 60bit string, using the phrase table and a 5gram english lm, disallowing reordering. table shows that these sentences are shorter than the mnemonic method (11.8 words versus words), without losing fluency. given a generated english sequence, we can deterministically reconstruct the original 60bit input string, using the above phrase table in reverse. sentence passwords from the previous method contain characters on average (including spaces). classic studies by shannon (1951) and others estimate that printed english may ultimately be compressible to about one bit per character. this implies we might be able to produce shorter output (60 characters, including space) while maintaining normal english fluency. our next technique (frequency method) modifies the phrase table by assigning short bit codes to frequent words, and long bit codes to infrequent words. for example: din through yields note that the word din is now mapped to a 9bit sequence rather than a 3bit sequence. more precisely, we map each word to a random bit sequence of length max(1, log p(word) ) by changing variables and we can vary between smooth but long sentences ( and 0) to xkcdstyle phrases ( and 15). table shows example sentences we obtain with and 2.5, yielding sentences of words on average. in ancient times, people recorded long, historical epics using poetry, to enhance memorability. we follow this idea by turning each systemassigned 60bit string into a short, distinct english poem. our format is the rhyming iambic tetrameter couplet: the poem contains two lines of eight syllables each. lines are in iambic meter, i.e., their syllables have the stress pattern 01010101, where represents an unstressed syllable, and represents a stressed syllable. we also allow 01010100, to allow a line to end in a word like angela. the two lines end in a pair of rhyming words. words rhyme if their phoneme sequences match from the final stressed vowel onwards. we obtain stress patterns and phoneme sequences from the cmu pronunciation dictionary. monosyllabic words cause trouble, because their stress often depends on context (greene et al., 2010). for example, eighth is stressed in eighth street, but not in eighth avenue. this makes it hard to guarantee that automaticallygenerated lines will scan as intended. we therefore eject all monosyllabic words from the vocabulary, except for six unstressed ones (a, an, and, the, of, or). here is a sample poem password: the legendary japanese subsidiaries overseas meter and rhyme constraints make it difficult to use the moses machine translation toolkit to search for fluent output, as we did above; the decoder state must be augmented with additional shortand longdistance information (genzel et al., 2010). instead, we build a large finitestate acceptor (fsa) with a path for each legal poem. in each path, the second line of the poem is reversed, so that we can enforce rhyming locally. the details of our fsa construction are as follows. first, we create a finitestate transducer (fst) that maps each input english word onto four sequences that capture its essential properties, e.g. : create create eyt create 1r 0r create eyt 1r 0r here, eyt represents the rhymeclass of words like create and debate. the r indicates a stress pattern in the righttoleft direction. we then compose this fst with an fsa that only accepts sequences of the form: x x 1r 0r 1r 0r 1r 0r 1r 0r where x and x are identical rhyme classes (e.g., eyt and eyt). it remains to map an arbitrary 60bit string onto a path in the fsa. let k be the integer representation of the 60bit string. if the fsa contains exactly paths, we can easily select the kth path using the following method. at each node n of the fsa, we store the total number of paths from n to the final statethis takes linear time if we visit states in reverse topological order. we then traverse the fsa deterministically from the start state, using k to guide the path selection. our fsa actually contains paths, far more than the required we can say that the information capacity of the english rhyming iambic tetrameter couplet is bits! some are very good: fortunately, because our fsa contains over a million times the required paths, we can avoid these bad outputs. for any particular 60bit string, we have a million poems to choose from, and we output only the best one. more precisely, given a 60bit input string k, we extract not only the kth fsa path, but also the k i paths, with i ranging from to 999,999. we explicitly list out these paths, reversing the second half of each, and score them with our 5gram lm. we output the poem with the 1best lm score. table shows sample outputs. to reconstruct the original 60bit string k, we first find the fsa path corresponding to the userrecalled english string (with second half reversed). we use depthfirst search to find this path. once we have the path, it is easy to determine which numbered path it is, lexicographically speaking, using the nodelabeling scheme above to recover k. we designed two experiments to compare our methods. the first experiment tests the memorability of passwords. we asked participants to memorize a password from a randomly selected method and recall it two days later. to give more options to users, in all experiments, we omit the first letter mnemonic, due to its low performance in early tests. table 4: memorability of passwords generated by our methods. recalls indicates how many participants returned to type their memorized english sequences, and correct recalls tells how many sequences were accurately remembered. method name user preference xkcd all letter method frequency method poetry table 5: user preferences among passwords generated by our methods. we let them select from the 10best passwords according to the lm score for a given 60bit code. note that this flexibility is not available for xkcd, which produces only one password per code. users participated in this experiment, returned to recall the password, and successfully recalled the complete password. table shows that the poetry and xkcd methods yield passwords that are easiest to remember. in the second experiment, we present a separate set of users with passwords from each of the four methods. we ask which they would prefer to use, without requiring any memorization. table shows that users prefer sentences over poetry, and poetry over xkcd. table shows that the poetry and xkcd methods yield passwords that are easiest to memorize. complete sentences generated by the all letter and frequency methods are harder to memorize. at the same time table shows that people like the sentences better than xkcd, so it seems that they overestimate their ability to memorize a sentence of words. here are typical mistakes (s systemgenerated, r as recalled by user): (s) still looking for ruben sierra could be in central michigan (r) i am still looking for ruben sierra in central michigan (s) that we were required to go to college more than action movies (r) we are required to go to college more than action movies (s) no dressing allowed under canon law in the youth group (r) no dresses allowed under canon law for youth groups users remember the gist of a sentence very well, but have trouble reproducing the exact wording. postexperiment interview reveal this to be partly an effect of overconfidence. users put little mental work into memorizing sentences, beyond choosing among the 10best alternatives presented to them. by contrast, they put much more work into memorizing an xkcd phrase, actively building a mental image or story to connect the four otherwise unrelated words. actually, we can often automatically determine that a userrecalled sequence is wrong. for example, when we go to reconstruct the 60bit input string from a userrecalled sequence, we may find that we get a 62bit string instead. we can then automatically prod the user into trying again, but we find that this is not effective in practice. an intriguing direction is to do automatic errorcorrection, i.e., take the userrecalled sequence and find the closest match among the english sequences producible by the method. of course, it is a challenge to do this with 1best outputs of an mt system that uses heuristic beam search, and we must also ensure that security is maintained. we may also investigate new ways to rerank nbest lists. language model scoring is a good start, but we may prefer vivid, concrete, or other types of words, or we may use text data associated with the user (papers, emails) for secure yet personalized password generation. gasser (1975), crawford and aycock (2008), and shay et al. (2012) describe systems that produce meaningless but pronounceable passwords, such as tufritvi however, their systems can only assign distinct passwords. jeyaraman and topkara (2005) suggest generating a random sequence of characters, and finding a mnemonic for it in a text corpus. a limited corpus means they again have a small space of systemassigned passwords. we propose a similar method in section 2.2, but we automatically synthesize a new mnemonic word sequence. (2012) use a method similar to xkcd with small dictionaries. this leads to longer nonsense sequences that can be difficult to remember. we introduced several methods for generating secure passwords in the form of english word sequences. we learned that long sentences are seemingly easy to remember, but actually hard to reproduce, and we also learned that our poetry method produced relatively short, memorable passwords that are liked by users.", "summary": "how to memorize a random 60bit string ghazvininejad et al. a bit of fun for today this paper has been the source of many articles around the net over the last couple of weeks (though not many have dug into the actual algorithms ). inspired by an xkcd cartoon , the challenge is to convert a randomly generated 60bit string into a memorable sequence of english words that can deterministically be used to recover the bit string and hence be used as a password. including the xkcd algorithm as a baseline, the authors develop four additional approaches and evaluate them all for ease of human memorisation. the xkcd and poetry methods perform the best under this test, so let\u2019s look at those two in more details. 60bit xkcdinspired passwords xkcd issue start with a randomly chosen 60bit password. divide this up into four 15bit segments, and use each as an index into a 32,768 word dictionary. (the xkcd original used a 44bit password, 11bit segments, and a word dictionary). this produces passwords such as fees wesley inmate decentralization,\u2019 photos bros nan plain,\u2019 and \u2019embarass debating gaskell jennie.\u2019 these passwords are nonsensical, but have the advantage of only being four words long. three variations are tried (firstletter mnemonic, allletter method, and frequency method) that encode bits or bitsequences as letters and then generate sentences from them. i particularly like the alllettermethod that pairs english words with bitphrases and then uses the moses machine translation toolkit to search for the 1best translation of the 60bit input string, using this phrase table and a 5gram english language model. ingenious, and it produces great phrases for example, fox news networks are seeking views from downtown streets. users ultimately found these sentences harder to remember though due to their length. poetrybased passwords my first original poetry contribution so far in the morning paper: if i can make my password rhyme, then learning it takes much less time. don\u2019t worry, i suspect it will also be my last ;). in ancient times, people recorded long, historical epics using poetry, to enhance memorability. we follow this idea by turning each systemassigned 60bit string into a short, distinct english poem. our format is the rhyming iambic tetrameter couplet. (two lines of eight syllables, stress pattern 01010101, and lines ending in a pair of rhyming words.) so now all you have to do is find an algorithm that generates memorable rhyming iambic tetrameter couplets that correspond to a unique 60bit code! the authors take this daunting sounding challenge in their stride. first they create a finite state transducer that translates english words into sequences capturing their essential properties. finite state transducers (fsts) are quite common in language processing. an fst is a finitestate automaton that produces an output tape as well as reading from its input tape. from the wikipedia page : the two tapes of a transducer are typically viewed as an input tape and an output tape. on this view, a transducer is said to transduce (i.e. translate) the contents of its input tape to its output tape, by accepting a string on its input tape and generating another string on its output tape. it may do so nondeterministically, and it may produce more than one output for each input string. a transducer may also produce no output for a given input string, in which case it is said to reject the input. in general, a transducer computes a relation between two formal languages. for our purposes though, we can simply think of this stage as mapping each english word to a set of encodings. for the sample word create,\u2019 these encodings would be: // create eyt // create at end of a line, eyt rhyming pattern 1r 0r // create in the second line (r for reverse) eyt 1r 0r // create at end of the second line (r for reverse) a finite state acceptor is constructed with a path\u2019 for each legal poem. this only accepts sequences of the form: x x 1r 0r 1r 0r 1r 0r 1r 0r (the second line is generated in reverse order so that rhyming can be enforced locally x stands for any rhyme pattern, e.g. it remains to map an arbitrary 60bit string onto a path in the fsa. let k be the integer representation of the 60bit string. if the fsa contains exactly paths, we can easily select the kth path using the following method. at each node n of the fsa, we store the total number of paths from n to the final statethis takes linear time if we visit states in reverse topological order. we then traverse the fsa deterministically from the start state, using k to guide the path selection. the generated fsa actually contains paths, giving more than a million poem choices for each 60bit string. this gives opportunity to use the 5gram language model again to output the best one: more precisely, given a 60bit input string k, we extract not only the kth fsa path, but also the k i paths, with i ranging from to 999,999. we explicitly list out these paths, reversing the second half of each, and score them with our 5gram lm. we output the poem with the 1best lm score. for example: diversity inside replied, retreats or colours justified to reconstruct the original 60bit string k, we first find the fsa path corresponding to the userrecalled english string (with second half reversed). we use depthfirst search to find this path. once we have the path, it is easy to determine which numbered path it is, lexicographically speaking, using the nodelabeling scheme above to recover k."}, {"document": "neural sequence models are widely used to model timeseries data. equally ubiquitous is the usage of beam search (bs) as an approximate inference algorithm to decode output sequences from these models. bs explores the search space in a greedy leftright fashion retaining only the topb candidates resulting in sequences that differ only slightly from each other. producing lists of nearly identical sequences is not only computationally wasteful but also typically fails to capture the inherent ambiguity of complex ai tasks. to overcome this problem, we propose diverse beam search (dbs), an alternative to bs that decodes a list of diverse outputs by optimizing for a diversityaugmented objective. we observe that our method finds better top1 solutions by controlling for the exploration and exploitation of the search space implying that dbs is a better search algorithm. moreover, these gains are achieved with minimal computational or memory overhead as compared to beam search. to demonstrate the broad applicability of our method, we present results on image captioning, machine translation and visual question generation using both standard quantitative metrics and qualitative human studies. our method consistently outperforms bs and previously proposed techniques for diverse decoding from neural sequence models. in the last few years, recurrent neural networks (rnns), long shortterm memory networks (lstms) or more generally, neural sequence models have become the standard choice for modeling timeseries data for a wide range of applications such as speech recognition (graves et al., 2013), machine translation (bahdanau et al., 2014), conversation modeling (vinyals le, 2015), image and video captioning (vinyals et al., 2015; venugopalan et al., 2015), and visual question answering (antol et al., 2015). rnn based sequence generation architectures model the conditional probability, pr(yx) of an output sequence y (y1, , yt ) given an input x (possibly also a sequence); where the output tokens yt are from a finite vocabulary, v maximum a posteriori (map) inference for rnns is te task of finding the most likely output sequence given the input. since the number of possible sequences grows as vt , exact inference is nphard so approximate inference algorithms like beam search (bs) are commonly employed. bs is a heuristic graphsearch algorithm that maintains the b topscoring partial sequences expanded in a greedy lefttoright fashion. shows a sample bs search tree.. despite the widespread usage of bs, it has long been understood that solutions decoded by bs are generic and lacking in diversity (finkel et al., 2006; gimpel et al., 2013;. steam black locomotive. down track train tracks traveling is the with near track down. a steam engine train travelling down train tracks. a steam engine train travelling down tracks. a steam engine train travelling through a forest. a steam engine train travelling through a lush green forest. a steam engine train travelling through a lush green countryside a train on a train track with a sky background.. a steam engine travelling down train tracks. a steam engine train travelling through a forest. an old steam engine train travelling down train tracks. an old steam engine train travelling through a forest. a black train is on the tracks in a wooded area. a black train is on the tracks in a rural area.. single engine train rolling down the tracks. a steam locomotive is blowing steam. a locomotive drives along the tracks amongst trees and bushes. an old fashion train with steam coming out of its pipe. a black and red train moving down a train track.. an engine is coming down the train track. figure 1: comparing image captioning outputs decoded by bs and our method, diverse beam search (dbs) we notice that bs captions are nearduplicates with similar shared paths in the search tree and minor variations in the end. in contrast, dbs captions are significantly diverse and similar to the interhuman variability in describing images.. li et al., 2015; li jurafsky, 2016). to illustrate this, a comparison of captions provided by humans (bottom) and bs (topmost) are shown in fig. while this behavior of bs is disadvantageous for many reasons, we highlight the three most crucial ones here:. i) the production of nearidentical beams make bs a computationally wasteful algorithm, with essentially the same computation being repeated for no significant gain in performance.. ii) due to lossevaluation mismatch i.e. improvements in posteriorprobabilities not necessarily corresponding to improvements in taskspecific metrics, it is common practice (vinyals et al., 2015; karpathy feifei, 2015; ferraro et al., 2016) to deliberately throttle bs to become a poorer optimization algorithm by using reduced beam widths. this treatment of an optimization algorithm as a hyperparameter is not only intellectually dissatisfying but also has a significant practical sideeffect it leads to the decoding of largely bland, generic, and safe outputs, e.g. always saying i don\u2019t know in conversation models (corrado, 2015).. iii) most importantly, lack of diversity in the decoded solutions is fundamentally crippling in ai problems with significant ambiguity e.g. there are multiple ways of describing an image or responding in a conversation that are correct and it is important to capture this ambiguity by finding several diverse plausible hypotheses.. overview and contributions. to address these shortcomings, we propose diverse beam search (dbs) a general framework to decode a list of diverse sequences that can be used as an alternative to bs. at a high level, dbs decodes diverse lists by dividing the beam budget into groups and enforcing diversity between groups of beams. drawing from recent work in the probabilistic graphical models literature on diverse mbest (divmbest) map inference (batra et al., 2012; prasad et al., 2014; kirillov et al., 2015), we optimize an objective that consists of two terms the sequence likelihood under the model and a dissimilarity term that encourages beams across groups to differ. this diversityaugmented model score is optimized in a doubly greedy manner greedily optimizing along both time (like bs) and groups (like divmbest).. to summarize, our primary technical contribution is diverse beam search, a doubly greedy approximate inference algorithm for decoding diverse sequences. our method consistently outperforms bs while being comparable in terms of both runtime and memory requirements. we report results on image captioning, machine translation and visual question generation to demonstrate the broad applicability of dbs. we find that dbs results in improvements on both oracle taskspecific and diversityrelated metrics against baselines. we conduct human studies to evaluate the role of diversity in human preferences between bs and dbs for image captions. we also analyze the parameters of dbs and show they are robust over a wide range of values. finally, we also show that our method is general enough to incorporate various forms for the dissimilarity term.. overall, our algorithm is simple to implement and consistently outperforms bs in a wide range of domains without sacrificing efficiency. our implementation is available at https://github.. com/ashwinkalyan/dbs. also, a demo of dbs on imagecaptioning is available at dbs. we begin with a refresher on bs, before describing our generalization, diverse beam search. for notational convenience, let [n] denote the set of natural numbers from to n and let v[n] index the first n elements of a vector v rm,. rnns are trained to estimate the likelihood of sequences of tokens from a finite dictionary v given an input x. the rnn updates its internal state and estimates the conditional probability distribution over the next output given the input and all previous output tokens. we denote the logarithm of this conditional probability distribution over all tokens at time t as (yt) log pr(ytyt1, to simplify notation, we index () with a single variable yt; but it should be clear that it depends on the previous outputs, y[t1] from the context. the logprobability of a partial solution (i.e. the sum of logprobabilities of all previous tokens decoded) can now be written as (y[t]) [t] (y ). the decoding problem is then the task of finding a sequence y that maximizes (y).. as each output is conditioned on all the previous outputs, decoding the optimal lengtht sequence in this setting can be viewed as map inference on t order markov chain with the t nodes corresponding to output tokens. not only does the size of the largest factor in such a graph grow as vt , but also requires wastefully forwarding of the rnn repeatedly to compute entries in the factors. thus, approximate algorithms are employed.. beam search. the most prevalent method for approximate decoding is bs, which stores the topb highly scoring candidates at each time step; where b is known as the beam width. let us denote the set of b solutions held by bs at the start of time t as y[t1] y1,[t1], at each time step, bs considers all possible single token extensions of these beams given by the set yt y[t1] v and selects the b most likely extensions. more formally, at each step,. y[t] argmax y1,[t],...,yb,[t]yt b[b] (yb,[t]) s.t. the above objective can be trivially maximized by sorting all b v members of yt by their logprobabilities and selecting the topb. this process is repeated until time t and the most likely sequence is selected by ranking the b beams based on logprobabilities.. while this method allows for multiple sequences to be explored in parallel, most completions tend to stem from a single highly valued beam resulting in outputs that are typically only minor perturbations of a single sequence. to overcome this shortcoming, we consider augmenting the objective in eq. with a dissimilarity term (y[t]) that measures the diversity between candidate sequences. jointly optimizing for all b candidates at each time step is intractable as the number of possible solutions grows with vb (which can easily reach for typical language modeling settings). to avoid this joint optimization problem, we divide the beam budget b into g groups and greedily optimize each group using beam search while holding previous groups fixed. this doubly greedy approximation along both time and across groups turns (y[t]) into a function of only the current group\u2019s possible extensions. we detail the specifics of our approach in this section.. let y[t], the set of all b beams at time t be partitioned into g disjoint and nonempty subsets y g[t], g[g]. without loss of generality, consider an equal partition such that each group containsb b/g groups. beam search can be applied to each group to produceb solutions; however, each group would produce identical outputs.. further consider a modification to the objective in eq. which adds a dissimilarity term (y 1[t], , y g1 [t] )[y] measuring the dissimilarity of group g against prior groups if token y is cho. sen to extend any of the beams in the group g. the exact form of () can vary and discussion of this choice is dealt in section as we optimize each group with the previous groups fixed, extending group g at time t amounts to a standard bs using dissimilarity augmented logprobabilities and can be written as:. y g[t] argmax yg 1,[t] ,...,yg b,[t]y g t. b[b] (ygb,[t]) g ( y 1[t], this approach, which we call diverse beam search (dbs) is detailed in algorithm an example run of dbs is shown in figure for decoding imagecaptions. in the example, b6 and g3 and so, each group performs a smaller, diversityaugmented bs of size in the snapshot shown, group is being stepped forward and the diversity augmented score of all words in the dictionary is computed conditioned on previous groups. the score of all words are adjusted by their similarity to previously chosen words birds\u2019, the\u2019 and an\u2019 (algorithm 1, line 5). the optimal continuations are then found by standard bs (algorithm 1, line 6).. algorithm 1: diverse beam search perform a diverse beam search with g groups using a beam width of b for t 1, // perform one step of beam search for first group without diversity y 1[t] argmax(y11,[t],...,y1b,[t]) b[b] (y b,[t]) for g 2, g do // augment logprobabilities with diversity penalty (ygb,[t]) (y g b,[t]) g(y [t], , y g1 [t] )[y g b,t] b [b],y g b,[t] y. g and g // perform one step of beam search for the group. y g[t] argmax(yg1,[t],...,ygb,[t]) b[b] (y g b,[t]). return set of b solutions, y[t ] g g1 y g [t ]. in summary, dbs works in a doubly greedy manner enabling us to incorporate diversity in beam search. moreover, as the first group is not conditioned on other groups during optimization, our method is guaranteed to be at least as good as a beam search of size b/g. the task of generating diverse structured outputs from probabilistic models has been studied extensively (park ramanan, 2011; batra et al., 2012; kirillov et al., 2015; prasad et al., 2014). (2012) formalized this task for markov random fields as the divmbest problem and presented a greedy approach which solves for outputs iteratively, conditioning on previous solutions to induce diversity. (2015) show how these solutions can be found jointly for certain kinds of energy functions. the techniques developed by kirillov are not directly applicable to decoding from rnns, which do not satisfy the assumptions made.. most related to our proposed approach is that of gimpel et al. (2013) who apply the divmbest approach to machine translation using beam search as a blackbox inference algorithm. to obtain diverse solutions, beam searches of arbitrary size are sequentially performed while retaining the topscoring candidate and using it to update the diversity term. this approach is extremely wasteful because in each iteration only one solution returned by beam search is kept. consequently, the iterative method is time consuming and is poorly suited for batch processing or producing a large number of solutions. our algorithm avoids these shortcomings by integrating diversity within bs such that no beams are discarded. by running multiple beam searches in parallel and at staggered time offsets, we obtain large time savings making our method comparable to classical bs. one potential advantage over our method is that more complex diversity measures at the sentencelevel can be incorporated. however, as observed empirically by us and li et al. (2015), initial words tend to significantly impact the diversity of the resultant sequences suggesting that later words may not contribute significantly to diverse inference.. diverse decoding for rnns. some efforts have been made to produce diverse decodings from recurrent models for conversation modeling and machine translation.. in this context, our work is closely related to li jurafsky (2016), who propose a bs diversification heuristic to overcome the shortcomings of gimpel et al. this discourages sequences from sharing common roots, implicitly resulting in diverse lists. introducing diversity through a modified objective as in dbs rather than a heuristic provides easier generalization to incorporate different notions of diversity and control for the explorationexploitation tradeoff as detailed in section furthermore, we find that dbs outperforms this method.. through a novel decoding objective that maximizes mutual information between inputs and predicted outputs, li et al. (2015) penalize decoding generic, input independent sequences. this is achieved by training an additional target language model. although this work and dbs share the same goals (producing diverse decodings), the techniques developed are disjoint and complementary li et al. (2015) develops a new model (rnn translation model with an rnn target language model), while dbs is a modified inference algorithm that can be applied to any model where bs is applicable. combination of these complementary techniques is left as interesting future work. we first explain the baselines and evaluation metrics used in this paper. next, we proceed to the analysis of the effects of dbs parameters. further, we report results on imagecaptioning, machine translation and visual question generation. although results are reported on these tasks, it should be noted that dbs is a taskagnostic algorithm that can replace bs to decode diverse solutions.. baselines. we compare with beam search and the following existing methods:. li jurafsky (2016): this work modifies bs by introducing an intrasibling rank. for each partial solution, the set of v continuations are sorted and assigned intrasibling ranks k [l] in order of decreasing logprobabilities, t(yt). the logprobability of an extenstion is then reduced in proportion to its rank, and continuations are resorted under these modified logprobabilities to select the topb diverse beam extensions.. li et al. (2015): these models are decoded using a modified objective, p (yx) u(y), where u(y) is an unconditioned target sequence model. this additional term penalizes generic input independent decoding.. both works use secondary mechanisms such as rerankers to pick a single solution from the generated lists. as we are interested in evaluating the quality of the generated lists and in isolating the gains due to diverse decoding, we do not implement any rerankers. instead, we simply sort the list based on logprobability. we compare to our own implementations of these methods as none are publicly available.. we evaluate the performance of the generated lists using the following two metrics that quantify complementary details:. oracle accuracy: oracle or topk accuracy for some taskspecific metric like bleu is the maximum value of the metric over a list of k potential solutions. it is an upper bound on the. potential impact diversity plays in finding relevant solutions.. diversity statistics: we count the number of distinct ngrams present in the list of generated outputs. (2015), we divide these counts by the total number of words generated to bias against long sentences.. simultaneous improvements in both metrics indicate that output lists have increased diversity without sacrificing fluency and correctness with respect to target tasks. human preference studies which compare image captions produced by dbs and bs also compare these methods. finally, we discuss the role of diversity by relating it to intrinsic details contained in images. in this section, we study the impact of the number of groups, the strength of diversity penalty, and various forms of diversity functions for language models. further discussion and experimental details are included in the supplementary materials.. settinggb allows for the maximum exploration of the space, while setting g1 reduces our method to bs, resulting in increased exploitation of the searchspace around the 1best decoding. thus, increasing the number of groups enables us to explore various modes of the model. empirically, we find that maximum exploration correlates with improved oracle accuracy and hence use gb to report results unless mentioned otherwise.. diversity strength (). the diversity strength specifies the tradeoff between the joint logprobability and the diversity terms. as expected, we find that a higher value of produces a more diverse list; however, excessively high values of can overpower model probability and result in grammatically incorrect outputs. we set by performing a grid search on the validation set for all experiments. we find a wide range of values (0.2 to 0.8) work well for most tasks and datasets.. choice of diversity function (). in section 3, we defined () as a function over a set of partial solutions that outputs a vector of similarity scores for potential beam completions. assuming that each of the previous groups can influece the completion of the current group independently, we can simplify the dissimilarity term (y 1[t], , y. g1 t] ) by summing over each group\u2019s contributions asg1. this factorized term can now take various forms ranging from simple hamming. diversity to neural embedding based penalties. we discuss some of these below:. this form penalizes the selection of tokens used in previous groups proportional to the number of times it was selected before.. cumulative diversity. once two sequences have diverged sufficiently, it seems unnecessary and perhaps harmful to restrict that they cannot use the same words at the same time. to encode this backingoff\u2019 of the diversity penalty we introduce cumulative diversity which keeps a count of identical words used at every time step, indicative of overall dissimilarity. specifically, (y h[t])[y g [t]] exp( t bb i[y h b, 6y g b, ])/ where is a temperature parameter control. ling the strength of the cumulative diversity term and i[] is the indicator function. the current group is penalized for producing the same ngrams as previous. groups, regardless of alignment in time similar to gimpel et al. this is proportional to the number of times each ngram in a candidate occurred in previous groups. unlike hamming diversity, ngrams capture higher order structures in the sequences.. neuralembedding diversity. while all the previous diversity functions discussed above perform exact matches, neural embeddings such as word2vec (mikolov et al., 2013) can penalize semantically similar words like synonyms. this is incorporated in each of the previous diversity functions by replacing the hamming similarity with a soft version obtained by computing the cosine similarity between word2vec representations. when using with ngram diversity, the representation of the ngram is obtained by summing the vectors of the constituent words.. each of these various forms encode different notions of diversity. hamming diversity ensures different words are used at different times, but can be circumvented by small changes in sequence alignment. while ngram diversity captures higher order statistics, it ignores sentence alignment. neuralembedding based encodings can be seen as a semantic blurring of either the hamming or ngram metrics, with word2vec representation similarity propagating diversity penalties not only. to exact matches but also to close synonyms. we find that using any of the above functions help outperform bs in the tasks we examine; hamming diversity achieves the best oracle performance despite its simplicity. a comparison of the performance of these functions for imagecaptioning is provided in the supplementary. we evaluate on two datasets coco (lin et al., 2014) and pascal50s (vedantam et al., 2015). we use the public splits as in karpathy feifei (2015) for coco. pascal50s is used only for testing save validation images used to tune hyperparameters. we train a captioning model (vinyals et al., 2015) using the neuraltalk21 code repository.. as it can be observed from table 1, dbs outperforms both bs and li jurafsky (2016) on both datasets. we observe that gains on pascal50s are more pronounced (7.24 and oracle20 improvements against bs and li jurafsky (2016)) than coco. this suggests diverse predictions are especially advantageous when there is a mismatch between training and testing sets making dbs a better inference strategy in realworld applications.. table also shows the number of distinct ngrams produced by different techniques. our method produces significantly more distinct ngrams (almost increase in the number of 4grams produced) as compared to bs. we also note that our method tends to produce slightly longer captions compared to beam search on average. moreover, on the pascal50s test split we observe that dbs finds more likely top1 solutions on average dbs obtains a maximum logprobability of as against got by bs of same beam width. while the performance of dbs is guaranteed to be better than a bs of size b/g, this experimental evidence suggests that using dbs as a replacement to bs leads to better or at least comparable performance.. human studies. to evaluate human preference between captions generated by dbs and bs, we perform a human study via amazon mechanical turk using all images of pascal50s. for each image, both dbs and standard bs captions are shown to different users. they are then asked which of the two robots understands the image better displaying intelligent humanlike behavior? in this forcedchoice test, dbs captions were preferred over bs of the time.. is diversity always needed? while these results show that diversity in outputs is important for systems that interact with consumers, is diversity always beneficial? while images with many objects (e.g., a park or a living room) can be described in multiple ways, the same is not true when there are few objects (e.g., a close up of a cat or a selfie). this notion is studied by ionescu et al. (2016), which defines a difficulty score: the human response time for solving a visual search task. on the pascal50s dataset, we observe a positive correlation ( 0.73) between difficulty scores and humans preferring dbs to bs. while dbs is generally preferred by humans for difficult images, there seems to be no such strong preference between the two in easier images. details of both the human study and the correlation experiments are provided in the supplementary.. 1https://github.com/karpathy/neuraltalk2 dataset and models. we use the englishfrench parallel data from the europarl corpus as the training set. we report results on newstest2013 and newstest2014 and use the newstest2012 to tune dbs parameters. we train a encoderdecoder architecture as proposed in bahdanau et al. (2014) using the dl4mttutorial2 code repository. the encoder consists of a bidirectional recurrent network (gated recurrent unit) with attention. we use sentence level bleu scores (papineni et al., 2002) to compute oracle metrics and report distinct ngrams similar to imagecaptioning. from table 2, we see that dbs consistently outperforms standard baselines with respect to both metrics. we also report results on another novel task visual question generation (mostafazadeh et al., 2016). we use the vqa dataset (antol et al., 2015) to train a model similar to image captioning. instead of captions, the training set now consists of questions per image. vqa requires the model to reason about multiple problems that are central to vision like the position and color of an object, relationships between objects and natural language. similarly, learning to ask the right questions pertinent to the image also requires the model to reason about these finer aspects making question generation an interesting task.. while using beam search to sample outputs results in similarly worded questions (see fig. 3), dbs brings out the details captured by the model in other modes. this promotes diverse questions of different types as defined by antol et al. we observe that the number of question types generated per image increases from to by employing dbs (at b 6). beam search is the most commonly used approximate inference algorithm to decode sequences from rnns; however, it suffers from a lack of diversity. producing multiple highly similar and generic outputs is not only wasteful in terms of computation but also detrimental for tasks with inherent ambiguity like image captioning. in this work, we presented diverse beam search, which describes. 2https://github.com/nyudl/dl4mttutorial. beam search as an optimization problem and augments the objective with a diversity term. the result is a doubly greedy\u2019 approximate algorithm that produces diverse decodings while using about the same time and resources as beam search. our method consistently outperforms beam search and other baselines across all our experiments without extra computation or taskspecific overhead. dbs is taskagnostic and can be applied to any case where bs is used making it applicable in multiple domains. our implementation will be made publicly available.", "summary": "[code]([url]), [live demo]([url]/) ([code for demo site]( [url])). diverse beam search (dbs) is an alternative to beam search (bs). decodes diverse lists by dividing the beam budget b (e.g. 3) and enforcing diversity between groups of beams.. for every time step t iterate over all groups. 2) partial beams y1[t] \\y1b,t : b \\in [b]\\ using bs with nll. in 2nd group find partial beams y2b,t using bs with partial beam score taken to be the sum of nll and the distance between the partial beam and the partial beams in 1st group. the distance is multiplied by a factor \\lambdat. for group g the distance is measured between the partial beam ygb,t and all the partial beams in all groups that were already optimized for current time step. oracle accuracy: maximum value of the metric (bleu) over a list of final beams. diversity statistics: number of distinct ngrams in all final beams. gb allows for the maximum exploration and found to improve oracle accuracy.. \\lambda \\in [0.20.8]. distance between partial beam and all other groups is broken to a sum of the distances with each group:. \\delta(y1[t],\\ldots,yg1[t]) \\sumg1h1\\delta(yh[t]). individual \\delta(yh[t])[ygb,t] is taken to be one of:. hamming (gives best oracle performance): proportional to the number of times latest token in ygb,t was selected as latest token in beams in yh[t].. cumulative: cancels out hamming: \\exp\\(\\sum\\tau \\in t \\sumb \\in b \\mathbb1[yhb,\\tau ngram: number of times each ngram in a candidate occurred in previous groups. neuralembedding: in all previous methods replace hamming similarity with cosine of word2vec of token (or sum of word2vec of ngram tokens). once a beam reaches eos you need to stop comparing it with other groups. using dbs cause results to be longer. you can reduce length by adding a penalty to length"}, {"document": "neural machine translation, a recently proposed approach to machine translation based purely on neural networks, has shown promising results compared to the existing approaches such as phrasebased statistical machine translation. despite its recent success, neural machine translation has its limitation in handling a larger vocabulary, as training complexity as well as decoding complexity increase proportionally to the number of target words. in this paper, we propose a method based on importance sampling that allows us to use a very large target vocabulary without increasing training complexity. we show that decoding can be efficiently done even with the model having a very large target vocabulary by selecting only a small subset of the whole target vocabulary. the models trained by the proposed approach are empirically found to match, and in some cases outperform, the baseline models with a small vocabulary as well as the lstmbased neural machine translation models. furthermore, when we use an ensemble of a few models with very large target vocabularies, we achieve performance comparable to the state of the art (measured by bleu) on both the englishgerman and englishfrench translation tasks of wmt\u201914. neural machine translation (nmt) is a recently introduced approach to solving machine translation (kalchbrenner and blunsom, 2013; bahdanau et al., 2015; sutskever et al., 2014). in neural machine translation, one builds a single neural network that reads a source sentence and generates. the whole neural network is jointly trained to maximize the conditional probability of a correct translation given a source sentence, using the bilingual corpus. the nmt models have shown to perform as well as the most widely used conventional translation systems (sutskever et al., 2014; bahdanau et al., 2015).. neural machine translation has a number of advantages over the existing statistical machine translation system, specifically, the phrasebased system (koehn et al., 2003). first, nmt requires a minimal set of domain knowledge. for instance, all of the models proposed in (sutskever et al., 2014), (bahdanau et al., 2015) or (kalchbrenner and blunsom, 2013) do not assume any linguistic property in both source and target sentences except that they are sequences of words. second, the whole system is jointly trained to maximize the translation performance, unlike the existing phrasebased system which consists of many separately trained features whose weights are then tuned jointly. lastly, the memory footprint of the nmt model is often much smaller than the existing system which relies on maintaining large tables of phrase pairs.. despite these advantages and promising results, there is a major limitation in nmt compared to the existing phrasebased approach. that is, the number of target words must be limited. this is mainly because the complexity of training and using an nmt model increases as the number of target words increases.. a usual practice is to construct a target vocabulary of the k most frequent words (a socalled shortlist), where k is often in the range of 30k (bahdanau et al., 2015) to 80k (sutskever et al., 2014). any word not included in this vocabulary is mapped to a special token representing an unknown word [unk]. this approach works well when there are only a few unknown words in the target sentence, but it has been observed. that the translation performance degrades rapidly as the number of unknown words increases (cho et al., 2014a; bahdanau et al., 2015).. in this paper, we propose an approximate training algorithm based on (biased) importance sampling that allows us to train an nmt model with a much larger target vocabulary. the proposed algorithm effectively keeps the computational complexity during training at the level of using only a small subset of the full vocabulary. once the model with a very large target vocabulary is trained, one can choose to use either all the target words or only a subset of them.. we compare the proposed algorithm against the baseline shortlistbased approach in the tasks of englishfrench and englishgerman translation using the nmt model introduced in (bahdanau et al., 2015). the empirical results demonstrate that we can potentially achieve better translation performance using larger vocabularies, and that our approach does not sacrifice too much speed for both training and decoding. furthermore, we show that the model trained with this algorithm gets the best translation performance yet achieved by single nmt models on the wmt\u201914 englishfrench translation task. limited vocabulary problem. in this section, we briefly describe an approach to neural machine translation proposed recently in (bahdanau et al., 2015). based on this description we explain the issue of limited vocabularies in neural machine translation. neural machine translation is a recently proposed approach to machine translation, which uses a single neural network trained jointly to maximize the translation performance (forcada and neco, 1997; kalchbrenner and blunsom, 2013; cho et al., 2014b; sutskever et al., 2014; bahdanau et al., 2015).. neural machine translation is often implemented as the encoderdecoder network. the encoder reads the source sentence x (x1, , xt ) and encodes it into a sequence of hidden states h (h1, , ht ):. then, the decoder, another recurrent neural network, generates a corresponding translation y (y1, , yt ) based on the encoded sequence of hidden states h:. the whole model is jointly trained to maximize the conditional logprobability of the correct translation given a source sentence with respect to the parameters of the model:. arg max n n1 tn t1 log p(ynt ynt, xn),. where (xn, yn) is the nth training pair of sentences, and tn is the length of the nth target sentence (yn). in this paper, we use a specific implementation of neural machine translation that uses an attention mechanism, as recently proposed in (bahdanau et al., 2015).. in (bahdanau et al., 2015), the encoder in eq. (1) is implemented by a bidirectional recurrent neural network such that. they used a gated recurrent unit for f (see, e.g., (cho et al., 2014b)).. the decoder, at each time, computes the context vector ct as a convex sum of the hidden states (h1, , ht ) with the coefficients 1, where a is a feedforward neural network with a single hidden layer.. a new hidden state zt of the decoder in eq. (3) is computed based on the previous hidden state zt1, previous generated symbol yt1 and the computed. the decoder also uses the gated recurrent unit, as the encoder does.. the probability of the next target word in eq. p(yt yt, x) z. exp wt (yt1, zt, ct) bt ,. where is an affine transformation followed by a nonlinear activation, and wt and bt are respectively the target word vector and the target word bias. z is the normalization constant computed by. where v is the set of all the target words. for the detailed description of the implementation, we refer the reader to the appendix of (bahdanau et al., 2015). one of the main difficulties in training this neural machine translation model is the computational complexity involved in computing the target word probability (eq. more specifically, we need to compute the dot product between the feature (yt1, zt, ct) and the word vector wt as many times as there are words in a target vocabulary in order to compute the normalization constant (the denominator in eq. this has to be done for, on average, words per sentence, which easily becomes prohibitively expensive even with a moderate number of possible target words. furthermore, the memory requirement grows linearly with respect to the number of target words. this has been a major hurdle for neural machine translation, compared to the existing nonparametric approaches such as phrasebased translation systems.. recently proposed neural machine translation models, hence, use a shortlist of 30k to 80k most frequent words (bahdanau et al., 2015; sutskever et al., 2014). this makes training more feasible, but comes with a number of problems. first of all, the performance of the model degrades heavily if the translation of a source sentence requires many words that are not included in the shortlist (cho et al., 2014a). this also affects the performance evaluation of the system which is often measured by bleu. second, the first issue becomes more. problematic with languages that have a rich set of words such as german or other highly inflected languages.. there are two modelspecific approaches to this issue of large target vocabulary. the first approach is to stochastically approximate the target word probability. this has been proposed recently in (mnih and kavukcuoglu, 2013; mikolov et al., 2013) based on noisecontrastive estimation (gutmann and hyvarinen, 2010). in the second approach, the target words are clustered into multiple classes, or hierarchical classes, and the target probability p(ytyt, x) is factorized as a product of the class probability p(ctyt, x) and the intraclass word probability p(ytct, yt, x). this reduces the number of required dotproducts into the sum of the number of classes and the words in a class. these approaches mainly aim at reducing the computational complexity during training, but do not often result in speedup when decoding a translation during test time.1. other than these modelspecific approaches, there exist translationspecific approaches. a translationspecific approach exploits the properties of the rare target words. for instance, luong et al. proposed such an approach for neural machine translation (luong et al., 2015). they replace rare words (the words that are not included in the shortlist) in both source and target sentences into corresponding oovn tokens using the word alignment model. once a source sentence is translated, each oovn in the translation will be replaced based on the source word marked by the corresponding oovn.. it is important to note that the modelspecific approaches and the translationspecific approaches are often complementary and can be used together to further improve the translation performance and reduce the computational complexity. in this paper, we propose a modelspecific approach that allows us to train a neural machine translation model with a very large target vocabulary. with the proposed approach, the compu. 1this is due to the fact that the beam search requires the conditional probability of every target word at each time step regardless of the parametrization of the output probability.. tational complexity of training becomes constant with respect to the size of the target vocabulary. furthermore, the proposed approach allows us to efficiently use a fast computing device with limited memory, such as a gpu, to train a neural machine translation model with a much larger target vocabulary.. as mentioned earlier, the computational inefficiency of training a neural machine translation model arises from the normalization constant in eq. in order to avoid the growing complexity of computing the normalization constant, we propose here to use only a small subset v of the target vocabulary at each update. the proposed approach is based on the earlier work of (bengio and senecal, 2008).. let us consider the gradient of the logprobability of the output in eq. the gradient is composed of a positive and negative part:. log p(yt yt, x) (8) e(yt) k:ykv p(yk yt, x)e(yk),. where we define the energy e as. the second, or negative, term of the gradient is in essence the expected gradient of the energy:. the main idea of the proposed approach is to approximate this expectation, or the negative term of the gradient, by importance sampling with a small number of samples. given a predefined proposal distribution q and a set v of samples from q, we approximate the expectation in eq. this approach allows us to compute the normalization constant during training using only a small subset of the target vocabulary, resulting in much lower computational complexity for each parameter update. intuitively, at each parameter update,. we update only the vectors associated with the correct word wt and with the sampled words in v once training is over, we can use the full target vocabulary to compute the output probability of each target word.. although the proposed approach naturally addresses the computational complexity, using this approach naively does not guarantee that the number of parameters being updated for each sentence pair, which includes multiple target words, is bounded nor can be controlled. this becomes problematic when training is done, for instance, on a gpu with limited memory.. in practice, hence, we partition the training corpus and define a subset v of the target vocabulary for each partition prior to training. before training begins, we sequentially examine each target sentence in the training corpus and accumulate unique target words until the number of unique target words reaches the predefined threshold the accumulated vocabulary will be used for this partition of the corpus during training. we repeat this until the end of the training set is reached. let us refer to the subset of target words used for the ith partition by v i this may be understood as having a separate proposal distribution qi for each partition of the training corpus. the distribution qi assigns equal probability mass to all the target words included in the subset v i , and zero probability mass to all the other words, i.e.,. this choice of proposal distribution cancels out the correction term logq(yk) from the importance weight in eqs. (10)(11), which makes the proposed approach equivalent to approximating the exact output probability in eq. wt (yt1, zt, ct) bt k:ykv exp wk (yt1, zt, ct) bk. it should be noted that this choice of q makes the estimator biased.. the proposed procedure results in speed up against usual importance sampling, as it exploits the advantage of modern computers in doing matrixmatrix vs matrixvector multiplications. the parametrization of the output probability in eq. (6) can be understood as arranging the vectors associated with the target words such that the dot product between the most likely, or correct, target word\u2019s vector and the current hidden state is maximized. the exponentiation followed by normalization is simply a process in which the dot products are converted into proper probabilities.. as learning continues, therefore, the vectors of all the likely target words tend to align with each other but not with the others. this is achieved exactly by moving the vector of the correct word in the direction of (yt1, zt, ct), while pushing all the other vectors away, which happens when the gradient of the logarithm of the exact output probability in eq. our approximate approach, instead, moves the word vectors of the correct words and of only a subset of sampled target words (those included in v ). once the model is trained using the proposed approximation, we can use the full target vocabulary when decoding a translation given a new source sentence. although this is advantageous as it allows the trained model to utilize the whole vocabulary when generating a translation, doing so may be too computationally expensive, e.g., for realtime applications.. since training puts the target word vectors in the space so that they align well with the hidden state of the decoder only when they are likely to be a correct word, we can use only a subset of candidate target words during decoding. this is similar to what we do during training, except that at test time, we do not have access to a set of correct target words.. the most nave way to select a subset of candidate target words is to take only the topk most frequent target words, where k can be adjusted to meet the computational requirement. this, however, effectively cancels out the whole purpose of training a model with a very large target vocabulary. instead, we can use an existing word alignment model to align the source and target words in the training corpus and build a dictionary. with the dictionary, for each source sentence, we construct a target word set consisting of the kmost frequent words (according to the estimated unigram probability) and, using the dictionary, at most k likely target words for each source word. k and k may be chosen either to meet the computational requirement or to maximize the translation performance on the development set. we call a subset constructed in either of these ways a candidate list. in the experiments, we evaluate the proposed approach with the neural machine translation model called rnnsearch (bahdanau et al., 2015) (see sec. in this model, as a part of decoding process, we obtain the alignments between the target words and source locations via the alignment model in eq. (5).. we can use this feature to infer the source word to which each target word was most aligned (indicated by the largest t in eq. this is especially useful when the model generated an [unk] token. once a translation is generated given a source sentence, each [unk] may be replaced using a translationspecific technique based on the aligned source word. for instance, in the experiment, we try replacing each [unk] token with the aligned source word or its most likely translation determined by another word alignment model. other techniques such as transliteration may also be used to further improve the performance (koehn, 2010). we evaluate the proposed approach in englishfrench and englishgerman translation tasks. we trained the neural machine translation models using only the bilingual, parallel corpora made available as a part of wmt\u201914. for each pair, the datasets we used are:. englishfrench:2 common crawl news commentary gigaword europarl v7 un. englishgerman: common crawl news commentary europarl v7. 2the preprocessed data can be found and downloaded from http://wwwlium.univlemans.fr/ schwenk/nnmtsharedtask/readme.. to ensure fair comparison, the englishfrench corpus, which comprises approximately million sentences, is identical to the one used in (kalchbrenner and blunsom, 2013; bahdanau et al., 2015; sutskever et al., 2014). as for englishgerman, the corpus was preprocessed, in a manner similar to (peitz et al., 2014; li et al., 2014), in order to remove many poorly translated sentences.. we evaluate the models on the wmt\u201914 test set (newstest 2014),3 while the concatenation of newstest2012 and newstest2013 is used for model selection (development set). table presents data coverage w.r.t. the vocabulary size, on the target side.. unless mentioned otherwise, all reported bleu scores (papineni et al., 2002) are computed with the multibleu.perl script4 on the cased tokenized translations. as a baseline for englishfrench translation, we use the rnnsearch model proposed by (bahdanau et al., 2015), with 30k source and target words.5 another rnnsearch model is trained for englishgerman translation with 50k source and target words.. for each language pair, we train another set of rnnsearch models with much larger vocabularies of 500k source and target words, using the proposed approach. we call these models rnnsearchlv. we vary the size of the shortlist used during training ( in sec. 3to compare with previous submissions, we use the filtered test sets.. 4https://github.com/mosessmt/ mosesdecoder/blob/master/scripts/ generic/multibleu.perl. 5the authors of (bahdanau et al., 2015) gave us access to their trained models. we chose the best one on the validation set and resumed training.. 15k and 30k for englishfrench, and 15k and 50k for englishgerman. we later report the results for the best performance on the development set, with models generally evaluated every twelve hours. the training speed is approximately the same as for rnnsearch. using a ti or titan black gpu, we could process 100k minibatches of sentences in about and hours respectively for 15k and 50k.. for both language pairs, we also trained new models, with 15k and 50k, by reshuffling the dataset at the beginning of each epoch. while this causes a nonnegligible amount of overhead, such a change allows words to be contrasted with different sets of other words each epoch.. to stabilize parameters other than the word embeddings, at the end of the training stage, we freeze the word embeddings and tune only the other parameters for approximately two more days after the peak performance on the development set is observed. this helped increase bleu scores on the development set.. we use beam search to generate a translation given a source. during beam search, we keep a set of hypotheses and normalize probabilities by the length of the candidate sentences, as in (cho et al., 2014a).6 the candidate list is chosen to maximize the performance on the development set, for k 15k, 30k, 50k and k 10, 3.2, we test using a bilingual dictionary to accelerate decoding and to replace unknown words in translations. the bilingual dictionary is built using fast align (dyer et al., 2013). we use the dictionary only if a word starts with a lowercase letter, and otherwise, we copy the source word directly. this led to better performance on the development sets.. note on ensembles for each language pair, we began training four models from each of which two points corresponding to the best and secondbest performance on the development set were collected. we continued training from each point, while keeping the word embeddings fixed, until the best development performance was reached, and took the model at this point as a single model in an ensemble. this procedure resulted in a total of eight models from which we averaged the lengthnormalized logprobabilities. since much of training had been shared, the composition of. 6these experimental details differ from (bahdanau et al., 2015).. such ensembles may be suboptimal. this is supported by the fact that higher crossmodel bleu scores (freitag et al., 2014) are observed for models that were partially trained together. in table 2, we present the results obtained by the trained models with very large target vocabularies, and alongside them, the previous results reported in (sutskever et al., 2014), (luong et al., 2015), (buck et al., 2014) and (durrani et al., 2014). without translationspecific strategies, we can clearly see that the rnnsearchlv outperforms the baseline rnnsearch.. in the case of the englishfrench task, rnnsearchlv approached the performance level of the previous best single neural machine translation (nmt) model, even without any translationspecific techniques (sec. with these, however, the rnnsearchlv outperformed it. the performance of the rnnsearchlv is also better than that of a standard phrasebased translation system (cho et al., 2014b). furthermore, by combining models, we were able to achieve a translation performance comparable to the state of the art, measured in bleu.. for englishgerman, the rnnsearchlv out. performed the baseline before unknown word replacement, but after doing so, the two systems performed similarly. we could reach higher largevocabulary singlemodel performance by reshuffling the dataset, but this step could potentially also help the baseline. in this case, we were able to surpass the previously reported best translation result on this task by building an ensemble of models.. with 15k, the rnnsearchlv performance worsened a little, with best bleu scores, without reshuffling, of and respectively for englishfrench and englishgerman.. the englishgerman ensemble described in this paper has also been used for the shared translation task of the 10th workshop on statistical machine translation (wmt\u201915), where it was ranked first in terms of bleu score. the translations by this ensemble can be found online.7 in table 3, we present the timing information of decoding for different models. clearly, decoding from rnnsearchlv with the full target vocab. 7http://matrix.statmt.org/matrix/ output/1774?runid4079. if we use a candidate list for decoding each translation, the speed of decoding substantially improves and becomes close to the baseline rnnsearch.. a potential issue with using a candidate list is that for each source sentence, we must rebuild a target vocabulary and subsequently replace a part of the parameters, which may easily become timeconsuming. we can address this issue, for instance, by building a common candidate list for multiple source sentences. by doing so, we were able to match the decoding speed of the baseline rnnsearch model. for englishfrench ( 30k), we evaluate the influence of the target vocabulary when translating the test sentences by using the union of a fixed set of 30k common words and (at most) k likely candidates for each source word according to the dictionary. results are presented in figure with k (not shown), the performance of the system is comparable to the baseline when not replacing the unknown words (30.12), but there is not as much improvement when doing so (31.14). as the large vocabulary model does not predict [unk] as much during training, it is less likely to generate it when decoding, limiting the effectiveness of the postprocessing step in this case. with k 1, which limits the diversity of allowed uncommon words, bleu is not as good as with moderately larger k , which indicates that our models can, to some degree, correctly choose between rare alternatives. if we rather use k 50k, as we did for testing based on validation performance, the improvement over k is approximately bleu.. when validating the choice of k, we found it to be correlated with the value of used during. for example, on the englishfrench validation set, with 15k (and k 10), the bleu score is with k 15k, but drops to and respectively for k 30k and 50k. for 30k, the score increases moderately from k 15k to k 50k. a similar effect was observed for englishgerman and on the test sets. as our implementation of importance sampling does not apply the usual correction to the gradient, it seems beneficial for the test vocabularies to resemble those used during training. in this paper, we proposed a way to extend the size of the target vocabulary for neural machine translation. the proposed approach allows us to train a model with much larger target vocabulary without any substantial increase in computational complexity. it is based on the earlier work in (bengio and senecal, 2008) which used importance sampling to reduce the complexity of computing the normalization constant of the output word probability in neural language models.. on englishfrench and englishgerman translation tasks, we observed that the neural machine translation models trained using the proposed method performed as well as, or better than, those using only limited sets of target words, even when replacing unknown words. as performance of the rnnsearchlv models increased when only a selected subset of the target vocabulary was used during decoding, this makes the proposed learning algorithm more practical.. when measured by bleu, our models showed translation performance comparable to the. stateoftheart translation systems on both the englishfrench task and englishgerman task. on the englishfrench task, a model trained with the proposed approach outperformed the best single neural machine translation (nmt) model from (luong et al., 2015) by approximately bleu point. the performance of the ensemble of multiple models, despite its relatively less diverse composition, is approximately bleu points away from the best system (luong et al., 2015). on the englishgerman task, the best performance of bleu by our model is higher than that of the previous state of the art (20.67) reported in (buck et al., 2014).. finally, we release the source code used in our experiments to encourage progress in neural machine translation.8 the authors would like to thank the developers of theano (bergstra et al., 2010; bastien et al., 2012). we acknowledge the support of the following agencies for research funding and computing support: nserc, calcul quebec, compute canada, the canada research chairs, cifar and samsung.", "summary": "tldr; the authors propose an importancesampling approach to deal with large vocabularies in nmt models. during training, the corpus is partitioned, and for each partition only target words occurring in that partition are chosen. to improve decoding speed over the full vocabulary, the authors build a dictionary mapping from source sentence to potential target vocabulary. the authors evaluate their approach on standard mt tasks and perform better than the baseline models with smaller vocabulary.. computing partition function is the bottleneck. use samplingbased approach.. dealing with large vocabulary during training is separate from dealing with large vocab during decoding. training is handled with importance sampling. decoding is handled with sourcebased candidate list.. decoding with candidate list takes around 0.12s (0.05) per token on cpu (gpu). without target list 0.8s (0.25s).. issue: candidate list is depended on source sentence, so it must be recomputed for each sentence. reshuffling the data set is expensive as new partitions need to be calculated (not necessary, but improved scores).. how is the corpus partitioned? whats the effect of the partitioning strategy?. the authors say that they replace unk tokens using another word alignment model but dont go into detail what this is. the results show that doing this results in much larger score bump than increasing the vocab does. (the authors do this for all comparison models though).. reshuffling the dataset also results in a significant performance bump, but this operation is expensive. imo the authors should take all these into account when reporting performance numbers. a single training update may be a lot faster, but the setup time increases. id wouldve like to see the authors assign a global time budget to train/test and then compare the models based on that.. the authors only briefly mentioned that rebuilding the target vocab for each source sentence is an issue and how they solve it, no details given."}, {"document": "a generative recurrent neural network is quickly trained in an unsupervised manner to model popular reinforcement learning environments through compressed spatiotemporal representations. the world model\u2019s extracted features are fed into compact and simple policies trained by evolution, achieving state of the art results in various environments. we also train our agent entirely inside of an environment generated by its own internal world model, and transfer this policy back into the actual environment. interactive version of paper: https://worldmodels.github.io humans develop a mental model of the world based on what they are able to perceive with their limited senses, learning abstract representations of both spatial and temporal aspects of sensory inputs. for instance, we are able to observe a scene and remember an abstract description thereof [7, 67]. our decisions and actions are influenced by our internal predictive model. for example, what we perceive at any given moment seems to be governed by our predictions of the future [52, 59]. one way of understanding the predictive model inside our brains is that it might not simply be about predicting the future in general, but predicting future sensory data given our current motor actions [38, 48]. we are able to instinctively act on this predictive model and perform fast reflexive behaviours when we face danger [55], without the need to consciously plan out a course of action [52].. for many reinforcement learning (rl) problems [37, 96, 106], an artificial rl agent may also benefit from a predictive model (m) of the future [95, 104] (modelbased rl). the backpropagation algorithm [39, 50, 103] can be used to train a large m in form of a neural network (nn). in partially observable environments, we can implement m through a recurrent neural network (rnn) [49, 74, 75, 78] to allow for better predictions based on memories of previous observation sequences.. figure 1: we build probabilistic generative models of openai gym [5] environments. these models can mimic the actual environments (left). we test trained policies in the actual environments (right).. in fact, our m will be a large rnn that learns to predict the future given the past in an unsupervised manner. m\u2019s internal representations of memories of past observations and actions are perceived and exploited by another nn called the controller (c) which learns through rl to perform some task without a teacher. a small and simple c limits c\u2019s credit assignment problem to a comparatively small search space, without sacrificing the capacity and expressiveness of the large and complex m.. 32nd conference on neural information processing systems (nips 2018), montral, canada.. ar x. iv :1. we combine several key concepts from a series of papers from on rnnbased world models and controllers [74, 75, 76, 78, 83] with more recent tools from probabilistic modelling, and present a simplified approach to test some of those key concepts in modern rl environments [5]. experiments show that our approach can be used to solve a challenging race car navigation from pixels task that previously has not been solved using more traditional methods.. most existing modelbased rl approaches learn a model of the rl environment, but still train on the actual environment. here, we also explore fully replacing an actual rl environment with a generated one, training our agent\u2019s controller c only inside of the environment generated by its own internal world model m, and transfer this policy back into the actual environment.. to overcome the problem of an agent exploiting imperfections of the generated environments, we adjust a temperature parameter of m to control the amount of uncertainty of the generated environments. we train c inside of a noisier and more uncertain version of its generated environment, and demonstrate that this approach helps prevent c from taking advantage of the imperfections of m. we will also discuss other related works in the modelbased rl literature that share similar ideas of learning a dynamics model and training an agent using this model. our simple model is inspired by our own cognitive system. our agent has a visual sensory component v that compresses what it sees into a small representative code. it also has a memory component m that makes predictions about future codes based on historical information. finally, our agent has a decisionmaking component c that decides what actions to take based only on the representations created by its vision and memory components.. the environment provides our agent with a high dimensional input observation at each time step. this input is usually a 2d image frame that is part of a video sequence. the role of v is to learn an abstract, compressed representation of each observed input frame. here, we use a variational autoencoder (vae) [42, 71] as v to compress each image frame into a latent vector z.. while v\u2019s role is to compress what the agent sees at each time frame, we also want to compress what happens over time. the rnn m serves as a predictive model of future z vectors that v is expected to produce. since many complex environments are stochastic in nature, we train our rnn to output a probability density function p(z) instead of a deterministic prediction of z.. in our approach, we approximate p(z) as a mixture of gaussian distribution, and train m to output the probability distribution of the next latent vector zt1 given the current and past information made available to it. more specifically, the rnn will model p (zt1 at, zt, ht), where at is the action taken at time t and ht is the hidden state of the rnn at time t. during sampling, we can adjust a temperature parameter to control model uncertainty, as done in previous work [28]. we will find that adjusting to be useful for training our controller later on. this approach is known as a mixture density network [3] combined with an rnn (mdnrnn) [24], and has been applied in the past for sequence generation problems such as generating handwriting [24] and sketches [28].. c is responsible for determining the course of actions to take in order to maximize the expected cumulative reward of the agent during a rollout of the environment. in our experiments, we deliberately. make c as simple and small as possible, and train it separately from v and m, so that most of our agent\u2019s complexity resides in v and m. c is a simple single layer linear model that maps zt and ht directly to action at at each time step: at wc [zt ht] bc. in this linear model, wc and bc are the parameters that map the concatenated input vector [zt ht] to the output action vector at.. this minimal design for c also offers important practical benefits. advances in deep learning provided us with the tools to train large, sophisticated models efficiently, provided we can define a wellbehaved, differentiable loss function. v and m are designed to be trained efficiently with the backpropagation algorithm using modern gpu accelerators, so we would like most of the model\u2019s complexity, and model parameters to reside in v and m. the number of parameters of c, a linear model, is minimal in comparison. this choice allows us to explore more unconventional ways to train c for example, even using evolution strategies (es) [70, 87] to tackle more challenging rl tasks where the credit assignment problem is difficult.. to optimize the parameters of c, we chose the covariancematrix adaptation evolution strategy (cmaes) [29, 30] as our optimization algorithm since it is known to work well for solution spaces of up to a few thousand parameters. we evolve parameters of c on a single machine with multiple cpu cores running multiple rollouts of the environment in parallel. for more information about the models, training procedures, and experiment configurations, please see the supplementary materials. in this section, we describe how we can train the agent model described earlier to solve a car racing task. to our knowledge, our agent is the first known to solve this task.1. frame compressor v and predictive model m can help us extract useful representations of space and time. by using these features as inputs of c, we can train a compact c to perform a continuous control task, such as learning to drive from pixel inputs for a topdown car racing environment called carracingv0 [44]. in this environment, the tracks are randomly generated for each trial, and our agent is rewarded for visiting as many tiles as possible in the least amount of time. the agent controls three continuous actions: steering left/right, acceleration, and brake.. algorithm training procedure in our experiments. collect 10,000 rollouts from a random policy. train vae (v) to encode frames into z rnz train mdnrnn (m) to model p (zt1 at, zt, ht). evolve controller (c) to maximize the expected cumulative reward of a rollout.. to train v, we first collect a dataset of 10k random rollouts of the environment. we have first an agent acting randomly to explore the environment multiple times, and record the random actions at taken and the resulting observations from the environment. we use this dataset to train our vae to encode each frame into low dimensional latent vector z by minimizing the difference between a given frame and the reconstructed version of the frame produced by the decoder from z. we can now use our trained v to preprocess each frame at time t into zt to train our m. using this preprocessed data, along with the recorded random actions at taken, our mdnrnn can now be trained to model p (zt1 at, zt, ht) as a mixture of gaussians. in this experiment, v and m have no knowledge about the actual reward signals from the environment. their task is simply to compress and predict the sequence of image frames observed. only c has access to the reward information from the environment. since there are a mere parameters inside the linear c, evolutionary algorithms such as cmaes are well suited for this optimization task.. 1we find this task interesting because although it is not difficult to train an agent to wobble around randomly generated tracks and obtain a mediocre score, carracingv0 defines solving as getting average reward of over consecutive trials, which means the agent can only afford very few driving mistakes.. 2although in principle, we can train v and m together in an endtoend manner, we found that training each separately is more practical, achieves satisfactory results, and does not require exhaustive hyperparameter tuning. as images are not required to train m on its own, we can even train on large batches of long sequences of latent vectors encoding the entire frames of an episode to capture longer term dependencies, on a single gpu. training an agent to drive is not a difficult task if we have a good representation of the observation. previous works [35, 46] have shown that with a good set of handengineered information about the observation, such as lidar information, angles, positions and velocities, one can easily train a small feedforward network to take this handengineered input and output a satisfactory navigation policy. for this reason, we first want to test our agent by handicapping c to only have access to v but not m, so we define our controller as at wc zt bc.. although the agent is still able to navigate the race track in this setting, we notice it wobbles around and misses the tracks on sharper corners, e.g., see figure (right). this handicapped agent achieved an average score of 251, in line with the performance of other agents on openai gym\u2019s leaderboard [44] and traditional deep rl methods such as a3c [36, 41]. adding a hidden layer to c\u2019s policy network helps to improve the results to 141, but not enough to solve this environment. the representation zt provided by v only captures a representation at a moment in time and does not have much predictive power. in contrast, m is trained to do one thing, and to do it really well, which is to predict zt1. since m\u2019s prediction of zt1 is produced from the rnn\u2019s hidden state ht at time t, ht is a good candidate for a feature vector we can give to our agent. combining zt with ht gives c a good representation of both the current observation, and what to expect in the future.. we see that allowing the agent to access both zt and ht greatly improves its driving capability. the driving is more stable, and the agent is able to seemingly attack the sharp corners effectively. furthermore, we see that in making these fast reflexive driving decisions during a car race, the agent does not need to plan ahead and roll out hypothetical scenarios of the future. since ht contain information about the probability distribution of the future, the agent can just reuse the rnn\u2019s internal representation instinctively to guide its action decisions. like a formula one driver or a baseball player hitting a fastball [52], the agent can instinctively predict when and where to navigate in the heat of the moment.. our agent is able to achieve a score of 21, effectively solving the task and obtaining new state of the art results. previous attempts [36, 41] using deep rl methods obtained average scores of range, and the best reported solution on the leaderboard obtained an average score of traditional deep rl methods often require preprocessing of each frame, such as employing edgedetection [36], in addition to stacking a few recent frames [36, 41] into the input. in contrast, our agent\u2019s v and m take in a stream of raw rgb pixel images and directly learn a spatiotemporal representation. to our knowledge, our method is the first reported solution to solve this task.. since our agent\u2019s world model is able to model the future, we can use it to come up with hypothetical car racing scenarios on its own. we can use it to produce the probability distribution of zt1 given the current states, sample a zt1 and use this sample as the real observation. we can put our trained c back into this generated environment. figure (left) shows a screenshot of the generated car racing environment. the interactive version of this work includes a demo of the generated environments. we have just seen that a policy learned inside of the real environment appears to somewhat function inside of the generated environment. this begs the question can we train our agent to learn inside of its own generated environment, and transfer this policy back to the actual environment?. if our world model is sufficiently accurate for its purpose, and complete enough for the problem at hand, we should be able to substitute the actual environment with this world model. after all, our agent does not directly observe the reality, but merely sees what the world model lets it see. in this experiment, we train an agent inside the environment generated by its world model trained to mimic a vizdoom [40] environment. in doomtakecoverv0 [62], the agent must learn to avoid fireballs shot by monsters from the other side of the room with the sole intent of killing the agent. the cumulative reward is defined to be the number of time steps the agent manages to stay alive during a rollout. each rollout of the environment runs for a maximum of time steps, and the task is considered solved if the average survival time over consecutive rollouts is greater than time steps. the setup of our vizdoom experiment is largely the same as the car racing task, except for a few key differences. in the car racing task, m is only trained to model the next zt. since we want to build a world model we can train our agent in, our m model here will also predict whether the agent dies in the next frame (as a binary event donet), in addition to the next frame zt.. since m can predict the done state in addition to the next observation, we now have all of the ingredients needed to make a full rl environment to mimic doomtakecoverv0 [62]. we first build an openai gym environment interface by wrapping a gym.env [5] interface over our m as if it were a real gym environment, and then train our agent inside of this virtual environment instead of using the actual environment. thus in our simulation, we do not need the v model to encode any real pixel frames during the generation process, so our agent will therefore only train entirely in a more efficient latent space environment. both virtual and actual environments share an identical interface, so after the agent learns a satisfactory policy inside of the virtual environment, we can easily deploy this policy back into the actual environment to see how well the policy transfers over.. here, our rnnbased world model is trained to mimic a complete game environment designed by human programmers. by learning only from raw image data collected from random episodes, it learns how to simulate the essential aspects of the game, such as the game logic, enemy behaviour, physics, and also the 3d graphics rendering. we can even play inside of this generated environment.. unlike the actual game environment, however, we note that it is possible to add extra uncertainty into the virtual environment, thus making the game more challenging in the generated environment. we can do this by increasing the temperature parameter during the sampling process of zt1. by increasing the uncertainty, our generated environment becomes more difficult compared to the actual environment. the fireballs may move more randomly in a less predictable path compared to the actual game. sometimes the agent may even die due to sheer misfortune, without explanation.. after training, our controller learns to navigate around the virtual environment and escape from deadly fireballs launched by monsters generated by m. our agent achieved an average score of time steps in the virtual environment. we then took the agent trained inside of the virtual environment and tested its performance on the original vizdoom environment. the agent obtained an average score of time steps, far beyond the required score of time steps, and also much higher than the score obtained inside the more difficult virtual environment. the full results are listed in table we see that even though v is not able to capture all of the details of each frame correctly, for instance, getting the number of monsters correct, c is still able to learn to navigate in the real environment. as the virtual environment cannot even keep track of the exact number of monsters in the first place, an agent that is able to survive a noisier and uncertain generated environment can thrive in the original, cleaner environment. we also find agents that perform well in higher temperature settings generally perform better in the normal setting. in fact, increasing helps prevent our controller from taking advantage of the imperfections of our world model. we will discuss this in depth in the next section. in our childhood, we may have encountered ways to exploit video games in ways that were not intended by the original game designer [9]. players discover ways to collect unlimited lives or health, and by taking advantage of these exploits, they can easily complete an otherwise difficult game. however, in the process of doing so, they may have forfeited the opportunity to learn the skill required to master the game as intended by the game designer. in our initial experiments, we noticed that our agent discovered an adversarial policy to move around in such a way so that the monsters in this virtual environment governed by m never shoots a single fireball during some rollouts. even when there are signs of a fireball forming, the agent moves in a way to extinguish the fireballs.. because m is only an approximate probabilistic model of the environment, it will occasionally generate trajectories that do not follow the laws governing the actual environment. as we previously pointed out, even the number of monsters on the other side of the room in the actual environment is not exactly reproduced by m. for this reason, our world model will be exploitable by c, even if such exploits do not exist in the actual environment.. as a result of using m to generate a virtual environment for our agent, we are also giving the controller access to all of the hidden states of m. this is essentially granting our agent access to all of the internal states and memory of the game engine, rather than only the game observations that the player gets to see. therefore our agent can efficiently explore ways to directly manipulate the hidden states of the game engine in its quest to maximize its expected cumulative reward. the weakness of this approach of learning a policy inside of a learned dynamics model is that our agent can easily find an adversarial policy that can fool our dynamics model it will find a policy that looks good under our dynamics model, but will fail in the actual environment, usually because it visits states where the model is wrong because they are away from the training distribution.. this weakness could be the reason that many previous works that learn dynamics models of rl environments do not actually use those models to fully replace the actual environments [8, 60]. like in the m model proposed in [74, 75, 78], the dynamics model is deterministic, making it easily exploitable by the agent if it is not perfect. using bayesian models, as in pilco [10], helps to address this issue with the uncertainty estimates to some extent, however, they do not fully solve the problem. recent work [57] combines the modelbased approach with traditional modelfree rl training by first initializing the policy network with the learned policy, but must subsequently rely on modelfree methods to finetune this policy in the actual environment.. to make it more difficult for our c to exploit deficiencies of m, we chose to use the mdnrnn as the dynamics model of the distribution of possible outcomes in the actual environment, rather than merely predicting a deterministic future. even if the actual environment is deterministic, the mdnrnn would in effect approximate it as a stochastic environment. this has the advantage of allowing us to train c inside a more stochastic version of any environment we can simply adjust the temperature parameter to control the amount of randomness in m, hence controlling the tradeoff between realism and exploitability.. using a mixture of gaussian model may seem excessive given that the latent space encoded with the vae model is just a single diagonal gaussian distribution. however, the discrete modes in a mixture density model are useful for environments with random discrete events, such as whether a monster decides to shoot a fireball or stay put. while a single diagonal gaussian might be sufficient to encode individual frames, an rnn with a mixture density output layer makes it easier to model the logic behind a more complicated environment with discrete random states.. for instance, if we set the temperature parameter to a very low value of 0.1, effectively training our c with an m that is almost identical to a deterministic lstm, the monsters inside this generated environment fail to shoot fireballs, no matter what the agent does, due to mode collapse. m is not able to transition to another mode in the mixture of gaussian model where fireballs are formed and shot. whatever policy learned inside of this generated environment will achieve a perfect score of most of the time, but will obviously fail when unleashed into the harsh reality of the actual world, underperforming even a random policy.. by making the temperature an adjustable parameter of m, we can see the effect of training c inside of virtual environments with different levels of uncertainty, and see how well they transfer over to the actual environment. we experiment with varying of the virtual environment, training an agent inside of this virtual environment, and observing its performance when inside the actual environment.. in table 2, while we see that increasing of m makes it more difficult for c to find adversarial policies, increasing it too much will make the virtual environment too difficult for the agent to learn anything, hence in practice it is a hyperparameter we can tune. the temperature also affects the types of strategies the agent discovers. for example, although the best score obtained is with 1.15, increasing a notch to results in a lower score but at the same time a less risky strategy with a lower variance of returns. for comparison, the best reported score [62] is there is extensive literature on learning a dynamics model, and using this model to train a policy. many basic concepts first explored in the 1980s for feedforward neural networks (fnns) [56, 58, 72, 104, 105] and in the 1990s for rnns [74, 75, 76, 78] laid some of the groundwork for learning to think [83]. the more recent pilco [10, 53] is a probabilistic modelbased search policy method designed to solve difficult control problems. using data collected from the environment, pilco uses a gaussian process (gp) model to learn the system dynamics, and uses this model to sample many trajectories in order to train a controller to perform a desired task, such as swinging up a pendulum.. while gps work well with a small set of low dimension data, their computational complexity makes them difficult to scale up to model a large history of high dimensional observations. other recent works [12, 17] use bayesian neural networks instead of gps to learn a dynamics model. these methods have demonstrated promising results on challenging control tasks [32], where the states well defined, and the observation is relatively low dimensional. here we are interested in modelling dynamics observed from high dimensional visual data, as a sequence of raw pixel frames.. in robotic control applications, the ability to learn the dynamics of a system from observing only camerabased video inputs is a challenging but important problem. early work on rl for active vision trained an fnn to take the current image frame of a video sequence to predict the next frame [85], and use this predictive model to train a foveashifting control network trying to find targets in a visual scene. to get around the difficulty of training a dynamical model to learn directly from highdimensional pixel images, researchers explored using neural networks to first learn a compressed representation of the video frames. recent work along these lines [99, 100] was able to train controllers using the bottleneck hidden layer of an autoencoder as lowdimensional feature vectors to control a pendulum from pixel inputs. learning a model of the dynamics from a compressed latent space enable rl algorithms to be much more dataefficient [15, 101].. video game environments are also popular in modelbased rl research as a testbed for new ideas. previous work [51] used a feedforward convolutional neural network (cnn) to learn a forward simulation model of a video game. learning to predict how different actions affect future states in the environment is useful for gameplay agents, since if our agent can predict what happens in the future given its current state and action, it can simply select the best action that suits its goal. this has been demonstrated not only in early work [58, 85] (when compute was a million times more expensive than today) but also in recent studies [13] on several competitive vizdoom environments.. the works mentioned above use fnns to predict the next video frame. we may want to use models that can capture longer term time dependencies. rnns are powerful models suitable for sequence modelling [24]. using rnns to develop internal models to reason about the future has been explored as early as [74], and then further explored in [75, 76, 78]. a more recent work [83] presented a unifying framework for building an rnnbased general problem solver that can learn a world model of its environment and also learn to reason about the future using this model. subsequent works have used rnnbased models to generate many frames into the future [8, 11, 25, 60], and also as an internal model to reason about the future [68, 90, 102].. in this work, we used evolution strategies (es) to train our controller, as this offers many benefits. for instance, we only need to provide the optimizer with the final cumulative reward, rather than the entire history. es is also easy to parallelize we can launch many instances of rollout with different solutions to many workers and quickly compute a set of cumulative rewards in parallel. recent works [14, 26, 73, 94] have demonstrated that es is a viable alternative to traditional deep rl methods on many strong baselines. before the popularity of deep rl methods [54], evolutionbased algorithms have been shown to be effective at solving rl tasks [18, 21, 22, 88, 92]. evolutionbased algorithms have even been able to solve difficult rl tasks from high dimensional pixel inputs [1, 31, 45, 63]. we have demonstrated the possibility of training an agent to perform tasks entirely inside of its simulated latent space world. this approach offers many practical benefits. for instance, video game engines typically require heavy compute resources for rendering the game states into image frames, or calculating physics not immediately relevant to the game. we may not want to waste cycles training an agent in the actual environment, but instead train the agent as many times as we want inside its simulated environment. agents that are trained incrementally to simulate reality may prove to be useful for transferring policies back to the real world. our approach may complement sim2real approaches outlined in previous work [4, 33].. the choice of implementing v as a vae and training it as a standalone model also has its limitations, since it may encode parts of the observations that are not relevant to a task. after all, unsupervised learning cannot, by definition, know what will be useful for the task at hand. for instance, our vae reproduced unimportant detailed brick tile patterns on the side walls in the doom environment, but failed to reproduce taskrelevant tiles on the road in the car racing environment. by training together with an m that predicts rewards, the vae may learn to focus on taskrelevant areas of the image, but the tradeoff here is that we may not be able to reuse the vae effectively for new tasks without retraining. learning taskrelevant features has connections to neuroscience as well. primary sensory neurons are released from inhibition when rewards are received, which suggests that they generally learn taskrelevant features, rather than just any features, at least in adulthood [65].. in our experiments, the tasks are relatively simple, so a reasonable world model can be trained using a dataset collected from a random policy. but what if our environments become more sophisticated? in any difficult environment, only parts of the world are made available to the agent only after it learns how to strategically navigate through its world. for more complicated tasks, an iterative training procedure is required. we need our agent to be able to explore its world, and constantly collect new observations so that its world model can be improved and refined over time. future work will incorporate an iterative training procedure [83], where our controller actively explores parts of the environment that is beneficial to improve its world model. an exciting research direction is to look at ways to incorporate artificial curiosity and intrinsic motivation [61, 64, 77, 80, 81] and information seeking [23, 86] abilities in an agent to encourage novel exploration [47]. in particular, we can augment the reward function based on improvement in compression quality [77, 80, 81, 83].. another concern is the limited capacity of our world model. while modern storage devices can store large amounts of historical data generated using an iterative training procedure, our lstm [20, 34] based world model may not be able to store all of the recorded information inside of its weight connections. while the human brain can hold decades and even centuries of memories to some resolution [2], our neural networks trained with backpropagation have more limited capacity and suffer from issues such as catastrophic forgetting [16, 43, 69]. future work will explore replacing the vae and mdnrnn with higher capacity models [27, 89, 93, 97, 98], or incorporating an external memory module [19, 107], if we want our agent to learn to explore more complicated worlds.. like early rnnbased cm systems [74, 75, 76, 78], ours simulates possible futures time step by time step, without profiting from humanlike hierarchical planning or abstract reasoning, which often ignores irrelevant spatiotemporal details. however, the more general learning to think [83] approach is not limited to this rather naive approach. instead it allows a recurrent c to learn to address subroutines of the recurrent m, and reuse them for problem solving in arbitrary computable ways, e.g., through hierarchical planning or other kinds of exploiting parts of m\u2019s programlike weight matrix. a recent one big net [84] extension of the cm approach collapses c and m into a single network, and uses powerplaylike [82, 91] behavioural replay (where the behaviour of a teacher net is compressed into a student net [79]) to avoid forgetting old prediction and control skills when learning new ones. experiments with those more general approaches are left for future work. we would like to thank blake richards, kory mathewson, chris olah, kai arulkumaran, denny britz, kyle mcdonald, ankur handa, elwin ha, nikhil thorat, daniel smilkov, alex graves, douglas eck, mike schuster, rajat monga, vincent vanhoucke, jeff dean and natasha jaques for their thoughtful feedback, and for offering their valuable insights from their respective areas of expertise. in this section we will describe in more details the models and training methods used in this work. table 4: doomtakecoverv0 parameter count. a.2 variational autoencoder. we trained a convolutional variational autoencoder (convvae) model as our agent\u2019s v, as illustrated in figure (left). unlike vanilla autoencoders, enforcing a gaussian prior over the latent vector z also limits the amount of information capacity for compressing each frame, but this gaussian prior also makes the world model more robust to unrealistic z vectors generated by m.. as the environment may give us observations as high dimensional pixel images, we first resize each image to 64x64 pixels and use this resized image as v\u2019s observation. each pixel is stored as three floating point values between and to represent each of the rgb channels. the convvae takes in this 64x64x3 input tensor and passes it through convolutional layers to encode it into low dimension vectors and , each of size nz the latent vector z is sampled from the gaussian prior n(, i). in the car racing task, nz is while for the doom task nz is the latent vector z is passed through of deconvolution layers used to decode and reconstruct the image.. each convolution and deconvolution layer uses a stride of the layers are indicated in the diagram in italics as activationtype output channels x filter size. all convolutional and deconvolutional layers use relu activations except for the output layer as we need the output to be between and we trained the model for epoch over the data collected from a random policy, using l2 distance between the input image and the reconstruction to quantify the reconstruction loss we optimize for, in addition to kl loss. to implement m, we use an lstm [34] recurrent neural network combined with a mixture density network [3] as the output layer, as illustrated in figure (right). we use this network to model the probability distribution of the next z in the next time step as a mixture of gaussian distribution. this approach is very similar to previous work [24] in the unconditional handwriting generation section and also the decoderonly section of sketchrnn [28]. the only difference is that we did not model the correlation parameter between each element of z, and instead had the mdnrnn output a diagonal covariance matrix of a factored gaussian distribution.. unlike the handwriting and sketch generation works, rather than using the mdnrnn to model the pdf of the next pen stroke, we model instead the pdf of the next latent vector z. we would sample from this pdf at each time step to generate the environments. in the doom task, we also use the mdnrnn to predict the probability of whether the agent has died in this frame. if that probability is above 50, then we set done to be true in the virtual environment. given that death is a low probability event at each time step, we find the cutoff approach to be more stable compared to sampling from the bernoulli distribution.. the mdnrnns were trained for epochs on the data collected from a random policy agent. in the car racing task, the lstm used hidden units, in the doom task hidden units. in both tasks, we used gaussian mixtures and did not model the correlation parameter, hence z is sampled from a factored mixture of gaussian distributions.. when training the mdnrnn using teacher forcing from the recorded data, we store a precomputed set of and for each of the frames, and sample an input z n(, ) each time we construct a training batch, to prevent overfitting our mdnrnn to a specific sampled z. for both environments, we applied tanh nonlinearities to clip and bound the action space to the appropriate ranges. for instance, in the car racing task, the steering wheel has a range from to 1.0, the acceleration pedal from to 1.0, and the brakes from to in the doom environment, we converted the discrete actions into a continuous action space between to 1.0, and divided this range into thirds to indicate whether the agent is moving left, staying where it is, or moving to the right. we would give c a feature vector as its input, consisting of z and the hidden state of the mdnrnn. in the car racing task, this hidden state is the output vector h of the lstm, while for the doom task it is both the cell vector c and the output vector h of the lstm. we used covariancematrix adaptation evolution strategy (cmaes) [29] to evolve c\u2019s weights. following the approach described in evolving stable strategies [26], we used a population size of 64, and had each agent perform the task times with different initial random seeds. the agent\u2019s fitness value is the average cumulative reward of the random rollouts. the diagram below (left) charts the best performer, worst performer, and mean fitness of the population of agents at each generation:. since the requirement of this environment is to have an agent achieve an average score above over random rollouts, we took the best performing agent at the end of every generations, and tested it over random rollout scenarios to record this average on the red line. after generations, an agent was able to achieve an average score of over random rollouts. we used random rollouts rather than because each process of the core machine had been configured to run times already, effectively using a full generation of compute after every generations to evaluate the best agent times. in the figure (left) below, we plot the results of same agent evaluated over rollouts:. we also experimented with an agent that has access to only the z vector from the vae, but not the rnn\u2019s hidden states. we tried variations, where in the first variation, c maps z directly to the action space a. in second variation, we attempted to add a hidden layer with tanh activations between z and a, increasing the number of model parameters of c to 1443, making it more comparable with the original setup. these results are shown in in the figure (right). we conducted a similar experiment on the generated doom environment we called doomrnn. please note that we have not actually attempted to train our agent on the actual vizdoom environment, and had only used vizdoom for the purpose of collecting training data using a random policy. doomrnn is more computationally efficient compared to vizdoom as it only operates in latent space without the need to render an image at each time step, and we do not need to run the actual doom game engine.. in our virtual doomrnn environment we increased the temperature slightly and used to make the agent learn in a more challenging environment. the best agent managed to obtain an average score of over random rollouts. this is the highest score of the red line in figure (left). this same agent achieved an average score of over random rollouts when deployed to the actual doomtakecoverv0 [62] environment, as shown in figure (right).", "summary": "the takehome message is that the challenge of reinforcement learning for environments with highdimensional and partial observations is learning a good representation of the environment. this means learning a sensory features extractor v to deal with the highly dimensional observation (pixels for example). but also learning a temporal representation m of the environment dynamics to deal with the partial observability. if provided with such representations, learning a controller so as to maximize a reward is really easy (single linear layer evolved with cmaes). authors call these representations a world model since they can use the learned environments dynamics to simulate rollouts. they show that policies trained inside the world model transfer well back to the real environment provided that measures are taken to prevent the policy from exploiting the world models inaccuracies. learning the world model. in this work they propose to learn these representations offline in an unsupervized manner in order to be more efficient.. they use a vae for v that they train exclusively with the reconstruction loss, that way the learned representations are independent of the reward and can be used alongside any reward. they then train m as mixturedensitynetworkrnn to predict the next sensory features (as extracted by the vae) and possibly the done condition and the reward and thus learn the dynamics of the environment in the vaes latent space (which is likely simpler there than in the pixel space).. note that the vaes latent space is a single gaussian (adding stochasticity makes it more robust to the next state outputs of m), whereas m outputs next states in a mixture of gaussians. indeed, an image is likely to have one visual encoding, yet it can have multiple and different future scenarii which are captured by the multimodal output of m. the agent is provided with the visual features and ms hidden state (temporal features).. to avoid that the agent exploits this imperfect simulator they increase its dynamics stochasticity by playing with \\tau the sampling temperature of zt1 in m. if exploration is important in the environment the initial random policy might fail to collect data in all the relevant part of the environment and an iterative version of algorithm might be required (see [url]/ for a discussion on the different iterative methods) for the data collection.. by training v independently of m it might fail to encode all the information relevant to the task. another option would be to train v and m concurrently so that the reward and zt1s prediction loss (or next state reconstruction loss) of m flows through v (that would also be trained with its own reconstruction loss). the tradeoff is that now v is tuned to a particular reward and cannot be reused.. the authors argue that since ht is such that it can predict zt1, it contains enough insight about the future for the agent not needing to plan ahead and just doing reflexive actions based on ht. this is interesting but the considered tasks (driving, dodging fireball) are still very reflexive and do not require much planning. when trained on the true env, a simple controller with the v and m representations achieve sota on carracing. v m is better than v alone. when trained inside the world model, its dynamics stochasticity must be tuned in order for the policy to transfer well and perform well on the real env: too little stochasticity and the agent overfits to the world model flaws and does not transfer to the real env, too much and the agent becomes riskaverse and robust but suboptimal. additional ressources thorough interactive blog post with additional experiments and discussions: [url]/"}, {"document": "how can we explain the predictions of a blackbox model? in this paper, we use influence functions a classic technique from robust statistics to trace a model\u2019s prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. to scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and hessianvector products. we show that even on nonconvex and nondifferentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. on linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visuallyindistinguishable trainingset attacks. a key question often asked of machine learning systems is why did the system make this prediction? we want models that are not just highperforming but also explainable. by understanding why a model does what it does, we can hope to improve the model (amershi et al., 2015), discover new science (shrikumar et al., 2016), and provide endusers with explanations of actions that impact them (goodman flaxman, 2016).. however, the bestperforming models in many domains e.g., deep neural networks for image and speech recognition (krizhevsky et al., 2012) are complicated, blackbox models whose predictions seem hard to explain. work on interpreting these blackbox models has focused on understanding how a fixed model leads to particular predictions, e.g., by locally fitting a simpler model around the test. 1stanford university, stanford, ca. correspondence to: pang wei koh pangweics.stanford.edu, percy liang pliangcs.stanford.edu.. proceedings of the th international conference on machine learning, sydney, australia, pmlr 70, copyright by the author(s).. point (ribeiro et al., 2016) or by perturbing the test point to see how the prediction changes (simonyan et al., 2013; li et al., 2016b; datta et al., 2016; adler et al., 2016). these works explain the predictions in terms of the model, but how can we explain where the model came from?. in this paper, we tackle this question by tracing a model\u2019s predictions through its learning algorithm and back to the training data, where the model parameters ultimately derive from. to formalize the impact of a training point on a prediction, we ask the counterfactual: what would happen if we did not have this training point, or if the values of this training point were changed slightly?. answering this question by perturbing the data and retraining the model can be prohibitively expensive. to overcome this problem, we use influence functions, a classic technique from robust statistics (cook weisberg, 1980) that tells us how the model parameters change as we upweight a training point by an infinitesimal amount. this allows us to differentiate through the training to estimate in closedform the effect of a variety of training perturbations.. despite their rich history in statistics, influence functions have not seen widespread use in machine learning; to the best of our knowledge, the work closest to ours is wojnowicz et al. (2016), which introduced a method for approximating a quantity related to influence in generalized linear models. one obstacle to adoption is that influence functions require expensive second derivative calculations and assume model differentiability and convexity, which limits their applicability in modern contexts where models are often nondifferentiable, nonconvex, and highdimensional. we address these challenges by showing that we can efficiently approximate influence functions using secondorder optimization techniques (pearlmutter, 1994; martens, 2010; agarwal et al., 2016), and that they remain accurate even as the underlying assumptions of differentiability and convexity degrade.. influence functions capture the core idea of studying models through the lens of their training data. we show that they are a versatile tool that can be applied to a wide variety of seemingly disparate tasks: understanding model behavior, debugging models, detecting dataset errors, and creating visuallyindistinguishable adversarial training examples that can flip neural network test predictions, the training set analogue of goodfellow et al. consider a prediction problem from some input space x (e.g., images) to an output space y (e.g., labels). we are given training points z1, for a point z and parameters , let l(z, ) be the loss, and let 1n n i1 l(zi, ) be the empirical risk. the empirical risk minimizer is given by def arg min n n i1 l(zi, ).. assume that the empirical risk is twicedifferentiable and strictly convex in ; in section we explore relaxing these assumptions. our goal is to understand the effect of training points on a model\u2019s predictions. we formalize this goal by asking the counterfactual: how would the model\u2019s predictions change if we did not have this training point?. let us begin by studying the change in model parameters due to removing a point z from the training set. formally, this change is z , where z def arg min zi 6z l(zi, ). however, retraining the model for each removed z is prohibitively slow.. fortunately, influence functions give us an efficient approximation. the idea is to compute the parameter change if z were upweighted by some small , giving us new parameters ,z def arg min n n i1 l(zi, ) l(z, ). a classic result (cook weisberg, 1982) tells us that the influence of upweighting z on the parameters is given by. where h def 1n n i12l(zi, ) is the hessian and is positive definite (pd) by assumption. in essence, we form a quadratic approximation to the empirical risk around and take a single newton step; see appendix a for a derivation. since removing a point z is the same as upweighting it by 1n , we can linearly approximate the parameter change due to removing z by computing z 1niup,params(z), without retraining the model.. next, we apply the chain rule to measure how upweighting z changes functions of in particular, the influence of upweighting z on the loss at a test point ztest again has a closedform expression:. 1we fold in any regularization terms into l. let us develop a finergrained notion of influence by studying a different counterfactual: how would the model\u2019s predictions change if a training input were modified?. for a training point z (x, y), define z def (x , y). consider the perturbation z z , and let z,z be the empirical risk minimizer on the training points with z in place of z. to approximate its effects, define the parameters resulting from moving mass from z onto z: ,z,z def arg min n n i1 l(zi, ) l(z, ) l(z, ). an analogous calculation to (1) yields:. d ,z,z d iup,params(z) iup,params(z). as before, we can make the linear approximation z,z 1n (iup,params(z) iup,params(z)), giving us a closedform estimate of the effect of z z on the model. analogous equations also apply for changes in y. while influence functions might appear to only work for infinitesimal (therefore continuous) perturbations, it is important to note that this approximation holds for arbitrary : the upweighting scheme allows us to smoothly interpolate between z and z this is particularly useful for working with discrete data (e.g., in nlp) or with discrete label changes.. if x is continuous and is small, we can further approximate (3). assume that the input domain x rd, the parameter space rp, and l is differentiable in and x. as 0,l(z, )l(z, ) [xl(z, )], wherexl(z, ) rpd. we thus have z,z 1nh [xl(z, )]. differentiating w.r.t. and applying the chain rule gives us. ipert,loss(z, ztest) def l(ztest, z,z) l(ztest, )h1 xl(z, ).. ipert,loss(z, ztest) tells us the approximate effect that z z has on the loss at ztest. by setting in the direction of ipert,loss(z, ztest), we can construct local perturbations of z that maximally increase the loss at ztest. in section 5.2, we will use this to construct trainingset attacks. finally, we note that ipert,loss(z, ztest) can help us identify the features of z that are most responsible for the prediction on ztest. to find the training points most relevant to a test point, it is common to look at its nearest neighbors in euclidean. (2016)); if all points have the same norm, this is equivalent to choosing x with the largest x xtest. for intuition, we compare this to iup,loss(z, ztest) on a logistic regression model and show that influence is much more accurate at accounting for the effect of training.. let p(y x) (yx), with y 1, and (t) 1exp(t) we seek to maximize the probability of the training set. for a training point z (x, y), l(z, ) log(1 exp(yx)), l(z, ) (yx)yx, and h 1n n i1 (. ytesty (ytestxtest) (yx) xtesth1 x.. we highlight two key differences from x xtest. first, (yx) gives points with high training loss more influence, revealing that outliers can dominate the model parameters. second, the weighted covariance matrix h1 measures the resistance of the other training points to the removal of z; if l(z, ) points in a direction of little variation, its influence will be higher since moving in that direction will not significantly increase the loss on other training points. as we show in fig 1, these differences mean that influence functions capture the effect of model training much more accurately than nearest neighbors. there are two computational challenges to using iup,loss(z, ztest) l(ztest, )h1 l(z, ). first, it requires forming and inverting h n n i12l(zi, ), the hessian of the empirical risk. with n training points and rp, this requires o(np2 p3) operations, which. is too expensive for models like deep neural networks with millions of parameters. second, we often want to calculate iup,loss(zi, ztest) across all training points zi.. the first problem is wellstudied in secondorder optimization. the idea is to avoid explicitly computing h1. stead, we use implicit hessianvector products (hvps) to efficiently approximate stest def h1. compute iup,loss(z, ztest) stest l(z, ). this also solves the second problem: for each test point of interest, we can precompute stest and then efficiently compute stest l(zi, ) for each training point zi.. we discuss two techniques for approximating stest, both relying on the fact that the hvp of a single term in h, [2l(zi, )]v, can be computed for arbitrary v in the same time that l(zi, ) would take, which is typically o(p) (pearlmutter, 1994).. conjugate gradients (cg). the first technique is a standard transformation of matrix inversion into an optimization problem. since h by assumption, h v arg mint t ht v. t. we can solve this with cg approaches that only require the evaluation of ht, which takeso(np) time, without explicitly formingh. while an exact solution takes p cg iterations, in practice we can get a good approximation with fewer iterations; see martens (2010) for more details.. stochastic estimation. with large datasets, standard cg can be slow; each iteration still goes through all n training points. we use a method developed by agarwal et al. (2016) to get an estimator that only samples a single point per iteration, which results in significant speedups.. dropping the subscript for clarity, leth1j def j i0(i h)i, the first j terms in the taylor expansion of h1. rewrite this recursively as h1j i (i h)h j1. from the validity of the taylor expansion, h1j h1 as j the key is that at each iteration, we can substitute the full h with a draw from any unbiased (and fastertocompute) estimator of h to form hj since e[h1j ] h1j , we still have e[h j ] h1.. in particular, we can uniformly sample zi and use 2l(zi, ) as an unbiased estimator of h this gives us the following procedure: uniformly sample t points zs1 , , zst from the training data; define h v v; and recursively compute h1j v v ( i 2l(zsj , ) ) h1j1v, taking h t v as our final unbiased estimate of h1v. we pick t to be large enough such that ht stabilizes, and to reduce variance we repeat this procedure r times and average results. empirically, we found this significantly faster than cg.. we note that the original method of agarwal et al. (2016) dealt only with generalized linear models, for which [2l(zi, )]v can be efficiently computed in o(p) time. in our case, we rely on pearlmutter (1994)\u2019s more general algorithm for fast hvps, described above, to achieve the same time complexity.3. with these techniques, we can compute iup,loss(zi, ztest) on all training points zi in o(np rtp) time; we show in section that empirically, choosing rt o(n) gives accurate results. similarly, we compute ipert,loss(zi, ztest) 1nl(ztest, ) h1 xl(zi, ) with two matrixvector products: we first compute stest, then stestxl(zi, ), with the same hvp trick. these computations are easy to implement in autograd systems like tensorflow (abadi et al., 2015) and theano (theano d. team, 2016), as users need only specify l; the rest is automatically handled. recall that influence functions are asymptotic approximations of leaveoneout retraining under the assumptions that (i) the model parameters minimize the empirical risk, and that (ii) the empirical risk is twicedifferentiable and. that i,2l(zi, ) i; if this is not true, we can scale the loss down without affecting the parameters. in some cases, we can get an upper bound on2l(zi, ) (e.g., for linear models and bounded input), which makes this easy. otherwise, we treat the scaling as a separate hyperparameter and tune it such that the taylor expansion converges.. 3to increase stability, especially with nonconvex models (see section 4.2), we can also sample a minibatch of training points at each iteration, instead of relying on a single training point.. strictly convex. here, we empirically show that influence functions are accurate approximations (section 4.1) that provide useful information even when these assumptions are violated (sections 4.2, 4.3). influence functions assume that the weight on a training point is changed by an infinitesimally small to investigate the accuracy of using influence functions to approximate the effect of removing a training point and retraining, we compared 1niup,loss(z, ztest) with l(ztest, z) l(ztest, ) (i.e., actually doing leaveoneout retraining). with a logistic regression model on 10class mnist,4 the predicted and actual changes matched closely (fig 2left).. the stochastic approximation from agarwal et al. (2016) was also accurate with r repeats and t 5, iterations (fig 2mid). since each iteration only requires one hvp [2l(zi, )]v, this runs quickly: in fact, we accurately estimated h1v without even looking at every data point, since n 55, rt. surprisingly, even r worked; while results were noisier, it was still able to identify the most influential points. in section 2, we took as the global minimum. in practice, if we obtain our parameters by running sgd with early stopping or on nonconvex objectives, as a result, h could have negative eigenvalues. we show that influence functions on still give meaningful results in practice.. our approach is to form a convex quadratic approximation of the loss around , i.e., l(z, ) l(z, ) 4we trained with lbfgs (liu nocedal, 1989), with l2 regularization of 0.01, n 55, 000, and p 7, parameters.. l(z, )( ) ( ) (hi)( ). here, is a damping term that we add ifh has negative eigenvalues; this corresponds to adding l2 regularization on we then calculate iup,loss using l. if is close to a local minimum, this is correlated with the result of taking a newton step from after removing weight from z (see appendix b).. we checked the behavior of iup,loss in a nonconvergent, nonconvex setting by training a convolutional neural network for 500k iterations.5 the model had not converged and h was not pd, so we added a damping term with even in this difficult setting, the predicted and actual changes in loss were highly correlated (pearson\u2019s r 0.86, fig 2right). what happens when the derivatives of the loss, l and 2l, do not exist? in this section, we show that influence functions computed on smooth approximations to nondifferentiable losses can predict the behavior of the original, nondifferentiable loss under leaveoneout retraining. the robustness of this approximation suggests that we can train nondifferentiable models and swap out nondifferentiable components for smoothed versions for the purposes of calculating influence.. to see this, we trained a linear svm on the same 1s vs. 7s mnist task in section 5the network had sets of convolutional layers with tanh() nonlinearities, modeled after the allconvolutional network from (springenberg et al., 2014). for speed, we used of the mnist training set and only 2,616 parameters, since repeatedly retraining the network was expensive. training was done with minibatches of examples and the adam optimizer (kingma ba, 2014). the model had not converged after 500k iterations; training it for another 500k iterations, using a full training pass for each iteration, reduced train loss from to imizing hinge(s) max(0, s); this simple piecewise linear function is similar to relus, which cause nondifferentiability in neural networks. we set the derivatives at the hinge to and calculated iup,loss. as one might expect, this was inaccurate (fig 3bleft): the second derivative carries no information about how close a support vector z is to the hinge, so the quadratic approximation of l(z, ) is linear (up to regularization), which leads to iup,loss(z, ztest) overestimating the influence of z.. for the purposes of calculating influence, we approximated hinge(s) with smoothhinge(s, t) t log(1exp( 1st )), which approaches the hinge loss as t (fig 3a). using the same svm weights as before, we found that calculating iup,loss using smoothhinge(s, 0.001) closely matched the actual change due to retraining in the original hinge(s) (pearson\u2019s r 0.95; fig 3bmid) and remained accurate over a wide range of t (fig 3bright). by telling us the training points responsible for a given prediction, influence functions reveal insights about how models rely on and extrapolate from the training data. in this section, we show that two models can make the same correct predictions but get there in very different ways.. we compared (a) the stateoftheart inception v3 network (szegedy et al., 2016) with all but the top layer frozen6 to (b) an svm with an rbf kernel on a dog vs. fish image classification dataset we extracted from imagenet (russakovsky et al., 2015), with training examples for each class. freezing neural networks in this way is not uncom. 6we used pretrained weights from keras (chollet, 2015).. mon in computer vision and is equivalent to training a logistic regression model on the bottleneck features (donahue et al., 2014). we picked a test image both models got correct (fig 4top) and used smoothhinge(, 0.001) to compute the influence for the svm.. as expected, iup,loss in the rbf svm varied inversely with raw pixel distance, with training images far from the test image in pixel space having almost no influence. the inception influences were much less correlated with distance in pixel space (fig 4left). looking at the two most helpful images (most positive iup,loss) for each model in fig 4right, we see that the inception network picked up on the distinctive characteristics of clownfish, whereas the rbf svm patternmatched training images superficially.. moreover, in the rbf svm, fish (green points) close to the test image were mostly helpful, while dogs (red) were mostly harmful, with the rbf acting as a soft nearest neighbor function (fig 4left). in contrast, in the inception network, fish and dogs could be helpful or harmful for correctly classifying the test image as a fish; in fact, some of the most helpful training images were dogs that, to the model, looked very different from the test fish (fig 4top). in this section, we show that models that place a lot of influence on a small number of points can be vulnerable to training input perturbations, posing a serious security risk in realworld ml systems where attackers can influence the training data (huang et al., 2011). recent work has generated adversarial test images that are visually indistinguish. able from real test images but completely fool a classifier (goodfellow et al., 2015; moosavidezfooli et al., 2016). we demonstrate that influence functions can be used to craft adversarial training images that are similarly visuallyindistinguishable and can flip a model\u2019s prediction on a separate test image. to the best of our knowledge, this is the first proofofconcept that visuallyindistinguishable training attacks can be executed on otherwise highlyaccurate neural networks.. the key idea is that ipert,loss(z, ztest) tells us how to modify training point z to most increase the loss on ztest. concretely, for a target test image ztest, we can construct zi, an adversarial version of a training image zi, by initializing zi : zi and then iterating zi : (zi sign(ipert,loss(zi, ztest))), where is the step size and projects onto the set of valid images that share the same bit representation with zi. after each iteration, we retrain the model. this is an iterated, trainingset analogue of the methods used by, e.g., goodfellow et al. (2016) for testset attacks.. we tested these training attacks on the same inception network on dogs vs. fish from section 5.1, choosing this pair of animals to provide a stark contrast between the classes. we set and ran the attack for iterations on each test image. as before, we froze all but the top layer for training; note that computing ipert,loss still involves differentiating through the entire network. originally, the model correctly classified / test images. for each of these test images, considered separately, we tried to find a visuallyindistinguishable perturbation (i.e., same bit representation) to a single training image, out of 1,800 total training images, that would flip the model\u2019s prediction. we were able to do this on (57) of the test images. by perturbing training images for each test image, we could flip predictions on of the test images; and if we perturbed training images, we could flip all but of the the above results are from attacking each test image separately, i.e., using a different training set to attack each test image. we also tried to attack multiple test images simultaneously by increasing their average loss, and found that single training image perturbations could simultaneously flip multiple test predictions as well (fig 5).. we make three observations about these attacks. first, though the change in pixel values is small, the change in the final inception feature layer is significantly larger: using l2 distance in pixel space, the training values change by less than of the mean distance of a training point to its class centroid, whereas in inception feature space, the change is on the same order as the mean distance. this leaves open the possibility that our attacks, while visuallyimperceptible, can be detected by examining the feature space. second, the attack tries to perturb the training ex. ample in a direction of low variance, causing the model to overfit in that direction and consequently incorrectly classify the test images; we expect attacking to be harder as the number of training examples grows. third, ambiguous or mislabeled training images are effective points to attack: the model has low confidence and thus high loss on them, making them highly influential (recall section 2.3). for example, the image in fig contains both a dog and a fish and is highly ambiguous; as a result, it is the training example that the model is least confident on (with a confidence of 77, compared to the next lowest confidence of 90).. this attack is mathematically equivalent to the gradientbased training set attacks explored by biggio et al. (2012); mei zhu (2015b) and others in the context of different models. (2012) constructed a dataset poisoning attack against a linear svm on a twoclass mnist task, but had to modify the training points in an obviously distinguishable way to be effective. measuring the magnitude of ipert,loss gives model developers a way of quantifying how vulnerable their models are to trainingset attacks. domain mismatch where the training distribution does not match the test distribution can cause models with high training accuracy to do poorly on test data (bendavid et al., 2010). we show that influence functions can identify the training examples most responsible for the errors, helping model developers identify domain mismatch.. as a case study, we predicted whether a patient would be readmitted to hospital. domain mismatches are common in biomedical data, e.g., different hospitals serve different populations, and models trained on one population can do poorly on another (kansagara et al., 2011). we used logistic regression to predict readmission with a balanced training dataset of 20k diabetic patients from us hospitals, each represented by features (strack et al., 2014).7. 7hospital readmission was defined as whether a patient would be readmitted within the next days. out of the children under age in this dataset were readmitted. to induce a domain mismatch, we filtered out children who were not readmitted, leaving out of readmitted. this caused the model to wrongly classify many children in the test set. our aim is to identify the children in the training set as being responsible for these errors.. as a baseline, we tried the common practice of looking at the learned parameters to see if the indicator variable for being a child was obviously different. however, this did not work: 14/127 features had a larger coefficient.. picking a random child ztest that the model got wrong, we calculatediup,loss(zi, ztest) for each training point zi. this clearly highlighted the training children, each of whom were times as influential as the next most influential examples. the child in the training set who was not readmitted had a very positive influence, while the other had very negative influences. moreover, calculating ipert,loss on these children showed that the child\u2019 indicator variable contributed significantly to the magnitude of iup,loss. labels in the real world are often noisy, especially if crowdsourced (frenay verleysen, 2014), and can even be adversarially corrupted. even if a human expert could recognize wrongly labeled examples, it is impossible in many applications to manually review all of the training data. we show that influence functions can help human experts prioritize their attention, allowing them to inspect only the examples that actually matter.. the key idea is to flag the training points that exert the most influence on the model. because we do not have access to the test set, we measure the influence of zi with iup,loss(zi, zi), which approximates the error incurred on zi if we remove zi from the training set.. our case study is email spam classification, which relies. graphic (e.g., age, race, gender), administrative (e.g., length of hospital stay), or medical (e.g., test results).. on userprovided labels and is also vulnerable to adversarial attack (biggio et al., 2011). we flipped the labels of a random of the training data and then simulated manually inspecting a fraction of the training points, correcting them if they had been flipped. using influence functions to prioritize the training points to inspect allowed us to repair the dataset (fig 6, blue) without checking too many points, outperforming the baselines of checking points with the highest train loss (fig 6, green) or at random (fig 6, red). no method had access to the test data. the use of influencebased diagnostics originated in statistics in the 70s and 80s, driven by seminal papers by cook and others (cook, 1977; cook weisberg, 1980; 1982), though similar ideas appeared even earlier in other forms, e.g., the infinitesimal jackknife (jaeckel, 1972). earlier work focused on removing training points from linear models, with later work extending this to more general models and a wider variety of perturbations (cook, 1986; thomas cook, 1990; chatterjee hadi, 1986; wei et al., 1998). most of this prior work focused on experiments with small datasets, e.g., n and p in cook weisberg (1980), with special attention therefore paid to exact solutions, or if not possible, characterizations of the error terms.. influence functions have not been used much in the ml literature, with some exceptions. christmann steinwart (2004); debruyne et al. (2014) use influence functions to study model robustness and to do fast crossvalidation in kernel methods. (2016) uses matrix sketching to estimate cook\u2019s distance, which is closely related to influence; they focus on prioritizing training points for human attention and derive meth. ods specific to generalized linear models.. as noted in section 5.2, our trainingset attack is mathematically equivalent to an approach first explored by biggio et al. (2012) in the context of svms, with followup work extending the framework and applying it to linear and logistic regression (mei zhu, 2015b), topic modeling (mei zhu, 2015a), and collaborative filtering (li et al., 2016a). these papers derived the attack directly from the kkt conditions without considering influence, though for continuous data, the end result is equivalent. influence functions additionally let us consider attacks on discrete data (section 2.2), but we have not tested this empirically. our work connects the literature on trainingset attacks with work on adversarial examples (goodfellow et al., 2015; moosavidezfooli et al., 2016), visuallyimperceptible perturbations on test inputs.. in contrast to trainingset attacks, cadamuro et al. (2016) consider the task of taking an incorrect test prediction and finding a small subset of training data such that changing the labels on this subset makes the prediction correct. they provide a solution for ols and gaussian process models when the labels are continuous. our work with influence functions allow us to solve this problem in a much larger range of models and in datasets with discrete labels. we have discussed a variety of applications, from creating trainingset attacks to debugging models and fixing datasets. underlying each of these applications is a common tool, the influence function, which is based on a simple idea we can better understand model behavior by looking at how it was derived from its training data.. at their core, influence functions measure the effect of local changes: what happens when we upweight a point by an infinitesimallysmall ? this locality allows us to derive efficient closedform estimates, and as we show, they can be surprisingly effective. however, we might want to ask about more global changes, e.g., how does a subpopulation of patients from this hospital affect the model? since influence functions depend on the model not changing too much, how to tackle this is an open question.. it seems inevitable that highperforming, complex, blackbox models will become increasingly prevalent and important. we hope that the approach presented here of looking at the model through the lens of the training data will become a standard part of the toolkit of developing, understanding, and diagnosing machine learning.. the code and data for replicating our experiments is available on github http://bit.ly/gtinfluence and codalab http://bit.ly/clinfluence.", "summary": "goal: identifying training points most responsible for a given prediction.. given training points z1, \\dots, zn, let loss function be \\frac1n\\sumi1nl(zi, \\theta) a function called influence function let us compute the parameter change if z were upweighted by some small \\epsilon. \\hat\\theta\\epsilon, z : \\arg \\min\\theta \\in \\theta \\frac1n\\sumi1n l(zi, \\theta) \\epsilon l(z, \\theta). \\mathcali\\textup, params(z) : \\fracd\\hat\\theta\\epsilon, zd\\epsilon h\\hat\\theta1 \\mathcali\\textup, params(z) shows how uplifting one point z affect the estimate of the parameters \\theta. furthermore, we could determine how uplifting z affect the loss estimate of a test point through chain rule. \\mathcali\\textup, loss(z, z\\texttest) abla\\theta l(z\\texttest, \\hat\\theta)\\top \\mathcali\\textup, params(z) apart from lifting one training point, change of the parameters with the change of a training point could also be estimated. \\fracd\\hat\\theta\\epsilon, z\\delta, zd\\epsilon \\mathcali\\textup, params(z\\delta) \\mathcali\\textup, params(z). this measures how purturbation \\delta to training point z affect the parameter estimation \\theta.. section describes some practicals about efficient implementing.. this set of tool could be used for some interpretable machine learning tasks."}, {"document": "dealing with sparse rewards is one of the biggest challenges in reinforcement learning (rl). we present a novel technique called hindsight experience replay which allows sampleefficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. it can be combined with an arbitrary offpolicy rl algorithm and may be seen as a form of implicit curriculum. we demonstrate our approach on the task of manipulating objects with a robotic arm. in particular, we run experiments on three different tasks: pushing, sliding, and pickandplace, in each case using only binary rewards indicating whether or not the task is completed. our ablation studies show that hindsight experience replay is a crucial ingredient which makes training possible in these challenging environments. we show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task. the video presenting our experiments is available at https://goo.gl/smrqni.reinforcement learning (rl) combined with neural networks has recently led to a wide range of successes in learning policies for sequential decisionmaking problems. this includes simulated environments, such as playing atari games (mnih et al., 2015), and defeating the best human player at the game of go (silver et al., 2016), as well as robotic tasks such as helicopter control (ng et al., 2006), hitting a baseball (peters and schaal, 2008), screwing a cap onto a bottle (levine et al., 2015), or door opening (chebotar et al., 2016). however, a common challenge, especially for robotics, is the need to engineer a reward function that not only reflects the task at hand but is also carefully shaped (ng et al., 1999) to guide the policy optimization. (2017) use a cost function consisting of five relatively complicated terms which need to be carefully weighted in order to train a policy for stacking a brick on top of another one. the necessity of cost engineering limits the applicability of rl in the real world because it requires both rl expertise and domainspecific knowledge. moreover, it is not applicable in situations where we do not know what admissible behaviour may look like. it is therefore of great practical relevance to develop algorithms which can learn from unshaped reward signals, e.g. a binary signal indicating successful task completion. one ability humans have, unlike the current generation of modelfree rl algorithms, is to learn almost as much from achieving an undesired outcome as from the desired one. imagine that you are learning how to play hockey and are trying to shoot a puck into a net. you hit the puck but it misses the net on the right side. the conclusion drawn by a standard rl algorithm in such a situation would be that the performed sequence of actions does not lead to a successful shot, and little (if anything) would be learned. it is however possible to draw another conclusion, namely that this sequence of actions would be successful if the net had been placed further to the right. in this paper we introduce a technique called hindsight experience replay (her) which allows the algorithm to perform exactly this kind of reasoning and can be combined with any offpolicy rl algorithm. it is applicable whenever there are multiple goals which can be achieved, e.g. achieving each state of the system may be treated as a separate goal. not only does her improve the sample efficiency in this setting, but more importantly, it makes learning possible even if the reward signal is sparse and binary. our approach is based on training universal policies (schaul et al., 2015a) which take as input not only the current state, but also a goal state. the pivotal idea behind her is to replay each episode with a different goal than the one the agent was trying to achieve, e.g. one of the goals which was achieved in the episode. in this section we introduce reinforcement learning formalism used in the paper as well as rl algorithms we use in our experiments. we consider the standard reinforcement learning formalism consisting of an agent interacting with an environment. to simplify the exposition we assume that the environment is fully observable. an environment is described by a set of states s, a set of actions a, a distribution of initial states p(s ), a reward function r : s a r, transition probabilities p(s t1 s t , a t ), and a discount factor [0, 1]. a deterministic policy is a mapping from states to actions: : s a. every episode starts with sampling an initial state s at every timestep t the agent produces an action based on the current state: a t (s t ). then it gets the reward r t r(s t , a t ) and the environments new state is sampled from the distribution p(s t , a t ). a discounted sum of future rewards is called a return: let denote an optimal policy i.e. q (s, a) q (s, a) for every s s, a a and any policy all optimal policies have the same qfunction which is called optimal qfunction and denoted q it is easy to show that it satisfies the following equation called the bellman equation: deep qnetworks (dqn) (mnih et al., 2015) is a modelfree rl algorithm for discrete action spaces. here we sketch it only informally, see mnih et al. in dqn we maintain a neural network q which approximates q q is defined as q (s) argmax aa q(s, a). q is a policy which with probability takes a random action (sampled uniformly from a) and takes the action q (s) with probability during training we generate episodes using greedy policy w.r.t. the current approximation of the actionvalue function q. the transition tuples (s t , a t , r t , s t1 ) encountered during training are stored in the socalled replay buffer. the generation of new episodes is interleaved with neural network training. the network is trained using minibatch gradient descent on the loss l which encourages the approximated qfunction to satisfy the bellman equation: , where y t r t max a a q(s t1 , a ) and the tuples (s t , a t , r t , s t1 ) are sampled from the replay buffer in order to make this optimization procedure more stable the targets y t are usually computed using a separate target network which changes at a slower pace than the main network. a common practice the targets yt depend on the network parameters but this dependency is ignored during backpropagation. is to periodically set the weights of the target network to the current weights of the main network (e.g. (2015)) or to use a polyakaveraged (polyak and juditsky, 1992) version of the main network instead (lillicrap et al., 2015). deep deterministic policy gradients (ddpg) (lillicrap et al., 2015) is a modelfree rl algorithm for continuous action spaces. here we sketch it only informally, see lillicrap et al. in ddpg we maintain two neural networks: a target policy (also called an actor) : s a and an actionvalue function approximator (called the critic) q : s a r. the critics job is to approximate the actors actionvalue function q episodes are generated using a behavioral policy which is a noisy version of the target policy, e.g. the critic is trained in a similar way as the qfunction in dqn but the targets y t are computed using actions outputted by the actor, i.e. the actor is trained with minibatch gradient descent on the loss l a e s q(s, (s)), where s is sampled from the replay buffer. actor parameters can be computed by backpropagation through the combined critic and actor networks. universal value function approximators (uvfa) (schaul et al., 2015a) is an extension of dqn to the setup where there is more than one goal we may try to achieve. let g be the space of possible goals. every goal g g corresponds to some reward function r g : s a r. every episode starts with sampling a stategoal pair from some distribution p(s , g). the goal stays fixed for the whole episode. at every timestep the agent gets as input not only the current state but also the current goal : s g a and gets the reward r t r g (s t , a t ). the qfunction now depends not only on a stateaction pair but also on a goal schaul et al. (2015a) show that in this setup it is possible to train an approximator to the qfunction using direct bootstrapping from the bellman equation (just like in case of dqn) and that a greedy policy derived from it can generalize to previously unseen stateaction pairs. the extension of this approach to ddpg is straightforward. consider a bitflipping environment with the state space s 0, n and the action space a 0, 1, , n for some integer n in which executing the ith action flips the ith bit of the state. for every episode we sample uniformly an initial state as well as a target state and the policy gets a reward of as long as it is not in the target state, i.e. standard rl algorithms are bound to fail in this environment for n because they will never experience any reward other than notice that using techniques for improving exploration (e.g. vime (houthooft et al., 2016), countbased exploration (ostrovski et al., 2017) or bootstrapped dqn (osband et al., 2016)) does not help here because the real problem is not in lack of diversity of states being visited, rather it is simply impractical to explore such a large state space. the standard solution to this problem would be to use a shaped reward function which is more informative and guides the agent towards the goal, e.g. while using a shaped reward solves the problem in our toy environment, it may be difficult to apply to more complicated problems. we investigate the results of reward shaping experimentally in sec. instead of shaping the reward we propose a different solution which does not require any domain knowledge. consider an episode with a state sequence s , , s t which implies that the agent received a reward of at every timestep. the pivotal idea behind our approach is to reexamine this trajectory with a different goal while this trajectory may not help us learn how to achieve the state g, it definitely tells us something about how to achieve the state s t this information can be harvested by using an offpolicy rl algorithm and experience replay where we replace g in the replay buffer by s t in addition we can still replay with the original goal g left intact in the replay buffer. with this modification at least half of the replayed trajectories contain rewards different from and learning becomes much simpler. compares the final performance of dqn with and without this additional replay technique which we call hindsight experience replay (her). dqn without her can only solve the task for n while dqn with her easily solves the task for n up to see appendix a for the details of the experimental setup. note that this approach combined with powerful function approximators (e.g., deep neural networks) allows the agent to learn how to achieve the goal g even if it has never observed it during training. we more formally describe our approach in the following sections. we are interested in training agents which learn to achieve multiple different goals. we follow the approach from universal value function approximators (schaul et al., 2015a), i.e. we train policies and value functions which take as input not only a state s s but also a goal g g. moreover, we show that training an agent to perform multiple tasks can be easier than training it to perform only one task (see sec. for details) and therefore our approach may be applicable even if there is only one task we would like the agent to perform (a similar situation was recently observed by pinto and gupta (2016)). we assume that every goal g g corresponds to some predicate f g : s 0, and that the agents goal is to achieve any state s that satisfies f g (s) in the case when we want to exactly specify the desired state of the system we may use s g and f g (s) [s g]. the goals can also specify only some properties of the state, e.g. suppose that s r and we want to be able to achieve an arbitrary state with the given value of x coordinate. in this case g r and f g ((x, y)) [x g]. moreover, we assume that given a state s we can easily find a goal g which is satisfied in this state. more formally, we assume that there is given a mapping m : s g s.t. notice that this assumption is not very restrictive and can usually be satisfied. in the case where each goal corresponds to a state we want to achieve, i.e. g s and f g (s) [s g], the mapping m is just an identity. for the case of 2dimensional state and 1dimensional goals from the previous paragraph this mapping is also very simple m((x, y)) x. a universal policy can be trained using an arbitrary rl algorithm by sampling goals and initial states from some distributions, running the agent for some number of timesteps and giving it a negative reward at every timestep when the goal is not achieved, i.e. this does not however work very well in practice because this reward function is sparse and not very informative. in order to solve this problem we introduce the technique of hindsight experience replay which is the crux of our approach. the idea behind hindsight experience replay (her) is very simple: after experiencing some episode s , s , , s t we store in the replay buffer every transition s t s t1 not only with the original goal used for this episode but also with a subset of other goals. notice that the goal being pursued influences the agents actions but not the environment dynamics and therefore we can replay each trajectory with an arbitrary goal assuming that we use an offpolicy rl algorithm like dqn (mnih et al., 2015), ddpg (lillicrap et al., 2015), naf (gu et al., 2016) or sdqn (metz et al., 2017). one choice which has to be made in order to use her is the set of additional goals used for replay. in the simplest version of our algorithm we replay each trajectory with the goal m(s t ), i.e. the goal which is achieved in the final state of the episode. we experimentally compare different types and quantities of additional goals for replay in sec. in all cases we also replay each trajectory with the original goal pursued in the episode. for a more formal description of the algorithm. given: an offpolicy rl algorithm a, e.g. dqn, ddpg, naf, sdqn a strategy s for sampling goals for replay, e.g. r(s, a, g) [f g (s) 0] initialize a e.g. initialize neural networks initialize replay buffer r for episode 1, m do sample a goal g and an initial state s for t 0, t do sample an action a t using the behavioral policy from a: denotes concatenation execute the action a t and observe a new state s t1 end for for t 0, t do r t : r(s t , a t , g) store the transition (s t g, a t , r t , s t1 g) in r standard experience replay sample a set of additional goals for replay g : s(current episode) for g g do sample a minibatch b from the replay buffer r perform one step of optimization using a and minibatch b end for end for her may be seen as a form of implicit curriculum as the goals used for replay naturally shift from ones which are simple to achieve even by a random agent to more difficult ones. however, in contrast to explicit curriculum, her does not require having any control over the distribution of initial environment states. not only does her learn with extremely sparse rewards, in our experiments it also performs better with sparse rewards than with shaped ones (see sec. these results are indicative of the practical challenges with reward shaping, and that shaped rewards would often constitute a compromise on the metric we truly care about (such as binary success/failure). the video presenting our experiments is available at https://goo.gl/smrqni. this section is organized as follows. we introduce multigoal rl environments we use for the experiments as well as our training procedure. we compare the performance of ddpg with and without her. we check if her improves performance in the singlegoal setup. we analyze the effects of using shaped reward functions. we compare different strategies for sampling additional goals for her. we show the results of the experiments on the physical robot. the are no standard environments for multigoal rl and therefore we created our own environments. we decided to use manipulation environments based on an existing hardware robot to ensure that the challenges we face correspond as closely as possible to the real world. in all experiments we use a 7dof fetch robotics arm which has a twofingered parallel gripper. the robot is simulated using the mujoco (todorov et al., 2012) physics engine. the whole training procedure is performed in the simulation but we show in sec. that the trained policies perform well on the physical robot without any finetuning. policies are represented as multilayer perceptrons (mlps) with rectified linear unit (relu) activation functions. training is performed using the ddpg algorithm (lillicrap et al., 2015) with adam (kingma and ba, 2014) as the optimizer. for improved efficiency we use workers which average the parameters after every update. see appendix a for more details and the values of all hyperparameters. we consider different tasks: in this task a box is placed on a table in front of the robot and the task is to move it to the target location on the table. the robot fingers are locked to prevent grasping. the learned behaviour is a mixture of pushing and rolling. in this task a puck is placed on a long slippery table and the target position is outside of the robots reach so that it has to hit the puck with such a force that it slides and then stops in the appropriate place due to friction. this task is similar to pushing but the target position is in the air and the fingers are not locked. to make exploration in this task easier we recorded a single state in which the box is grasped and start half of the training episodes from this state states: the state of the system is represented in the mujoco physics engine and consists of angles and velocities of all robot joints as well as positions, rotations and velocities (linear and angular) of all objects. goals: goals describe the desired position of the object (a box or a puck depending on the task) with some fixed tolerance of i.e. g r where s if the state after the execution of the action a in the state s. we compare sparse and shaped reward functions in sec. stategoal distributions: for all tasks the initial position of the gripper is fixed, while the initial position of the object and the target are randomized. see appendix a for details. this was necessary because we could not successfully train any policies for this task without using the demonstration state. we have later discovered that training is possible without this trick if only the goal position is sometimes on the table and sometimes in the air. observations: in this paragraph relative means relative to the current gripper position. the policy is given as input the absolute position of the gripper, the relative position of the object and the target , as well as the distance between the fingers. the qfunction is additionally given the linear velocity of the gripper and fingers as well as relative linear and angular velocity of the object. we decided to restrict the input to the policy in order to make deployment on the physical robot easier. actions: none of the problems we consider require gripper rotation and therefore we keep it fixed. action space is 4dimensional. three dimensions specify the desired relative gripper position at the next timestep. we use mujoco constraints to move the gripper towards the desired position but jacobianbased control could be used instead the last dimension specifies the desired distance between the fingers which are position controlled. strategy s for sampling goals for replay: unless stated otherwise her uses replay with the goal corresponding to the final state in each episode, i.e. we compare different strategies for choosing which goals to replay with in sec. in order to verify if her improves performance we evaluate ddpg with and without her on all tasks. moreover, we compare against ddpg with countbased exploration (strehl and littman, 2005;kolter and ng, 2009;tang et al., 2016;bellemare et al., 2016;ostrovski et al., 2017). for her we store each transition in the replay buffer twice: once with the goal used for the generation of the episode and once with the goal corresponding to the final state from the episode (we call this strategy final). we perform ablation studies of different strategies s for choosing goals for replay, here we include the best version from sec. in the plot for comparison. an episode is considered successful if the distance between the object and the goal at the end of the episode is less than 7cm for pushing and pickandplace and less than 20cm for sliding. the results are averaged across random seeds and shaded areas represent one standard deviation. the red curves correspond to the future strategy with k from sec. while the blue one corresponds to the final strategy. it is clear that ddpg without her is unable to solve any of the tasks and ddpg with countbased exploration is only able to make some progress on the sliding task. on the other hand, ddpg with her solves all tasks almost perfectly. it confirms that her is a crucial element which makes learning from sparse, binary rewards possible. the target position is relative to the current object position. the successful deployment on a physical robot (sec. 4.6) confirms that our control model produces movements which are reproducible on the physical robot despite not being fully physically plausible. we discretize the state space and use an intrinsic reward of the form / n , where is a hyperparameter and n is the number of times the given state was visited. the discretization works as follows. we take the relative position of the box and the target and then discretize every coordinate using a grid with a stepsize which is a hyperparameter. we have performed a hyperparameter search over 0.032, 0.064, 0.125, 0.25, 0.5, 1, 2, 4, 8, 16, 32, 1cm, 2cm, 4cm, 8cm. the best results were obtained using and 1cm and these are the results we report. we also evaluated dqn (without her) on our tasks and it was not able to solve any of them. does her improve performance even if there is only one goal we care about? in this section we evaluate whether her improves performance in the case where there is only one goal we care about. to this end, we repeat the experiments from the previous section but the goal state is identical in all episodes. it is clear that ddpgher performs much better than pure ddpg even if the goal state is identical in all episodes. more importantly, comparing fig. we can also notice that her learns faster if training episodes contain multiple goals, so in practice it is advisable to train on multiple goals even if we care only about one of them. so far we only considered binary rewards of the form in this section we check how the performance of ddpg with and without her changes if we replace this reward with one which is shaped. we considered reward functions of the form r(s, a, g) where s is the state of the environment after the execution of the action a in the state s and 0, 1, p 1, are hyperparameters. surprisingly neither ddpg, nor ddpgher was able to successfully solve any of the tasks with any of these reward functions .our results are consistent with the fact that successful applications of rl to difficult manipulation tasks which does not use demonstrations usually have more complicated reward functions than the ones we tried (e.g. the following two reasons can cause shaped rewards to perform so poorly: (1) there is a huge discrepancy between what we optimize (i.e. a shaped reward function) and the success condition (i.e. : is the object within some radius from the goal at the end of the episode); (2) shaped rewards penalize for inappropriate behaviour (e.g. moving the box in a wrong direction) which may hinder exploration. it can cause the agent to learn not to touch the box at all if it can not manipulate it precisely and we noticed such behaviour in some of our experiments. our results suggest that domainagnostic reward shaping does not work well (at least in the simple forms we have tried). of course for every problem there exists a reward which makes it easy (ng et al., 1999) but designing such shaped rewards requires a lot of domain knowledge and may in some cases not be much easier than directly scripting the policy. this strengthens our belief that learning from sparse, binary rewards is an important problem. how many goals should we replay each trajectory with and how to choose them? in this section we experimentally evaluate different strategies (i.e. 1) for choosing goals to use with her. so far the only additional goals we used for replay were the ones corresponding to pickandplace figure 6: ablation study of different strategies for choosing additional goals for replay. the top row shows the highest (across the training epochs) test performance and the bottom row shows the average test performance across all training epochs. on the right top plot the curves for final, episode and future coincide as all these strategies achieve perfect performance on this task. the final state of the environment and we will call this strategy final. apart from it we consider the following strategies: future replay with k random states which come from the same episode as the transition being replayed and were observed after it, episode replay with k random states coming from the same episode as the transition being replayed, random replay with k random states encountered so far in the whole training procedure. all of these strategies have a hyperparameter k which controls the ratio of her data to data coming from normal experience replay in the replay buffer. the plots comparing different strategies and different values of k can be found in fig. we can see from the plots that all strategies apart from random solve pushing and pickandplace almost perfectly regardless of the values of k. in all cases future with k equal or performs best and it is the only strategy which is able to solve the sliding task almost perfectly. the learning curves for future with k can be found in fig. it confirms that the most valuable goals for replay are the ones which are going to be achieved in the near future notice that increasing the values of k above degrades performance because the fraction of normal replay data in the buffer becomes very low. we took a policy for the pickandplace task trained in the simulator (version with the future strategy and k from sec. 4.5) and deployed it on a physical fetch robot without any finetuning. the box position was predicted using a separately trained cnn using raw fetch head camera images. see appendix b for details. initially the policy succeeded in out of trials. it was not robust to small errors in the box position estimation because it was trained on perfect state coming from the simulation. after retraining the policy with gaussian noise (std1cm) added to observations the success rate increased to 5/5. the video showing some of the trials is available at https://goo.gl/smrqni. the technique of experience replay has been introduced in lin (1992) and became very popular after it was used in the dqn agent playing atari (mnih et al., 2015). prioritized experience replay (schaul et al., 2015b) is an improvement to experience replay which prioritizes transitions in the replay buffer in order to speed up training. it it orthogonal to our work and both approaches can be easily combined. learning simultaneously policies for multiple tasks have been heavily explored in the context of policy search, e.g. schmidhuber and huber (1990); caruana (1998); da silva et al. learning offpolicy value functions for multiple tasks was investigated by foster and dayan (2002) and sutton et al. our work is most heavily based on schaul et al. (2015a) who considers training a single neural network approximating multiple value functions. learning simultaneously to perform multiple tasks has been also investigated for a long time in the context of hierarchical reinforcement learning, e.g. bakker and schmidhuber (2004); vezhnevets et al. our approach may be seen as a form of implicit curriculum learning (elman, 1993;bengio et al., 2009). while curriculum is now often used for training neural networks (e.g. zaremba and sutskever (2014); graves et al. (2016)), the curriculum is almost always handcrafted. the problem of automatic curriculum generation was approached by schmidhuber (2004) who constructed an asymptotically optimal algorithm for this problem using program search. another interesting approach is powerplay (schmidhuber, 2013;srivastava et al., 2013) which is a general framework for automatic task selection. (2017) consider a setup where there is a fixed discrete set of tasks and empirically evaluate different strategies for automatic curriculum generation in this settings. another approach investigated by sukhbaatar et al. (2017) uses selfplay between the policy and a tasksetter in order to automatically generate goal states which are on the border of what the current policy can achieve. our approach is orthogonal to these techniques and can be combined with them. we have also tried replaying the goals which are close to the ones achieved in the near future but it has not performed better than the future strategy the qfunction approximator was trained using exact observations. it does not have to be robust to noisy observations because it is not used during the deployment on the physical robot. we introduced a novel technique called hindsight experience replay which makes possible applying rl algorithms to problems with sparse and binary rewards. our technique can be combined with an arbitrary offpolicy rl algorithm and we experimentally demonstrated that with dqn and ddpg. we showed that her allows training policies which push, slide and pickandplace objects with a robotic arm to the specified positions while the vanilla rl algorithm fails to solve these tasks. we also showed that the policy for the pickandplace task performs well on the physical robot without any finetuning. as far as we know, it is the first time so complicated behaviours were learned using only sparse, binary rewards.", "summary": "hindsight experience replay(her) is a sample efficient technique to learn from sparse rewards. idea assume a footballer misses the goal narrowly. even though the player does not get any reward(in terms of goal), the player realizes that had the goal post been shifted a bit, it would have resulted in a goal(reward). the same intuition is applied for the rl agent let us say that the true goal state was g while the agent ends up in the state s. while the action sequence is not useful for reaching the goal state g, it is indeed useful for reaching state s. hence the trajectory could be replayed with the goal as s(and not g). technical details multigoal policy trained using universal value function approximation (uvfa). every episode starts by sampling a start state and a goal state. each goal has a different reward function. policy uses both the current state and the current goal state and leads to a state transition sequence s1, s2,, sn. each of these transitions si si1 are stored in a buffer with both the original goal and a subset of the other goals. for the goal selection, following strategies are tried: future goal state is the state k steps after observing the state transition. final goal state is the final state of the current episode. episode k random states are selected from the current episode. randon k states are selected randomly. any offpolicy algorithm can be used. specifically, ddpg is used. experiments robotic arm simulated using mujoco for push, slide and pick and place tasks. ddpg with and without her evaluated on the tasks. ddpg with the her variant significantly outperforms the baseline in all the cases."}, {"document": "what makes images similar? to measure the similarity between images, they are typically embedded in a featurevector space, in which their distance preserve the relative dissimilarity. however, when learning such similarity embeddings the simplifying assumption is commonly made that images are only compared to one unique measure of similarity. a main reason for this is that contradicting notions of similarities cannot be captured in a single space. to address this shortcoming, we propose conditional similarity networks (csns) that learn embeddings differentiated into semantically distinct subspaces that capture the different notions of similarities. csns jointly learn a disentangled embedding where features for different similarities are encoded in separate dimensions as well as masks that select and reweight relevant dimensions to induce a subspace that encodes a specific similarity notion. we show that our approach learns interpretable image representations with visually relevant semantic subspaces. further, when evaluating on triplet questions from multiple similarity notions our model even outperforms the accuracy obtained by training individual specialized networks for each notion separately.understanding visual similarities between images is a key problem in computer vision. to measure the similarity between images, they are embedded in a featurevector space, in which their distances preserve the relative dissimilarity. commonly, convolutional neural networks are trained to transform images into respective featurevectors. we refer to these as similarity networks. when learning such networks from pairwise or triplet (dis)similarity constraints, the simplifying assumption is commonly made that objects are compared according to one unique measure of similarity. however, objects have various attributes and can be compared according to a multitude of semantic aspects. similar category similar color similar occasion figure example illustrating how objects can be compared according to multiple notions of similarity. here, we demonstrate three intuitive concepts, which are challenging to combine for a machine vision algorithm that has to embed objects in a feature space where distances preserve the relative dissimilarity: shoes are of the same category; red objects are more similar in terms of color; sneakers and tshirts are stylistically closer. an illustrative example to consider is the comparison of coloured geometric shapes, a task toddlers are regularly exposed to with benefits to concept learning. consider, that a red triangle and a red circle are very similar in terms of color, more so than a red triangle and a blue triangle. however, the triangles are more similar to one another in terms of shape than the triangle and the circle. an optimal embedding should minimize distances between perceptually similar objects. in the example above and also in the practical example in figure this creates a situation where the same two objects are semantically repelled and drawn to each other at the same time. a standard triplet embedding ignores the sources of similarity and cannot jointly satisfy the competing semantic aspects. thus, a successful embedding necessarily needs to take the visual concept into account that objects are compared to. one way to address this issue is to learn separate triplet networks for each aspect of similarity. however, the idea is wasteful in terms of parameters needed, redundancy of parameters, as well as the associated need for training data. the proposed conditional similarity network consists of three key components: first, a learned convolutional neural network as feature extractor that learns the disentangled embedding, i.e., different dimensions encode features for specific notions of similarity. second, a condition that encodes according to which visual concept images should be compared. third, a learned masking operation that, given the condition, selects the relevant embedding dimensions that induce a subspace which encodes the queried visual concept. in this work, we introduce conditional similarity networks (csns) a joint architecture to learn a nonlinear embeddings that gracefully deals with multiple notions of similarity within a shared embedding using a shared feature extractor. different aspects of similarity are incorporated by assigning responsibility weights to each embedding dimension with respect to each aspect of similarity. this can be achieved through a masking operation leading to separate semantic subspaces. figure provides an overview of the proposed framework. images are passed through a convolutional network and projected into a nonlinear embedding such that different dimensions encode features for specific notions of similarity. subsequent masks indicate which dimensions of the embedding are responsible for separate aspects of similarity. we can then compare objects according to various notions of similarity by selecting an appropriate masked subspace. in the proposed approach the convolutional network that learns the disentangled embedding as well as the masks that learn to select relevant dimensions are trained jointly. in our experiments we evaluate the quality of the learned embeddings by their ability to embed unseen triplets. we demonstrate that csns clearly outperform single triplet networks, and even sets of specialist triplet networks where a lot more parameters are available and each network is trained towards one single similarity notion. further we show csns make the representation interpretable by encoding different similarities in separate dimensions. our contributions are a) formulating conditional similarity networks, an approach that allows to to learn nonlinear embeddings that incorporate multiple aspects of similarity within a shared embedding using a shared feature extractor, b) demonstrating that the proposed approach outperforms standard triplet networks and even sets of specialist triplet networks in a variety of hard predictive visual tasks and c) demonstrating that our approach successfully disentangles the embedding features into meaningful dimensions so as to make the representation interpretable. similarity based learning has emerged as a broad field of interest in modern computer vision and has been used in many contexts. disconnected from the input image, triplet based similarity embeddings, can be learned using crowdkernels [24]. [21] introduce a probabilistic treatment for triplets and learn an adaptive crowd kernel. similar work has been generalized to multipleviews and clustering settings by amid and ukkonen [1] as well as van der maaten and hinton [23]. a combination of triplet embeddings with input kernels was presented by wilber et al. [27], but this work did not include joint feature and embedding learning. an early approach to connect input features with embeddings has been to learn image similarity functions through ranking [4]. a foundational line of work combining similarities with neural network models to learn visual features from similarities revolves around siamese networks [6,10], which use pairwise distances to learn embeddings discriminatively. in contrast to pairwise comparisons, triplets have a key advantage due to their flexibility in capturing a variety of higherorder similarity constraints rather than the binary similar/dissimilar statement for pairs. neural networks to learn visual features from triplet based similarities have been used by wang et al. [17] for face verification and finegrained visual categorization. a key insight from these works is that semantics as captured by triplet embeddings are a natural way to represent complex classstructures when dealing with problems of highdimensional categorization and greatly boost the ability of models to share information between classes. disentangling representations is a major topic in the recent machine learning literature and has for example been tackled using boltzmann machines by reed et al. [5] propose information theoretical factorizations to improve unsupervised adversarial networks. within this stream of research, the work closest to ours is that of karaletsos et al. [12] on representation learning which introduces a joint generative model over inputs and triplets to learn a factorized latent space. however, the focus of that work is the generative aspect of disentangling representations and proof of concept applications to lowdimensional data. our work introduces a convolutional embedding architecture that forgoes the generative pathway in favor of exploring applications to embed highdimensional image data. we thus demonstrate that the generative interpretation is not required to reap the benefits of conditional similarity networks and demonstrate in particular their use in common computer vision tasks. a theme in our work is the goal of modeling separate similarity measures within the same system by factorizing (or disentangling) latent spaces. we note the relation of these goals to a variety of approaches used in representation learning. multiview learning [20,26] has been used for 3d shape inference and shown to generically be a good way to learn factorized latent spaces. multiple kernel learning [3,19] employs information encoded in different kernels to provide predictions using the synthesized complex feature space and has also been used for similaritybased learning by mcfee and lanckriet [15]. multitask learning approaches [7] are used when information from disparate sources or using differing assumptions can be combined beneficially for a final prediction task. indeed, our gating mechanism can be interpreted as an architectural novelty in neural networks for multitask triplet learning. similar to our work, multiliniear networks [14] also strive to factorize representations, but differ in that they ignore weak additional information. an interesting link also exists to multiple similarity learning [2], where category specific similarities are used to approximate a finegrained global embedding. our global factorized embeddings can be thought of as an approach to capture similar information in a shared space directly through feature learning. we also discuss the notion of attention in our work, by employing gates to attend to the mentioned subspaces of the inferred embeddings when focusing on particular visual tasks. this term may be confused with spatial attention such as used in the draw model [9], but bears similarity insofar as it shows that the ability to gate the focus of the model on relevant dimensions (in our case in latent space rather than observed space) is beneficial both to the semantics and to the quantitative performance of our model. the masking operation selects relevant embedding dimensions, given a condition index. masking can be seen as a soft gating function, to attend to a particular concept. our goal is to learn a nonlinear feature embedding f (x), from an image x into a feature space r d , such that for a pair of images x and x , the euclidean distance between f (x ) and f (x ) reflects their semantic dissimilarity. in particular, we strive for the distance between images of semantically similar objects to be small, and the distance between images of semantically different objects to be large. this relationship should hold independent of imaging conditions. we consider y f (x) to be an embedding of observed images x into coordinates in a feature space y. here, f (x) w g(x) clarifies that the embedding function is a composition of an arbitrarily nonlinear function g() and a linear projection w , for w r db , where d denotes the dimensions of the embedding and b stands for the dimensions of the output of the nonlinear function g(). in general, we denote the parameters of function f (x) by , denoting all the filters and weights. apart from observing images x, we are also given a set of triplet constraints sampled from an oracle such as a crowd. we define triplet constraints in the following. given an unknown conditional similarity function s c (, ), an oracle such as a crowd can compare images x , x and x according to condition c. a condition is defined as a certain notion of similarity according to which images can be compared. figure gives a few example notions according to which images of fashion products can be compared. the condition c serves as a switch between attented visual concepts and can effectively gate between different similarity functions s c using image x as reference, the oracle can apply s c (x , x ) and s c (x , x ) and decide whether x is more similar to x or to x conditioned on c. the oracle then returns an ordering over these two distances, which we call a triplet t. a triplet is defined as the set of indices reference image, more distant image, closer image, e.g. we define the set of all triplets related to condition c as: we do not have access to the exhaustive set t , but can sample ktimes from it using the oracle to yield a finite sample the feature space spanned by our model is given by function f (). to learn this nonlinear embedding and to be consistent with the observed triplets, we define a loss function l t () over triplets to model the similarity structure over images. the triplet loss commonly used is where is the euclidean distance between the representations of images x i and x j the scalar margin h helps to prevent trivial solutions. the generic triplet loss is not capable of capturing the structure induced by multiple notions of similarities. to be able to model conditional similarities, we introduce masks m over the embedding with m r dnc where n c is the number of possible notions of similarities. we define a set of parameters m of the same dimension as m such that m (), with denoting a rectified linear unit so that () max0, as such, we denote m c to be the selection of the cth mask column of dimension d (in pseudocode m c m[:, c]). the mask plays the role of an elementwise gating function selecting the relevant dimensions of the embedding required to attend to a particular concept. the role of the masking operation is visually sketched in figure the masked distance function between two images x i and x j is given by while appearing to be a small technical change, the inclusion of a masking mechanism for the tripletloss has a highly nontrivial effect. the mask induces a subspace over the relevant embedding dimensions, effectively attending only to the relevant dimensions for the visual concept being queried. in the loss function above, that translates into a modulated cost phasing out euclidean distances between irrelevant featuredimensions while preserving the lossstructure of the relevant ones. given an triplet t i, j, l defined over indices of the observed images and a corresponding conditionindex c, the final triplet loss function l t () is given by: we want to encourage embeddings to be drawn from a unit ball to maintain regularity in the latent space. we encode this in an embedding loss function l w given by: the separate subspaces are computed as f (x)m c to prevent the masks from expanding the embedding and to encourage sparse masks, we add a loss to regulate the masks: without these terms, an optimization scheme may choose to inflate embeddings to create space for new data points instead of learning appropriate parameters to encode the semantic structure. we define a lossfunction l csn for training csns by putting together the defined loss functions. given images x, triplet constraints with associated condition t, c as well as parameters for the masks m and the embedding function , the csn loss is defined as the parameters and weight the contributions of the triplet terms against the regular embedding terms. in our paper, the nonlinear embedding function is defined as f (x) w g(x), where g(x) is a convolutional neural network. in the masked learning procedure the masks learn to select specific dimensions in the embedding that are associated with a given notion of similarity. at the same time, f () learns to encode the visual features such that different dimensions in the embedding encode features associated to specific semantic notions of similarity. then, during test time each image can be mapped into this embedding by f (). by looking at the different dimensions of the images representation, one can reason about the different semantic notions of similarity. we call a feature space spanned by a function with this property disentangled , as it preserves the separation of the similarity notions through test time. we focus our experiments on evaluating the semantic structure of the learned embeddings and their subspaces as well as the underlying convolutional filters. we perform experiments on two different datasets. first, for illustrative purposes we use a dataset of fonts collected by bernhardsson. the dataset contains million images of single characters in gray scale with a size of by pixels each. the dataset exhibits variations according to font style and character type. in particular, it contains different characters in 50,000 fonts, from which we use the first 1,000. second, we use the zappos50k shoe dataset [28] collected by yu and grauman. the dataset contains 50,000 images of individual richly annotated shoes, with a size of by pixels each, which we resize to by the images exhibit multiple complex variations. in particular, we are looking into four different characteristics: the type of the shoes (i.e., shoes, boots, sandals or slippers), the suggested gender of the shoes (i.e., for women, men, girls or boys), the height of the shoes heels (numerical measurements from to inches) and the closing mechanism of the shoes (buckle, pull on, slip on, hook and loop or laced up). we also use the shoes brand information to perform a finegrained classification test. to supervise and evaluate the triplet networks, we sample triplet constraints from the annotations of the datasets. for the font dataset, we sample triplets such that two characters are of the same type or font and one is different. for the zappos dataset, we sample triplets in an analogous way for the three categorical attributes. for the heel heights we have numerical measurements so that for each triplet we pick two shoes with similar height and one with different height. first, we split the images into three parts: for training, for validation and in the test set. then, we sample triplets within each set. for each attribute we collect 200k train, 20k validation and 40k test triplets. as initial model for our experiments we use a convnet pretrained on imagenet. all model variants are finetuned on the same set of triplets and only differ in the way they are trained. we compare four different approaches, which are schematically illustrated in figure standard triplet network: the common approach to learn from triplet constrains is a single convolutional network where the embedding layer receives supervision from the triplet loss defined in equation as such, it aims to learn from all available triplets jointly as if they come from a single measure of similarity. set of task specific triplet networks: second, we compare to a set of n c separate triplet network experts, each of which is trained on a single notion of similarity. this overcomes the simplifying assumption that all comparisons come from a single measure of similarity. however, this comes at the cost of significantly more parameters. this is the best model achievable with currently available methods. we compare two variants of conditional similarity networks. both extend a standard triplet network with a masking operation on the embedding vector and supervise the network with the loss defined in equation the first variant learns the convolutional filters and the embedding. we show the four different model variants used in our experiments with the example of three objects being compared according to two contradictory notions of similarity, green and red. (a) a standard triplet network that treats all triplets equally (b) a set of ncmany triplet network experts specialized on green or red, respectively (c) a csn with masks preset to be disjoint, so that in the embedding each dimension encodes a feature for a specific notion of similarity (d) a learned csn, where the masks are learned to select features relevant to the respective notion of similarity. masks are predefined to be disjoint between the different notions of similarity. this ensures the learned embedding is fully disentangled, because each dimension must encode features that describe a specific notion of similarity. conditional similarity networks learned masks: the second variant learns the convolutional filters, the embedding and the mask parameters together. this allows the model to learn unique features for the subspaces as well as features shared across tasks. this variant has the additional benefit that the learned masks can provide interesting insight in how different similarity notions are related. we train different convolutional networks for the two datasets. for the font dataset, we use a variant of the vgg architecture [18] with layers of by convolutions and two fully connected layers, which we train from scratch. for the zappos dataset we finetune an layer deep residual network [11] that is pretrained on imagenet [8]. we remove one downsampling module to adjust for the smaller image size. we train the networks with a minibatch size of and optimize using adam [13] with 5e5, and for all our experiments we use an embedding dimension of and the weights for the embedding losses are 5e3 and 5e4. in each minibatch we sample triplets uniformly and for each condition in equal proportions. we train each model for epochs and perform early stopping in that we evaluate the snapshot with highest validation performance on the test set. for our csn variants, we use two masks over the embedding for the fonts dataset and four masks for the zappos dataset, one mask per similarity notion. for models with predefined masks, we allocate 1/n c th of the embedding dimensions to one task. when learning masks, we initialize m using a normal distribution with mean and variance. following the relu, this results in initial mask values that induce random subspaces for each similarity measure. we observe that different random subspaces perform better than a setup where all subspaces start from the same values. masks that are initialized as disjoint analogous to the predefined masks perform similar to random masks, but are not able to learn shared features. we visually explore the learned embeddings regarding their consistency according to respective similarity notions. we stress that all of these semantic representations are taking place within a shared space produced by the same network. the representations are disentangled so that each dimension encodes a feature for a specific notion of similarity. this allows us to use a simple masking operation to look into a specific semantic subspace. figure shows embeddings of the two subspaces in the fonts dataset, which we project down to two dimensions using tsne [22]. the learned features are successfully disentangled such that the dimensions selected by the first mask describe the character type (left) and those selected by the second mask the font style (right). figures and show embeddings of the four subspaces learned with a csn on the zappos50k dataset. figure 5(a) shows the subspace encoding features for the closure mechanism of the shoes. figure 5(b) shows the subspace attending to the type of the shoes. the embedding clearly separates the different types of shoes into boots, slippers and so on. highlighted areas reveal some interesting details. for example, the highlighted region on the upper right side shows nearby images of the same type (shoes) that are completely different according to all other aspects. this means the selected feature dimensions successfully focus only on the type aspect and do not encode any of the other notions. figure 7(a) shows the subspace for suggested gender for the shoes. the subspace separates shoes that are for female and male buyers as well as shoes for adult or youth buyers. the learned submanifold occupies a rotated square with axes defined by gender and age. finally, figure 7(b) shows a continuous embedding of heel heights, which is a subtle visual feature. the key feature of csns is the fact that they can learn separated semantic subspaces in the embeddings using the masking mechanism. we visualize the masks for our common model choices in figure we show the traditional triplet loss, where each dimension is equally taken into account for each triplet. further, we show predefined masks that are used to factorize the embedding into fully disjoint features. lastly, we show a learned mask. interestingly, the masks are very sparse in accordance with the 2d embeddings presented in the previous section, confirming that the concepts are lowdimensional. further, although many additional dimensions are available, the model learned to share some of the features across concepts. this demonstrates that csns can learn to only use the required number of dimensions via relevance determination, reducing the need for picking the right embedding dimensionality. to evaluate the quality of the learned embeddings by the different model variants, we test how well they generalize to unseen triplets. in particular, we perform triplet prediction on a testset of holdout triplets from the zappos50k dataset. we first train each model on a fixed set of triplets, where triplets are sourced from the four different notions of similarity. after convergence, we evaluate for each triplet with associated query i, j, l, c in the testset whether the distance between i and l is smaller than between i and j according to concept/query c. since this is a binary task, random guessing would perform at an error rate of the error rates for the different models are shown in table standard triplet networks fail to capture finegrained similarity and only reach an error rate of the set of task specific triplet networks greatly improves on that, achieving an error rate of this shows that simply learning a single space cannot capture multiple similarity notions. however, this comes at a the cost of n c figure visualization of the masks: left: in standard triplet networks, each dimension is equally taken into account for each triplet. center: the conditional similarity network allows to focus on a subset of the embedding to answer a triplet question. here, each mask focuses on one fourth. right: for learned masks, it is evident that the model learns to switch off different dimensions per question. further, a small subset is shared across tasks. times more model parameters. conditional similarity networks with fixed disjoint masks achieve an error rate of 10.79, clearly outperforming both the single triplet network as well as the set of specialist networks, which have a lot more parameters available for learning. this means by factorizing the embedding space into separate semantic subspaces, csns can successfully capture multiple similarity notions without requiring substantially more parameters. moreover, csns benefit from learning all concepts jointly within one model, utilizing shared structure between the concepts while keeping the subspaces separated. csns with learned masks achieve an error rate of improving performance even further. this indicates the benefits from allowing the model to determine the relevant dimensions and to share features across concepts. triplet prediction results: we evaluate how many triplets of the test set are satisfied in the learned embeddings. triplets come from four different similarity notions. the proposed conditional similarity network clearly outperforms standard triplet networks that treat each triplet as if it came from the same similarity notion. moreover, csns even outperform sets of specialist triplet networks where a lot more parameters are available during training and each network is specifically trained towards one similarity notion. csns with learned masks provide the best performance. error further, we evaluate the impact of the number of unique triplets available during training on performance. we compare models trained on 5, 25, and thousand triplets per concept. figure shows that triplet networks generally improve with more available triplets. further, csns with fixed masks consistently outperform set of specialized triplet networks. lastly, csns with learned masks figure triplet prediction performance with respect to number of unique training triplets available. csns with fixed masks consistently outperform the set of specialized triplet networks. csns with learned masks generally require more triplets, since they need to learn the embedding as well as the masks. however, when enough triplets are available, they provide the best performance. generally require more triplets, since they need to learn the embedding as well as the masks. however, when enough triplets are available, they provide the best performance. we now evaluate how the different learning approaches affect the visual features of the networks. we compare standard triplet networks to csns. both are initialized from the same imagenet pretrained residual network and finetuned using the same triplets and with their respective losses as described in section we evaluate the features learned by the two approaches, by subsequently performing brand classification on the zappos dataset. in particular, we keep all convolutional filters fixed and replace the last embedding layer for both networks with one hidden and one softmax classification layer. we select the brands in the zappos dataset with the most examples and train with a standard multiclass classification approach using the brands as classes. it is noteworthy that the triplets used for the finetuning do not contain brand information. the results are shown in table the residual network trained on imagenet leads to very good initial visual features for general classification tasks. starting from the pretrained model, we observe that the standard triplet learning approach decreases the quality of the visual features, while csns retain most of the information. in the triplet prediction experiment in section standard triplet networks do not perform well, as they are naturally limited by the fact that contradicting notions cannot be satisfied in one single space. this classification result documents that the problem reaches even deeper. the contradicting gradients do not stop at the embedding layer, instead, they expose the entire network to inconsistent learning signals and hurt the underlying convolutional features. in this work, we propose conditional similarity networks to learn nonlinear embeddings which incorporate multiple aspect of similarity within a shared embedding. the learned embeddings are disentangled such that each embedding dimension encodes semantic features for a specific aspect of similarity. this allows to compare objects according to various notions by selecting an appropriate subspace using an elementwise mask. we demonstrate that csns clearly outperform single triplet networks, and even sets of specialist triplet networks where a lot more parameters are available and each network is trained towards one similarity notion. further, instead of being a blackbox predictor, csns are qualitatively highly interpretable as evidenced by our exhibition of the semantic submanifolds they learn. moreover, they provide a featureexploration mechanism through the learned masks which surfaces the structure of the private and shared features between the different similarity aspects. lastly, we empirically find that naively training a triplet network with triplets generated through different similarity notions does not only limit the ability to correctly embed triplets, it also hurts the underlying convolutional features and thus generalization performance. the proposed csns are a simple to implement and easy to train endtoend alternative to resolve these problems. for future work, it would be interesting to consider learning from unlabeled triplets with a clustering mechanism to discover similarity substructures in an unsupervised way.", "summary": "problem statement a common way of measuring image similarity is to embed them into feature spaces where distance acts as a proxy for similarity. but this feature space can capture one (or a weighted combination) of the many possible notions of similarity. what if contracting notions of similarity could be captured at the same time in terms of semantically distinct subspaces. the paper proposes a new architecture called as conditional similarity networks (csns) which learns a disentangled embedding such that the features, for different notions of similarity, are encoded into separate dimensions. it jointly learns masks (or feature extractors) that select and reweights relevant dimensions to induce a subspace that encodes a specific notion of similarity. conditional similarity networks given an image, x, learn a nonlinear feature embedding f(x) such that for any images x1 and x2, the euclidean distance between f(x1) and f(x2) reflects their similarity. conditional similarity triplets given a triplet of images (x1, x2, x3) and a condition c (the notion of similarity), an oracle (say crowd) is used to determmine if x1 is more similar to x2 or x3 as per the given criteria c. in general, for images i, j, l, the triplet t is ordered i, j, l c if i is more similar to j than l. learning from triplets define a loss function lt() to model the similarity structure over the triplets. lt(i, j, l) max0, d(i, j) d(i, l) h where d is the euclidean distance function and h is the similarity scalar margin to prevent trivial solutions. to model conditional similarities, masks m are defined as m () where is the relu unit and is a set of parameters to be learnt. mc denotes the selection of the cth mask column from feature vector. it thus acts as an elementwise gating function which selects the relevant dimensions of the embedding to attend to a particular similarity concept. the euclidean function d now computes the masked distance (f(i, c)mc) between the two given images. two regularising terms are also added l2 norm for d and l1 norm for m. experiments datasets fonts dataset by bernhardsson million by 64pixel grey scale images. zappos50k shoe dataset contains 50,000 images of individual richly annotated shoes. characteristics of interest: type of the shoes (i.e., shoes, boots, sandals or slippers) suggested gender of the shoes (i.e., for women, men, girls or boys) height of the shoes\u2019 heels (0 to inches) closing mechanism of the shoes (buckle, pull on, slip on, hook and loop or laced up) models initial model for the experiments is a convnet pretrained on imagenet standard triplet network learn from all available triplets jointly as if they have the same notion of similarity. set of task specific triplet networks train n separate triplet networks such that each is trained on a single notion of similarity. needs far more parameters and compute. conditional similarity networks fixed disjoint masks in this version, only the convolutional filters and the embedding is learnt and masks are predefined to be disjoint. aims to learn a fully disjoint embedding. conditional similarity networks learned masks learns all the components conv filters, embedding and the masks. refer paper for details on hyperparameters. results visual exploration of the learned subspaces (tsne visualisation) show that network successfully disentangles different features in the embedded vector space. the learned masks are very sparse and share dimensions. this shows that csns may learn to only use the required number of dimensions thereby doing away with the need of picking the right size of embedding. order of performance: csns with learned masks csns with fixed masks taskspecific networks standard triplet network. though csns with learned masks require more training data. csns also outperform standard triplet network when used as off the shelf features for (brand) classification task and is very close to the performance of resnet trained on imagenet. this shows that while csn retained most of the information in the original network, the training mechanism of standard triplet network hurts the underlying conv features and their generalising capability"}, {"document": "data structure selection and tuning is laborious but can vastly improve an applications performance and memory footprint. some data structures share a common interface and enjoy multiple implementations. we call them darwinian data structures (dds), since we can subject their implementations to survival of the fittest. we introduce artemis a multiobjective, cloudbased searchbased optimisation framework that automatically finds optimal, tuned dds modulo a test suite, then changes an application to use that dds. artemis achieves substantial performance improvements for every project in java projects from dacapo benchmark, popular projects and uniformly sampled projects from github. for execution time, cpu usage, and memory consumption, artemis finds at least one solution that improves all measures for (37/43) of the projects. the median improvement across the best solutions is 4.8, 10.1, for runtime, memory and cpu usage. these aggregate results understate artemiss potential impact. some of the benchmarks it improves are libraries or utility functions. two examples are gson, a ubiquitous java serialization framework, and xalan, apaches xml transformation tool. artemis improves gson by 16.5, and for memory, runtime, and cpu; artemis improves xalans memory consumption by every client of these projects will benefit from these performance improvements.under the immense time pressures of industrial software development, developers are heeding one part of knuths advice: they are avoiding premature optimisation. indeed, developers appear to be avoiding optimisation altogether and neglecting the critical when selecting data structures from libraries, in particular, they tend to rely on defaults and neglect potential optimisations that alternative implementations or tuning parameters can offer. this, despite the impact that data structure selection and tuning can have on application performance and defects. consider three examples. selecting an implementation that creates unnecessary temporary objects for the programs workload [46]. selecting a combination of scala data structures that scaled better, reducing execution time from to minutes [36]. avoiding the use of poor implementation, such as those in the oracle bug database that leak memory [48]. optimisation is timeconsuming, especially on large code bases. an optimisation for one version of a program can break or become a deoptimisation in the next release. another reason developers may avoid optimisation are development fads that focus on fast solutions, like premature optimisation is the horror of all evil and hack until it works [18]. in short, optimisation is expensive and its benefits unclear for many projects. developers need automated help. data structures are a particularly attractive optimisation target because they have a welldefined interface; many are tunable; and different implementations of a data structure usually represent a particular tradeoff between time and storage, making some operations faster but more spaceconsuming or slower but more spaceefficient. for instance, an ordered list makes retrieving the entire dataset in sorted order fast, but inserting new elements slow, whilst a hash table allows for quick insertions and retrievals of specific items, but listing the entire set in order is slow. we introduce darwinian data structures, distinct data structures that are interchangeable because they share an abstract data type and can be tuned. the darwinian data structure optimisation problem is the problem of finding an optimal implementation and tuning for a darwinian data structure used in an input program. we aim to help developers perform optimisation cheaply, focusing solving the data structure optimisation problem. we present artemis, a cloudbased optimisation framework that identifies darwinian data structures and, given a test suite, automatically searches arxiv:1706.03232v3 [cs.se] aug for optimal combinations of implementations and parameters for them. artemis is languageagnostic; we have instantiated it for java and c, and present optimisation results for both languages (section 5). artemis search is multiobjective, seeking to simultaneously improve a programs execution time, memory usage, and cpu usage while passing all the test suites. artemis scales to large code bases because is uses a genetic algorithm on those regions of its search space with the most solutions (section 4.4). artemis is the first technique to apply multiobjective optimisation to the darwinian data structure selection and tuning problem. artemis promises to change the economics of data structure optimisation. given a set of darwinian data structures, artemis can search for optimal solutions in the background on the cloud, freeing developers to focus on new features. artemis makes economical small optimizations, such as a few percent, that would not pay for the developer time spent realizing them. and sometimes, of course, artemis, by virtue of being used, will find unexpectedly big performance gains. artemis is a sourcetosource transformer. when artemis finds a solution, the program variants it produces differ from the original program only at constructor calls and relevant type annotations. thus, artemis variants are amenable, by design, to programmer inspection and do not increase technical debt [7]. to ease inspection, artemis generates a diff for each changes it makes. developers can inspect these diffs and decide which to keep and which to reject. we report results on popular diverse github projects, on dacapo benchmark which was constructed to be representative, and a corpus of github projects, filtered to meet artemiss constraints and sampled uniformly at random. in this study, artemis achieves substantial performance improvements for all projects in its corpus. in terms of execution time, cpu usage, and memory consumption, artemis finds at least one solution for out of projects that improves all measures. across all produced optimal solutions, the median improvement for execution time is 4.8, memory consumption and cpu usage this result is for various corpora, but it is highly likely to generalise to arbitrary programs because of the care we took to build a representative corpus (section 5.1). these aggregate results understate artemiss potential impact. some of our benchmarks are libraries or utilities. all of their clients will enjoy any improvements artemis finds for them. three examples are the apache projects powerful xslt processor xalan, googlehttpjavaclient, the unbiquitious java library for accessing web resources, and googles inmemory file system jimfs. section shows that artemis improved xalans memory consumption by 23.5, while leaving its execution time unchanged; artemis improved googlehttpjavaclients execution time by and its cpu usage by 39.6; finally, artemis improved jimfss execution time by and its cpu usage by 10.7, while leaving its memory consumption unchanged. our principal contributions follow: we formalise the darwinian data structure selection and optimisation problem ds (section 3). we implement artemis, a multilanguage optimisation framework that automatically discovers and optimises suboptimal data structures and their parameters. listing contains a code snippet from googlehttpjavaclient , a popular java library for accessing efficiently resources on the web. in the listing 1, getaslist packages http headers and is invoked frequently from other methods because they use it every time they construct an http request. thus, its functionality is important for the performance of googlehttpjavaclient. listing uses arraylist to instantiate the result variable. however, other list implementations share the same functionality but different nonfunctional properties. thus, replacing arraylist with other list implementations may affect the performance of the program. considering the variant created when replacing arraylist (listing 1, line 4) with linkedlist, when we compare it with the original program against the same test set for runs (section 4), we see that the googlehttpjavaclient achieves a median 46, with confidence interval [45.6, 46.3] improvement in execution time (section 5). artemis, our optimization framework, automatically discovers underperforming data structures and replaces them with better choices using searchbased techniques (section 4.4). first, it automatically creates a store of data structures from the languages collection api library (section 4.1). then, artemis traverses the programs ast to identify which of those data structures are used and exposes them as parameters to the artemiss optimizer (section 4.4) by transforming line into where d is the tag that refers to the exposed parameter associated with the defined data structure type (section 4). listing does not specify the initial capacity size of the arraylist, so the default size was used. if the instantiated list object contains less than items, the default capacity can result in memory bloat. if the list object contains more than items, the default capacity can slow the execution time; more memory realllocation operations will happen. therefore, an appropriate value must be chosen to find a good tradeoff between memory and execution time. https://github.com/google/googlehttpjavaclient artemis automatically exposes such arguments as tunable parameters, then adjusts them to improve the runtime performance of the program. for instance, artemis changes line to the code below: where s refers to the exposed parameter associated with the amount of preallocated memory. this section defines the darwinian data structure and parameter optimisation problem we solve in this paper. definition (abstract data type). an abstract data type (adt) is class of objects whose logical behavior is defined by a set of values and a set of operations [10]. a data structure concretely implements an adt. for the corpus of programs c and the adt a, the data structure extraction function dse(a, c) returns all data structures that implement a in c. this function is integral to the definition that follows. in words, darwinian data structures are darwinian in the sense that they can be replaced to produce program mutants whose fitness we can evaluate. the adt a has darwinian data structures when it has more than one data structure that are equivalent over the operations the adt defines. in java, list is an adt and arraylist, which implements it, is a data structure. linkedlist also implements list, so both arraylist and linkedlist are darwinian. for the adt a and the corpus c, darwinian data structures are interchangeable. thus, we can search the variants of p c formed by substituting one darwinian data structure for another to improve ps nonfunctional properties, like execution time, memory consumption or cpu usage. just as we needed a function to extract an adts data structures from a corpus for definition 2, we need a function that returns the adt that a data structure implements: when d dse(a, c), let adte(d, c) a. let d bind fully scopequalified declarations of names to darwinian data structures in c. we use d when creating variants of a program via substitution. we are interested not just searching the space of darwinian data structures, but also tuning them via their constructor parameters. to this end, we assume without loss of generality that a defines a single constructor c and we let n.c(x) denote calling identifier ns constructor c with parameters x : to create a variant of p c that differs from p only in its k bindings of names to darwinian data structures or their constructor initialization parameter, we define (p, (n, this vectorbased definition simultaneously considers all possible rebinding of names to darwinian data structures in p; it is also cumbersome, compared to its pointsubstitution analogue. we could not, however, simply present a scalar definition and then quantify over all potential ddss substitutions, as doing so would not surface synergies and antagonisms among the substitutions. the artemiss optimisation framework solves the darwinian data structure selection problem. figure illustrates the architecture with its three main components: the darwinian data structures store generator (ddssg), the extractor, and the optimiser. artemis takes the languages collection api library, the users application source code and a test suite as input to generate an optimised version of the code with a new combination of data structures. the ddssg builds a store that contains data structure transformation rules. the extractor uses this store to discover potential data structure transformations and exposes them as tunable parameters to the optimiser (see section 4.2). the optimiser uses a multiobjective genetic search algorithm (nsgaii [13]) to tune the parameters [2527, 44, 45] and to provide optimised solutions (see section 4.4). a regression test suite is used to maintain the correctness of the transformations and to evaluate the nonfunctional properties of interest. artemis uses a builtin profiler that measures execution time, memory and cpu usage. artemis relies on testing to define a performance search space and to preserve semantics. artemis therefore can only be applied to programs with a test suite. ideally, this test suite would comprise both a regression test suite with high code coverage for maintaining the correctness of the program and a performance test suite to simulate the reallife behaviour of the program and ensure that all of the common features are covered [4]. even though performance test suites are a more appropriate and logical choice for evaluating the nonfunctional properties of the program, most real world programs in github do not provide such performance test suite. for this reason, we use the regression test suites to evaluate the nonfunctional properties of the github projects of this study whenever a performance test suite is not available. artemis needs a darwinian data structure store (ddss) from which to choose when creating variants. let a be a set of adts known to be darwinian. a developer can augment this set; figure shows those that artemis knows by default. for our corpus c of java benchmarks augmented with jdk libraries over a, to build the default ddss for java, artemis extracts and traverses each projects class hierarchy, similar to the one illustrated in figure this hierarchy shows potential darwinian data structures of a specific interface. when this traversal finishes, artemis extracts all the implementations of a particular darwinian data structure; e.g., list, arraylist, linkedlist. artemis considers these implementations mutually replaceable. for java, a default ddss is provided by artemis, which the developer can edit. for other languages, the ddss can be provided manually by the user and this step can be skipped. the optimiser, described next, uses the store during its search. the developer can also extend the store with custom usersupplied implementations or with implementations from other thirdparty libraries such as google guava collections , fastutil and apache commons collections the extractor takes as input the program ps source code, identifies darwinian data structures in p modulo its store (section 4.1), when implementing artemis, we encountered coding practices that vastly increase the search space. many turn out to be known bad practices [43]. in lines and 8, we see two linkedlist variables that the extractor marks darwinian and candidates for replacement by their equivalent arraylist implementation. in these lines, user is violating the precept to program to the interface, here list, but is, instead, declaring the variable to have the concrete, data structure not adt, type linkedlist. this bad practice [43] adds dependencies to the code, limiting code reuse. they are especially problematic for artemis, because they force artemis to apply multiple transformations to replace and optimise the data structure. further, func3 takes a linkedlist as a parameter, not list, despite the fact that it only calls the get method defined by list on this parameter. this instance of violating the program to the interface precept triggers a compilation error if artemis navely changes func1s type. artemis transforms the code to reduce the optimisers search space and handle these bad practices. artemis supports thee transformations parserless, supertype, and profiler. the parserless mode changes greadily each appearance of a darwinian implementation. when optimising list, it exhaustively tries every implementation of list for every list variable. it is parserless, since it needs only a regular expression to identify rewritings. this makes it simple, easily portable to other languages, and fast, so it is artemis default. however, it generates invalid programs and a large search space. artemis sypertype transformation converts the type of a darwinian implementation to that of their darwinian adt, for example linkedlistt listt on lines, 2,7,8 and for listing 2, this tranformation exposes only two dds to the optimiser and produces only syntactically valid code. to implement this transformation, artemis invokes eclipses refactoring functionality via its api, then validates the result. artemis aims to be languageagnostic without any additional dependencies on language specific tools. for this case, artemis auto performs this transformation by adding the supertype as an equivalent parameter in the store of data structures. whenever the ast visitor traverses a variable or parameter declaration expression it may replace the darwinian data structure with its supertype. all data structures are equal, but some data structures are more equal than others ; some dds affect a programs performance more than others, as when one stores only a few, rarely accessed items. to rank dds, artemis profiles its input program to identify costly methods. the extractor uses this info to identify the subset of a programs dds worth considering for optimisation. artemis instrumentation is particularly important for large programs. the optimiser searches a combination of data structures that improves the performance of the initial program while keeps the original functionality. practically, we can represent all those data structures as parameters that can be tuned using search based software engineering approaches [19]. because of the nature of the various conflicting performance objectives, the problem we faced here requires a multiobjective optimisation approach to search the (near) optimal solutions. an array of integers is used to represent the tuning parameters. each parameter refers either to a darwinian data structure or to the initial size of that data structure. if the parameter refers to a data structure, its value represents the index in the list of darwinian data structures. the optimiser keeps additional mapping information to distinguish the types of the parameters. for each generation, the nsgaii applies tournament selection, followed by a uniform crossover and a uniform mutation operation. in our experiments, we designed fitness functions to capture execution time, memory consumption, and cpu usage. after fitness evaluation, artemis applies standard nondominated selection to form the next generation. artemis repeats this process until the solutions in a generation converge. at this point, artemis returns all nondominated solutions in the final population. we used ga because the search space is huge. let d be the definitions of darwinian data structures in program p. let i be the number of implementations for a particular d d. the size of the search space is: artemis provides optimisation as a cloud service. to use the service, developers only need to provide the source code of their project in a maven build format and a performance test suite invoked by mvn test. artemis returns the optimised source code and a performance report. artemis exposes a restful api that developers can use to edit the default store of darwinian data structures. the api also allows developers to select other search based algorithms; the optimiser uses nsgaii by default. to use our tool from the command line, a simple command is used: ./artemis inputprogramsrc where this command defaults to artemiss built in ddssg. artemis writes the source of an optimized variant of its input for each measure. artemis also supports optional parameters to customise its processing. to demonstrate the performance improvements that artemis automatically achieves and its broad applicability, we applied it to three corpora: popular github projects, projects from the dacapo benchmark, and projects, filtered to meet artemiss requirements, then sampled uniformly at random from github. to show also that artemisis languageagnostic, we applied it to optimise guetzli (section 5.3), a jpeg encoder written in c. artemis requires projects with capable build systems and an extensive test suites. these two requirements entail that artemis be able to build and run the project against its test suite. artemis is https://github.com/google/guetzli languageagnostic but is currently only instantiated for java and c, so it requires java or c programs. our first corpus comprises eight popular github projects. we selected these eight to have good test suites and be diverse. we defined popular to be projects that received at least stars on github. we deemed a test suite to be good if its line coverage met or exceeded this corpus contains projects, usually wellwritten, optimised and peer codereviewed by experienced developers. we applied artemis on those projects to investigate whether it can provide a better combination of data structures than those selected by experienced human developers. this first corpus might not be representative, precisely because of the popularity of its benchmarks. to address this threat to validity, we turned to the dacapo benchmarks [5]. the authors of dacapo built it, from the ground up, to be representative. the goal was to provide the research community with realistic, large scale java benchmarks that contain a good methodology for java evaluation. dacapo contains open source, clientside java benchmarks (version 9.12) and they come with builtin extensive evaluation. each benchmark provides accurate measurements for execution time and memory consumption. dacapo first appeared in to work with java v.1.5 and has not been further updated to work with newer versions of java. for this reason, we faced difficulties in compiling all the benchmarks and the total number of benchmarks were reduced to out of in this corpus we use the following five: fop, avrora, xalan, pmd and sunflow (figure 4). because of its age and the fact that we are only using subset of it, our dacapo benchmark may not be representative. to counter this threat, we uniformly sampled projects from github. we discarded those that did not meet artemiss constraints, like being equipped with a build system, until we collected projects. those projects are diverse, both in domain and size. the selected projects include static analysers, testing frameworks, web clients, and graph processing applications. their sizes vary from to 94k lines of code with a median of their popularity varies from to stars with a median of stars per project. the median number of tests is and median line coverage ratio is collectively, we systematically built these corpora to be representative in order to demontrate the general applicably of the artemis optimization framework. the full list of the programs used in this experimental study are available online in the projects website. experiments were conducted using microsoft azure tm d4v2 machines with one intel e52673v3 cpu featuring cores and 14gb of dram and built with oracle jdk and ubuntu lts. performance measurements may lead to incorrect results if not handled carefully [1]. thus, a statistical rigorous performance evaluation is required [16,23,28]. to mitigate instability and incorrect results, we differentiate vm startup and steadystate. we ran our experiments in a fresh azure vm that contained only the jvm and the subject. we use junit, which runs an entire test suite in a single jvm. we manually identified and dropped startup runs, then we spotchecked the results to confirm that the rest of the runs achieved a steady state and were exhibiting low variance. all https://darwinianoptimiser.com/corpus of the means and medians we reported fall within the computed interval with confidence. to assure the accuracy and reduce the bias in the measurement, program profiling period was set as seconds, and each generated solution was run for more than simulations. also we use mann whitney u test [15] to examine if the improvement is statistically significant. to measure the memory consumption and cpu usage of a subject program, we use the popular jconsole profiler because it directly handles jdk statistics and provides elegant api. we extended jconsole to monitor only those system processes belonging to the test suite. we use maven surefire plugin to measure the test suites execution time because it reports only the execution time of each individual test, excluding the measurement overhead that other maven plugins may introduce. for the optimiser, we chose an initial population size of and a maximum number of function evaluations. we used the tournament selection (based on ranking and crowding distance), simulated binary crossover (with crossover probability 0.8) and polynomial mutation (with the mutation probability 0.1). we determined these settings from calibration trials to ensure the maturity of the results. since nsgaii is stochastic, we ran each experiment times to obtain statistical significant results. artemis aims to improve all objectives at the same time. therefore the first research question we would like to answer is: to answer rq1, we applied artemis to our corpus. we inspected the generated optimal solutions from runs of each subject by examining the dominate relation between the optimal and inital solutions regarding the optimisation objectives. we introduce the terms strictly dominate relation and nondominated relation to describe the relation. defined by zitzler et al. [49], a solution strictly dominates another solution if it outperforms the latter in all measures. a solution is nondominated with another solution if both outperform the other in at least one of the measures. for dacapo, artemis found at least one strictly dominant solution for out of projects; it found no such solution for sunflow. it found solutions, from which are strictly dominant (median is solutions per project) and are nondominated (median is solutions per project). for the popular github projects, artemis found at least one strictly dominant solution for all projects. the total number of solutions found is and of them are strictly dominant (median is solutions per project) and are nondominated (median is solutions per project). for the sampled github projects, artemis found a strictly dominant solution for out of projects, but found no solution for projects rubixverifier, epubcheck, dworker, telegrambots and fqueue. it found of which of them are strictly dominant (median is solutions per project) and are non dominant http://openjdk.java.net/tools/svc/jconsole/ http://maven.apache.org/components/surefire/mavensurefireplugin/ (median is solutions per project). with these results, we answer rq1 affirmatively: finding1: artemis finds optimised variants that outperform the original program in at least one measure for all programs in our representative corpus. this finding understates artemiss impact. not only did it improve at least one measure for all programs, artemis found solutions that improve all measures for of the programs. having found that artemis finds performance improvements, we ask how good are these improvements with: rq2: what is the average improvement that artemis provides for each program? though artemis aims to improve all candidates measures, it cannot achieve that if improvements are antagonistic. in some domains, it is more important to significantly improve one of the measures than to improve slightly all measures; e.g., a high frequency trading application may want to pay the cost of additional memory overhead in order to improve the execution time. our intuition is that the optimiser will find many solutions on the paretofront and at least one of them will improve each measure significantly. we answer rq2 quantitatively. we report the maximum improvement (median value with confidence interval) for execution time, memory and cpu usage for each subject of the three corpora. we use bar charts with error bars to plot the three measures for each program. in y axis, we represent the percentage of improvement for each measure. a value less than represents an improvement and a value greater than means degradation; e.g., memory consumption implies that the solution consumes of the memory used in the input program. selected popular github programs. figure 3a presents the three measures of the solutions when the execution time is minimised, for each program from the popular github programs. we observe that artemis improves the execution time of every program. googlehttpjavaclients execution time was improved the most; its execution time was reduced by m46, ci [45.6, 46.3]. we also notice that this execution time improvement did not affect negatively the other measures, but instead the cpu usage was reduced by m41.6, ci [39.6, 43.6] and memory consumption remained almost the same. the other interesting program to notice from this graph is solo, a blogging system written in java; its execution time improved slightly by but its memory consumption increased by finally, for this set of solutions, the median execution time improvement is 14.13, whilst memory consumption slightly increased by and cpu usage decreased by for those programs, artemis extracted a median of data structures and the optimal solutions had a median of data structures changes from the original versions of the program. figure 3b shows the solutions optimised for memory consumption. we notice that artemis improves the memory consumption for all programs, with a median value of the execution time was improved by a median value of for these solutions, while the median value of cpu usage is slightly increased by we notice that solo has the best improvement by m31.1, ci dacapo. figure presents all solutions optimised for execution time and memory consumption for the dacapo benchmark. we used only two measures for the dacapo benchmark as those were the ones built in the benchmark suite. we chose not to extend or edit the profiling method of dacapo, to avoid the risk of affecting the validity of its existing, well tested profiling process. artemis found solutions that improve the execution time for every program without affecting significantly the memory consumption, except project xalan which had improvement (m4.8, ci [4.6, 5.7] in execution time but with an increase (5.8, ci [3.5, 7]) in memory consumption. all solutions for optimised memory consumption did not affect execution time, except for a slight increase for program fop. finally, for this set of solutions, the median percentage of execution time improvement is 4.8, and for memory consumption. for this set of programs, sampled github programs. figure presents all solutions optimised for execution time, memory consumption and cpu usage for the sampled github programs. as with the previous corpora, artemis found solutions that improved each measure significantly. artemis improves the median value of execution time across all projects by 4.6, memory consumption by and cpu usage by artemis found solutions with antagonistic improvement for projects jafka and documents4j. artemis found a solution that improves the execution time of jafka, a distributed publishsubscribe messaging system, by m12, ci [11.2, 13.6], but also increases its memory consumption by m23.6, ci [21.4, 25.7]. it also found a solution that improves the memory consumption of documents4j (m38, ci [38, 41]) but introduced extra cpu usage m26.1, ci [24.2, 28]. a median of data structures were extracted and the optimal solutions had a median of data structures changes from the original versions of the program. observing again the numbers across the three corpora, we can say that they are quite consistent, showing that artemis finds optimal solutions that improve significantly the different optimisation measures. we also see that the number of darwinian data structures extracted (between and 18) and the optimal solutions dds changes (between and 5) are quite similar for the three corpora. analysing all results from the corpora we conclude the discussion of rq2 with: finding2: artemis improves the median across all programs in our corpus by execution time, memory consumption, and cpu usage. we ask this question to understand which changes artemis makes to a program. applied across all optimal solutions. we see that the most common transformation for all measures is replacing arraylist with linkedlist, it appears 91, and times respectevely across all measures. this transformation indicates that most developers prefer to use arraylist in their code, which in general is considered to be faster, neglecting use cases in which linkedlist performs better; e.g., when the program has many list insertion or removal operations. except hashmap to linkedhashmap, the other transformations happen relatively rare in the optimal solutions. last, the median number of lines artemis changes is finding3: artemis extracted a median of darwinian data structures from each program and the optimal solutions had a median of data structure changes from the original versions of the program. what is the cost of using artemis? in order for artemis to be practical and useful in realworld situations, it is important to understand the cost of using it. the aforementioned experimental studies reveal that, even for the popular programs, the existing selection of the data structure and the setting of its parameters may be suboptimal. therefore, optimising the data structures and their parameters can still provide significant improvement on nonfunctional properties. to answer this research question, the cost of artemis for optimising a program is measured by the cost of computational resources it uses. in this study, we used a microsoft azure tm d4v2 machine, which costs per hour at a standard payasyougo rate , to conduct all experiments. the experiments show that an optimisation process takes hours on average for all studied subjects. the program graphjet and jimfs are the most and the least timeconsuming programs respectively, with hours and minutes optimisation time. accordingly, the average cost of applying artemis for the subjects studied is 1.25, with a range from to the experimental results show that overall cost of using artemis is negligible compared to a human software engineer, with the assumption that a competent software engineer can find those optimisation in a reasonable time. artemis transforms the selection of data structure and sets parameters by rewriting source code, thereby allowing human developers to easily investigate its changes and gain insight about the usage of data structures and the characteristics of the program. to show the versatility of the artemis framework, we ask rq2, rq3 and rq4 over google guetzli, a very popular jpeg encoder written in c. we used the stl containers and their operations as darwinian data structures. more specifically, we considered the push back and emplace back as equivalent implementations of the same functionality and exposed those as tunable parameters to artemiss optimiser. we collected a random sample of images (available online ) and used it to construct a performance suite that evaluates the execution time of guetzli. we answer rq2 by showing that artemis found an optimal solution that improves execution time by we answer rq3 by showing that artemis extracted and tuned parameters and found an optimal solution with parameter changes. artemis spent hours (costs 0.62) to find optimal solutions which is between the limits reported in rq4. last, we spent approximately days to extend artemis to support c, using the parserless mode. section discusses the steps we took to address the threats to the external validity of the results we present here. in short, we built three subcorpora, each more representative than the last, for a total of programs, diverse in size and domain. the biggest threat to the internal validity of our work is the difficulty of taking accurate performance measurements of applications running on vm, like the jvm. section details the steps, drawn from best practice, we took to address this threat. in essence, we conducted calibration experiments to adjust the parameters such that the http://darwinianoptimiser.com/corpus algorithm converges quickly and stops after the results become stable. for measuring the nonfunctional properties, we carefully chose jconsole profiler that directly gathers runtime information from jdk, such that the measurement error is minimised. moreover, we carefully tuned jconsole to further improve the precision of the measurements by maximising its sampling frequency such that it does not miss any measurements while minimising the cpu overhead. to cater for the stochastic nature of artemis and to provide the statistic power for the results, we ran each experiment times and manually checked that experiments had a steady state and exhibited low variance. multiobjective darwinian data structure selection and optimisation stands between two areas: searchbased software engineering and data structure performance optimisation. previous work applies genetic programming [21,29,30,35,37,39] to either improve the functionality (bug fixing) [11,17] or nonfunctional properties of a program [8,21,25,35,38,44]. their approaches use existing code as the code base and replace some of the source code in the program under optimisation with the code from the code base. however, many of these approaches rely on the plastic surgery hypothesis [2], which assumes that the solutions exist in the code base. artemis, on the other hand, does not rely on this hypothesis. artemis can harvest darwinian data structures both from the program, but also from external code repositiories; further, artemis relies on a set of transformation rules that it can automatically exhaustively extract from library code or documentation. [45] introduced a mutationbased method to expose deep parameters, similar to those we optimise in this paper, from the program under optimisation, and tuned these parameters along with shallow parameters to improve the time and memory performance of the program. though the idea of exposing additional tunable parameter is similar to artemis, their approach did not optimise data structure selection, which can sometimes be more rewarding than just tuning the parameters. moreover, they applied their approach to a memory management library to benefit that librarys clients. the extent of improvement usually depends on how much a program relies on that library. in contrast, artemis directly applies to the source code of the program, making no assumptions about which libraries the program uses, affording artemis much wider applicability. a body of work [3, 14, 3234, 42, 47] has attempted to identify bloat arising from data structures. [40,41] introduced a semantic profiler that provides online collectionusage semantics for java programs. they instrumented java virtual machine (jvm) to gather the usage statistics of collection data structures. using heuristics, they suggest a potentially better choice for a data structure for a program. though developers can add heuristics, if they lack sufficient knowledge about the data structures, they may bias the heuristics and jeopardise the effectiveness of the approach. artemis directly uses the performance of a data structure profiled against a set of performance tests to determine the optimal choices of data structures. therefore, artemis does not depend on expert human knowledge about the internal implementation and performance differences of data structures to formulate heuristics. artemis relies on carefullychosen performance tests to minimse bias. furthermore, artemis directly modifies the program instead of providing hints, thus users can use the finetuned program artemis generates without any additional manual adjustment. other frameworks provide users with manually or automatically generated selection heuristics to improve the data structure selection process. jitds [12] exploits declarative specifications embedded by experts in data structures to adapt them. collectionswitch [9] uses data and userdefined performance rules to select other data structure variants. brainy [22] provides users with machine learning cost models that guide the selection of data structures. artemis does not require expert annotations, userdefined rules or any machine learning knowledge. storage strategies [6] changes vms to optimize their performance on collections that contain a single primitive type; artemis rewrites source code and handles userdefined types and does not make vm modifications. [31] introduced a collection data structure replacement and optimisation framework named seeds. their framework replaces the collection data structures in java applications with other data structures exhaustively and automatically select the most energy efficient one to improve the overall energy performance of the application. conceptually artemis extends this approach to optimise both the data structures and their initialization parameters. artemis also extends the optimisation objectives from single objective to triple objectives and used pareto nondominated solutions to show the tradeoffs between these objectives. due to a much larger search space in our problem, the exhaustive exploration search that used by seeds is not practical, therefore we adopted metaheuristic search. furthermore, artemis directly transforms the source code of the programs whilst seeds transforms the bytecode, so artemis provides developers more intuitive information about what was changed and teaches them to use more efficient data structures. moreover, artemis can be more easily applied to other languages as it does not depend on language specific static analysers and refactoring tools such as wala [20] and eclipse ides refactoring tools. in order to support another language we just need the grammar of that language and to implement a visitor that extracts a programs darwinian data structures. we note that antlr, which artemis uses, currently provides many available grammar languages apart from the novelties mentioned above, this is the largest empirical study to our knowledge compared to similar work. in the studies mentioned above, only to subjects were included in the experiments. our study included the dacapo benchmark, sampled github subjects and wellwritten popular subjects to show the effectiveness of artemis, therefore our results are statistically more meaningful. developers frequently use underperformed data structures and forget to optimise them with respect to some critical nonfunctional properties once the functionalities are fulfilled. in this paper, we introduced artemis, a novel multiobjective multilanguage searchbased framework that automatically selects and optimises darwinian data structures and their arguments in a given program. artemis is language agnostic, meaning it can be easily adapted to any programming language; extending artemis to support c took approximately days. given as input a data structure store with darwinian implementations, it can automatically detect and optimise them along with any additional parameters to improve the nonfunctional properties of the given program. in a large empirical study on dacapo benchmarks, randomly sampled projects and wellwritten popular github projects, artemis found strong improvement for all of them. on extreme cases, artemis found improvement on execution time, improvement on memory consumption, and improvement on cpu usage. artemis found such improvements making small changes in the source code; the median number of lines artemis changes is thus, artemis is practical and can be easily used on other projects. at last, we estimated the cost of optimising a program in machine hours. with a price of per machine hour, the cost of optsimising any subject in this study is less than 8, with an average of therefore, we conclude that artemis is a practical tool for optimising the data structures in large realworld programs.", "summary": "darwinian data structure selection basios et al., fse\u201918 graphit may have caught your attention for the success of its approach, but i suspect for many readers it\u2019s not something you\u2019ll be immediately applying. darwinian data structures (ddss) on the other hand looks to be of immediate interest to many java and c projects (and generalises beyond those languages). what i would have called an adt (e.g., a list), the authors call darwinian data structures. the darwinian\u2019 part comes from the fact that adts have multiple concrete implementations, and artemis, a multiobjective, cloudbased searchbased optimisation framework finds the best implementation class (e.g. arraylist, linkedlist) for your specific use case. it does this using the nsgaii genetic algorithmbased optimiser in the current implementation. in brief, artemis finds the places in your code where you are using an adt, and explores the possible concrete instantiation space for those adts using your test suite as a guide to performance. then it outputs the transformed source. you might be wondering whether e.g. linkedlist vs arraylist makes that big a difference in most real world projects: artemis achieves substantial performance improvements for every project in java projects from dacapo benchmark, popular projects, and uniformly sampled projects from github. for execution time, cpu usage, and memory consumption, artemis finds at least one solution that improves all measures for (37/43) of the projects. the median improvement across the best solutions is 4.8, 10.1, and for runtime, memory, and cpu usage. for example, consider this code from googlehttpjavaclient, which currently uses arraylist : switching to linkedlist and comparing performance over the same test set for runs, we get a median reduction in execution time. we are interested not just in searching the space of darwinian data structures, but also tuning them via their constructor parameters. for example, choosing an appropriate initial capacity size for an arraylist. endtoend artemis works like this: there\u2019s a oneoff upfront exercise to analyse the collections library / libraries of interest and build a dictionary that describes the search space. then given the source code and test suite of a project artemis explores the ast to find uses of ddss, outputting a templated version of the source code with replacement points for each usage. a search algorithm is then used to find the best choice in each location, with the test suite being used to judge performance. finding candidate program points for dds substitution given an ast, it\u2019s easy to find declarations using the abstract data type (e.g. list), but in the code bases under study the authors also found many cases where programmers had overspecified, using a concrete type for variable and parameter type declarations, e.g. artemis will apply further transformations to replace these with the abstract type instead, thus permitting dds exploration. many programs make extensive use of collection types, resulting in a very large overall search space. artemis profiles the input program while running the test suite to identify the highest value points in the program to explore and thus prunes the search space. profiling is done using the jconsole profiler. searching for the best parameters the overall search space for a given dds consists of all the possible concrete implementation types, together with the parameter spaces for their respective constructor arguments. for each generation, nsgaii applies tournament selection, followed by a uniform crossover and a uniform mutation operation. in our experiments, we designed fitness functions to capture execution time, memory consumption, and cpu usage. after fitness evaluation, artemis applies standard nondominated selection to form the next generation. artemis repeats this process until the solutions in a generation converge. at this point, artemis returns all nondominated solutions in the final population. in the evaluation, the initial population size is set to 30, with a limit of function evaluations. to assess fitness artemis relies on running the test suite. therefore the results will only apply to production use cases to the extent that your test suite mirrors production usage. even though performance test suites are a more appropriate and logical choice for evaluating the nonfunctional properties of the program, most real world programs in github do not provide a performance suite. for this reason, we use the regression test suites to evaluate the nonfunctional properties of the github projects of this study whenever a performance test suite is not available. test suite execution time is measured using the maven surefire plugin, with profiling done by jconsole. each generated solution is run for at least simulations, with startup/ jvm warmup runs not counting towards this total. (see [virtual machine warmup blows hot and cold]( [url]"}, {"document": "adversarial examples have attracted significant attention in machine learning, but the reasons for their existence and pervasiveness remain unclear. we demonstrate that adversarial examples can be directly attributed to the presence of nonrobust features: features (derived from patterns in the data distribution) that are highly predictive, yet brittle and (thus) incomprehensible to humans. after capturing these features within a theoretical framework, we establish their widespread existence in standard datasets. finally, we present a simple setting where we can rigorously tie the phenomena we observe in practice to a misalignment between the (humanspecified) notion of robustness and the inherent geometry of the data. the pervasive brittleness of deep neural networks [sze14; eng19b; hd19; ath18] has attracted significant attention in recent years. particularly worrisome is the phenomenon of adversarial examples [big13; sze14], imperceptibly perturbed natural inputs that induce erroneous predictions in stateoftheart classifiers. previous work has proposed a variety of explanations for this phenomenon, ranging from theoretical models [sch18; bpr18] to arguments based on concentration of measure in highdimensions [gil18; mdm18; sha19a]. these theories, however, are often unable to fully capture behaviors we observe in practice (we discuss this further in section 5).. more broadly, previous work in the field tends to view adversarial examples as aberrations arising either from the high dimensional nature of the input space or statistical fluctuations in the training data [sze14; gss15; gil18]. from this point of view, it is natural to treat adversarial robustness as a goal that can be disentangled and pursued independently from maximizing accuracy [mad18; shs19; sug19], either through improved standard regularization methods [tg16] or pre/postprocessing of network inputs/outputs [ues18; cw17a; he17].. in this work, we propose a new perspective on the phenomenon of adversarial examples. in contrast to the previous models, we cast adversarial vulnerability as a fundamental consequence of the dominant supervised learning paradigm. specifically, we claim that:. adversarial vulnerability is a direct result of our models\u2019 sensitivity to wellgeneralizing features in the data.. recall that we usually train classifiers to solely maximize (distributional) accuracy. consequently, classifiers tend to use any available signal to do so, even those that look incomprehensible to humans. after all, the presence of a tail or ears is no more natural to a classifier than any other equally predictive feature. in fact, we find that standard ml datasets do admit highly predictive yet imperceptible features. our models learn to rely on these nonrobust features, leading to adversarial perturbations that exploit this dependence.1. our hypothesis also suggests an explanation for adversarial transferability: the phenomenon that adversarial perturbations computed for one model often transfer to other, independently trained models. since any two models are likely to learn similar nonrobust features, perturbations that manipulate such features will apply to both. finally, this perspective establishes adversarial vulnerability as a humancentric phenomenon, since, from the standard supervised learning point of view, nonrobust features can be as important as robust ones. it also suggests that approaches aiming to enhance the interpretability of a given model by enforcing priors for its explanation [mv15; oms17; smi17] actually hide features that are meaningful and predictive to standard models. as such, producing humanmeaningful explanations that remain faithful to underlying models cannot be pursued independently from the training of the models themselves.. to corroborate our theory, we show that it is possible to disentangle robust from nonrobust features in standard image classification datasets. specifically, given any training dataset, we are able to construct:. a robustified version for robust classification (figure 1a)2 we demonstrate that it is possible to effectively remove nonrobust features from a dataset. concretely, we create a training set (semantically similar to the original) on which standard training yields good robust accuracy on the original, unmodified test set. this finding establishes that adversarial vulnerability is not necessarily tied to the standard training framework, but is also a property of the dataset.. a nonrobust version for standard classification (figure 1b)2. we are also able to construct a training dataset for which the inputs are nearly identical to the originals, but all appear incorrectly labeled. in fact, the inputs in the new training set are associated to their labels only through small adversarial perturbations (and hence utilize only nonrobust features). despite the lack of any predictive humanvisible information, training on this dataset yields good accuracy on the original, unmodified test set. this demonstrates that adversarial perturbations can arise from flipping features in the data that are useful for classification of correct inputs (hence not being purely aberrations).. finally, we present a concrete classification task where the connection between adversarial examples and nonrobust features can be studied rigorously. this task consists of separating gaussian distributions, and is loosely based on the model presented in tsipras et al. [tsi19], while expanding upon it in a few ways. first, adversarial vulnerability in our setting can be precisely quantified as a difference between the intrinsic data geometry and that of the adversary\u2019s perturbation set. second, robust training yields a classifier which utilizes a geometry corresponding to a combination of these two. lastly, the gradients of standard models can be significantly more misaligned with the interclass direction, capturing a phenomenon that has been observed in practice in more complex scenarios [tsi19]. we begin by developing a framework, loosely based on the setting proposed by tsipras et al. [tsi19], that enables us to rigorously refer to robust and nonrobust features. in particular, we present a set of definitions which allow us to formally describe our setup, theoretical results, and empirical evidence.. we consider binary classification3, where inputlabel pairs (x, y) x are sampled from a (data) distribution d; the goal is to learn a classifier c : x which predicts a label y corresponding to a given input x.. 1it is worth emphasizing that while our findings demonstrate that adversarial vulnerability does arise from nonrobust features, they do not preclude the possibility of adversarial vulnerability also arising from other phenomena [tg16; sch18]. for example, nakkiran [nak19a] constructs adversarial examples that do not exploit nonrobust features (and hence do not allow one to learn a generalizing model from them). still, the mere existence of useful nonrobust features suffices to establish that without explicitly discouraging models from utilizing these features, adversarial vulnerability will remain an issue.. 2the corresponding datasets for cifar10 are publicly available at http://git.io/advdatasets. 3our framework can be straightforwardly adapted though to the multiclass setting.. we define a feature to be a function mapping from the input space x to the real numbers, with the set of all features thus being f f : x r. for convenience, we assume that the features in f are shifted/scaled to be meanzero and unitvariance (i.e., so that e(x,y)d [ f (x)] and e(x,y)d [ f (x)2] 1), in order to make the following definitions scaleinvariant4. note that this formal definition also captures what we abstractly think of as features (e.g., we can construct an f that captures how furry an image is).. useful, robust, and nonrobust features. we now define the key concepts required for formulating our framework. to this end, we categorize features in the following manner:. useful features: for a given distributiond, we call a feature f useful ( 0) if it is correlated with the true label in expectation, that is if. (1) we then define d( f ) as the largest for which feature f is useful under distribution d. (note that if a feature f is negatively correlated with the label, then f is useful instead.) crucially, a linear classifier trained on useful features can attain nontrivial generalization performance.. robustly useful features: suppose we have a useful feature f (d( f ) 0). we refer to f as a robust feature (formally a robustly useful feature for 0) if, under adversarial perturbation (for some specified set of valid perturbations ), f remains useful. formally, if we have that. useful, nonrobust features: a useful, nonrobust feature is a feature which is useful for some bounded away from zero, but is not a robust feature for any these features help with classification in the standard setting, but may hinder accuracy in the adversarial setting, as the correlation with the label can be flipped.. in our framework, a classifier c (f, w, b) is comprised of a set of features f f , a weight vector w, and a scalar bias b. for a given input x, the classifier predicts the label y as. for convenience, we denote the set of features learned by a classifier c as fc. 4this restriction can be straightforwardly removed by simply shifting/scaling the definitions.. standard training. training a classifier is performed by minimizing a loss function (via empirical risk minimization (erm)) that decreases with the correlation between the weighted combination of the features and the label. the simplest example of such a loss is when minimizing classification loss, no distinction exists between robust and nonrobust features: the only distinguishing factor of a feature is its usefulness. furthermore, the classifier will utilize any useful feature in f to decrease the loss of the classifier.. in the presence of an adversary, any useful but nonrobust features can be made anticorrelated with the true label, leading to adversarial vulnerability. therefore, erm is no longer sufficient to train classifiers that are robust, and we need to explicitly account for the effect of the adversary on the classifier. to do so, we use an adversarial loss function that can discern between robust and nonrobust features [mad18]:. for an appropriately defined set of perturbations since the adversary can exploit nonrobust features to degrade classification accuracy, minimizing this adversarial loss (as in adversarial training [gss15; mad18]) can be viewed as explicitly preventing the classifier from learning a useful but nonrobust combination of features.. we want to note that even though the framework above enables us to formally describe and predict the outcome of our experiments, it does not necessarily capture the notion of nonrobust features exactly as we intuitively might think of them. for instance, in principle, our theoretical framework would allow for useful nonrobust features to arise as combinations of useful robust features and useless nonrobust features [goh19b]. these types of constructions, however, are actually precluded by our experimental results (in particular, the classifiers trained in section would not generalize). this shows that our experimental findings capture a stronger, more finegrained statement than our formal definitions are able to express. we view bridging this gap as an interesting direction for future work. the central premise of our proposed framework is that there exist both robust and nonrobust features that constitute useful signals for standard classification. we now provide evidence in support of this hypothesis by disentangling these two sets of features.. on one hand, we will construct a robustified dataset, consisting of samples that primarily contain robust features. using such a dataset, we are able to train robust classifiers (with respect to the standard test set) using standard (i.e., nonrobust) training. this demonstrates that robustness can arise by removing certain features from the dataset (as, overall, the new dataset contains less information about the original training set). moreover, it provides evidence that adversarial vulnerability is caused by nonrobust features and is not inherently tied to the standard training framework.. on the other hand, we will construct datasets where the inputlabel association is based purely on nonrobust features (and thus the corresponding dataset appears completely mislabeled to humans). we show that this dataset suffices to train a classifier with good performance on the standard test set. this indicates that natural models use nonrobust features to make predictions, even in the presence of robust features. these features alone are actually sufficient for nontrivial generalizations performance on natural images, which indicates that they are indeed valuable features, rather than artifacts of finitesample overfitting.. a conceptual description of these experiments can be found in figure 5just as for the other parts of this model, we use this loss for simplicity onlyit is straightforward to generalize to more practical loss function such as logistic or hinge loss. recall that the features a classifier learns to rely on are based purely on how useful these features are for (standard) generalization. thus, under our conceptual framework, if we can ensure that only robust features are useful, standard training should result in a robust classifier. unfortunately, we cannot directly manipulate the features of very complex, highdimensional datasets. instead, we will leverage a robust model and modify our dataset to contain only the features that are relevant to that model.. in terms of our formal framework (section 2), given a robust (i.e., adversarially trained [mad18]) model c we aim to construct a distribution dr which satisfies:. e(x,y)dr [ f (x) y] e(x,y)d [ f (x) y] if f fc otherwise,. where fc again represents the set of features utilized by c. conceptually, we want features used by c to be as useful as they were on the original distribution d while ensuring that the rest of the features are not useful under dnr.. we will construct a training set for dr via a onetoone mapping x xr from the original training set for d. in the case of a deep neural network, fc corresponds to exactly the set of activations in the penultimate layer (since these correspond to inputs to a linear classifier). to ensure that features used by the model are equally useful under both training sets, we (approximately) enforce all features in fc to have similar values for both x and xr through the following optimization:. where x is the original input and g is the mapping from x to the representation layer. we optimize this objective using gradient descent in input space6.. since we don\u2019t have access to features outside fc, there is no way to ensure that the expectation in (5) is zero for all f fc. to approximate this condition, we choose the starting point of gradient descent for the optimization in (6) to be an input x0 which is drawn fromd independently of the label of x (we also explore sampling x0 from noise in appendix d.1). this choice ensures that any feature present in that input will. 6we follow [mad18] and normalize gradient steps during this optimization. experimental details are provided in appendix c.. not be useful since they are not correlated with the label in expectation over x0. the underlying assumption here is that, when performing the optimization in (6), features that are not being directly optimized (i.e., features outside fc) are not affected. we provide pseudocode for the construction in figure (appendix c).. given the new training set for dr (a few random samples are visualized in figure 2a), we train a classifier using standard (nonrobust) training. we then test this classifier on the original test set (i.e. the results (figure 2b) indicate that the classifier learned using the new dataset attains good accuracy in both standard and adversarial settings as a control, we repeat this methodology using a standard (nonrobust) model for c in our construction of the dataset. sample images from the resulting nonrobust dataset dnr are shown in figure 2athey tend to resemble more the source image of the optimization x0 than the target image x. we find that training on this dataset leads to good standard accuracy, yet yields almost no robustness (figure 2b). we also verify that this procedure is not simply a matter of encoding the weights of the original modelwe get the same results for both dr and dnr if we train with different architectures than that of the original models.. overall, our findings corroborate the hypothesis that adversarial examples can arise from (nonrobust) features of the data itself. by filtering out nonrobust features from the dataset (e.g. by restricting the set of available features to those used by a robust model), one can train a significantly more robust model using standard training. the results of the previous section show that by restricting the dataset to only contain features that are used by a robust model, standard training results in classifiers that are significantly more robust. this suggests that when training on the standard dataset, nonrobust features take on a large role in the resulting learned classifier. here we set out to show that this role is not merely incidental or due to finitesample overfitting. in particular, we demonstrate that nonrobust features alone suffice for standard generalization i.e., a model trained solely on nonrobust features can perform well on the standard test set.. to show this, we construct a dataset where the only features that are useful for classification are nonrobust features (or in terms of our formal model from section 2, all features f that are useful are nonrobust). to accomplish this, we modify each inputlabel pair (x, y) as follows. we select a target class t either (a) uniformly at random among classes (hence features become uncorrelated with the labels) or (b) deterministically according to the source class (e.g. using a fixed permutation of labels). then, we add a small adversarial perturbation to x in order to ensure it is classified as t by a standard model. where lc is the loss under a standard (nonrobust) classifier c and is a small constant. the resulting inputs are nearly indistinguishable from the originals (appendix d figure 9)to a human observer, it thus appears that the label t assigned to the modified input is simply incorrect. the resulting inputlabel pairs (xadv, t) make up the new training set (pseudocode in appendix c figure 6).. now, since xadv x is small, by definition the robust features of xadv are still correlated with class y (and not t) in expectation over the dataset. after all, humans still recognize the original class. on the other hand, since every xadv is strongly classified as t by a standard classifier, it must be that some of the nonrobust features are now strongly correlated with t (in expectation).. in the case where t is chosen at random, the robust features are originally uncorrelated with the label t (in expectation), and after the adversarial perturbation can be only slightly correlated (hence being significantly. 7in an attempt to explain the gap in accuracy between the model trained on dr and the original robust classifier c, we test distributional shift, by reporting results on the robustified test set in appendix d.3.. 8in order to gain more confidence in the robustness of the resulting model, we attempt several diverse attacks in appendix d.2.. less useful for classification than before) formally, we aim to construct a dataset drand where :. e(x,y)drand [y f (x)] if f nonrobustly useful under d, otherwise. in contrast, when t is chosen deterministically based on y, the robust features actually point away from the assigned label t. in particular, all of the inputs labeled with class t exhibit nonrobust features correlated with t, but robust features correlated with the original class y. thus, robust features on the original training set provide significant predictive power on the training set, but will actually hurt generalization on the standard test set. viewing this case again using the formal model, our goal is to construct ddet such that. e(x,y)ddet [y f (x)] if f nonrobustly useful under d, if f robustly useful under d r otherwise ( f not useful under d)11. we find that standard training on these datasets actually generalizes to the original test set, as shown in table 1). this indicates that nonrobust features are indeed useful for classification in the standard setting. remarkably, even training on ddet (where all the robust features are correlated with the wrong class), results in a wellgeneralizing classifier. this indicates that nonrobust features can be picked up by models during standard training, even in the presence of robust features that are predictive one of the most intriguing properties of adversarial examples is that they transfer across models with different architectures and independently sampled training sets [sze14; pmg16; crp19]. 9goh [goh19a] provides an approach to quantifying this robust feature leakage and finds that one can obtain a (small) amount of test accuracy by leveraging robust feature leakage on drand.. 10note that the optimization procedure we describe aims to merely approximate this condition, where we once again use trained models to simulate access to robust and nonrobust features.. note that regardless how useful a feature is on ddet, since it is useless on d it cannot provide any generalization benefit on the unaltered test set.. 12additional results and analysis (e.g. training curves, generating drand and ddet with a robust model, etc.) d.6 and d.5 13we also show that the models trained on drand and ddet generalize to cifar10.1 [rec19] in appendix d.7.. that this phenomenon can in fact be viewed as a natural consequence of the existence of nonrobust features. recall that, according to our main thesis, adversarial examples can arise as a result of perturbing wellgeneralizing, yet brittle features. given that such features are inherent to the data distribution, different classifiers trained on independent samples from that distribution are likely to utilize similar nonrobust features. consequently, an adversarial example constructed by exploiting the nonrobust features learned by one classifier will transfer to any other classifier utilizing these features in a similar manner.. in order to illustrate and corroborate this hypothesis, we train five different architectures on the dataset generated in section (adversarial examples with deterministic labels) for a standard resnet50 [he16]. our hypothesis would suggest that architectures which learn better from this training set (in terms of performance on the standard test set) are more likely to learn similar nonrobust features to the original classifier. indeed, we find that the test accuracy of each architecture is predictive of how often adversarial examples transfer from the original model to standard classifiers with that architecture (figure 3). in a similar vein, nakkiran [nak19a] constructs a set of adversarial perturbations that is explicitly nontransferable and finds that these perturbations cannot be used to learn a good classifier. these findings thus corroborate our hypothesis that adversarial transferability arises when models learn similar brittle features of the underlying dataset. the experiments from the previous section demonstrate that the conceptual framework of robust and nonrobust features is strongly predictive of the empirical behavior of stateoftheart models on realworld datasets. in order to further strengthen our understanding of the phenomenon, we instantiate the framework in a concrete setting that allows us to theoretically study various properties of the corresponding model. our model is similar to that of tsipras et al. [tsi19] in the sense that it contains a dichotomy between robust and nonrobust features, but extends upon it in a number of ways:. the adversarial vulnerability can be explicitly expressed as a difference between the inherent data metric and the metric.. robust learning corresponds exactly to learning a combination of these two metrics.. the gradients of adversarially trained models align better with the adversary\u2019s metric.. we study a simple problem of maximum likelihood classification between two gaussian distributions. in particular, given samples (x, y) sampled from d according to. our goal is to learn parameters (, ) such that. where (x; , ) represents the gaussian negative loglikelihood (nll) function. intuitively, we find the parameters , which maximize the likelihood of the sampled data under the given model. classification under this model can be accomplished via likelihood test: given an unlabeled sample x, we predict y as. in turn, the robust analogue of this problem arises from replacing (x; y , ) with the nll under adversarial perturbation. the resulting robust parameters r can be written as. a detailed analysis of this setting is in appendix ehere we present a highlevel overview of the results.. (1) vulnerability from metric misalignment (nonrobust features). note that in this model, one can rigorously make reference to an inner product (and thus a metric) induced by the features. in particular, one can view the learned parameters of a gaussian (, ) as defining an inner product over the input space given by x, y (x )1(y ). this in turn induces the mahalanobis distance, which represents how a change in the input affects the features learned by the classifier. this metric is not necessarily aligned with the metric in which the adversary is constrained, the 2norm. actually, we show that adversarial vulnerability arises exactly as a misalignment of these two metrics.. theorem (adversarial vulnerability from misalignment). consider an adversary whose perturbation is determined by the lagrangian penalty form of (12), i.e.. max (x ; y , ) c 2,. where c min() is a constant trading off nll minimization and the adversarial constraint14. then, the adversarial loss ladv incurred by the nonrobustly learned (, ) is given by:. and, for a fixed tr() k the above is minimized by kd i.. in fact, note that such a misalignment corresponds precisely to the existence of nonrobust features, as it indicates that small changes in the adversary\u2019s metric along certain directions can cause large changes under the datadependent notion of distance established by the parameters. this is illustrated in figure 4, where misalignment in the featureinduced metric is responsible for the presence of a nonrobust feature in the corresponding classification problem.. (2) robust learning. the optimal (nonrobust) maximum likelihood estimate is , and thus the vulnerability for the standard mle estimate is governed entirely by the true data distribution. the following theorem characterizes the behaviour of the learned parameters in the robust problem. in fact, we can prove (section e.3.4) that performing (sub)gradient descent on the inner maximization (also known as adversarial training [gss15; mad18]) yields exactly r. we find that as the perturbation budget is increased, the metric induced by the learned features mixes and the metric induced by the features. theorem (robustly learned parameters). just as in the nonrobust case, r , i.e. the true mean is learned. for the robust covariance r, there exists an 0, such that for any [0, 0),. the effect of robust optimization under an 2constrained adversary is visualized in figure as e grows, the learned covariance becomes more aligned with identity. for instance, we can see that the classifier learns to be less sensitive in certain directions, despite their usefulness for natural classification.. (3) gradient interpretability. [tsi19] observe that gradients of robust models tend to look more semantically meaningful. it turns out that under our model, this behaviour arises as a natural consequence of theorem in particular, we show that the resulting robustly learned parameters cause the gradient of the linear classifier and the vector connecting the means of the two distributions to better align (in a worstcase sense) under the inner product.. theorem (gradient alignment). let f (x) and fr(x) be monotonic classifiers based on the linear separator induced by standard and 2robust maximum likelihood classification, respectively. the maximum angle formed between the gradient of the classifier (wrt input) and the vector connecting the classes can be smaller for the robust model:. min ,x fr(x) x fr(x) min ,x f (x) x f (x) 14the constraint on c is to ensure the problem is concave. 15note: as discussed in appendix e.3.3, we study a slight relaxation of (12) that approaches exactness exponentially fast as d figure illustrates this phenomenon in the twodimensional case. with 2bounded adversarial training the gradient direction (perpendicular to the decision boundary) becomes increasingly aligned under the inner product with the vector between the means ().. our theoretical analysis suggests that rather than offering any quantitative classification benefits, a natural way to view the role of robust optimization is as enforcing a prior over the features learned by the classifier. in particular, training with an 2bounded adversary prevents the classifier from relying heavily on features which induce a metric dissimilar to the metric. the strength of the adversary then allows for a tradeoff between the enforced prior, and the datadependent features.. robustness and accuracy. note that in the setting described so far, robustness can be at odds with accuracy since robust training prevents us from learning the most accurate classifier (a similar conclusion is drawn in [tsi19]). however, we note that there are very similar settings where nonrobust features manifest themselves in the same way, yet a classifier with perfect robustness and accuracy is still attainable. concretely, consider the distributions pictured in figure in appendix d.10. it is straightforward to show that while there are many perfectly accurate classifiers, any standard loss function will learn an accurate yet nonrobust classifier. only when robust training is employed does the classifier learn a perfectly accurate and perfectly robust decision boundary. several models for explaining adversarial examples have been proposed in prior work, utilizing ideas ranging from finitesample overfitting to highdimensional statistical phenomena [gil18; fff18; for19; tg16; sha19a; mdm18; sha19b; gss15; bpr18]. the key differentiating aspect of our model is that adversarial perturbations arise as wellgeneralizing, yet brittle, features, rather than statistical anomalies or effects of poor statistical concentration. in particular, adversarial vulnerability does not stem from using a specific model class or a specific training method, since standard training on the robustified data distribution of section leads to robust models. at the same time, as shown in section 3.2, these nonrobust features are sufficient to learn a good standard classifier. we discuss the connection between our model and others in detail in appendix a. we discuss additional related work in appendix b. in this work, we cast the phenomenon of adversarial examples as a natural consequence of the presence of highly predictive but nonrobust features in standard ml datasets. we provide support for this hypothesis by. explicitly disentangling robust and nonrobust features in standard datasets, as well as showing that nonrobust features alone are sufficient for good generalization. finally, we study these phenomena in more detail in a theoretical setting where we can rigorously study adversarial vulnerability, robust training, and gradient alignment.. our findings prompt us to view adversarial examples as a fundamentally human phenomenon. in particular, we should not be surprised that classifiers exploit highly predictive features that happen to be nonrobust under a humanselected notion of similarity, given such features exist in realworld datasets. in the same manner, from the perspective of interpretability, as long as models rely on these nonrobust features, we cannot expect to have model explanations that are both humanmeaningful and faithful to the models themselves. overall, attaining models that are robust and interpretable will require explicitly encoding human priors into the training process. here, we describe other models for adversarial examples and how they relate to the model presented in this paper.. concentration of measure in highdimensions. an orthogonal line of work [gil18; fff18; mdm18; sha19a], argues that the high dimensionality of the input space can present fundamental barriers on classifier robustness. at a high level, one can show that, for certain data distributions, any decision boundary will be close to a large fraction of inputs and hence no classifier can be robust against small perturbations. while there might exist such fundamental barriers to robustly classifying standard datasets, this model cannot fully explain the situation observed in practice, where one can train (reasonably) robust classifiers on standard datasets [mad18; rsl18; wk18; xia19; crk19].. [sch18] propose a theoretical model under which a single sample is sufficient to learn a good, yet nonrobust classifier, whereas learning a good robust classifier requires o( d) samples. under this model, adversarial examples arise due to insufficient information about the true data distribution. however, unless the adversary is strong enough (in which case no robust classifier exists), adversarial inputs cannot be utilized as inputs of the opposite class (as done in our experiments in section 3.2). we note that our model does not explicitly contradict the main thesis of schmidt et al. in fact, this thesis can be viewed as a natural consequence of our conceptual framework. in particular, since training models robustly reduces the effective amount of information in the training data (as nonrobust features are discarded), more samples should be required to generalize robustly.. boundary tilting. tanay and griffin [tg16] introduce the boundary tilting model for adversarial examples, and suggest that adversarial examples are a product of overfitting. in particular, the model conjectures that adversarial examples are possible because the class boundary extends beyond the submanifold of sample data and can beunder certain circumstanceslying close to it. consequently, the authors suggest that mitigating adversarial examples may be a matter of regularization and preventing finitesample overfitting. in contrast, our empirical results in section suggest that adversarial inputs consist of features inherent to the data distribution, since they can encode generalizing information about the target class.. inspired by this hypothesis and concurrently to our work, kim, seo, and jeon [ksj19] present a simple classification task comprised of two gaussian distributions in two dimensions. they experimentally show that the decision boundary tends to better align with the vector between the two means for robust models. this is a special case of our theoretical results in section (note that this exact statement is not true beyond two dimensions, as discussed in section 4.). fawzi, moosavidezfooli, and frossard [fmf16] and ford et al. [for19] argue that the adversarial robustness of a classifier can be directly connected to its robustness under (appropriately scaled) random noise. while this constitutes a natural explanation of adversarial vulnerability given the classifier robustness to noise, these works do not attempt to justify the source of the latter.. at the same time, recent work [lec19; crk19; for19] utilizes random noise during training or testing to construct adversarially robust classifiers. in the context of our framework, we can expect the added noise to disproportionately affect nonrobust features and thus hinder the model\u2019s reliance on them.. local linearity. goodfellow, shlens, and szegedy [gss15] suggest that the local linearity of dnns is largely responsible for the existence of small adversarial perturbations. while this conjecture is supported by the effectiveness of adversarial attacks exploiting local linearity (e.g., fgsm [gss15]), it is not sufficient to fully characterize the phenomena observed in practice. in particular, there exist adversarial examples that violate the local linearity of the classifier [mad18], while classifiers that are less linear do not exhibit greater robustness [acw18].. piecewiselinear decision boundaries. [sha19b] prove that the geometric structure of the classifier\u2019s decision boundaries can lead to sparse adversarial perturbations. however, this result does not take into account the distance to the decision boundary along these direction or feasibility constraints on the input domain. as a result, it cannot meaningfully distinguish between classifiers that are brittle to small adversarial perturbations and classifiers that are moderately robust.. theoretical constructions which incidentally exploit nonrobust features. bubeck, price, and razenshteyn [bpr18] and nakkiran [nak19b] propose theoretical models where the barrier to learning robust classifiers is, respectively, due to computational constraints or model complexity. in order to construct distributions that admit accurate yet nonrobust classifiers they (implicitly) utilize the concept of nonrobust features. namely, they add a lowmagnitude signal to each input that encodes the true label. this allows a classifier to achieve perfect standard accuracy, but cannot be utilized in an adversarial setting as this signal is susceptible to small adversarial perturbations. we describe previously proposed models for the existence of adversarial examples in the previous section. here we discuss other work that is methodologically or conceptually similar to ours.. distillation. the experiments performed in section can be seen as a form of distillation. there is a line of work, known as model distillation [hvd14; fur18; bcn06], where the goal is to train a new model to mimic another already trained model. this is typically achieved by adding some regularization terms to the loss in order to encourage the two models to be similar, often replacing training labels with some other target based on the already trained model. while it might be possible to successfully distill a robust model using these methods, our goal was to achieve it by only modifying the training set (leaving the training process unchanged), hence demonstrating that adversarial vulnerability is mainly a property of the dataset. closer to our work is dataset distillation [wan18] which considers the problem of reconstructing a classifier from an alternate dataset much smaller than the original training set. this method aims to produce inputs that directly encode the weights of the already trained model by ensuring that the classifier\u2019s gradient with respect to these inputs approximates the desired weights. (as a result, the inputs constructed do not resemble natural inputs.) this approach is orthogonal to our goal since we are not interested in encoding the particular weights into the dataset but rather in imposing a structure to its features.. adversarial transferabiliy. in our work, we posit that a potentially natural consequence of the existence of nonrobust features is adversarial transferability [pap17; liu17; pmg16]. a recent line of work has considered this phenomenon from a theoretical perspective, confined to simple models, or unbounded perturbations [crp19; zou18]. [tra17] study transferability empirically, by finding adversarial subspaces, (orthogonal vectors whose linear combinations are adversarial perturbations). the authors find that there is a significant overlap in the adversarial subspaces between different models, and identify this as a source of transferability. in our work, we provide a potential reason for this overlapthese directions correspond to nonrobust features utilized by models in a similar manner.. universal adversarial perturbations moosavidezfooli et al. [moo17] construct perturbations that can cause misclassification when applied to multiple different inputs. more recently, jetley, lord, and torr [jlt18] discover input patterns that are meaningless to humans and can induce misclassification, while at the same time being essential for standard classification. these findings can be naturally cast into our framework by considering these patterns as nonrobust features, providing further evidence about their pervasiveness.. manipulating dataset features ding et al. [din19] perform synthetic transformations on the dataset (e.g., image saturation) and study the performance of models on the transformed dataset under standard and robust training. while this can be seen as a method of restricting the features available to the model during. training, it is unclear how well these models would perform on the standard test set. [gei19] aim to quantify the relative dependence of standard models on shape and texture information of the input. they introduce a version of imagenet where texture information has been removed and observe an improvement to certain corruptions. for our experimental analysis, we use the cifar10 [kri09] and (restricted) imagenet [rus15] datasets. attaining robust models for the complete imagenet dataset is known to be a challenging problem, both due to the hardness of the learning problem itself, as well as the computational complexity. we thus restrict our focus to a subset of the dataset which we denote as restricted imagenet. to this end, we group together semantically similar classes from imagenet into superclasses shown in table we train and evaluate only on examples corresponding to these classes. we use the resnet50 architecture for our baseline standard and adversarially trained classifiers on cifar10 and restricted imagenet. for each model, we grid search over three learning rates (0.1, 0.01, 0.05), two batch sizes (128, 256) including/not including a learning rate drop (a single order of magnitude) and data augmentation. we use the standard training parameters for the remaining parameters. the hyperparameters used for each model are given in table to obtain robust classifiers, we employ the adversarial training methodology proposed in [mad18]. specifically, we train against a projected gradient descent (pgd) adversary constrained in 2norm starting from the original image. [mad18] we normalize the gradient at each step of pgd to ensure that we move a fixed distance in 2norm per step. unless otherwise specified, we use the values of e provided in table to train/evaluate our models. we used steps of pgd with a step size of /5. in section 3.1, we describe a procedure to construct a dataset that contains features relevant only to a given (standard/robust) model. to do so, we optimize the training objective in (6). unless otherwise specified, we initialize xr as a different randomly chosen sample from the training set. (for the sake of completeness, we also try initializing with a gaussian noise instead as shown in table 7.) we then perform normalized gradient descent (2norm of gradient is fixed to be constant at each step). at each step we clip the input xr to in the [0, 1] range so as to ensure that it is a valid image. details on the optimization procedure are shown in table we provide the pseudocode for the construction in figure c.5 nonrobust features suffice for standard classification. to construct the dataset as described in section 3.2, we use the standard projected gradient descent (pgd) procedure described in [mad18] to construct an adversarial example for a given input from the dataset (7). perturbations are constrained in 2norm while each pgd step is normalized to a fixed step size. the details for our pgd setup are described in table we provide pseudocode in figure attack parameters cifar10 restricted imagenet d.1 detailed evaluation of models trained on robust dataset. in section 3.1, we generate a robust training set by restricting the dataset to only contain features relevant to a robust model (robust dataset) or a standard model (nonrobust dataset). this is performed by choosing either a random input from the training set or random noise16 and then performing the optimization procedure described in (6). the performance of these classifiers along with various baselines is shown in table we observe that while the robust dataset constructed from noise resembles the original, the corresponding nonrobust does not (figure 7). this also leads to suboptimal performance of classifiers trained on this dataset (only standard accuracy) potentially due to a distributional shift.. 16we use 10k steps to construct the dataset from noise, instead to using 1k steps done when the input is a different training set image (cf. to verify the robustness of our classifiers trained on the robust dataset, we evaluate them with strong attacks [car19]. in particular, we try up to steps of projected gradient descent (pgd), increasing steps until the accuracy plateaus, and also try the cw2 loss function [cw17b] with steps. for each attack we search over step size. we find that over all attacks and step sizes, the accuracy of the model does not drop by more than 2, and plateaus at for both pgd and cw2 (the value given in figure 2). we show a plot of accuracy in terms of the number of pgd steps used in figure in section 3.1, we observe that an erm classifier trained on a robust training dataset dr (obtained by restricting features to those relevant to a robust model) attains nontrivial robustness (cf. in table 8, we evaluate the adversarial accuracy of the model on the corresponding robust training set (the samples which the classifier was trained on) and test set (unseen samples from dr, based on the test set). we find that the drop in robustness comes from a combination of generalization gap (the robustness on the dr test set is worse than it is on the robust training set) and distributional shift (the model performs better on the robust test set consisting of unseen samples from dr than on the standard test set containing unseen samples from d).. d.4 classification based on nonrobust features. figure shows sample images from d, drand and ddet constructed using a standard (nonrobust) erm classifier, and an adversarially trained (robust) classifier.. in table 9, we repeat the experiments in table based on datasets constructed using a robust model. note that using a robust model to generate the ddet and drand datasets will not result in nonrobust features that are strongly predictive of t (since the prediction of the classifier will not change). thus, training a model on these datasets leads to poor accuracy on the standard test set from d.. observe from figure that models trained on datasets derived from the robust model show a decline in test accuracy as training progresses. in table 9, the accuracy numbers reported correspond to the last iteration, and not the best performance. this is because we have no way to crossvalidate in a meaningful way as the validation set itself comes from drand or ddet, and not from the true data distribution d. thus, validation accuracy will not be predictive of the true test accuracy, and thus will not help determine when to early stop. d.6 performance of erm classifiers on relabeled test set. in table 10), we evaluate the performance of classifiers trained on ddet on both the original test set drawn from d, and the test set relabelled using t(y) (y 1) mod c. observe that the classifier trained on ddet constructed using a robust model actually ends up learning permuted labels based on robust features (indicated by high test accuracy on the relabelled test set). [rec19] have constructed an unseen but distributionshifted test set for cifar10. they show that for many previously proposed models, accuracy on the cifar10.1 test set can be predicted as a linear function of performance on the cifar10 test set.. as a sanity check (and a safeguard against any potential adaptive overfitting to the test set via hyperparameters, historical test set reuse, etc.) we note that the classifiers trained on ddet and drand achieve and generalization on the cifar10.1 test set, respectively. this demonstrates nontrivial generalization, and actually perform better than the linear fit would predict (given their accuracies on the cifar10 test set). in this section, we develop a framework for studying nonrobust features by studying the problem of maximum likelihood classification between two gaussian distributions. we first recall the setup of the problem, then present the main theorems from section first we build the techniques necessary for their proofs. we consider the setup where a learner receives labeled samples from two distributions, n (, ), and n (, ). the learner\u2019s goal is to be able to classify new samples as being drawn from d1 or d2 according to a maximum likelihood (mle) rule.. a simple coupling argument demonstrates that this problem can actually be reduced to learning the parameters , of a single gaussiann (, ), and then employing a linear classifier with weight in the standard setting, maximum likelihoods estimation learns the true parameters, and , and thus the learned classification rule is c(x) 1x1 in this work, we consider the problem of adversarially robust maximum likelihood estimation. in particular, rather than simply being asked to classify samples, the learner will be asked to classify adversarially perturbed samples x , where is chosen to maximize the loss of the learner. our goal is to derive the parameters , corresponding to an adversarially robust maximum likelihood estimate of the parameters of n (, ). note that since we have access to (indeed, the learner can just run nonrobust mle to get access), we work in the space where is a diagonal matrix, and we restrict the learned covariance to the set of diagonal matrices.. we denote the parameters of the sampled gaussian by rd, and diag(u)u rd. we use min(x) to represent the smallest eigenvalue of a square matrix x, and (; x) to represent the gaussian negative loglikelihood for a single sample x. for convenience, we often use v x , and r we also define the operator to represent the vectorization of the diagonal of a matrix. in particular, for a matrix x rdd, we have that x v rd if vi xii. we focus on the case where b2(e) for some e 0, i.e. the ball, corresponding to the following minimax problem:. we first derive the optimal adversarial perturbation for this setting (section e.3.1), and prove theorem (section e.3.2). we then propose an alternate problem, in which the adversary picks a linear operator to be applied to a fixed vector, rather than picking a specific perturbation vector (section e.3.3). we argue via gaussian concentration that the alternate problem is indeed reflective of the original model (and in particular, the two become equivalent as d ). in particular, we propose studying the following in place of (13):. min , max mm exn ( ,) [(, ; x m(x ))] (14) wherem m rdd : mij i j, exn ( ,) [ mv22 ] e2 our goal is to characterize the behavior of the robustly learned covariance in terms of the true covariance matrix and the perturbation budget the proof is through danskin\u2019s theorem, which allows us to use any maximizer of the inner problem m in computing the subgradient of the inner minimization. after showing the applicability of danskin\u2019s theorem (section e.3.4) and then applying it (section e.3.5) to prove our main results (section e.3.7). our three main results, which we prove in the following section, are presented below.. first, we consider a simplified version of (13), in which the adversary solves a maximization with a fixed lagrangian penalty, rather than a hard constraint. in this setting, we show that the loss contributed by. the adversary corresponds to a misalignment between the data metric (the mahalanobis distance, induced by 1), and the metric:. theorem (adversarial vulnerability from misalignment). consider an adversary whose perturbation is determined by the lagrangian penalty form of (12), i.e.. max (x ; y , ) c 2,. where c min() is a constant trading off nll minimization and the adversarial constraint17. then, the adversarial loss ladv incurred by the nonrobustly learned (, ) is given by:. and, for a fixed tr() k the above is minimized by kd i.. we then return to studying (14), where we provide upper and lower bounds on the learned robust covariance matrix :. theorem (robustly learned parameters). just as in the nonrobust case, r , i.e. the true mean is learned. for the robust covariance r, there exists an 0, such that for any [0, 0),. finally, we show that in the worst case over mean vectors , the gradient of the adversarial robust classifier aligns more with the interclass vector:. theorem (gradient alignment). let f (x) and fr(x) be monotonic classifiers based on the linear separator induced by standard and 2robust maximum likelihood classification, respectively. the maximum angle formed between the gradient of the classifier (wrt input) and the vector connecting the classes can be smaller for the robust model:. min ,x fr(x) x fr(x) min ,x f (x) x f (x) in the first section, we have shown that the classification between two gaussian distributions with identical covariance matrices centered at and can in fact be reduced to learning the parameters of a single one of these distributions.. thus, in the standard setting, our goal is to solve the following problem:. note that in this setting, one can simply find differentiate with respect to both and , and obtain closed forms for both (indeed, these closed forms are, unsurprisingly, and ). here, we consider the existence of a malicious adversary who is allowed to perturb each sample point x by some the goal of the adversary is to maximize the same loss that the learner is minimizing.. e.3.1 motivating example: 2constrained adversary. we first consider, as a motivating example, an 2constrained adversary. that is, the adversary is allowed to perturb each sampled point by : in this case, the minimax problem being solved is the following:. the following lemma captures the optimal behaviour of the adversary: 17the constraint on c is to ensure the problem is concave.. lemma in the minimax problem captured in (15) (and earlier in (13)), the optimal adversarial perturbation is given by. where v x , and is set such that in this context, we can solve the inner maximization problem with lagrange multipliers. in the following we write b2() for brevity, and discard terms not containing as well as constant factors freely:. now we can solve (17) using the aforementioned lagrange multipliers. in particular, note that the maximum of (17) is attained at the boundary of the ball thus, we can solve the following system of two equations to find , rewriting the norm constraint as 2:. for clarity, we write v x : then, combining the above, we have that. our final result for the maximizer of the inner problem, where is set according to the norm constraint.. e.3.2 variant with fixed lagrangian (theorem 1). to simplify the analysis of theorem 1, we consider a version of (15) with a fixed lagrangian penalty, rather than a norm constraint: max (x ; y , ) c note then, that by lemma 1, the optimal perturbation is given by. (c i)1 we now proceed to the proof of theorem theorem (adversarial vulnerability from misalignment). consider an adversary whose perturbation is determined by the lagrangian penalty form of (12), i.e.. max (x ; y , ) c 2,. where c min() is a constant trading off nll minimization and the adversarial constraint18. then, the adversarial loss ladv incurred by the nonrobustly learned (, ) is given by:. and, for a fixed tr() k the above is minimized by kd i. 18the constraint on c is to ensure the problem is concave.. we begin by expanding the gaussian negative loglikelihood for the relaxed problem:. recall that we are considering the vulnerability at the mle parameters and :. evn (0,i) [ v (c i)1 v v1/2 ( c23 2c )1 1/2 v ] evn (0,i) [ v (c i)1 v v (c i)2 v ] evn (0,i) [ v22 v iv v (c i)1 v v (c i)2 v ]. evn (0,i) [ v22 v ( i (c i)1 )2 v ] tr [( i (c i)1 )2] d. this shows the first part of the theorem. it remains to show that for a fixed k tr(), the adversarial risk is minimized by kd i:. where i are the eigenvalues of now, we have that i k by assumption, so by optimality conditions, we have that minimizes the above if i 1, i.e. (c i 1)3 then, by solving analytically, we find that. c i (c i 1)3 c j (c j 1)3. admits only one real solution, i j. scaling to satisfy the trace constraint yields kd i, which concludes the proof. our motivating example (section e.3.1) demonstrates that the optimal perturbation for the adversary in the 2constrained case is actually a linear function of v, and in particular, that the optimal perturbation can be expressed as dv for a diagonal matrix d. note, however, that the problem posed in (15) is not actually. a minimax problem, due to the presence of the expectation between the outer minimization and the inner maximization. motivated by this and (19), we define the following robust problem:. min , max mm exn ( ,) [(, ; x mv)] , (20) wherem m rdd : mij i j, exn ( ,) [ mv22 ] e2 first, note that this objective is slightly different from that of (15). in the motivating example, is constrained to always have norm, and thus is normalizer on a persample basis inside of the expectation. in contrast, here the classifier is concerned with being robust to perturbations that are linear in v, and of squared norm in expectation.. note, however, that via the result of laurent and massart [lm00] showing strong concentration for the norms of gaussian random variables, in high dimensions this bound on expectation has a corresponding highprobability bound on the norm. in particular, this implies that as d , mv2 almost surely, and thus the problem becomes identical to that of (15). we now derive the optimal m for a given (, ):. consider the minimax problem described by (20), i.e.. min , max mm exn ( ,) [(, ; x mv)] then, the optimal action m of the inner maximization problem is given by. m ( i)1 , (21) where again is set so that m m. proof. we accomplish this in a similar fashion to what was done for , using lagrange multipliers:. m ( i)1 , where is a constant depending on and enforcing the expected squarednorm constraint.. indeed, note that the optimal m for the adversary takes a nearidentical form to the optimal (19), with the exception that is not sampledependent but rather varies only with the parameters. the main tool in proving our key results is danskin\u2019s theorem [dan67], a powerful theorem from minimax optimization which contains the following key result:. theorem (danskin\u2019s theorem). suppose (x, z) : r z r is a continuous function of two arguments, where z rm is compact. then, if for every z z, (x, z) is convex and differentiable in x, and x is continuous:. the subdifferential of f (x) is given by. where conv() represents the convex hull operation, and z0 is the set of maximizers defined as. in short, given a minimax problem of the form minx maxyc f (x, y) where c is a compact set, if f (, y) is convex for all values of y, then rather than compute the gradient of g(x) : maxyc f (x, y), we can simply find a maximizer y for the current parameter x; theorem ensures thatx f (x, y) xg(x). note thatm is trivially compact (by the heineborel theorem), and differentiability/continuity follow rather straightforwardly from our reparameterization (c.f. (22)), and so it remains to show that the outer minimization is convex for any fixed m.. convexity of the outer minimization. note that even in the standard case (i.e. nonadversarial), the gaussian negative loglikelihood is not convex with respect to (, ). thus, rather than proving convexity of this function directly, we employ the parameterization used by [das19]: in particular, we write the problem in terms of t and m under this parameterization, we show that the robust problem is convex for any fixed m.. lemma under the aforementioned parameterization of t and m 1, the following gaussian robust negative loglikelihood is convex:. to prove this, we show that the likelihood is convex even with respect to a single sample x; the result follows, since a convex combination of convex functions remains convex. we begin by looking at the likelihood of a single sample x n (, ):. in terms of the aforementioned t and m, and for convenience defining a (i m)2:. xatxmax ) log ( exp ( xatxmax )). from here, following an identical argument to [das19] equation (3.7), we find that. that the loglikelihood is indeed convex with respect to [. the previous two parts show that we can indeed apply danskin\u2019s theorem to the outer minimization, and in particular that the gradient of f at m m is in the subdifferential of the outer minimization problem. we proceed by writing out this gradient explicitly, and then setting it to zero (note that since we have shown f is convex for all choices of perturbation, we can use the fact that a convex function is globally minimized its subgradient contains zero). we continue from above, plugging in (21) for m and using (22) to write the gradients of with respect to t and m.. [t m ] exn ( ,). using this fact, we derive an implicit expression for the robust covariance matrix note that for the sake of brevity, we now use m to denote the optimal adversarial perturbation (previously defined as m in (21)). this implicit formulation forms the foundation of the bounds given by our main results.. lemma the minimax problem discussed throughout this work admits the following (implicit) form of solution:. where is such that m m, and is thus dependent on rewriting (23) in the standard parameterization (with respect to , ) and reexpanding a (i m)2 yields:. now, note that the equations involving and are completely independent, and thus can be solved separately. in terms of , the relevant system of equations is a a 0, where multiplying by the inverse a gives that. this tells us that the mean learned via 2robust maximum likelihood estimation is precisely the true mean of the distribution.. now, in the same way, we set out to find by solving the relevant system of equations:. now, we make use of the woodbury matrix identity in order to write (i m) as. thus, we can revisit (25) as follows:. we now apply the quadratic formula to get an implicit expression for (implicit since technically depends on ):. this concludes the proof. we now attempt to characterize the shape of as a function of first, we use the fact that e[xv2] tr(x2) for standard normallydrawn v. thus, is set such that tr(m2) , i.e:. now, consider as a function of observe that for min() , we have that m must be positive semidefinite, and thus decays smoothly from (at 1min ) to zero (at ). similarly, for max() , decays smoothly as decreases. note, however, that such values of would necessarily make m negative semidefinite, which would actually help the loglikelihood. thus, we can exclude this case; in particular, for the remainder of the proofs, we can assume max() also observe that the zeros of in terms of are only at using this, we can show that there exists some for which, for all 0, the only corresponding possible valid value of is where 1min this idea is formalized in the following lemma.. lemma for every , there exists some for which, for all [0, 0) the only admissible value of is such that min() , and thus such that m is positive semidefinite.. we prove the existence of such an by lower bounding (in terms of ) for any finite that does not make m psd. providing such a lower bound shows that for small enough (in particular, less than this lower bound), the only corresponding values of are as desired in the statement19.. in particular, if m is not psd, then there must exist at least one index k such that kk 1, and thus (kk 1)2 for all we can thus lower bound (27) as:. by contradiction, it follows that for any min()2, the only admissible is such that m is psd, i.e. according to the statement of the lemma.. 19since our only goal is existence, we lose many factors from the analysis that would give a tighter bound on in the regime [0, 0), note that is inversely proportional to (i.e. this allows us to get a qualitative view of (26): as the allowed perturbation value increases, the robust covariance resembles the identity matrix more and more, and thus assigns more and more variance on initially lowvariance features. the term indicates that the robust model also adds uncertainty proportional to the square root of the initial variancethus, lowvariance features will have (relatively) more uncertainty in the robust case. indeed, our main result actually follows as a (somewhat loose) formalization of this intuition.. e.3.7 proof of main theorems. first, we give a proof of theorem 2, providing lower and upper bounds on the learned robust covariance in the regime [0, 0). theorem (robustly learned parameters). just as in the nonrobust case, r , i.e. the true mean is learned. for the robust covariance r, there exists an 0, such that for any [0, 0),. we have already shown that in the robust case (c.f. we choose to be as described, i.e. the largest for which the set : tr(2m) , 1/max() has only one element (which, as we argued, must not be less than 1/min()). we have argued that such an must exist.. we prove the result by combining our early derivation (in particular, (25) and (26)) with upper and lower bound on , which we can compute based on properties of the trace operator. we begin by deriving a lower bound on by linear algebraic manipulation (given in appendix e.3.8), we get the following bound:. now, we can use (25) in order to remove the dependency of on :. applying this to (29) yields:. note that we can simplify this bound significantly by writing d min() tr(), which does not affect the result (beyond rescaling the valid regime (0, 0)), and gives:. next, we follow a similar methodology (appendix e.3.8) in order to upper bound :. note that by (25) and positive semidefiniteness of m, it must be that min() min(). thus, we can simplify the previous expression, also substituting d min():. these bounds can be straightforwardly combined with lemma 4, which concludes the proof.. using this theorem, we can now show theorem 3:. theorem (gradient alignment). let f (x) and fr(x) be monotonic classifiers based on the linear separator induced by standard and 2robust maximum likelihood classification, respectively. the maximum angle formed between the gradient of the classifier (wrt input) and the vector connecting the classes can be smaller for the robust model:. min ,x fr(x) x fr(x) min ,x f (x) x f (x) to prove this, we make use of the following lemmas:. for two positive definite matrices a and b with (a) (b), we have that (a b) max(a), (b). we proceed by contradiction:. (a) (a b) max(a) (min(a) min(b)) min(a) (max(a) max(b)). which is false by assumption. this concludes the proof.. lemma (straightforward). for a positive definite matrix a and k 0, we have that. (a k i) (a) (a k a) (a).. lemma (angle induced by positive definite matrix; folklore). for a positive definite matrix a with condition number , we have that. these two results can be combined to prove the theorem. finally, note that (30) is a strictly decreasing function in , and as such, we have shown the theorem. 20a proof can be found in https://bit.ly/2l6jdat lower bound.. tr(m2). min() tr(m2) by the definition of tr(). min() d tr(m)2 by cauchyschwarz. min() d. [ tr ( i)1 d2 ]2 amhm inequality.", "summary": "present a followup work to their paper on the tradeoff between accuracy and robustness. specifically, given a feature f(x) computed from input x, the feature is considered predictive if. \\mathbbe(x,y) \\sim \\mathcald[y f(x)] \\geq \\rho;. similarly, a predictive feature is robust if. \\mathbbe(x,y) \\sim \\mathcald\\left[\\inf\\delta \\in \\delta(x) yf(x \\delta)\\right] \\geq \\gamma.. this means, a feature is considered robust if the worstcase correlation with the label exceeds some threshold \\gamma; here the worstcase is considered within a predefined set of allowed perturbations \\delta(x) relative to the input x. obviously, there also exist predictive features, which are however not robust according to the above definition. present two simple algorithms for obtaining adapted datasets which contain only robust or only nonrobust features. the main idea of these algorithms is that an adversarially trained model only utilizes robust features, while a standard model utilizes both robust and nonrobust features. based on these datasets, they show that nonrobust, predictive features are sufficient to obtain high accuracy; similarly training a normal model on a robust dataset also leads to reasonable accuracy but also increases robustness. experiments were done on cifar10. these observations are supported by a theoretical toy dataset consisting of two overlapping gaussians; i refer to the paper for details.. also find this summary at [davidstutz.de]([url]/)."}, {"document": "in recent years, supervised learning with convolutional networks (cnns) has seen huge adoption in computer vision applications. comparatively, unsupervised learning with cnns has received less attention. in this work we hope to help bridge the gap between the success of cnns for supervised learning and unsupervised learning. we introduce a class of cnns called deep convolutional generative adversarial networks (dcgans), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. additionally, we use the learned features for novel tasksdemonstrating their applicability as general image representations.learning reusable feature representations from large unlabeled datasets has been an area of active research. in the context of computer vision, one can leverage the practically unlimited amount of unlabeled images and videos to learn good intermediate representations, which can then be used on a variety of supervised learning tasks such as image classification. we propose that one way to build good image representations is by training generative adversarial networks (gans) (goodfellow et al., 2014), and later reusing parts of the generator and discriminator networks as feature extractors for supervised tasks. gans provide an attractive alternative to maximum likelihood techniques. one can additionally argue that their learning process and the lack of a heuristic cost function (such as pixelwise independent meansquare error) are attractive to representation learning. gans have been known to be unstable to train, often resulting in generators that produce nonsensical outputs. there has been very limited published research in trying to understand and visualize what gans learn, and the intermediate representations of multilayer gans. in this paper, we make the following contributions we propose and evaluate a set of constraints on the architectural topology of convolutional gans that make them stable to train in most settings. we name this class of architectures deep convolutional gans (dcgan) we use the trained discriminators for image classification tasks, showing competitive performance with other unsupervised algorithms. we visualize the filters learnt by gans and empirically show that specific filters have learned to draw specific objects. arxiv:1511.06434v2 [cs.lg] jan we show that the generators have interesting vector arithmetic properties allowing for easy manipulation of many semantic qualities of generated samples. representation learning from unlabeled data unsupervised representation learning is a fairly well studied problem in general computer vision research, as well as in the context of images. a classic approach to unsupervised representation learning is to do clustering on the data (for example using kmeans), and leverage the clusters for improved classification scores. in the context of images, one can do hierarchical clustering of image patches (coates ng, 2012) to learn powerful image representations. another popular method is to train autoencoders (convolutionally, stacked (vincent et al., 2010), separating the what and where components of the code (zhao et al., 2015), ladder structures (rasmus et al., 2015)) that encode an image into a compact code, and decode the code to reconstruct the image as accurately as possible. these methods have also been shown to learn good feature representations from image pixels. deep belief networks (lee et al., 2009) have also been shown to work well in learning hierarchical representations. generative image models are well studied and fall into two categories: parametric and nonparametric. the nonparametric models often do matching from a database of existing images, often matching patches of images, and have been used in texture synthesis (efros et al., 1999), superresolution (freeman et al., 2002) and inpainting (hays efros, 2007). parametric models for generating images has been explored extensively (for example on mnist digits or for texture synthesis (portilla simoncelli, 2000)). however, generating natural images of the real world have had not much success until recently. a variational sampling approach to generating images (kingma welling, 2013) has had some success, but the samples often suffer from being blurry. another approach generates images using an iterative forward diffusion process (sohldickstein et al., 2015). generative adversarial networks (goodfellow et al., 2014) generated images suffering from being noisy and incomprehensible. a laplacian pyramid extension to this approach (denton et al., 2015) showed higher quality images, but they still suffered from the objects looking wobbly because of noise introduced in chaining multiple models. a recurrent network approach (gregor et al., 2015) and a deconvolution network approach (dosovitskiy et al., 2014) have also recently had some success with generating natural images. however, they have not leveraged the generators for supervised tasks. one constant criticism of using neural networks has been that they are blackbox methods, with little understanding of what the networks do in the form of a simple humanconsumable algorithm. in the context of cnns, zeiler et. (zeiler fergus, 2014) showed that by using deconvolutions and filtering the maximal activations, one can find the approximate purpose of each convolution filter in the network. similarly, using a gradient descent on the inputs lets us inspect the ideal image that activates certain subsets of filters (mordvintsev et al.). historical attempts to scale up gans using cnns to model images have been unsuccessful. this motivated the authors of lapgan (denton et al., 2015) to develop an alternative approach to iteratively upscale low resolution generated images which can be modeled more reliably. we also encountered difficulties attempting to scale gans using cnn architectures commonly used in the supervised literature. however, after extensive model exploration we identified a family of architectures that resulted in stable training across a range of datasets and allowed for training higher resolution and deeper generative models. core to our approach is adopting and modifying three recently demonstrated changes to cnn architectures. the first is the all convolutional net (springenberg et al., 2014) which replaces deterministic spatial pooling functions (such as maxpooling) with strided convolutions, allowing the network to learn its own spatial downsampling. we use this approach in our generator, allowing it to learn its own spatial upsampling, and discriminator. second is the trend towards eliminating fully connected layers on top of convolutional features. the strongest example of this is global average pooling which has been utilized in state of the art image classification models (mordvintsev et al.). we found global average pooling increased model stability but hurt convergence speed. a middle ground of directly connecting the highest convolutional features to the input and output respectively of the generator and discriminator worked well. the first layer of the gan, which takes a uniform noise distribution z as input, could be called fully connected as it is just a matrix multiplication, but the result is reshaped into a 4dimensional tensor and used as the start of the convolution stack. for the discriminator, the last convolution layer is flattened and then fed into a single sigmoid output. for a visualization of an example model architecture. third is batch normalization (ioffe szegedy, 2015) which stabilizes learning by normalizing the input to each unit to have zero mean and unit variance. this helps deal with training problems that arise due to poor initialization and helps gradient flow in deeper models. this proved critical to get deep generators to begin learning, preventing the generator from collapsing all samples to a single point which is a common failure mode observed in gans. directly applying batchnorm to all layers however, resulted in sample oscillation and model instability. this was avoided by not applying batchnorm to the generator output layer and the discriminator input layer. the relu activation (nair hinton, 2010) is used in the generator with the exception of the output layer which uses the tanh function. we observed that using a bounded activation allowed the model to learn more quickly to saturate and cover the color space of the training distribution. within the discriminator we found the leaky rectified activation (maas et al., 2013) (xu et al., 2015) to work well, especially for higher resolution modeling. this is in contrast to the original gan paper, which used the maxout activation (goodfellow et al., 2013). replace any pooling layers with strided convolutions (discriminator) and fractionalstrided convolutions (generator). use batchnorm in both the generator and the discriminator. remove fully connected hidden layers for deeper architectures. use relu activation in generator for all layers except for the output, which uses tanh. use leakyrelu activation in the discriminator for all layers. we trained dcgans on three datasets, largescale scene understanding (lsun) (yu et al., 2015), imagenet1k and a newly assembled faces dataset. details on the usage of each of these datasets are given below. no preprocessing was applied to training images besides scaling to the range of the tanh activation function [1, 1]. all models were trained with minibatch stochastic gradient descent (sgd) with a minibatch size of all weights were initialized from a zerocentered normal distribution with standard deviation in the leakyrelu, the slope of the leak was set to in all models. while previous gan work has used momentum to accelerate training, we used the adam optimizer (kingma ba, 2014) with tuned hyperparameters. we found the suggested learning rate of 0.001, to be too high, using instead. additionally, we found leaving the momentum term at the figure 1: dcgan generator used for lsun scene modeling. a dimensional uniform distribution z is projected to a small spatial extent convolutional representation with many feature maps. a series of four fractionallystrided convolutions (in some recent papers, these are wrongly called deconvolutions) then convert this high level representation into a pixel image. notably, no fully connected or pooling layers are used. suggested value of resulted in training oscillation and instability while reducing it to helped stabilize training. as visual quality of samples from generative image models has improved, concerns of overfitting and memorization of training samples have risen. to demonstrate how our model scales with more data and higher resolution generation, we train a model on the lsun bedrooms dataset containing a little over million training examples. recent analysis has shown that there is a direct link between how fast models learn and their generalization performance (hardt et al., 2015). we show samples from one epoch of training ( fig.2), mimicking online learning, in addition to samples after convergence ( fig.3), as an opportunity to demonstrate that our model is not producing high quality samples via simply overfitting/memorizing training examples. no data augmentation was applied to the images. to further decrease the likelihood of the generator memorizing input examples (fig.2) we perform a simple image deduplication process. we fit a denoising dropout regularized relu autoencoder on 32x32 downsampled centercrops of training examples. the resulting code layer activations are then binarized via thresholding the relu activation which has been shown to be an effective information preserving technique (srivastava et al., 2014) and provides a convenient form of semantichashing, allowing for linear time deduplication visual inspection of hash collisions showed high precision with an estimated false positive rate of less than in additionally, the technique detected and removed approximately 275,000 near duplicates, suggesting a high recall. we scraped images containing human faces from random web image queries of peoples names. the people names were acquired from dbpedia, with a criterion that they were born in the modern era. this dataset has 3m images from 10k people. we run an opencv face detector on these images, keeping the detections that are sufficiently high resolution, which gives us approximately 350,000 face boxes. we use these face boxes for training. no data augmentation was applied to the images. figure 2: generated bedrooms after one training pass through the dataset. theoretically, the model could learn to memorize training examples, but this is experimentally unlikely as we train with a small learning rate and minibatch sgd. we are aware of no prior empirical evidence demonstrating memorization with sgd and a small learning rate. we use imagenet1k (deng et al., 2009) as a source of natural images for unsupervised training. we train on minresized center crops. no data augmentation was applied to the images. one common technique for evaluating the quality of unsupervised representation learning algorithms is to apply them as a feature extractor on supervised datasets and evaluate the performance of linear models fitted on top of these features. on the cifar10 dataset, a very strong baseline performance has been demonstrated from a well tuned single layer feature extraction pipeline utilizing kmeans as a feature learning algorithm. when using a very large amount of feature maps (4800) this technique achieves accuracy. an unsupervised multilayered extension of the base algorithm reaches accuracy to evaluate the quality of the representations learned by dcgans for supervised tasks, we train on imagenet1k and then use the discriminators convolutional features from all layers, maxpooling each layers representation to produce a spatial grid. these features are then flattened and concatenated to form a dimensional vector and a regularized linear l2svm classifier is trained on top of them. this achieves accuracy, out performing all kmeans based approaches. notably, the discriminator has many less feature maps (512 in the highest layer) compared to kmeans based techniques, but does result in a larger total feature vector size due to the many layers of spatial locations. the performance of dcgans is still less than that of exemplar cnns (dosovitskiy et al., 2015), a technique which trains normal discriminative cnns in an unsupervised fashion to differentiate between specifically chosen, aggressively augmented, exemplar samples from the source dataset. further improvements could be made by finetuning the discriminators representations, but we leave this for future work. additionally, since our dcgan was never trained on cifar10 this experiment also demonstrates the domain robustness of the learned features. table 1: cifar10 classification results using our pretrained model. our dcgan is not pretrained on cifar10, but on imagenet1k, and the features are used to classify cifar10 images. accuracy (netzer et al., 2011), we use the features of the discriminator of a dcgan for supervised purposes when labeled data is scarce. following similar dataset preparation rules as in the cifar10 experiments, we split off a validation set of 10,000 examples from the nonextra set and use it for all hyperparameter and model selection. uniformly class distributed training examples are randomly selected and used to train a regularized linear l2svm classifier on top of the same feature extraction pipeline used for cifar10. this achieves state of the art (for classification using labels) at test error, improving upon another modifcation of cnns designed to leverage unlabled data (zhao et al., 2015). additionally, we validate that the cnn architecture used in dcgan is not the key contributing factor of the models performance by training a purely supervised cnn with the same architecture on the same data and optimizing this model via random search over hyperparameter trials (bergstra bengio, 2012). it achieves a signficantly higher validation error. we investigate the trained generators and discriminators in a variety of ways. we do not do any kind of nearest neighbor search on the training set. nearest neighbors in pixel or feature space are (theis et al., 2015) by small image transforms. we also do not use loglikelihood metrics to quantitatively assess the model, as it is a poor (theis et al., 2015) metric. the first experiment we did was to understand the landscape of the latent space. walking on the manifold that is learnt can usually tell us about signs of memorization (if there are sharp transitions) and about the way in which the space is hierarchically collapsed. if walking in this latent space results in semantic changes to the image generations (such as objects being added and removed), we can reason that the model has learned relevant and interesting representations. the results are shown in fig.4. previous work has demonstrated that supervised training of cnns on large image datasets results in very powerful learned features (zeiler fergus, 2014). additionally, supervised cnns trained on scene classification learn object detectors (oquab et al., 2014). we demonstrate that an unsupervised dcgan trained on a large image dataset can also learn a hierarchy of features that are interesting. using guided backpropagation as proposed by (springenberg et al., 2014), we show in fig.5 that the features learnt by the discriminator activate on typical parts of a bedroom, like beds and windows. for comparison, in the same figure, we give a baseline for randomly initialized features that are not activated on anything that is semantically relevant or interesting. in addition to the representations learnt by a discriminator, there is the question of what representations the generator learns. the quality of samples suggest that the generator learns specific object representations for major scene components such as beds, windows, lamps, doors, and miscellaneous furniture. in order to explore the form that these representations take, we conducted an experiment to attempt to remove windows from the generator completely. on samples, window bounding boxes were drawn manually. on the second highest convolution layer features, logistic regression was fit to predict whether a feature activation was on a window (or not), by using the criterion that activations inside the drawn bounding boxes are positives and random samples from the same images are negatives. using this simple model, all feature maps with weights greater than zero ( in total) were dropped from all spatial locations. then, random new samples were generated with and without the feature map removal. the generated images with and without the window dropout are shown in fig.6, and interestingly, the network mostly forgets to draw windows in the bedrooms, replacing them with other objects. interpolation between a series of random points in z show that the space learned has smooth transitions, with every image in the space plausibly looking like a bedroom. in the 6th row, you see a room without a window slowly transforming into a room with a giant window. in the 10th row, you see what appears to be a tv slowly being transformed into a window. in the context of evaluating learned representations of words (mikolov et al., 2013) demonstrated that simple arithmetic operations revealed rich linear structure in representation space. one canonical example demonstrated that the vector(king) vector(man) vector(woman) resulted in a vector whose nearest neighbor was the vector for queen. we investigated whether similar structure emerges in the z representation of our generators. we performed similar arithmetic on the z vectors of sets of exemplar samples for visual concepts. experiments working on only single samples per concept were unstable, but averaging the z vector for three examplars showed consistent and stable generations that semantically obeyed the arithmetic. in addition to the object manipulation shown in (fig. 7), we demonstrate that face pose is also modeled linearly in z space (fig. these demonstrations suggest interesting applications can be developed using z representations learned by our models. it has been previously demonstrated that conditional generative models can learn to convincingly model object attributes like scale, rotation, and position (dosovitskiy et al., 2014). this is to our knowledge the first demonstration of this occurring in purely unsupervised figure 5: on the right, guided backpropagation visualizations of maximal axisaligned responses for the first learned convolutional features from the last convolution layer in the discriminator. notice a significant minority of features respond to beds the central object in the lsun bedrooms dataset. on the left is a random filter baseline. comparing to the previous responses there is little to no discrimination and random structure. figure 6: top row: unmodified samples from model. bottom row: the same samples generated with dropping out window filters. some windows are removed, others are transformed into objects with similar visual appearance such as doors and mirrors. although visual quality decreased, overall scene composition stayed similar, suggesting the generator has done a good job disentangling scene representation from object representation. extended experiments could be done to remove other objects from the image and modify the objects the generator draws. further exploring and developing the above mentioned vector arithmetic could dramatically reduce the amount of data needed for conditional generative modeling of complex image distributions. we propose a more stable set of architectures for training generative adversarial networks and we give evidence that adversarial networks learn good representations of images for supervised learning and generative modeling. there are still some forms of model instability remaining we noticed as models are trained longer they sometimes collapse a subset of filters to a single oscillating mode. : vector arithmetic for visual concepts. for each column, the z vectors of samples are averaged. arithmetic was then performed on the mean vectors creating a new vector y the center sample on the right hand side is produce by feeding y as input to the generator. to demonstrate the interpolation capabilities of the generator, uniform noise sampled with scale was added to y to produce the other samples. applying arithmetic in the input space (bottom two examples) results in noisy overlap due to misalignment. further work is needed to tackle this from of instability. we think that extending this framework figure 8: a turn vector was created from four averaged samples of faces looking left vs looking right. by adding interpolations along this axis to random samples we were able to reliably transform their pose. to other domains such as video (for frame prediction) and audio (pretrained features for speech synthesis) should be very interesting. further investigations into the properties of the learnt latent space would be interesting as well. we propose to apply standard classification metrics to a conditional version of our model, evaluating the conditional distributions learned. we trained a dcgan on mnist (splitting off a 10k validation set) as well as a permutation invariant gan baseline and evaluated the models using a nearest neighbor classifier comparing real data to a set of generated conditional samples. we found that removing the scale and bias parameters from batchnorm produced better results for both models. we speculate that the noise introduced by batchnorm helps the generative models to better explore and generate from the underlying data distribution. the results are shown in table which compares our models with other techniques. the dcgan model achieves the same test error as a nearest neighbor classifier fitted on the training dataset suggesting the dcgan model has done a superb job at modeling the conditional distributions of this dataset. at one million samples per class, the dcgan model outperforms infimnist (loosli et al., 2007), a hand developed data augmentation pipeline which uses translations and elastic deformations of training examples. the dcgan is competitive with a probabilistic generative data augmentation technique utilizing learned per class transformations (hauberg et al., 2015) while being more general as it directly models the data instead of transformations of the data. figure 9: sidebyside illustration of (from lefttoright) the mnist dataset, generations from a baseline gan, and generations from our dcgan", "summary": "what dcgans are just a different architecture of gans. in gans a generator network (g) generates images. a discriminator network (d) learns to differentiate between real images from the training set and images generated by g. dcgans basically convert the laplacian pyramid technique (many pairs of g and d to progressively upscale an image) to a single pair of g and d. how their d: convolutional networks. no pooling, instead strided layers. their g: starts with 100d noise vector. generates with linear layers 1024x4x4 values. then uses fractionally strided convolutions (move by per step) to upscale to 512x8x8. this is continued till cx32x32 or cx64x64. the last layer is a convolution to 3x32x32/3x64x64 (tanh activation). the fractionally strided convolutions do basically the same as the progressive upscaling in the laplacian pyramid. so its basically one laplacian pyramid in a single network and all upscalers are trained jointly leading to higher quality images. they use adam as their optimizer. to decrease instability issues they decreased the learning rate to (from 0.001) and the momentum/beta1 to (from 0.9). architecture of g using fractionally strided convolutions to progressively upscale the image. results high quality images. still with distortions and errors, but at first glance they look realistic. smooth interpolations between generated images are possible (by interpolating between the noise vectors and feeding these interpolations into g). the features extracted by d seem to have some potential for unsupervised learning. there seems to be some potential for vector arithmetics (using the initial noise vectors) similar to the vector arithmetics with wordvectors. to generate mean with sunglasses via vector(men) vector(sunglasses). generated images, bedrooms. generated images, faces. rough chapterwise notes introduction for unsupervised learning, they propose to use to train a gan and then reuse the weights of d. gans have traditionally been hard to train. approach and model architecture they use for d an convnet without linear layers, withput pooling layers (only strides), leakyrelus and batch normalization. they use for g relus (hidden layers) and tanh (output). details of adversarial training they trained on lsun, imagenet1k and a custom dataset of faces. they used adam with a learning rate of and momentum of they note that a higher momentum lead to oscillations. lsun 3m images of bedrooms. they use an autoencoder based technique to filter out 0.25m near duplicate images. faces they downloaded 3m images of 10k people. they extracted 350k faces with opencv. empirical validation of dcgans capabilities classifying cifar10 gans as a feature extractor they train a pair of g and d on imagenet1k. ds top layer has features. they train an svm on these features to classify the images of cifar10. they achieve a score of 82.8, better than unsupervised kmeans based methods, but worse than exemplar cnns. classifying svhn digits using gans as a feature extractor they reuse the same pipeline (d trained on cifar10, svm) for the streetview house numbers dataset. they use svhn images (with the features from d) to train the svm. they achieve test error. investigating and visualizing the internals of the networks walking in the latent space the performs walks in the latent space ( interpolate between input noise vectors and generate several images for the interpolation). they argue that this might be a good way to detect overfitting/memorizations as those might lead to very sudden (not smooth) transitions. visualizing the discriminator features they use guided backpropagation to visualize what the feature maps in d have learned (i.e. to which images they react). they can show that their lsunbedroom gan seems to have learned in an unsupervised way what beds and windows look like. forgetting to draw certain objects they manually annotated the locations of objects in some generated bedroom images. based on these annotations they estimated which feature maps were mostly responsible for generating the objects. they deactivated these feature maps and regenerated the images. that decreased the appearance of these objects. its however not as easy as one feature map deactivation leading to one object disappearing. they deactivated quite a lot of feature maps (200) and they objects were often still quite visible or replaced by artefacts/errors. vector arithmetic on face samples wordvectors can be used to perform semantic arithmetic (e.g. the unsupervised representations seem to be useable in a similar fashion. they generated images via g. they then picked several images that showed men with glasses and averaged these images noise vectors. they did with same with men without glasses and women without glasses. then they performed on these vectors men with glasses mean without glasses women without glasses to get womean with glasses"}, {"document": "as deep learning models are applied to increasingly diverse problems, a key bottleneck is gathering enough highquality training labels tailored to each task. users therefore turn to weak supervision, relying on imperfect sources of labels like pattern matching and userdefined heuristics. unfortunately, users have to design these sources for each task. this process can be time consuming and expensive: domain experts often perform repetitive steps like guessing optimal numerical thresholds and developing informative text patterns. to address these challenges, we present snuba, a system to automatically generate heuristics using a small labeled dataset to assign training labels to a large, unlabeled dataset in the weak supervision setting. snuba generates heuristics that each labels the subset of the data it is accurate for, and iteratively repeats this process until the heuristics together label a large portion of the unlabeled data. we develop a statistical measure that guarantees the iterative process will automatically terminate before it degrades training label quality. snuba automatically generates heuristics in under five minutes and performs up to f1 points better than the best known userdefined heuristics developed over many days. in collaborations with users at research labs, stanford hospital, and on open source datasets, snuba outperforms other automated approaches like semisupervised learning by up to f1 points.the success of machine learning for tasks like image recognition and natural language processing [12,14] has ignited interest in using similar techniques for a variety of tasks. however, gathering enough training labels is a major bottleneck in applying machine learning to new tasks. in response, there has been a shift towards relying on weak su figure 1: snuba uses a small labeled and a large unlabeled dataset to iteratively generate heuristics. it uses existing label aggregators to assign training labels to the large dataset. pervision, or methods that can assign noisy training labels to unlabeled data, like crowdsourcing [9,22,60], distant supervision [8,32], and userdefined heuristics [38,39,50]. over the past few years, we have been part of the broader effort to enhance methods based on userdefined heuristics to extend their applicability to text, image, and video data for tasks in computer vision, medical imaging, bioinformatics and knowledge base construction [4,39,50]. through our engagements with users at large companies, we find that experts spend a significant amount of time designing these weak supervision sources. as deep learning techniques are adopted for unconventional tasks like analyzing codebases and now commodity tasks like driving marketing campaigns, the few domain experts with required knowledge to write heuristics cannot reasonably keep up with the demand for several specialized, labeled training datasets. even machine learning experts, such as researchers at the computer vision lab at stanford, are impeded by the need to crowdsource labels before even starting to build models for novel visual prediction tasks [23,25]. this raises an important question: can we make weak supervision techniques easier to adopt by automating the process of generating heuristics that assign training labels to unlabeled data? the key challenge in automating weak supervision lies in replacing the human reasoning that drives heuristic development. in our collaborations with users with varying levels of machine learning expertise, we noticed that the process to develop these weak supervision sources can be fairly repetitive. for example, radiologists at the stanford hospital and clinics have to guess the correct threshold for each heuristic that uses a geometric property of a tumor to determine if it is malignant (example shown in figure 1). we instead take advantage of a small, labeled dataset to automatically generate noisy heuristics. though the labeled dataset is too small to train an end model, it has enough information to generate heuristics that can assign noisy labels to a large, unlabeled dataset and improve end model performance by up to f1 points. to aggregate labels from these heuristics, we improve over majority vote by relying on existing factor graphbased statistical techniques in weak supervision that can model the noise in and correlation among these heuristics [2,4,39,41,48,50]. however, these techniques were intended to work with userdesigned labeling sources and therefore have limits on how robust they are. automatically generated heuristics can be noisier than what these models can account for and introduce the following challenges: accuracy. users tend to develop heuristics that assign accurate labels to a subset of the unlabeled data. an automated method has to properly model this tradeoff between accuracy and coverage for each heuristic based only on the small, labeled dataset. empirically, we find that generating heuristics that each labels all the datapoints can degrade end model performance by up to f1 points. since each heuristic has limited coverage, users develop multiple heuristics that each labels a different subset to ensure a large portion of the unlabeled data receives a label. in an automated approach, we could mimic this by maximizing the number of unlabeled datapoints the heuristics label as a set. however, this approach can select heuristics that cover a large portion of the data but have poor performance. there is a need to account for both the diversity and performance of the heuristics as a set. empirically, balancing both aspects improves end model performance by up to f1 points compared to selecting the heuristic set that labels the most datapoints. termination condition. users stop generating heuristics when they have exhausted their domain knowledge. an automated method, however, can continue to generate heuristics that deteriorate the overall quality of the training labels assigned to the unlabeled data, such as heuristics that are worse than random for the unlabeled data. not accounting for performance on the unlabeled dataset can affect end model performance by up to f1 points. to address the challenges above, we introduce snuba, an automated system that takes as input a small labeled and a large unlabeled dataset and outputs probabilistic training labels for the unlabeled data, as shown in figure these labels can be used to train a downstream machine learning model of choice, which can operate over the raw data and generalize beyond the heuristics snuba generates to label any datapoint. users from research labs, hospitals and industry helped us design snuba such that it outperforms userdefined heuristics and crowdsourced labels by up to f1 points and f1 points in terms of end model performance. snuba maintains a set of heuristics that is used to assign labels to the unlabeled dataset. at each iteration, snuba appends a new heuristic to this set after going through the following components: synthesizer for accuracy. to address the tradeoff between the accuracy and coverage of each heuristic, the synthesizer (section 3.1) generates heuristics based on the labeled set and adjusts its labeling pattern to abstain if the heuristic has low confidence. the synthesizer relies on a small number of primitives, or features of the data, to generate multiple, simple models like decision trees, which improves over fitting a single model over primitives by f1 points. these primitives are userdefined and part of open source libraries [35,49] and data models in existing weak supervision frameworks [38,58]. primitives examples in our evaluation include bagofwords for text and bounding box attributes for images. to ensure that the set of heuristics is diverse and assigns highquality labels to a large portion of the unlabeled data, the pruner (section 3.2) ranks the heuristics the synthesizer generates by the weighted average of their performance on the labeled set and coverage on the unlabeled set. it selects the best heuristic at each iteration and adds it to the collection of existing heuristics. this method performs up to f1 points better than ranking heuristics by performance only. verifier to determine termination condition. the verifier uses existing statistical techniques to aggregate labels from the heuristics into probabilistic labels for the unlabeled datapoints [4,39,50]. however, the automated heuristic generation process can surpass the noise levels to which these techniques are robust to and degrade end model performance by up to f1 points. we develop a statistical measure that uses the small, labeled set to determine whether the noise in the generated heuristics is below the threshold these techniques can handle (section 4). we describe snuba, a system to automatically generate heuristics using a small labeled dataset to assign training labels to a large, unlabeled dataset in the weak supervision setting. a summary of our contributions are as follows: we describe the system architecture, the iterative process of generating heuristics, and the optimizers used in the three components (section 3). we also show that our automated optimizers can affect end model performance by up to f1 points (section 5). we present a theoretical guarantee that snuba will terminate the iterative process before the noise in heuristics surpasses the threshold to which statistical techniques are robust (section 4). this theoretical result translates to improving end model performance by up to f1 points compared to generating as many heuristics as possible (section 5). we evaluate our system in section by using snuba labels to train downstream models, which generalize beyond the heuristics snuba generates. we report on collaborations with stanford hospital and stanford computer vision lab, analyzing text, image, and multimodal data. we show that heuristics from snuba can improve over handcrafted heuristics developed over several days by up to f1 points. we compare to automated methods like semisupervised learning, which snuba outperforms by up to f1 points. we describe the input and output for snuba, introduce notation used in the rest of paper, and summarize statistical techniques snuba relies on to learn heuristic accuracies. the input to snuba is a labeled dataset ol with nl datapoints and an unlabeled dataset ou with nu figure 2: an overview of the snuba system. (1) the synthesizer generates a candidate set of heuristics based on the labeled dataset. 2the pruner selects the heuristic from the candidate set to add to the committed set. 3the verifier learns heuristic accuracies and passes appropriate feedback to the synthesizer to continue the iterative process. each datapoint is defined by its associated primitives, or characteristics of the data, and a label. the inputs to the system can be represented as , (for the labeled set ol), and , (for the unlabeled set ou ) where xi r d , y represent the primitives for a particular object and the true label, respectively. for convenience, we focus on the binary classification setting, in which y 1, and discuss the multiclass setting in section the primitives for each datapoint xi r d can be viewed as features of the data examples include numerical features such as area or perimeter of a tumor for image data. or onehot vectors for the bag of words representation for text data. for our collaborators using snuba, these primitives are usually part of data models in existing weak supervision systems and open source libraries [35,38,49,58]. for example, scikitimage includes functions to extract geometric properties from segmented images [49]. in our evaluation, we do not allow users to extend the set of primitives beyond those present in these data models and libraries, though they could be extended in principle. snuba outputs a probabilistic training label y p [y 1] [0, 1] for each datapoint in the unlabeled set, a weighted combination of labels from different heuristics. since snuba only relies on information about the data encoded in the primitives and does not take advantage of a complete representation of the data, it is advantageous to train a downstream model that has access to the entire input data space using probabilistic labels from snuba as training labels. these downstream models, such as a convolutional neural network (cnn) [26] for image classification or a longshort term memory (lstm) architecture [19] for natural language processing tasks, can operate over the raw data (e.g., the radiology image of a tumor from figure or complete sentences). we discuss specific end models and show that the end model generalizes beyond the heuristics by improving recall by up to points in section each heuristic snuba generates relies on one or more primitives and outputs a binary label or abstains for each datapoint in the unlabeled dataset (section 3.1). a single bad (but prolific) voter can compromise majority vote, which weights all heuristics equally [39]. snuba instead relies on existing statistical techniques (section 4) that can learn the accuracies of these heuristics without using ground truth labels and assign probabilistic labels to the unlabeled dataset accordingly [2,4,39,41,48,50]. we treat these statistical techniques as blackbox methods that learns heuristic accuracies and refer to them as label aggregators since they combine the labels the heuristics assign to generate a single probabilistic label per datapoint. however, since snuba can generate heuristics that are much noisier than the label aggregator can handle, it has to determine the conditions under which the aggregator operates successfully (section 4). the snuba process is iterative and generates a new heuristic specialized to the subset of the data that did not receive high confidence labels from the existing set of heuristics at each iteration. as shown in figure 2, the three components of snuba are the synthesizer (section 3.1) that generates a candidate set of heuristics, a pruner (section 3.2) that selects a heuristic to add to an existing committed set of heuristics, and a verifier (section 3.3) that assigns probabilistic labels to the data and passes the subset of the labeled data that received low confidence labels to the synthesizer for the next iteration. this process is repeated until the subset the verifier passes to the synthesizer is empty, or the verifier determines that the conditions for the label aggregator to operate successfully are violated (section 4). the snuba synthesizer takes as input the labeled set, or a subset of the labeled set after the first iteration, and outputs a candidate set of heuristics ( figure 2). first, we describe how the heuristics are generated using the labeled dataset and the different models the heuristic can be based on. then, we describe how the labeling pattern of the heuristics are adjusted to assign labels to only a subset of the unlabeled dataset. finally, we explore the tradeoffs between accuracy and coverage by comparing heuristics snuba generated to other automated methods. in snuba, users can select the model they want to base their heuristics on given the heuristic h follows the inputoutput form: heuristic models. in this paper, we focus on heuristics that are based on classification models that take as input one or more primitives and assign probabilistic labels p [y i 1] [0, 1] to the unlabeled datapoints. we consider three different ways of generating heuristics given a subset of the labeled data and a subset of primitives ( figure 3). decision stumps mimic the nested thresholdbased heuristics that users commonly write. to maintain the simplicity of the heuristic, we limit the depth of each tree to the number of primitives the heuristic depends on. the confidence each unlabeled datapoint receives is the fraction of labeled datapoints that belong to the same leaf. logistic regressor allows the heuristic to learn a single linear decision boundary. as shown in figure 3, it does not have to be parallel to the primitive axes, unlike decision trees. the confidence for an unlabeled datapoint is determined by the sigmoid function, whose parameters are learned using the labeled datapoints. knearest neighbor is based on a kdtree implementation of nearest neighbor and can lead to complex decision boundaries that neither decision trees nor logistic regressors can capture. unlike the previous heuristic models, it does not learn a parameter per primitive, but instead relies on the distribution of the labeled datapoints to decide the decision boundaries. the confidence for a unlabeled datapoint is a function of its distance from labeled datapoints. the user can replace the heuristic model with another function of choice as long as it follows the inputoutput criteria described earlier in this section. for example, decision trees that rely on bagofwords primitives represent heuristics that check whether a particular word, represented as a primitive, exists or not. we can improve performance of heuristics by modeling the tradeoff between heuristic accuracy and coverage. snuba return all subsets of size d from d forces heuristics to only assign labels to datapoints they have high confidence for and abstain for the rest. to measure confidences, snuba relies on the probabilistic label p [y i 1] that each heuristic model assigns to a datapoint. we define datapoints that heuristics have low confidence for as the points where for each heuristic, snuba selects a threshold that determines when a heuristic assigns a label, 1, and when it abstains, y the relation between and can be defined as: to choose the best threshold , we need a metric that models the tradeoffs between coverage and accuracy. we calculate the precision and recall of the heuristics on the labeled set with nl datapoints as a proxy for their performance on the unlabeled dataset. we define these metrics below: precision (p) the fraction of correctly labeled points over the total points labeled, recall (r) the fraction of correctly labeled points over the total number of points, f1 score the harmonic mean of p and r, p r to balance precision and recall, the snuba synthesizer selects for each heuristic that maximizes the f1 score on the labeled dataset ol (algorithm 1). the synthesizer iterates through (default 10) equally spaced values in (0, 0.5), calculates the f1 score the heuristic achieves, and selects the that maximizes f1 score. in case of ties, the synthesizer chooses the lower value for higher coverage. we find selecting based on f1 score outperforms a constant by up to f1 points (section 5). as an example, if the synthesizer uses a decision tree as the heuristic model, it trains a normal decision tree on the small labeled dataset and learns appropriate parameters for a specific subset of primitives (e.g., d means two primitives, or two rows of x in algorithm 1) to decide on a label. then, the synthesizer learns , which adjusts these decision tree thresholds to abstain for lowconfidence datapoints. this adjusted decision tree is then added as a heuristic to the candidate set, and the process is repeated for different subsets of primitives as inputs to the decision tree. we explore the tradeoffs that result from allowing the heuristics to abstain in terms of the effect on end model performance. we compare to automated baseline methods (more details in section 5.1) that assign labels to the entire unlabeled dataset. we generate a synthetic experiment (figure 4) using one of the datasets from our evaluation, the visual genome dataset [25] (more details in section 5.1). to study how snuba performs given varying amounts of unlabeled data, we set up the following simulation: given nl labeled datapoints, we varied the amount of unlabeled data available to snuba from nu to nu each of the methods assigned training labels to the unlabeled dataset, and this dataset was used to finetune the last layer of googlenet [47]. n l n u case: since snuba only labels a portion of the unlabeled data, the end model has fewer training labels to learn from compared to the other methods that do not abstain. since the unlabeled set is small in this situation (nl nu 100), the baseline methods have better end model performance. n l n u case: heuristics snuba generates continue to only assign labels with high confidence, leading to a smaller labeled training set than other methods, but high quality training labels for that portion. this is promising for machine learning applications in which the bottleneck lies in gathering enough training labels, while unlabeled data is readily available. semisupervised learning also performs better as the amount of unlabeled data increases; however, it still performs worse than snuba when the amount of unlabeled data is more than larger than labeled data since semisupervised methods do not abstain. snuba also outperforms these baseline methods when the unlabeled data is between to as much as labeled data (section 5). the pruner takes as input the candidate heuristics from the synthesizer and selects a heuristic to add to the committed set of heuristics ( figure 2). we want the heuristics in the committed set to be diverse in terms of the datapoints in the unlabeled set they label, but also ensure that it performs well for the datapoints it labels in the labeled dataset. a diverse heuristic is defined as one that labels points that have never received a label from any other heuristic. therefore, we want to be able to maximize the dissimilarity if w(jscore fscore) bestscore then between the set of datapoints a heuristic labels and the set of datapoints that previous heuristics in the committed set have already labeled. let nj 0, n u represent whether heuristic j from the candidate set has assigned labels to the datapoints in the unlabeled set. let n 0, n u represent whether any heuristic from the committed set has assigned a label to the datapoints in the unlabeled set. to measure the distance between these two vectors, we rely on the jaccard distance metric [20], the complement of jaccard similarity, as a standard measure of similarity between sets. for a particular heuristic hj in the candidate set, the generalized jaccard distance is defined as: to measure performance on the labeled dataset, snuba uses the f1 score of each heuristic in the candidate set, as defined in the previous section. as the final metric to rank heuristics, the pruner uses a weighted average of the jaccard distance and f1 score and selects the highest ranking heuristic from the candidate set and adds it to the committed set of heuristics. this process is described in algorithm for our experiments, we use both w for a simple average and w t n n u (percentage of unlabeled set with at least one label). the latter weights the f1 score more as coverage of the unlabeled dataset increases. we find that considering both performance on the labeled set and diversity on the unlabeled set improves over only considering diversity by up to f1 points and over only considering performance by up to f1 points in section the verifier uses the label aggregator (section 4) to learn accuracies of the heuristics in the committed set without any ground truth labels to produce a single, probabilistic training label for each datapoint in the unlabeled dataset. these probabilistic labels also represent how confident the label aggregator is about the assigned label. datapoints that have not received a single label from heuristics in the committed set will have a probabilistic label p [y 1] 0.5, equal chance of belonging to either class. p [y 1] close to represent datapoints with low confidence, which can result from scenarios with low accuracy heuristics labeling that datapoint, or multiple heuristics with similar accuracies disagreeing on the label for that datapoint. since snuba generates a new heuristic at each iteration, we want the new heuristic to assign labels to the subset that currently has low confidence labels. snuba identifies datapoints in the labeled set that receive low confidence labels from the label aggregator. it passes this subset to the synthesizer with the assumption that similar datapoints in the unlabeled dataset would have also received low confidence labels (algorithm 3). formally, we define low confidence labels as i where is the probabilistic label assigned by the label aggregator and (m 1) where the parameter (default ) controls the rate at which the definition of low confidence changes with number of heuristics in the committed set (m ). as the number of heuristics increases, we expect that fewer datapoints will have confidences near and adjust what is considered low confidence accordingly. we also compare to a weighted feedback approach in which the weights are the inverse of the label confidence (wv ) normalized across all datapoints. the iterative process terminates if: (1) the statistical measure discussed in section suggests the generative model in the synthesizer is not learning the accuracies of the heuristics properly, or (2) there are no low confidence datapoints, as defined by , in the small, labeled dataset. empirically, we find that (1) is a more popular termination condition than (2). in both cases, it is likely for some datapoints in the large, unlabeled set to not receive a label from any heuristic in the committed set; however, since snuba generates training labels, the downstream end model can generalize to assign labels to these datapoints. we discuss the extension of the snuba architecture to the multiclass setting, intuition behind the greedy approach, alternative heuristic models, and limitations of the system. while we focus on the binary setting, snuba can be extended to the multiclass setting without additional changes. we include an example of a threeclass classification task in [52]. statistics like f1 and jaccard score in the synthesizer and pruner are calculated using only overall accuracy and coverage, which apply to the multiclass setting. the label aggregator in the verifier can operate over multiclass labels [38,39] and pass feedback using the probabilistic label of the most likely class. our intuition behind generating heuristics greedily was to mimic the the user process of manually developing heuristics. the iterative approach tries to ensure each heuristic labels a subset of the data that does not have labels from existing heuristics and ensure a large portion of the datapoints receive high confidence labels. we use a statistical method to determine the optimal stopping condition for the iterative approach (section 4, figure 5). alternative heuristic models. while we only discuss three possible heuristic models in this paper, snuba can handle any heuristic model that follows the inputoutput schema described in section the user can therefore design different heuristic models that are specialized for their classification task. for example, the user can use a regex heuristic model that can perform more complex operations over bagofwords primitives than a decision tree. first, the performance of the snuba heuristics is bounded by the quality of the input primitives. for example, if the primitives for the tumor classification task only contained age, which was a poor signal of tumor malignancy, then the heuristics snuba generated would not assign high quality training labels. second, snuba heuristics can only rely on the input primitives and no external knowledge about the task, such as knowledge bases, which is a limitation compared to userdefined heuristics (more details in section 5.2.3). finally, snuba is likely to overfit and not perform well on the unlabeled dataset if the small, labeled dataset is not representative of the unlabeled dataset. for the tumor classification task, the images in the small, labeled set could be taken from one perspective while the ones in the larger, unlabeled dataset are from a different perspective. this can lead the distribution of the primitives to be significantly different across the two datasets and prevent snuba from generating high quality heuristics. we provide an overview of generative models [4,39,50,53] that serve as the label aggregator for snuba. as discussed in section 2, these models can learn the accuracies of the noisy heuristics without using any ground truth data and can assign probabilistic labels to the unlabeled data accordingly. however, these generative models are designed to model the noise in userdefined heuristics, which are much more accurate than automatically generated heuristics. specifically, the generative model assumes that heuristics always have accuracies better than 50; however, snubagenerated heuristics can easily violate this assumption as described in section therefore, a key challenge in snuba is recognizing whether the committed set includes heuristics that are worse than random for the unlabeled dataset without access to ground truth labels. we introduce a statistical measure in section that relies on the accuracies the generative model learns and the small labeled dataset. in section 4.4, we formally define this statistical measure and provide a theoretical guarantee that it will recognize when the generative model is not learning heuristic accuracies successfully. generative models are a popular approach to learn and model the accuracies of different labeling sources like userdefined heuristics and knowledge bases when data is labeled by a variety of sources [11,39]. in snuba, we could also rely on the accuracies of the heuristics on the small, labeled dataset,; however, this could degrade end model performance by up to f1 points (section 5). formally, the goal of the generative model is to estimate the true accuracies of the heuristics, r m , using the labels the heuristics assign to the unlabeled data, 1, 0, m n u it models the true class label y 1, n u for a datapoint as a latent variable in a probabilistic model and in the simplest case, assumes that each labeling source is independent. the generative model is expressed as a factor graph: where z is a partition function to ensure is a normalized distribution. the parameter r m is used to calculate the learned accuracies exp() 1exp() r m (defined pointwise). it is estimated by maximizing the marginal likelihood of the observed heuristics , using a method similar to contrastive divergence [18], alternating between using stochastic gradient descent and gibbs sampling [4,39]. the generative model assigns probabilistic training labels by computing y (y ) for each datapoint. these probabilistic training labels can be used to train any end model with noiseaware loss [38,39] min where oi ou is an object in the unlabeled dataset and are the probabilistic training labels. in our experiments, we adjust the loss functions of several popular machine learning models to the use the noiseaware variant. since the generative model requires no ground truth labels to learn heuristic accuracies, it has to solve an underdetermined problem where the heuristics could have accuracies or the generative model assumes that the labeling sources always perform better than random ( 0.5), which is a reasonable assumption for userdefined heuristics [4,39,50]. since snuba generates these heuristics automatically, it is possible for the heuristics to be accurate for the labeled set but violate the generative models assumption that an example of such a situation is shown in figure 5(a),(c) for two real datasets. the 8th and 12th heuristics, respectively, have an accuracy worse than on the unlabeled dataset. however, since the generative model does not know that this assumption has been violated, it learns an accuracy much greater than in both cases. if these heuristics are included in the generative model, the generated probabilistic training labels degrade end model performance by f1 and f1 points, respectively. snuba can take advantage of the small, labeled dataset to indirectly determine whether the generated heuristics are worse than random for the unlabeled dataset. we define the empirical accuracies of the heuristics a for i m.ij 1, 0, is the label heuristic i assigned to the jth datapoint in the labeled set ol, and ni is the number of datapoints wherei 1, our goal is to use the empirical accuracies, to estimate whether the learned accuracies, are close to the true accuracies, , defined as , the maximum absolute difference between the learned and true accuracies being less than , a positive constant to be set. toward this end, we define the to guarantee with high probability that the generative model learns accuracies within , we want to find , the largest allowed error between the learned and empirical accuracies, at each iteration. we discuss the exact form of in section we compare the measured error to the maximum allowable value of at each iteration, as shown in figure 5(b),(d). if the measured error is greater than , then we stop the iterative process of generating heuristics and use the probabilistic training labels generated at the previous iteration (since the heuristic generated at the current iteration led to measured error being greater than ). as shown in figure 5, this stopping point maps to the iteration at which the new heuristic generated has a true accuracy worse than for the unlabeled dataset (we only calculate for demonstration since we would not have access to ground truth labels for realworld tasks). intuitively, we expect that once the synthesizer generates a heuristic that is worse than random for the unlabeled dataset, it will never generate heuristics that will be helpful in labeling the data anymore. empirically, we observe that this is indeed the case as shown for two real tasks in figure 5(a) and (c). assuming that the objects in the labeled set ol are independent and identically distributed, we provide the following guarantee on the probability of the generative model learning the accuracies successfully: proposition 1: suppose we have m heuristics with empirical accuracies, accuracies learned by the generative model, and measured error for all m iterations. then, if each heuristic labels a minimum of datapoints at each iteration, the generative model will succeed in learning accuracies within across all iterations with probability we provide a formal proof for this proposition in [52]. we require each heuristic to assign labels to at least n datapoints to guarantee that the generative model will learn accuracies within of the true accuracies, given the measured error is less than for all iterations. we solve for the maximum allowed error at each iteration: this value is plotted against the value of the measured error in figure 5(b,d). snuba stops generating new heuristics when the measured error surpasses the allowed error. the above proposition relies only on the measured error to guarantee whether the generative model is learning accuracies successfully. we compare the performance of end models trained on labels generated by snuba and other baseline methods. we seek to experimentally validate the following claims: training labels from snuba outperform labels from automated baseline methods we compare snuba to models that generate heuristics using only the labeled data, such as boosting and decision trees, and semisupervised methods, which utilize both labeled and unlabeled datasets. snuba outperforms these methods by up to f1 points. we also compare to transfer learning using only the labeled dataset, which snuba outperforms by up to f1 points. training labels from snuba outperform those from userdeveloped heuristics we compare the performance of heuristics generated by snuba to heuristics developed by users. snuba can use the same amount of labeled data as users to generate heuristics and improve end model performance by up to f1 points. each component of snuba boosts overall system performance we evaluate separate components of the snuba system by changing how the parameter is chosen in the synthesizer, how the pruner selects a heuristic to add to the committed set, and different label aggregation methods in the verifier. compared to the complete snuba system, we observe that performance can degrade by up to f1 points by removing these components. we describe the datasets, baseline methods, performance metrics, and implementation details for snuba. we consider realworld applications and tasks over open source datasets for image, text, and multimodal classification. for each of the tasks, previous techniques to assign training labels included using crowdsourcing, userdefined functions, and decision trees based on a small, labeled dataset. table and additional details are in [52]. we focus on two realworld medical image classification tasks that we collaborated on with radiologists at stanford hospital and clinics. the bone tumor and mammogram tumor classification tasks demonstrate how snubagenerated heuristics compare to those developed by domain experts. the first dataset uses domainspecific primitives while the second relies on simple geometric primitives. working with graduate students in the stanford computer vision lab, we identify images of person riding bike. we use the visual genome database [25] with bounding box characteristics as primitives and study how snuba performs with severe class imbalance. text and multimodal classification. we applied snuba to text and multimodal datasets to study how well snuba operated in domains where humans could easily interpret and write rules over the raw data. we generate primitives by featurizing the text using a bagofwords representation. the mscoco dataset [30] had heuristics generated over captions and classification performed over associated images, and the imdb plot summary classification [1] is purely textbased. the twitter sentiment analysis dataset relied on crowdworkers for labels [31] while the chemicaldisease relation extraction task (cdr) [57] relies on external sources of information like knowledge bases. the hardware relation extraction task over richly formatted data classifies part numbers and electrical characteristics from specification datasheets as valid or not. we use visual, tabular, and structural primitives extracted using fonduer [58]. we compare to pruned decision tree [42] and boosting [16] (adaboost), which use the labeled dataset to generate one complex or multiple, simple decision trees, respectively. we compare to semisupervised learning [61], which uses both the labeled and unlabeled dataset to assign training labels and represents a single heuristic in the form of a blackbox model. for select tasks, we perform transfer learning using pretrained models. we use glove embeddings [36] for imbd and twitter only tune the last layer of a vggnet [43] for mscoco, and tune the weights of a googlenet [47] pretrained on imagenet [12] for visual genome and mammogram (more details in [52]). as shown in table 1, training labels for all tasks were previously generated by some userdriven labeling method, https://www.digikey.com such as userdefined heuristics, distant supervision, or crowdsourcing. these were developed by users, ranging from domain experts to machine learning practitioners and input to label aggregators we developed [38,50,51]. for tasks like cdr, bone tumor, and mammogram that required specific domain knowledge, the time taken for bioinformatics experts and radiologists to manually develop heuristics ranged from a few days to a few weeks. for tasks that did not require domain expertise, such as imdb and visual genome, graduate students wrote a small number of heuristics over a period of a few hours. in all cases, users encoded their domain knowledge in heuristics and evaluated their performance on a small, heldout labeled set in an iterative manner. we observe a maximum of d for our text and d for our image and multimodal tasks. since textbased tasks used a bagofwords representation, the primitives are sparse and number in the hundreds of thousands. we filter bagofwords primitives by only considering primitives that are active for both the labeled and unlabeled dataset, and for at least of the unlabeled dataset to ensure a minimim coverage for generated heuristics. the threshold had the best performance for our text datasets but this threshold can be userdefined in practice. for our imagebased tasks, we found that snuba never generated heuristics that relied on more than primitives as input, while for textbased tasks, it only generated heuristics that relied on a single primitive ( figure 6). heuristics rely on a small number of primitives since this limits their complexity and prevents them from overfitting to the small, labeled dataset. moreover, relying on multiple primitives can also lower the coverage of the heuristics, and a fairly accurate heuristic that relies on several primitives being present is filtered by the pruner, which relies on both coverage and performance. the relatively small number of table 3: precision (p), recall (r) and f1 scores for userdefined heuristics, snubagenerated heuristics, and end model trained on labels from snubagenerated heuristics. lift reported is from user to snuba heuristics, then snuba heuristics to end model. snuba heuristics have lower precision than users and end model improves recall. while snuba can generate training labels efficiently, they rely only on the userdefined primitives. the end model trained on these labels can use the raw data or representations of the data based on pretrained models. for example, the end model can operate over the entire raw image, sentence or representation from a pretrained model as opposed to measurements of the tumor, bagofwords representation, or bounding box coordinates. for image classification tasks, we use popular deep learning models like googlenet and vggnet that take the raw image as input, while for text tasks we use a model composed of a single embedding and a single lstm layer that take the raw text sentence(s) as input. these models take as input the probabilistic or binary training labels from snuba or the baseline methods and minimize the noiseaware loss, as defined in section while the tasks explored in this section are all binary classification, the system can be easily generalized to the multiclass case (section 3.4). we demonstrate that a downstream model trained on the labels from snuba generalizes beyond the snuba heuristics, improving recall by up to points (section 5.2.1), outperforms automated baseline methods by up to f1 points (section 5.2.2) and userdriven labeling by up to f1 points (section 5.2.3). one of the motivations for designing snuba is to efficiently label enough training data for training powerful, downstream machine learning models like neural networks. heuristics from snuba are not used directly for the classification task at hand because (1) they may not label the entire dataset due to abstentions, and (2) they are based only on the userdefined primitives and fail to take advantage of the raw data representation. for datasets like mscoco, the end model also operates over a different modality than the heuristics. to demonstrate the advantage of training an end model, we compare the performance of snuba heuristics to standard figure 7: snuba generates fewer heuristics than users for our image tasks and usually more for text tasks. end models trained on labels from snuba on a test set in table the end model improves over the heuristics performance by up to f1 points. the end model helps generalize beyond the heuristics, as a result of more powerful underlying models and access to raw data, and improves recall by up to points. table shows that snuba can outperform automated baseline methods by up to f1 points. snuba outperforms decision trees, which fit a single model to the labeled dataset, by f1 points on average, the largest improvement compared to other baselines. the method that performs the closest to snuba for most tasks is semisupervised learning, which takes advantage of both the unlabeled and unlabeled dataset, but fails to account for diversity, performing worse than snuba by f1 points on average. finally, compared to transfer learning which does not have to learn a representation of the data from scratch, snuba performs up to f1 points better using the same amount of labeled data. this demonstrates how for many tasks, using a larger training set with noisy labels is able to train a better end model from scratch than fine tuning a pretrained model with a small labeled dataset. we compare end model performance trained on labels snuba generates to labels from manually generated labeling sources in table and report the precision, recall, and f1 score of snubagenerated and userdefined heuristics in table the labels from the heuristics are combined using the snuba label aggregator, the generative model in section overall, snuba generates heuristics that perform up to f1 points better than userdefined heuristics. note that users develop heuristics that are very high precision, up to points. snubagenerated heuristics, on the other hand, balance both precision and recall. this supports the design of the system since the synthesizer optimizes for f1 score, which relies on both precision and recall, and the pruner optimizes for both accuracy and coverage, which are related to both precision and recall. for image domains, snuba generates fewer heuristics (figure 7) that depend on more primitives than userdefined heuristics. primitives for image domains are numerical and require guessing the correct threshold for heuristics, a process snuba automates while users guess manually. for the bone tumor classification task, the userdefined heuristics were manually tuned versions of decision trees fit to the labeled set. therefore, snuba only improves f1 points over this partially automated approach. for text datasets (mscoco and imdb), snuba generates almost as many heuristics as users since each heuristic relies only on a single primitive and improves f1 score by up to points (table 3). for cdr, users relied on distant supervision through the comparative toxicogenomics database [10]. snuba only relies on the primitives it has access to and cannot incorporate any external information, leading to f1 points lower performance than userdefined heuristics using distant supervision. finally, for hardware, snuba uses only labeled datapoints to generate heuristics while users had access to 47, 413, which leads to snuba performing f1 points worse in terms of end model performance. we evaluate the individual components of the snuba system and show how adjustments to each component can affect end model performance by up to f1 points. first, we compare how different heuristic models perform for select tasks in table and show how much better the best heuristic type (marked as 0) performs compares to alternate heuristic types. for textbased tasks, decision tree and logistic regressor based heuristics perform the same since they both rely on a single primitive and learn the same threshold to make a binary decision. these heuristic models essentially check whether a word exists in a sentence. next, we set to prevent heuristics from abstaining and set it to a constant 0.25, the midpoint of possible values (0, 0.5) (table 4). allowing heuristics to abstain can improve end model performance by up to f1 points and choosing the correct value can improve end model performance by up to f1 points. we show the performance of the pruner compared to only optimizing for either performance (with f1 score) or diversity (with jaccard distance) in table for text tasks, only optimizing for performance comes within f1 points of the snuba pruner since each heuristic selecting a different word automatically accounts for diversity. on the other hand, only optimizing for diversity in text domains can affect performance by up to f1 points since it could result in a large portion of the unlabeled dataset receiving lowquality labels. we also compare to weighting the f1 score by how much of the unlabeled dataset is covered, which performs closest to the simple average case for textbased tasks. this suggests that other domainspecific weighting schemes, like weighting coverage more than accuracy given sparse primitives can further improve performance. finally, we look at how learning heuristic accuracies for label aggregation compares to majority vote in table text domains in which the number of heuristics generated is more than 15, the majority vote score comes within f1 points of the snuba verifier. with a large number of heuristics, each datapoint receives enough labels that learning accuracies has little effect on the assigned labels [28]. we compare to using the empirical accuracies of the heuristics rather than learning accuracies based on labels assigned to the unlabeled data. this method performs worse than the snuba verifier by up to f1 points. we also generate heuristics till there are no more datapoints in the small, labeled dataset with low confidence labels and find that this can degrade end model performance by up to f1 points as shown in table we compare to passing a weighted version of the small, labeled dataset as feedback to the synthesizer instead of a subset and find it performs up to f1 points worse than passing a subset. we posit that heuristics fit to a weighted set can lead to more low confidence labels and eventually a higher rate of abstentions for the unlabeled dataset. we provide an overview of methods that label data automatically based on heuristics, use both labeled an unlabeled data, and aggregate noisy sources of labels. the inspiration for snuba comes from program synthesis, where programs are generated given access to a set of inputoutput pairs [15,44], reference implementations [3], or demonstrations [21]. the design is based loosely on counterexample guided inductive synthesis (cegis) in which a synthesizer generates programs, passes it to the verifier that decides whether the candidate program satisfies the given specifications, and passes relevant feedback to the synthesizer [15,21,44,46]. however, unlike snuba, such models only synthesize programs that match all the specified inputoutput pairs. other works also generate heuristics to help interpret the underlying data labels [54,55], but neither methods use unlabeled data since the programs generated either mimic the desired program perfectly or provide interpretations for existing labels. while snuba focuses on generating training labels for various domains, rule learning has been widely studied in the context of information extraction [33,45]. recent works can learn logical rules for knowledge base reasoning [59], interleave beam search with parameter learning [24], select rules from a restricted set using lasso regression [27], and use alternate gradientbased search to find parameters for probailistic logic [56]. while these methods are more sophisticated than snuba, they use a large amount of training data and rely directly on the generated rules for prediction. incorporating these methods into the snuba synthesizer could be interesting for future work, especially for textbased tasks. training label generation. focusing on the problem of generating training data, snorkel [38] is a system that relies on domain experts manually developing heuristics, patterns, or distant supervision rules to label data noisily. while users in snorkel rely on a small, labeled dataset to evaluate and refine their heuristics, snuba automatically generates heuristics using the labeled and unlabeled data it has access to. snorkel and snuba both use the generative model to aggregate heuristic labels, but snuba can generate heuristics that are noisier than the generative model can account for. therefore, it uses a statistical measure to determine when the generative model can be used (section 4). other methods that rely on imperfect sources of labels that are partially userdefined include heuristic patterns [6,17] and distant supervision [8,32], which relies on information present in knowledge bases. utilizing labeled and unlabeled data. to train a deep learning model with a small, labeled dataset, a common approach is using transfer learning, or retraining models that have been trained for different tasks that have abundant training data in the same domain [34]. however, this approach does not take advantage of any unlabeled data available. semisupervised learning leverages both labeled and unlabeled data, along with assumptions about lowdimensional structure and smoothness of the data to automatically assign labels to the unlabeled data [7,61]. unlike semisupervised learning, which generates a single blackbox model, snuba generates multiple, diverse heuristics to label the unlabeled data. moreover, as demonstrated in section 5, snuba performs better than a specific semisupervised model, label spreading [61], when the amount of unlabeled data is larger than than the amount of labeled data. cotraining [5] also takes advantage of both labeled and unlabeled data and trains two independent models on two separate views of the data. snuba does not require access to separate feature sets as views and can generate more than two heuristics (classifiers) that can be correlated with each other (section 4). combining noisy labels. combining labels from multiple sources like heuristics is wellstudied problem [11], especially in the context of crowdsourcing [9,22,60]. however, these methods assume the labeling sources are not generated automatically and requires a labeled dataset to learn the accuracies of the different sources. other methods, including our previous work [39,50,53], rely on generative models to learn accuracies and dependencies among labeling sources [2,41,48]. areas like data fusion [13,37,40] and truth discovery [29] also look at the problem of estimating how reliable different data sources are while utilizing probabilistic graphical models like snuba. snuba is a system to automatically generate heuristics using a small labeled dataset to assign training labels to a large, unlabeled dataset, which can be used to train a downstream model of choice. it iteratively generates heuristics that are accurate and diverse for the unlabeled dataset using the small, labeled dataset. snuba relies on a statistical measure to determine when generated heuristics are too noisy and therefore when to terminate the iterative process. we demonstrate how training labels from snuba outperform labels from semisupervised learning by up to f1 points and from userdefined heuristics by up to f1 points in terms of end model performance for tasks across various domains. our work suggests that there is potential to use a small amount of labeled data to make the process of generating training labels much more efficient.", "summary": "snuba: automating weak supervision to label training data varma r, vldb this week we\u2019re moving on from icml to start looking at some of the papers from vldb vldb is a huge conference, and once again i have a problem because my shortlist of that looks really interesting, i\u2019d love to read it papers runs to long at the moment! as a special bonus for me, i\u2019m actually going to be at vldb this year, where no doubt i\u2019ll learn about even more interesting things! by the time you get to read this, it should be the first (workshop) day of the conference the conference may have changed, but to bridge from icml to vldb i\u2019m going to start with a paper on very much the same theme as we\u2019ve been dipping into over the past couple of weeks: how to combine and learn from multiple noisy sources of data and labels. snuba is from the same stanford line as snorkel which we looked at last year. it\u2019s tackling the same fundamental problem: how to gather enough labeled data to train a model, and how to effectively use it in a weak supervision setting (supervised learning with noisy labels). in snorkel human experts write (noisy) labelling functions, aka heuristics, but in snuba the system itself generates its own heuristics! here\u2019s the setup: we have a small labeled dataset, but not big enough to learn an accurate classifier we use the labeled dataset to learn classifiers that have goodenough accuracy over subsets of the data (features) we use the learned classifiers to predict labels for a much larger unlabelled dataset we train a final model on the now noisily labelled large dataset, and this model shows increased performance it took me quite a while to get my head around this! we don\u2019t have enough labeled data to learn a good classifier, but we end up learning a good classifier anyway. the secret is in the selection, application, and aggregation of those intermediate learned heuristics. snuba automatically generates heuristics that each labels the subset of the data it is accurate for, and iteratively repeats this process until the heuristics together label a large portion of the unlabeled data users from research labs, hospitals and industry helped us design snuba such that it outperforms userdefined heuristics and crowdsourced labels by up to f1 point and f1 points in terms of end model performance. compared to snorkel of course, the key challenge in automating weak supervision lies in replacing the human reasoning that drives heuristic development. the big picture snuba has three main components: the synthesiser, the pruner, and the verifier. it maintains a committed set of heuristics which will be used in labelling, and in each iteration it synthesises new candidate heuristics, selects from among them to add to the committed set, and then verifies the results. the synthesiser uses the small labelled dataset to generate new candidate heuristics. it associates each heuristic with a labelling pattern to assign labels only where the heuristic has high confidence. allowing heuristics to abstain from labelling datapoints where they have low confidence is a key part of how snuba achieves its final performance. compared to noisily labelling everything the result is a smaller labelled dataset once the heuristic has been applied to the unlabelled data, but with higher confidence. not every candidate heuristic generated by the synthesiser ends up being used. it\u2019s the job of the pruner to select a heuristic to add from among them. we want the heuristics in the committed set to be diverse in terms of the datapoints in the unlabeled set they label, but also ensure that it performs well for the datapoints it labels in the labeled dataset. a diverse heuristic is defined as one that labels points that have never received a label from any other heuristics. the pruner selects for diversity by measuring the jaccard distance between the set of datapoints labelled by a candidate heuristic and the set of datapoints labelled so far. the verifier uses a generative model to learn the accuracies of the heuristics in the committed set and produce a single probabilistic training label for each data point in the unlabeled dataset. generative models are a popular approach to learn and model the accuracies of different labeling sources like userdefined heuristics and knowledge bases when data is labeled by a variety of sources. the generative model assumes that each heuristic performs better than random. for userdefined heuristics that\u2019s a reasonable assumption, but with machinegenerated heuristics that assumption could be violated. snuba use the small labeled dataset to indirectly determine whether the generated heuristics are likely to be worse than random on the unlabeled dataset. snuba keeps iterating (adding new heuristics) until the estimate of the accuracy of the new heuristic suggests it performs worse than random. supported heuristic models users can plug in their own heuristic models for heuristic generation, all that\u2019s required is that the model generates heuristics that output a probabilistic label over a subset of the data. snuba comes with three different models out of the box: decision stumps are decision trees limited to a certain depth logistic regressors learn a single linear decision boundary knearest neighbour relies on the distribution of the labeled datapoints to decide decision boundaries, with confidence based on the distance of an unlabelled point from a cluster. why does the end model performed better than a simple ensemble of the selected heuristics? at the end of this process, we have an aggregation of heuristics that assign probabilistic labels to a large portion of the unlabelled dataset. why not just use this aggregation as the final model?? one of the motivations for designing snuba is to efficiently label enough training data for training powerful, downstream machine learning models like neural networks. heuristics from snuba are not used directly for the classification task at hand because (1) they may not label the entire dataset due to abstentions, and (2) they are based only on the userdefined primitives and fail to take advantage of the raw data representation. for example, heuristics may be based on features such as measurements of a tumor (images), bagofwords representations (text), or bounding box coordinates. an end model can operate over the entire raw image, sentence, or representation. evaluation the evaluation demonstrates the following: training labels from snuba outperform labels from automated baseline methods by up to f1 points, and to transfer learning from the small labelled dataset by up to f1 points training labels from snuba outperform those from userdeveloped heuristics by up to f1 points. the heuristics developed by users have very high precision, but snuba claws back its advantage by improving recall through its diversity measures. each component of snuba plays its part in boosting overall system performance. experiments are conducted over a variety of different applications and datasets: in imagebased tasks snuba generated heuristics that used at most primitives, while for textbased tasks it generated heuristics that relied on only a single primitive. our work suggests that there is potential to use a small amount of labeled data to make the process of generating training labels much more efficient. the jaccard distance between two sets a and b is given by"}, {"document": "the goal of unsupervised imagetoimage translation is to map images from one domain to another without the ground truth correspondence between the two domains. stateofart methods learn the correspondence using large numbers of unpaired examples from both domains and are based on generative adversarial networks. in order to preserve the semantics of the input image, the adversarial objective is usually combined with a cycleconsistency loss that penalizes incorrect reconstruction of the input image from the translated one. however, if the target mapping is manytoone, e.g. aerial photos to maps, such a restriction forces the generator to hide information in lowamplitude structured noise that is undetectable by human eye or by the discriminator. in this paper, we show how such selfattacking behavior of unsupervised translation methods affects their performance and provide two defense techniques. we perform a quantitative evaluation of the proposed techniques and show that making the translation model more robust to the selfadversarial attack increases its generation quality and reconstruction reliability and makes the model less sensitive to lowamplitude perturbations. generative adversarial networks (gans) [7] have enabled many recent breakthroughs in image generation, such as being able to change visual attributes like hair color or gender in an impressively realistic way, and even generate highly realisticlooking faces of people that do not exist [13, 31, 14]. conditional gans designed for unsupervised imagetoimage translation can map images from one domain to another without pairwise correspondence and ground truth labels, and are widely used for solving such tasks as semantic segmentation, colorization, style transfer, and quality enhancement of images [34, 10, 19, 3, 11, 35, 4] and videos [2, 1]. these models learn the crossdomain mapping by ensuring that the translated image both looks like a true representative of the target domain, and also preserves the semantics of the input image, e.g. the shape and position of objects, overall layout etc. semantic preservation is usually achieved by enforcing cycleconsistency [34], i.e. a small error between the source image and its reverse reconstruction from the translated target image.. despite the success of cycleconsistent gans, they have a major flaw. the reconstruction loss forces the generator network to hide the information necessary to faithfully reconstruct the input image inside tiny perturbations of the translated image [5]. the problem is particularly acute in manytoone mappings, such as photos to semantic labels, where the model must reconstruct textures and colors lost during translation to the target domain. for example, figure 1\u2019s top row shows that even when the car is mapped incorrectly to semantic labels of building (gray) and tree (green), cyclegan is still. .c v. ] able to cheat and perfectly reconstruct the original car from hidden information. it also reconstructs road textures lost in the semantic map. this behavior is essentially an adversarial attack that the model is performing on itself, so we call it a selfadversarial attack.. in this paper, we extend the analysis of selfadversarial attacks provided in [5] and show that the problem is present in recent stateofart methods that incorporate cycle consistency. we provide two defense mechanisms against the attack that resemble the adversarial training technique widely used to increase robustness of deep neural networks to adversarial attacks [9, 16, 32]. we also introduce quantitative evaluation metrics for translation quality and reconstruction honesty that help to detect selfadversarial attacks and provide a better understanding of the learned crossdomain mapping. we show that due to the presence of hidden embeddings, state of the art translation methods are highly sensitive to highfrequency perturbations as illustrated in figure in contrast, our defense methods substantially decrease the amount of selfadversarial structured noise and thus make the mapping more reliant on the input image, which results in more interpretable translation and reconstruction and increased translation quality. importantly, robustifying the model against the selfadversarial attack makes it also less susceptible to the highfrequency perturbations which make it less likely to converge to a nonoptimal solution. unsupervised imagetoimage translation is one of the tasks of domain adaptation that received a lot of attention in recent years. current stateofart methods [34, 20, 11, 15, 4, 10] solve this task using generative adversarial networks [8] that usually consist of a pair of generator and discriminator networks that are trained in a minmax fashion to generate realistic images from the target domain and correctly classify real and fake images respectively.. the goal of imagetoimage translation methods is to map the image from one domain to another in such way that the output image both looks like a real representative of the target domain and contains the semantics of the input image. in the supervised setting, the semantic consistency is enforced by. the ground truth labels or pairwise correspondence. in case when there is no supervision, however, there is no such ground truth guidance, so using regular gan results in often realisticlooking but unreliable translations. in order to overcome this problem, current stateofart unsupervised translation methods incorporate cycleconsistency loss first introduced in [34] that forces the model to learn such mapping from which it is possible to reconstruct the input image.. recently, various methods have been developed for unimodal (cyclegan [34], unit [20], cogan [21] etc.) and multimodal (munit [11], stargan [4], bicyclegan [35]) imagetoimage translation. in this paper, we explore the problem of selfadversarial attacks in three of them: cyclegan, unit and munit. cyclegan is a unimodal translation method that consists of two domain discriminators and two generator networks; the generators are trained to produce realistic images from the corresponding domains, while the discriminators aim to distinguish indomain real images from the generated ones. the generatordiscriminator pairs are trained in a minmax fashion both to produce realistic images and to satisfy the cycleconsistency property. the main idea behind unit is that both domains share some common semantics, and thus can be encoded to the shared latent space. it consists of two encoderdecoder pairs that map images to the latent space and back; the crossdomain translation is then performed by encoding the image from the source domain to the latent space and decoding it with the decoder for the target domain. munit is a multimodal extension of unit that performs disentanglement of domainspecific (style space) and domainagnostic (content space) features. while the original munit does not use the explicit cycleconsistency loss, we found that cycleconsistency penalty significantly increases the quality of translation and helps the model to learn more reliable content disentanglement (see figure 2). thus, we used the munit with cycleconsistency loss in our experiments.. as illustrated in figure 2, adding cycleconsistency loss indeed helps to disentangle domainagnostic information and enhance the translation quality and reliability. however, such pixelwise penalty was shown [5] to force the generator to hide the domainspecific information that cannot be explicitly reconstructed from the translated image (i.e., shadows or color of the buildings from maps in mapstophotos example) in such way that it cannot be detected by the discriminator.. it has been known that deep neural networks [17], while providing higher accuracy in the majority of machine learning problems, are highly susceptible to the adversarial attacks [24, 29, 16, 23]. there exist multiple defense techniques that make neural networks more robust to the adversarial examples, such as adding adversarial examples to the training set or adversarial training [24, 22], distillation [25], ensemble adversarial training [30], denoising [18] and many more. moreover, [33] have shown that defending the discriminator in a gan setting increases the generation quality and prevents the model from converging to a nonoptimal solution. however, most adversarial defense techniques are developed for the classification task and are very hard to adapt to the generative setting. suppose we are given a number of samples from two image domains x pa and y pb the goal is to learn two mappings g : x pa y pb and f : y pb x pa. in order to learn the distributions pa and pb , two discriminators da and db are trained to classify whether the input image is a true representative of the corresponding domain or generated by g or f accordingly. the crossdistribution mapping is learned using the cycleconsistency property in form of a loss based on the pixelwise distance between the input image and its reconstruction. usually, the cycleconsistency loss can be described as following:. however, in case when domain a is richer than b, the mapping g : x pa y pb is manytoone (i.e. if for one image x pb there are multiple correct correspondences y pa), the generator is still forced to perfectly reconstruct the input even though some of the information of the input image is lost after the translation to the domain b. as shown in [5], such behavior of a cyclegan can be described as an adversarial attack, and in fact, for any given image it is possible to generate such structured noise that would lead to reconstruction of the target image [5].. in practice, cyclegan and other methods that utilize cycleconsistency loss add a very lowamplitude signal to the translation y that is invisible for a human eye. addition of a certain signal is enough to reconstruct the information of image x that should not be present in y. this makes methods that. incorporate cycleconsistency loss sensitive to lowamplitude highfrequency noise since that noise can destroy the hidden signal (shown in figure 3). in addition, such behavior can force the model to converge to a nonoptimal solution or even diverge since by adding structured noise the model cheats to minimize the reconstruction loss instead of learning the correct mapping. one approach to defend the model from a selfadversarial attack is to train it to be resistant to the perturbation of nature similar to the one produced by the hidden embedding. unfortunately, it is impoossible to separate the pure structured noise from the traslated image, so classic adversarial defense training cannot be used in this scenario. however, it is possible to prevent the model from learning to embed by adding perturbations to the translated image before reconstruction. the intuition behind this approach is that adding random noise of amplitude similar to the hidden signal disturbs the embedded message. this results in high reconstruction error, so the generator cannot rely on the embedding. the modified noisy cycleconsistency loss can be described as follows:. where (n) is some highfrequency perturbation function with parameters n. in our experiments we used lowamplitude gaussian noise with mean equal to zero. such a simplistic defense approach is very similar to the one proposed in [33] where the discriminator is defended from the generator attack by regularizing the discriminator objective using the adversarial vectors. in our setting, however, the attack is targeted on both the discriminator and the generator of opposite domain, which makes it harder to find the exact adversarial vector. which is why we regularize both the discriminator and generator using random noise. since adding noise to the input image is equivalent to penalizing large magnitude of the gradients of the loss function, this also forces the model to learn smoother boundaries and prevents it from overfitting. ideally, the selfadversarial attack should be detected by the discriminator, but this might be too hard for it since it never sees real and fake examples of the same content. in the supervised setting, this problem is naturally solved by conditioning the outputs on the ground truth labels. for example, a selfadversarial attack does not occur in conditional gans because the discriminator is conditioned on the ground truth class labels and is provided with real and fake examples of each class. in the unsupervised setting, however, there is no such information about the class labels, and the discriminator only receives unpaired real and fake examples from the domain. this task is significantly harder for the discriminator as it has to learn the distribution of the whole domain. one widely used defense strategy is adding the adversarial examples to the training set. while it is possible to model the adversarial attack of the generator, it is very time and memory consuming as it requires training an additional network that generates such examples at each step of training the gan. however, we can use the fact that cycleconsistency loss forces the model to minimize the difference between the input and reconstructed images, so we can use the reconstruction output to provide the fake example for the real input image as an approximation of the adversarial example.. thus, the defense during training can be formulated in terms of an additional guess discriminator that is very similar to the original gan discriminator, but receives as input two images input and reconstruction in a random order, and guesses which of the images is fake. as with the original discriminator, the guess discriminator dguess is trained to minimize its error while the generator aims to produce such images that maximize it. the guess discriminator loss or guess loss can be described as:. lguess gaguessx, f (g(x), with probability 1gaguessf (g(x)), x, with probability this loss resembles the class label conditioning in the conditional gan in the sense that the guess discriminator receives real and fake examples that are presumably of the same content, therefore the embedding detection task is significantly simplified.. in addition to the defense approaches described above, it is beneficial to use the fact that the relationship between the domains is onetomany. one naive solution to add such prior knowledge is by assigning a smaller weight to the reconstruction loss of the richer domain (e.g. photos in mapstophotos experiment). results of our experiments show substantial improvement in the generation quality when such a domain relation prior is used. in abundance of ganbased methods for unsupervised image translation, we limited our analysis to three popular stateofart models that cover both unimodal and multimodal translation cases: cyclegan[34], unit[20] and munit[11]. the details on model architectures and choice of hyperparameters used in our experiments can be found in the supplementary materials. to provide empirical evidence of our claims, we performed a sequence of experiments on three publicly available imagetoimage translation datasets. despite the fact that all three datasets are paired and hence the ground truth correspondence is known, the models that we used are not capable of using the groundtruth alignment by design and thus were trained in an unsupervised manner.. google aerial photo to maps dataset consisting of pairs of aerial photos and corresponding maps. in our experiments, we resized the images from pixels to pixels for munit and unit and to pixels for cyclegan. during training, the images were randomly cropped to for unit and munit and for cyclegan. the dataset is available at [6]. we used images for training and images for testing.. playing for data (gta)[26] dataset that consists of pairs of image frames and their semantic segmentation maps. we used a subset of frames (7500 images for training, images for testing) with daytime lighting resized to pixels, and randomly cropped with window size synaction [28] synthetic human action dataset consisting of a set of possible actions performed by different human renders. for our experiments, we used two actors and all existing actions to perform the translation from one actor to another; all other conditions such as background, lighting, viewpoint etc. are chosen to be the same for both domains. we used this dataset to test whether the selfadversarial attack is present in the onetoone setting. the original images were resized to and cropped to we split the data to images in each domain for training images for testing. the choice of aligned datasets was dictated by the need to quantitatively evaluate the translation quality which is impossible when the ground truth correspondence is unknown. however, even having the ground truth pairs does not solve the issue of quality evaluation in onetomany case, since for one input image there exist a large (possibly infinite) number of correct translations, so pixelwise comparison of the ground truth image and the output of the model does not provide a correct metric for the translation quality.. figure 3: actor translation example with cyclegan, cyclegan with noise and cyclegan with guess loss.. in order to overcome this issue, we adopted the idea behind the inception score [27] and trained the supervised pix2pix[12] model to perform manytoone mapping as an intermediate step in the evaluation. considering the gta dataset example, in order to evaluate the unsupervised mapping from segmentation maps to real frames (later on segmentation to real), we train the pix2pix model to translate from real to segmentation; then we feed it the output of the unsupervised model to perform honest reconstruction of the input segmentation map, and compute the intersection over union (iou) and mean classwise accuracy of the output of pix2pix when given a ground truth example and the output of the onetomany translation model. for any ground truth pair (ai, bi), the onetomany translation quality is computed as iou(pix(ga(bi)), pix(ai)), where pix() is the translation with pix2pix from a to b. the honest reconstruction is compared with the pix2pix translation of the ground truth image ai instead of the ground truth image itself in order to take into account the error produced by the pix2pix translation. reconstruction honesty. since it is impossible to acquire the structured noise produced as a result of a selfadversarial attack, there is no direct way to either detect the attack or measure the amount of information hidden in the embedding.. method mse sn cyclegan cyclegannoise cycleganguess table 1: results on synaction dataset: mean square error of the translation and sensitivity to noise.. in order to evaluate the presence of a selfadversarial attack, we developed a metric that we call quantized reconstruction honesty. the intuition behind this metric is that, ideally, the recon. struction error of the image of the richer domain should be the same as the onetomany translation error if given the same input image from the poorer domain. in order to measure whether the model is independent of the origin of the input image, we quantize the manytoone translation results in such way that it only contains the colors from the domainspecific palette. in our experiments, we approximate the quantized maps by replacing the colors of each pixel by the closest one from the palette. we then feed those quantized images to the model to acquire the honest reconstruction error, and compare it with the reconstruction error without quantization. the honesty metric for a onetomany reconstruction can be described as follows:. n n i1 ga(bgb(xi)c) yi ga(gb(xi)) yi, (4). where bc is a quantization operation, gb is a manytoone mapping, (xi, yi) is a ground truth pair of examples from domains a and b.. aside from the obvious consequences of the selfadversarial attack, such as convergence of the generator to a suboptimal solution, there is one more significant side effect of it extreme sensitivity to perturbations. figure shows how addition of lowamplitude gaussian noise effectively destroys the hidden embedding thus making a model that uses cycleconsistency loss unable to correctly reconstruct the input image. in order to estimate the sensitivity of the model, we add zeromean gaussian noise to the translation result before reconstruction and compute the reconstruction error. the sensitivity to noise of amplitude for a set of images xi pa is computed by the following formula:. where mse is the mean square pixelwise error. the overall sensitivity of a method is then computed as an area under curve of auc(sn()) b a sn(x)dx. in our experiments we chose a 0, b 0.2, n for google maps and gta experiments and n for the synaction experiment. in case when there is no structured noise in the translation, the reconstruction error. should be proportional to the amplitude of added noise, which is what we observe for the onetomany mapping using munit and cyclegan. surprisingly, unit translation is highly senstive to noise even in onetomany case. the manytoone mapping result (figure 3), in contrast, suggests that the structured noise is present, since the reconstruction error increases rapidly and quickly saturates at noise amplitude the results of onetomany and manytoone noisy reconstruction show that both noisy cyclegan and guess loss defense approaches make the cyclegan model more robust to highfrequency perturbations compared to the original cyclegan. the results of our experiments show that the problem of selfadversarial attacks is present in all three cycleconsistent methods we examined. surprisingly, the results on the synaction dataset had shown that selfadversarial attack appear even if the learned mapping is onetoone (table 1). both defense techniques proposed in section make cyclegan more robust to random noise and increase its translation quality (see tables 1, and 3). the noiseregularization defense helps the cyclegan model to become more robust both to small perturbations and to the selfadversarial attack. the guess loss approach, on the other hand, while allowing the model to hide some small portion of information about the input image (for example, road marking for the gta experiment), produces more interpretable and reliable reconstructions. since both defense techniques force the generators to rely more on the input image than on the structured noise, their results are more interpretable and provide deeper understanding of the methods reasoning. for example, since the training set did not contain any examples of a truck that is colored in white and green, at test time the guessloss cyclegan approximated the green part of the truck with the vegetation class color and the white part with the building class color (see section of the supplementary material); the reconstructed frame looked like a rough approximation of the truck despite the fact that the semantic segmentation map was wrong. this can give a hint about the limitations of the given training set. in this paper, we introduced the selfadversarial attack phenomenon of unsupervised imagetoimage translation methods the hidden embedding performed by the model itself in order to reconstruct the input image with high precision. we empirically showed that selfadversarial attack appears in models when the cycleconsistency property is enforced and the target mapping is manytoone. we provided the evaluation metrics that help to indicate the presence of selfadversarial attack, and a translation quality metric for onetomany mappings. we also developed two adversarial defense techniques that significantly reduce the hidden embedding and force the model to produce more honest results, which, in return, increases its translation quality. in our experiments, we used the implementation of cyclegan provided at https://github.com/ junyanz/pytorchcycleganandpix2pix. for all cyclegan models we used (original, noisy and guessloss based) we set all the cyclegan parameters to the default ones provided in the implementation except for the weights of the cycleconsistency loss.. the cyclegan parameters used in our experiments are:. generator architecture resnet with residual block layers discriminator architecture 3layer patchgan with patch size 70x70 weight initialization gaussian instance normalization gan objective lsgan optimizer adam with momentum learning rate with linear policy trained for epochs.. the parameters specific to the proposed defense techniques are:. for training with additive noise: standard deviation of noise that should lie in the interval [0, 1]. the higher is the value of , the harder it is for the model to perform the selfadversarial attack. we chose the minimal value which results in the reconstruction that lacks the highfrequency details that should be lost after the translation, such as road texture or color.. for the guess loss weight of the guess loss guess. we chose guess and the cycleconsistency losses weights a and b such that their corresponding loss values are of the similar magnitude during training. in other words, we choose the loss weights to be such that they all lie within one range and none of them dominates in the overall loss.. for the gta dataset, the defensespecific parameters are:. we performed the experiments with on the cyclegan with the smaller weights a and b that are proportional to the crossdomain relation as for the guess loss approach (e.g. a and b 3), and this resulted in unreliable translation.. cyclegan noise: 0.06, a 5, b cyclegan guess loss: guess 2, a 1.5, b for the synaction, the defensespecific parameters are:. cyclegan guess loss: guess 1, a 2, b for the google maps dataset, we used the following parameters:. cyclegan guess loss: guess 1, a 1, b we based our experiments on the unit and munit models on their original implementation: https://github.com/nvlabs/munit.. unit architecture and parameters are:. optimizer adam with momentum and second momentum initialization kaiming learning rate with step decay policy (decay weight 0.5, step size iterations) weight on image reconstruction loss weight on cycleconsistency loss weight of kl loss for cycle consistency discriminator 4layer multiscale lsgan with leaky relu activation function and scales. generator vae with relu activations, with filters in the first layer, downsampling. layers and residual blocks for the content encoder and decoder. padding reflect.. munit parameters are:. optimizer adam with momentum and second momentum initialization kaiming learning rate with step decay policy (decay weight 0.5, step size iterations) weight on image reconstruction loss weight on explicit cycleconsistency loss weight of kl loss for cycle consistency discriminator 4layer multiscale lsgan with leaky relu activation function and scales. generator vae with relu activations, with filters in the first layer, with filters in. mlp, downsampling layers and residual blocks for the content encoder and decoder. the code for the guess loss cyclegan and noisy cyclegan can be found in files cycleganguessmodel.py and cyclegannoisy.py respectively. in order to train or test the model, please add them to the folder models of the original cyclegan project (https:// github.com/junyanz/pytorchcycleganandpix2pix) and specify the model parameter as cycleganguess or cyclegannoisy instead of cyclegan.. statistics. translation results figures.", "summary": "domain translation for example, mapping from a summer to a winter scene, or from a photorealistic image to an object segmentation map is often performed by gans through something called cycle consistency loss. this model works by having, for each domain, a generator to map domain a into domain b, and a discriminator to differentiate between real images from domain b, and those that were constructed through the crossdomain generator. with a given image in domain a, training happens by using the ab generator to map it into domain b, and then then b a generator to map it back the original domain. these generators are then trained using two losses: one based on the bdomain discriminator, to push the generated image to look like it belongs from that domain, and another based on the l2 loss between the original domain a image, and the image you get on the other end when you translate it into b and back again. this paper addresses an effect (identified originally in an earlier paper) where in domains with a many to one mapping between domains (for example, mapping a realistic scene into a domain segmentation map, where information is inherently lost by translating pixels to object outlines), the cycle loss incentivizes the model to operate in a strange, steganographic way, where it saves information about the that would otherwise be lost in the form of lowamplitude random noise in the translated image. this lowamplitude information cant be isolated, but can be detected in a few ways. first, we can simply examine images and notice that information that could not have been captured in the lowerinformation domain is being perfectly reconstructed. second, if you add noise to the translation in the lowerinformation domain, in such a way as to not perceptibly change the translation to human eyes, this can cause the predicted image off of that translation to deteriorate considerably, suggesting that the model was using information that could be modified by such small additions of noise to do its reconstruction. the authors of this paper ask whether its possible to train models that dont perform this steganographic informationstoring (which they call self adversarial examples). a typical approach to such a problem would be to train generators to perform translations with and without the steganographic information, but even though we can prove the existence of the information, we cant isolate it in a way that would allow us to remove it, and thus create these kinds of training pairs. the two tactics the paper uses are: 1) simply training the generators to be able to translate a domainmapped image with noise as well as one without noise, in the hope that this would train it not use information that can be interfered with by the application of such noise. 2) in addition to a l2 cycle loss, adding a discriminator to differentiate between the backtranslated image and the original one. i believe the idea here is that if both of the encoders are adding in noise as a kind of secret signal, this would be a way for the discriminator to distinguish between the original and reconstructed image, and would thus be penalized. they find that both of these methods reduce the use of steganographic information, as determined both by sensitivity to noise (where less sensitivity of reconstruction to noise means less use of coded information) and reconstruction honesty (which constrains accuracy of reconstruction in many to one domains to be no greater than the prediction that a supervised predictor could make given the image from the compressed domain"}, {"document": "high computational complexity hinders the widespread usage of convolutional neural networks (cnns), especially in mobile devices. hardware accelerators are arguably the most promising approach for reducing both execution time and power consumption. one of the most important steps in accelerator development is hardwareoriented model approximation. in this paper we present ristretto, a model approximation framework that analyzes a given cnn with respect to numerical resolution used in representing weights and outputs of convolutional and fully connected layers. ristretto can condense models by using fixed point arithmetic and representation instead of floating point. moreover, ristretto finetunes the resulting fixed point network. given a maximum error tolerance of 1, ristretto can successfully condense caffenet and squeezenet to 8bit. the code for ristretto is available. the annually held ilsvrc competition has seen stateoftheart classification accuracies by deep networks such as alexnet by krizhevsky et al. (2012), vgg by simonyan zisserman (2015), googlenet (szegedy et al., 2015) and resnet (he et al., 2015). these networks contain millions of parameters and require billions of arithmetic operations.. various solutions have been offered to reduce the resourcerequirement of cnns. fixed point arithmetic is less resource hungry compared to floating point. moreover, it has been shown that fixed point arithmetic is adequate for neural network computation (hammerstrom, 1990). this observation has been leveraged recently to condense deep cnns. (2015) show that networks on datasets like cifar10 (10 images classes) can be trained in 16bit. further trimming of the same network uses as low as 7bit multipliers (courbariaux et al., 2014). another approach by courbariaux et al. (2016) uses binary weights and activations, again on the same network.. the complexity of deep cnns can be split into two parts. first, the convolutional layers contain more than of the required arithmetic operations. by turning these floating point operations into operations with small fixed point numbers, both the chip area and energy consumption can be significantly reduced. the second resourceintense layer type are fully connected layers, which contain over of the network parameters. as a nice byproduct of using bitwidth reduced fixed point numbers, the data transfer to offchip memory is reduced for fully connected layers. in this paper, we concentrate on approximating convolutional and fully connected layers only. using fixed point arithmetic is a hardwarefriendly way of approximating cnns. it allows the use of smaller processing elements and reduces the memory requirements without adding any computational overhead such as decompression.. even though it has been shown that cnns perform well with small fixed point numbers, there exists no thorough investigation of the delicate tradeoff between bitwidth reduction and accuracy loss. in this paper we present ristretto, which automatically finds a perfect balance between the bitwidth reduction and the given maximum error tolerance. ristretto performs a fast and fully automated trimming analysis of any given network. this posttraining tool can be used for applicationspecific trimming of neural networks.. ar x. iv :1. in the next two sections we discuss quantization of a floating point cnn to fixed point. moreover, we explain dynamic fixed point, and show how it can be used to further decrease network size while maintaining the classification accuracy.. the data path of fully connected and convolutional layers consists of a series of mac operations (multiplication and accumulation), as shown in figure the layer activations are multiplied with the network weights, and the results are accumulated to form the output. (2016), it is a good approach to use mixed precision, i.e., different parts of a cnn use different bitwidths.. in figure 1, m and n refer to the number of bits for layer outputs and layer weights, respectively. multiplication results are accumulated using an adder tree which gets thicker towards the end. the adder outputs in the first level aremn2 bits wide, and the bitwidth grows by bit in each level. in the last level, the bitwidth is m n lg2 x, where x is the number of multiplication operations per output value. in the last stage, the bias is added to form the layer output. for each network layer, we need to find the right balance between reducing the bitwidths (m and n) and maintaining a good classification accuracy. the different parts of a cnn have a significant dynamic range. in large layers, the outputs are the result of thousands of accumulations, thus the network parameters are much smaller than the layer outputs. fixed point has only limited capability to cover a wide dynamic range. dynamic fixed point (williamson, 1991; courbariaux et al., 2014) is a solution to this problem.. in dynamic fixed point, each number is represented as follows: (1)s 2fl b2. b denotes the bitwidth, s the sign bit, fl is the fractional length, and x the mantissa bits. the intermediate values in a network have different ranges. therefor it is desirable to assign fixed point numbers into groups with constant fl, such that the number of bits allocated to the fractional part is constant within that group. each network layer is split into two groups: one for the layer outputs, one for the layer weights. this allows to better cover the dynamic range of both layer outputs and weights, as weights are normally significantly smaller. on the hardware side, it is possible to realize dynamic fixed point arithmetic using bit shifters.. different hardware accelerators for deployment of neural networks have been proposed (motamedi et al., 2016; qiu et al., 2016; han et al., 2016a). the first important step in accelerator design is the compression of the network in question. in the next section we present ristretto, a tool which can condense any neural network in a fast and automated fashion. according to wikipedia, ristretto is \u2019a short shot of espresso coffee made with the normal amount of ground coffee but extracted with about half the amount of water\u2019. similarly, our compressor removes the unnecessary parts of a cnn, while making sure the essence the ability to predict image classes is preserved. with its strong community and fast training for deep cnns, caffe (jia et al., 2014) is an excellent framework to build on.. ristretto takes a trained model as input, and automatically brews a condensed network version. input and output of ristretto are a network description file (prototxt) and the network parameters. optionally, the quantized network can be finetuned with ristretto. the resulting fixed point model in caffeformat can then be used for a hardware accelerator. ristretto\u2019s quantization flow has five stages (figure 2) to compress a floating point network into fixed point. in the first step, the dynamic range of the weights is analyzed to find a good fixed point representation. for the quantization from floating point to fixed point, we use roundnearest. the second step runs several thousand images in forward path. the generated layer activations are analyzed to generate statistical parameters. ristretto uses enough bits in the integer part of fixed point numbers to avoid saturation of layer activations. next ristretto performs a binary search to find the optimal number of bits for convolutional weights, fully connected weights, and layer outputs. in this step, a certain network part is quantized, while the rest remains in floating point. since there are three network parts that should use independent bitwidths (weights of convolutional and fully connected layers as well as layer outputs), iteratively quantizing one network part allows us to find the optimal bitwidth for each part. once a good tradeoff between small number representation and classification accuracy is found, the resulting fixed point network is retrained. in order to make up for the accuracy drop incurred by quantization, the fixed point network is finetuned in ristretto. during this retraining procedure, the network learns how to classify images with fixed point parameters. since the network weights can only have discrete values, the main challenge consists in the weight update. we adopt the idea of previous work (courbariaux et al., 2015) which uses full precision shadow weights. small weight updates w are applied to the full precision weights w, whereas the discrete weights w are sampled from the full precision weights. the sampling during finetuning is done with stochastic rounding. this rounding scheme was successfully used by gupta et al. (2015) for weight updates of 16bit fixed point networks.. ristretto uses the finetuning procedure illustrated in figure for each batch, the full precision weights are quantized to fixed point. during forward propagation, these discrete weights are used to compute the layer outputs yl. each layer l turns its input batch xl into output yl, according to its function fl : (xl, w) yl. assuming the last layer computes the loss, we denote f as the overall cnn function.. the goal of back propagation is to compute the error gradient f/w with respect to each fixed point parameter. for parameter updates we use the adam rule by kingma ba (2015). as an important observation, we do not quantize layer outputs to fixed point during finetuning. we use floating point layer outputs instead, which enables ristretto to analytically compute the error gradient with respect to each parameter. in contrast, the validation of the network is done with fixed point layer outputs.. to achieve the best finetuning results, we used a learning rate that is an order of magnitude lower than the last full precision training iteration. since the choice of hyper parameters for retraining is crucial (bergstra bengio, 2012), ristretto relies on minimal human intervention in this step.. fast finetuning with fixed point parameters ristretto brews a condensed network with fixed point weights and fixed point layer activations. for simulation of the forward propagation in hardware, ristretto uses full floating point for accumulation. this follows the thought of gupta et al. (2015) and is conform with our description of the forward data path in hardware (figure 2). during finetuning, the full precision weights need to be converted to fixed point for each batch, but after that all computation can be done in floating point (figure 3). therefore ristretto can fully leverage optimized matrixmatrix multiplication routines for both forward and backward propagation. thanks to its fast implementation on the gpu, a fixed point caffenet can be tested on the ilsvrc validation dataset (50k images) in less than minutes (using one tesla k40 gpu). in this section we present the results of approximating 32bit floating point networks by condensed fixed point models. all classification accuracies were obtained running the respective network on the whole validation dataset. we present approximation results of ristretto for five different networks. first, we consider lenet (lecun et al., 1998) which can classify handwritten digits (mnist dataset). second, cifar10 full model provided by caffe is used to classify images into different classes. third, we condense caffenet, which is the caffe version of alexnet and classifies images into the imagenet categories. fourth, we use the bvlc version of googlenet (szegedy et al., 2015) to classify images of the same data set. finally, we approximate squeezenet (iandola et al., 2016), a recently proposed architecture with the classification accuracy of alexnet, but50x fewer parameters.. impact of dynamic fixed point we used ristretto to quantize caffenet (alexnet) into fixed point, and compare traditional fixed point with dynamic fixed point. to allow a simpler comparison, all layer outputs and network parameters share the same bitwidth. results show a good performance of static fixed point for as low as 18bit (figure 4). however, when reducing the bitwidth further, the accuracy starts to drop significantly, while dynamic fixed point has a stable accuracy.. we can conclude that dynamic fixed point performs significantly better for such a large network. with dynamic fixed point, we can adapt the number of bits allocated to integer and fractional part, according to the dynamic range of different parts of the network. we will therefore concentrate on dynamic fixed point for the subsequent experiments.. quantization of individual network parts in this section, we analyze the impact of quantization on different parts of a floating point cnn. table shows the classification accuracy when the layer outputs, the convolution kernels or the parameters of fully connected layers are quantized to dynamic fixed point.. in all three nets, the convolution kernels and layer activations can be trimmed to 8bit with an absolute accuracy change of only fully connected layers are more affected from trimming to 8bit weights, the absolute change is maximally interestingly, lenet weights can be trimmed to as low as 2bit, with absolute accuracy change below finetuning of all considered network parts here we report the accuracy of five networks that were condensed and finetuned with ristretto. all networks use dynamic fixed point parameters as well as dynamic fixed point layer outputs for convolutional and fully connected layers. lenet performs well in 2/4bit, while cifar10 and. the three imagenet cnns can be trimmed to 8bit (see table 2). surprisingly, these compressed networks still perform nearly as well as their floating point baseline. the relative accuracy drops of lenet, cifar10 and squeezenet are very small (0.6), whereas the approximation of the larger caffenet and googlenet incurs a slightly higher cost (0.9 and respectively). we hope we will further improve the finetuning results of these larger networks in the future.. the squeezenet architecture was developed by iandola et al. (2016) with the goal of a small cnn that performs well on the imagenet data set. ristretto can make the already small network even smaller, so that its parameter size is less than mb. this condensed network is wellsuited for deployment in smart mobile systems.. all five 32bit floating point networks can be approximated well in 8bit and 4bit fixed point. for a hardware implementation, this reduces the size of multiplication units by about one order of magnitude. moreover, the required memory bandwidth is reduced by 48x. finally, it helps to hold 48x more parameters in onchip buffers. the code for reproducing the quantization and finetuning results is available1.. a previous work by courbariaux et al. (2014) concentrates on training with limited numerical precision. they can train a dynamic fixed point network on the mnist data set using just 7bits to represent activations and weights. ristretto doesn\u2019t reduce the resource requirements for training, but concentrates on inference instead. ristretto can produce a lenet network with 2bit parameters and 4bit activations. our approach is different in that we train with high numerical precision, then quantize to fixed point, and finally finetune the fixed point network.. other works (courbariaux et al., 2016; rastegari et al., 2016) can reduce the bitwidth even further to as low as 1bit, using more advanced number encodings than dynamic fixed point. ristretto\u2019s strength lies in its capability to approximate a large number of existing floating point models on challenging data sets. for the five considered networks, ristretto can quantize activations and weights to 8bit or lower, at an accuracy drop below 2.3, compared to the floating point baseline.. while more sophisticated data compression schemes could be used to achieve higher network size reduction, our approach is very hardware friendly and imposes no additional overhead such as decompression. in this work we presented ristretto, a caffebased approximation framework for deep convolutional neural networks. the framework reduces the memory requirements, area for processing elements and overall power consumption for hardware accelerators. a large net like caffenet can be quantized to 8bit for both weights and layer outputs while keeping the network\u2019s accuracy change below compared to its 32bit floating point counterpart. ristretto is both fast and automated, and we release the code as an open source project.. ristretto is in its first development stage. we consider adding new features in the future: shared weights: fetching cookbook indices from offchip memory, instead of real values (han et al.,. 1https://github.com/pmgysel/caffe. network pruning as shown by the same authors. network binarization as shown by courbariaux et al. these additional features will help to reduce the bitwidth even further, and to reduce the computational complexity of trimmed networks.", "summary": "the authors present a framework that can quantize caffe models into 8bit and lower fixedpoint precision models, which is useful for lowering memory and energy consumption on embedded devices. the compression is an iterative algorithm that determines data statistics to figure out activation and parameter ranges that can be compressed, and conditionally optimizes convolutional weights, fully connected weights and activations given the compression of the other parts.. this work focuses on processing models already trained with high numerical precision (32 bits float) and compress them, as opposed to other work that tries to train directly with quantized operations.."}, {"document": "convolutional neural networks provide visual features that perform remarkably well in many computer vision applications. however, training these networks requires significant amounts of supervision. this paper introduces a generic framework to train deep networks, endtoend, with no supervision. we propose to fix a set of target representations, called noise as targets (nat), and to constrain the deep features to align to them. this domain agnostic approach avoids the standard unsupervised learning issues of trivial solutions and collapsing of features. thanks to a stochastic batch reassignment strategy and a separable square loss function, it scales to millions of images. the proposed approach produces representations that perform on par with stateoftheart unsupervised methods on imagenet and pascal voc.in recent years, convolutional neural networks, or convnets (fukushima, 1980;lecun et al., 1989) have pushed the limits of computer vision (krizhevsky et al., 2012;he et al., 2016), leading to important progress in a variety of tasks, like object detection (girshick, 2015) or image segmentation (pinheiro et al., 2015). key to this success is their ability to produce features that easily transfer to new domains when trained on massive databases of labeled images (razavian et al., 2014;oquab et al., 2014) or weaklysupervised data (joulin et al., 2016). however, human annotations may introduce unforeseen bias that could limit the potential of learned features to capture subtle information hidden in a vast collection of images. several strategies exist to learn deep convolutional features with no annotation (donahue et al., 2016). they either try to capture a signal from the source as a form of selfsupervision (doersch et al., 2015;wang gupta, 2015) or learn the underlying distribution of images (vincent et al., facebook ai research. correspondence to: piotr bojanowski bojanowskifb.com. while some of these approaches obtain promising performance in transfer learning (donahue et al., 2016;wang gupta, 2015), they do not explicitly aim to learn discriminative features. some attempts were made with retrieval based approaches (dosovitskiy et al., 2014) and clustering (yang et al., 2016;liao et al., 2016), but they are hard to scale and have only been tested on small datasets. unfortunately, as in the supervised case, a lot of data is required to learn good representations. in this work, we propose a novel discriminative framework designed to learn deep architectures on massive amounts of data. our approach is general, but we focus on convnets since they require millions of images to produce good features. similar to selforganizing maps (kohonen, 1982;martinetz schulten, 1991), we map deep features to a set of predefined representations in a low dimensional space. as opposed to these approaches, we aim to learn the features in a endtoend fashion, which traditionally suffers from a feature collapsing problem. our approach deals with this issue by fixing the target representations and aligning them to our features. these representations are sampled from a uninformative distribution and we use this noise as targets (nat). our approach also shares some similarities with standard clustering approches like kmeans (lloyd, 1982) or discriminative clustering (bach harchaoui, 2007). in addition, we propose an online algorithm able to scale to massive image databases like imagenet (deng et al., 2009). importantly, our approach is barely less efficient to train than standard supervised approaches and can reuse any optimization procedure designed for them. this is achieved by using a quadratic loss as in (tygert et al., 2017) and a fast approximation of the hungarian algorithm. we show the potential of our approach by training endtoend on imagenet a standard architecture, namely alexnet (krizhevsky et al., 2012) with no supervision. we test the quality of our features on several image classification problems, following the setting of donahue et al. we are on par with stateoftheart unsupervised and selfsupervised learning approaches while being much simpler to train and to scale. the paper is organized as follows: after a brief review of the related work in section 2, we present our approach in arxiv:1704.05310v1 [stat.ml] apr section we then validate our solution with several experiments and comparisons with standard unsupervised and selfsupervised approaches in section several approaches have been recently proposed to tackle the problem of deep unsupervised learning (coates ng, 2012;mairal et al., 2014;dosovitskiy et al., 2014). some of them are based on a clustering loss (xie et al., 2016;yang et al., 2016;liao et al., 2016), but they are not tested at a scale comparable to that of supervised convnet training. coates ng (2012) uses kmeans to pretrain convnets, by learning each layer sequentially in a bottomup fashion. in our work, we train the convnet endtoend with a loss that shares similarities with kmeans. closer to our work, dosovitskiy et al. (2014) proposes to train convnets by solving a retrieval problem. they assign a class per image and its transformation. in contrast to our work, this approach can hardly scale to more than a few hundred of thousands of images, and requires a customtailored architecture while we use a standard alexnet. another traditional approach for learning visual representations in an unsupervised manner is to define a parametrized mapping between a predefined random variable and a set of images. traditional examples of this approach are variational autoencoders (kingma welling, 2013), generative adversarial networks (goodfellow et al., 2014), and to a lesser extent, noisy autoencoders (vincent et al., 2010). in our work, we are doing the opposite; that is, we map images to a predefined random variable. this allows us to reuse standard convolutional networks and greatly simplifies the training. generative adversarial networks. among those approaches, generative adversarial networks (gans) (goodfellow et al., 2014;denton et al., 2015;donahue et al., 2016) share another similarity with our approach, namely they are explicitly minimizing a discriminative loss to learn their features. while these models cannot learn an inverse mapping, donahue et al. (2016) recently proposed to add an encoder to extract visual features from gans. like ours, their encoder can be any standard convolutional network. however, their loss aims at differentiating real and generated images, while we are aiming directly at differentiating between images. this makes our approach much simpler and faster to train, since we do not need to learn the generator nor the discriminator. recently, a lot of work has explored selfsupervison: leveraging supervision contained in the input signal (doersch et al., 2015;noroozi favaro, 2016;pathak et al., 2016). in the same vein as word2vec (mikolov et al., 2013), doersch et al. (2015) show that spatial context is a strong signal to learn visual features. noroozi favaro (2016) have further extended this work. others have shown that temporal coherence in videos also provides a signal that can be used to learn powerful visual features (agrawal et al., 2015;jayaraman grauman, 2015;wang gupta, 2015). in particular, wang gupta (2015) show that such features provide promising performance on imagenet. in contrast to our work, these approaches are domain dependent since they require explicit derivation of weak supervision directly from the input. many have also used autoencoders with a reconstruction loss (bengio et al., 2007;ranzato et al., 2007;masci et al., 2011). the idea is to encode and decode an image, while minimizing the loss between the decoded and original images. once trained, the encoder produces image features and the decoder can be used to generate images from codes. the decoder is often a fully connected network (ranzato et al., 2007) or a deconvolutional network (masci et al., 2011;zhao et al., 2016) but can be more sophisticated, like a pixelcnn network (van den oord et al., 2016). this family of unsupervised methods aims at learning a low dimensional representation of the data that preserves certain topological properties (kohonen, 1982;vesanto alhoniemi, 2000). in particular, neural gas (martinetz schulten, 1991) aligns feature vectors to the input data. each input datum is then assigned to one of these vectors in a winnertakesall manner. these feature vectors are in spirit similar to our target representations and we use a similar assignment strategy. in contrast to our work, the target vectors are not fixed and aligned to the input vectors. since we primarly aim at learning the input features, we do the opposite. discriminative clustering. many methods have been proposed to use discriminative losses for clustering (xu et al., 2004;bach harchaoui, 2007;krause et al., 2010;joulin bach, 2012). in particular, bach harchaoui (2007) shows that the ridge regression loss could be use to learn discriminative clusters. it has been successfully applied to several computer vision applications, like object discovery (joulin et al., 2010;tang et al., 2014) or video/text alignment (bojanowski et al., 2013;ramanathan et al., 2014). in this work, we show that a similar framework can be designed for neural networks. (2004), we address the empty assignment problems by restricting the set of possible reassignments to permutations rather than using global linear constrains the assignments. our assignments can be updated online, allowing our approach to scale to very large datasets. our approach takes a set of images, computes their deep features with a convolutional network and matches them to a set of predefined targets from a low dimensional space. the parameters of the network are learned by aligning the features to the targets. in this section, we present our model and discuss its relations with several clustering approaches including kmeans. figure shows an overview of our approach. we also show that it can be trained on massive datasets using an online procedure. finally, we provide all the implementation details. we are interested in learning visual features with no supervision. these features are produced by applying a parametrized mapping f to the images. in the presence of supervision, the parameters are learned by minimizing a loss function between the features produced by this mapping and some given targets, e.g., labels. in absence of supervision, there is no clear target representations and we thus need to learn them as well. more precisely, given a set of n images x i , we jointly learn the parameters of the mapping f , and some target vectors y i : where d is the dimension of target vectors. in the rest of the paper, we use matrix notations, i.e., we denote by y the matrix whose rows are the target representations y i , and by x the matrix whose rows are the images x i with a slight abuse of notation, we denote by f (x) the n d matrix of features whose rows are obtained by applying the function f to each image independently. choosing the loss function. in the supervised setting, a popular choice for the loss is the softmax function. however, computing this loss is linear in the number of targets, making it impractical for large output spaces (goodman, 2001). while there are workarounds to scale these losses to large output spaces, tygert et al. (2017) has recently shown that using a squared distance works well in many supervised settings, as long as the final activations are unit normalized. this loss only requires access to a single target per sample, making its computation independent of the number of targets. this leads to the following problem: where we still denote by f (x) the unit normalized features. using fixed target representations. directly solving the problem defined in eq. (2) would lead to a representation collapsing problem: all the images would be assigned to the same representation (xu et al., 2004). we avoid this issue by fixing a set of k predefined target representations and matching them to the visual features. more precisely, the matrix y is defined as the product of a matrix c containing these k representations and an assignment matrix p in 0, nk , i.e., y p c. note that we can assume that k is greater than n with no loss of generality (by duplicating representations otherwise). each image is assigned to a different target and each target can only be assigned once. this leads to a set p of constraints for the assignment matrices: this formulation forces the visual features to be diversified, avoiding the collapsing issue at the cost of fixing the target representations. predefining these targets is an issue if their number k is small, which is why we are interested in the case where k is at least as large as the number n of images. choosing the target representations. until now, we have not discussed the set of target representations stored in c. a simple choice for the targets would be to take k elements of the canonical basis of r d if d is larger than n, this formulation would be similar to the framework of dosovitskiy et al. (2014), and is impractical for large n. on the other hand, if d is smaller than n, this formulation is equivalent to the discriminative clustering approach of bach harchaoui (2007). choosing such targets makes very strong assumptions on the nature of the underlying problem. indeed, it assumes that each image belongs to a unique class and that all classes are orthogonal. while this assumption might be true for some classification datasets, it does not generalize to huge image collections nor capture subtle similarities between images belonging to different classes. since our features are unit normalized, another natural choice is to uniformly sample target vectors on the unit sphere. note that the dimension d will then directly influence the level of correlation between representations, i.e., the correlation is inversely proportional to the square root of d. using this noise as targets (nat), eq. 2is now equivalent to: this problem can be interpreted as mapping deep features to a uniform distribution over a manifold, namely the ddimension sphere. using k predefined representations is a discrete approximation of this manifold that justifies the restriction of the mapping matrices to the set p of 1to1 assignment matrices. in some sense, we are optimizing a crude approximation of the earth movers distance between the distribution of deep features and a given target distribution (rubner et al., 1998). relation to clustering approaches. using the same notations as in eq. 5, several clustering approaches share similarities with our method. in the linear case, spherical kmeans minimizes the same loss function w.r.t. p and c, i.e., max a key difference is the set q of assignment matrices: this set only guarantees that each data point is assigned to a single target representation. once we jointly learn the features and the assignment, this set does not prevent the collapsing of the data points to a single target representation. another similar clustering approach is diffrac (bach harchaoui, 2007). their loss is equivalent to ours in the case of unit normalized features. their set r of assignment matrices, however, is different: where c is some fixed parameter. while restricting the assignment matrices to this set prevents the collapsing issue, it introduces global constraints that are not suited for online optimization. this makes their approach hard to scale to large datasets. in this section, we describe how to efficiently optimize the cost function described in eq. in particular, we explore algorithm stochastic optimization of eq. require: t batches of images, for t 1, , t do obtain batch b and representations r compute approximated updates of the assignment matrix that are compatible with online optimization schemes, like stochastic gradient descent (sgd). updating the assignment matrix p directly solving for the optimal assignment requires to evaluate the distances between all the n features and the k representations. in order to efficiently solve this problem, we first reduce the number k of representations to n. this limits the set p to the set of permutation matrices, i.e., restricting the problem defined in eq. (5) to this set, the linear assignment problem in p can be solved exactly with the hungarian algorithm (kuhn, 1955), but at the prohibitive cost of o(n ). instead, we perform stochastic updates of the matrix. given a batch of samples, we optimize the assignment matrix p on its restriction to this batch. given a subset b of b distinct images, we only update the b b square sub matrix p b obtained by restricting p to these b images and their corresponding targets. in other words, each image can only be reassigned to a target that was previously assigned to another image in the batch. this procedure has a complexity of o(b ) per batch, leading to an overall complexity of o(nb ), which is linear in the number of data points. we perform this update before updating the parameters of our features, in an online manner. note that this simple procedure would not have been possible if k n; we would have had to also consider the k n unassigned representations. stochastic gradient descent. apart from the update of the assignment matrix p , we use the same optimization scheme as standard supervised approaches, i.e., sgd with batch normalization (ioffe szegedy, 2015). (2017), batch normalization plays a crucial role when optimizing the l loss, as it avoids exploding gradients. for each batch b of images, we first perform a forward pass to compute the distance between the images and the corresponding subset of target representations r. the hungarian algorithm is then used on these distances to obtain the optimal reassignments within the batch. once softmax square loss imagenet table comparison between the softmax and the square loss for supervised object classification on imagenet. the architecture is an alexnet. the features are unit normalized for the square loss (tygert et al., 2017). we report the accuracy on the validation set. the assignments are updated, we use the chain rule in order to compute the gradients of all our parameters. our optimization algorithm is summarized in algorithm our experiments solely focus on learning visual features with convnets. all the details required to train these architectures with our approach are described below. most of them are standard tricks used in the usual supervised setting. to ensure a fair empirical comparison with previous work, we follow wang gupta (2015) and use an alexnet architecture. we train it end to end using our unsupervised loss function. we subsequently test the quality of the learned visual feature by retraining a classifier on top. during transfer learning, we consider the output of the last convolutional layer as our features as in razavian et al. we use the same multilayer perceptron (mlp) as in krizhevsky et al. we observe in practice that preprocessing the images greatly helps the quality of our learned features. (2007), we use image gradients instead of the images to avoid trivial solutions like clustering according to colors. using this preprocessing is not surprising since most handmade features like sift or hog are based on image gradients (lowe, 1999;dalal triggs, 2005). in addition to this preprocessing, we also perform all the standard image transformations that are commonly applied in the supervised setting (krizhevsky et al., 2012), such as random cropping and flipping of images. we project the output of the network on the sphere as in tygert et al. the network is trained with sgd with a batch size of during the first t batches, we use a constant step size. after t batches, we use a linear decay of the step size, i.e., l t l0 1[tt0] unless mentioned otherwise, we permute the assignments within batches every epochs. for the transfer learning experiments, we follow the guideline described in donahue et al. we perform several experiments to validate different design choices in nat. we then evaluate the quality of our features by comparing them to stateoftheart unsupervised approaches on several auxiliary supervised tasks, namely object classification on imagenet and object classification and detection of pascal voc 2007(everingham et al., transfering the features. in order to measure the quality of our features, we measure their performance on transfer learning. we freeze the parameters of all the convolutional layers and overwrite the parameters of the mlp classifier with random gaussian weights. we precisely follow the training and testing procedure that is specific to each of the datasets following donahue et al. datasets and baselines. we use the training set of imagenet to learn our convolutional network (deng et al., 2009). this dataset is composed of 1, 281, images that belong to 1, object categories. for the transfer learning experiments, we also consider pascal voc in addition to fully supervised approaches (krizhevsky et al., 2012), we compare our method to several unsupervised approaches, i.e., autoencoder, gan and bigan as reported in donahue et al. we also compare to selfsupervised approaches, i.e., agrawal et al. (2016); wang gupta (2015) and zhang et al. finally we compare to stateoftheart handmade features, i.e., sift with fisher vectors (siftfv) (snchez et al., 2013). they reduce the fisher vectors to a 4, dimensional vector with pca, and apply an 8, unit 3layer mlp on top. in this section, we validate some of our design choices, like the loss function, representations and the influences of some parameters on the quality of our features. all the experiments are run on imagenet. softmax versus square loss. table compares the performance of an alexnet trained with a softmax and a square loss. we report the accuracy on the validation set. the square loss requires the features to be unit normalized to avoid exploding gradients. as previously observed by tygert et al. (2017), the performances are similar, hence validating our choice of loss function. effect of image preprocessing. in supervised classification, image preprocessing is not frequently used, and transformations that remove information are usually avoided. in the unsupervised case, however, we observe that is it is preferable to work with simpler inputs as clean highpass sobel acc1 table performance of supervised models with various image preprocessings applied. we train an alexnet on imagenet, and report classification accuracy. it avoids learning trivial features. in particular, we observe that using grayscale image gradients greatly helps our method, as mentioned in sec. in order to verify that this preprocessing does not destroy crucial information, we propose to evaluate its effect on supervised classification. we also compare with highpass filtering. table shows the impact of this preprocessing methods on the accuracy of an alexnet on the validation set of imagenet. none of these preprocessings degrade the perform significantly, meaning that the information related to gradients are sufficient for object classification. this experiment confirms that such preprocessing does not lead to a significant drop in the upper bound performance for our model. continuous versus discrete representations. we compare our choice for the target vectors to those commonly used for clustering, i.e., elements of the canonical basis of a k dimensional space. such discrete representation make a strong assumption on the underlying structure of the problem, that it can be linearly separated in k different classes. this assumption holds for imagenet giving a fair advantage to this discrete representation. we test this representation with k in , , , which is a range wellsuited for the 1, classes of imagenet. the matrix c contains n/k replications of k elements of the canonical basis. this assumes that the clusters are balanced, which is verified on imagenet. we compare these clusterlike representations to our continuous target vectors on the transfer task on imagenet. using discrete targets achieves an accuracy of 19, which is significantly worse that our best performance, i.e., a possible explanation is that binary vectors induce sharp discontinuous distances between representations. such distances are hard to optimize over and may result in early convergence to poorer local minima. evolution of the features. in this experiment, we are interested in understanding how the quality of our features evolves with the optimization of our cost function. during the unsupervised training, we freeze the network every epochs and learn a mlp classifier on top. we report the accuracy on the validation set of imagenet. on the left, we measure the accuracy on imagenet after training the features with different permutation rates there is a clear tradeoff with an optimum at permutations performed every epochs. on the right, we measure the accuracy on imagenet after training the features with our unsupervised approach as a function of the number of epochs. the performance improves with longer unsupervised training. of the unsupervised training. this suggests that optimizing our objective function correlates with learning transferable features, i.e., our features do not destroy useful classlevel information. on the other hand, the test accuracy seems to saturate after a hundred epochs. this suggests that the mlp is overfitting rapidly on pretrained features. effect of permutations. assigning images to their target representations is a crucial feature of our approach. in this experiment, we are interested in understanding how frequently we should update this assignment. indeed, updating the assignment, even partially, is relatively costly and may not be required to achieve good performance. figure shows the transfer accuracies on imagenet as a function of the frequency of these updates. the model is quite robust to choice of frequency, with a test accuracy always above interestingly, the accuracy actually degrades slightly with high frequency. a possible explanation is that the network overfits rapidly to its own output, leading to relatively worse features. in practice, we observe that updating the assignment matrix every epochs offers a good tradeoff between performance and accuracy. visualizing the filters. figure shows a comparison between the first convolutional layer of an alexnet trained with and without supervision. both take grayscale gradient images as input. the visualization are obtained by composing the sobel filtering with the filters of the first layer of the alexnet. unsupervised filters are slightly less sharp than their supervised counterpart, but still maintain edge and orientation information. nearest neighbor queries. our loss optimizes a distance between features and fixed vectors. this means that looking at the distance between features should provide some information about the type of structure that our model cap tures. given a query image x, we compute its feature f (x) and search for its nearest neighbors according to the distance. figure shows images and their nearest neighbors. the features capture relatively complex structures in images. objects with distinctive structures, like trunks or fruits, are well captured by our approach. however, this information is not always related to true labels. for example, the image of bird over the sea is matched to images capturing information about the sea or the sky rather than the bird. we report results on the transfer task both on imagenet and pascal voc in both cases, the model is trained on imagenet. imagenet classification. in this experiment, we evaluate the quality of our features for the object classification task of imagenet. note that in this setup, we build the unsupervised features on images that correspond to predefined image categories. even though we do not have access to category labels, the data itself is biased towards these classes. in order to evaluate the features, we freeze the layers up to the last convolutional layer and train the classifier with supervision. this experimental setting follows noroozi favaro (2016). we compare our model with several selfsupervised approaches (wang gupta, 2015;doersch et al., 2015;zhang et al., 2016) and an unsupervised approach, i.e., donahue et al. note that selfsupervised approaches use losses specifically designed for visual features. like bigans (donahue et al., 2016), nat does not make any assumption about the domain but of the structure of its features. table compares nat with these approaches. among unsupervised approaches, nat compares favorably to bigan (donahue et al., 2016). interestingly, the performance of nat are slightly better than selfsupervised acc1 random (noroozi favaro, 2016) siftfv (snchez et al., 2013) wang gupta (2015) doersch et al. (2016) noroozi favaro (2016) bigan (donahue et al., 2016) nat table comparison of the proposed approach to stateoftheart unsupervised feature learning on imagenet. a full multilayer perceptron is retrained on top of the features. we compare to several selfsupervised approaches and an unsupervised approach, i.e., bigan (donahue et al., 2016). noroozi favaro (2016) uses a significantly larger amount of features than the original alexnet. we report classification accuracy. methods, even though we do not explicitly use domainspecific clues in images or videos to guide the learning. while all the models provide performance in the range, it is not clear if they all learn the same features. finally, all the unsupervised deep features are outperformed by handmade features, in particular fisher vectors with sift descriptors. this baseline uses a slightly bigger mlp for the classifier and its performance can be improved by by bagging of these models. this difference of in accuracy shows that unsupervised deep features are still quite far from the stateofthearts among all unsupervised features. transferring to pascal voc we carry out a second transfer experiment on the pascal voc dataset, on the classification and detection tasks. the model is trained on imagenet. depending on the task, we finetune all layers in the network, or solely the classifier, following donahue et al. in all experiments, the parameters of the convolutional layers are initialized with the ones obtained with our unsupervised approach. the parameters of the classification layers are initialized with gaussian weights. we get rid of batch normalization layers and use a datadependent rescaling of the parameters (krhenbhl et al., 2015). table shows the comparison between our model and other unsupervised approaches. the results for other methods are taken from donahue et al. as with the imagenet classification task, our performance is on par with selfsupervised approaches, for both detection and classification. among purely unsupervised approaches, we outperform standard approaches like autoencoders or gans by a large margin. comparison of the proposed approach to stateoftheart unsupervised feature learning on voc classification and detection. we either fix the features after conv5 or we finetune the whole model. we compare to several selfsupervised and an unsupervised approaches. the gan and autoencoder baselines are from donahue et al. we report mean average prevision as customary on pascal voc. performs slightly better than the best performing bigan model (donahue et al., 2016). these experiments confirm our findings from the imagenet experiments. despite its simplicity, nat learns feature that are as good as those obtained with more sophisticated and dataspecific models. this paper presents a simple unsupervised framework to learn discriminative features. by aligning the output of a neural network to lowdimensional noise, we obtain features on par with stateoftheart unsupervised learning approaches. our approach explicitly aims at learning discriminative features, while most unsupervised approaches target surrogate problems, like image denoising or image generation. as opposed to selfsupervised approaches, we make very few assumptions about the input space. this makes our appproach very simple and fast to train. interestingly, it also shares some similarities with traditional clustering approaches as well as retrieval methods. while we show the potential of our approach on visual data, it will be interesting to try other domains. finally, this work only considers simple noise distributions and alignment methods. a possible direction of research is to explore target distributions and alignments that are more informative. this also would strengthen the relation between nat and methods based on distribution matching like the earth mover distance.", "summary": "convolutional neural networks are extremely good feature extractors in the sense that features extracted for one task (say image classification) can be easily transferred to another task (say image segmentation). existing unsupervised approaches do not aim to learn discriminative features and supervised approaches for discriminative features do not scale well. the paper presents an approach to learn features in an unsupervised setting by using a set of target representations called as noise as target (nat) which acts as a kind of proxy supervising signal. approach unsupervised setting given a collection of image x (x1, x2, , xn), we want to learn a parameterized mapping f such that f(xi) gives the features of image xi. we would jointly learn the target vectors yi (more on it later). loss function squared l2 norm is used as the distance measure while making sure that final activations are unit normalized. fixed target representation in the setting of the problem where we are learning both the features and the target representation, a trivial solution would be the one where all the input images map to the same target and are assigned the same representation. no discriminative features are learned in this case. to avoid such situations, a set of k predefined target representations are chosen and each image is mapped to one of these k representations (based on the features). there is an assumption that k n so that each image is assigned a different target. one simple choice of target representation is the standard onehot vector which implies that all the class (and by extension, the associated images) are orthogonal and equidistant from each other. but this is not a reasonable approximation as not all the image pairs are equally similar or dissimilar. instead, the target vectors are uniformly sampled from a ddimensional unit sphere, where d is the dimensionality of the feature representation. that is, the idea is to map the features to the manifold of the ddimensional l2 sphere by using the k predefined representations as for the discrete approximation of the manifold. since each data point (image) is mapped to a new point on the manifold, the algorithm is suited for online training as well. optimisation for the training, the number of target k is reduced to the number of images n and an assignment matrix p is learned which ensures that the mapping between the image to target is 1to1. the resulting optimisation equation can be solved using the hungarian algorithm but at a highcost o(n3). an optimisation is to take a batch of b images and update the square matrix pb for dimension bxb (made of the images and their corresponding targets). this reduces the overall complexity of o(nb2). other optimisation techniques, that are common to supervised learning, like batch norm used in this setting as well. implementation detail used alexnet with nats to train the unsupervised model. an mlp is trained on these features to learn the classifier. standard preprocessing techniques like random cropping/flipping are used. experimental details dataset imagenet for training the alexnet architecture with the proposed approach. pascal voc for transfer learning experiments. baselines unsupervised approaches like autoencoder, gan, bigan selfsupervised sota models using handmade features sift with fisher vector. observation using squared loss instead of softmax does not deteriorate the performance too much. the authors compare the effect of using discrete vs continuous target representations for transfer learning. for the discrete representation, elements of the canonical basis of a kdimensional space (k1000, 10000, 100000) are used. experiments demonstrate that ddimensional continuous vectors perform much better than the discrete vectors. while training the unsupervised network, its features were extracted after every iterations to evaluate the performance on transfer learning task. the test accuracy increases up to around iterations then saturate. comparing the visualization of the first convolutional layer filters (for alexnet with and without supervision) shows that while unsupervised filters are less sharp, they maintain the edge and orientation information. the proposed unsupervised method outperforms all the unsupervised baselines and is competitive with respect to the supervised baseline. but it is still far behind the model using handcrafted features. for transfer learning, on pascal voc, the proposed approach beats the supervised baseline and works at par with the supervised approach. notes the paper proposed a simple unsupervised framework for learning discriminative features without having to rely on proxy tasks like image generation and without having to make an assumption about the input domain. the key aspect of the proposed approach is that each image is assigned to a unique point in the ddimensional manifold which means images could be very close to each other on the manifold while being quite distinct in reality. it is interesting to see that such a simple strategy is able to give such good results."}, {"document": "modeling the distribution of natural images is a landmark problem in unsupervised learning. this task requires an image model that is at once expressive, tractable and scalable. we present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. architectural novelties include fast twodimensional recurrent layers and an effective use of residual connections in deep recurrent networks. we achieve loglikelihood scores on natural images that are considerably better than the previous state of the art. our main results also provide benchmarks on the diverse imagenet dataset. samples generated from the model appear crisp, varied and globally coherent. generative image modeling is a central problem in unsupervised learning. probabilistic density models can be used for a wide variety of tasks that range from image compression and forms of reconstruction such as image inpainting (e.g., see figure 1) and deblurring, to generation of new images. when the model is conditioned on external information, possible applications also include creating images based on text descriptions or simulating future frames in a planning task. one of the great advantages in generative modeling is that there are practically endless amounts of image data available to learn from. however, because images are high dimensional and highly structured, estimating the distribution of natural images is extremely challenging.. one of the most important obstacles in generative mod. proceedings of the rd international conference on machine learning, new york, ny, usa, copyright by the author(s).. occluded completions original. image completions sampled from a pixelrnn.. eling is building complex and expressive models that are also tractable and scalable. this tradeoff has resulted in a large variety of generative models, each having their advantages. most work focuses on stochastic latent variable models such as vae\u2019s (rezende et al., 2014; kingma welling, 2013) that aim to extract meaningful representations, but often come with an intractable inference step that can hinder their performance.. one effective approach to tractably model a joint distribution of the pixels in the image is to cast it as a product of conditional distributions; this approach has been adopted in autoregressive models such as nade (larochelle murray, 2011) and fully visible neural networks (neal, 1992; bengio bengio, 2000). the factorization turns the joint modeling problem into a sequence problem, where one learns to predict the next pixel given all the previously generated pixels. but to model the highly nonlinear and longrange correlations between pixels and the complex conditional distributions that result, a highly expressive sequence model is necessary.. recurrent neural networks (rnn) are powerful models that offer a compact, shared parametrization of a series of conditional distributions. rnns have been shown to excel at hard sequence problems ranging from handwriting generation (graves, 2013), to character prediction (sutskever et al., 2011) and to machine translation (kalchbrenner blunsom, 2013). a twodimensional rnn has produced very promising results in modeling grayscale images and textures (theis bethge, 2015).. in this paper we advance twodimensional rnns and ap. ply them to largescale modeling of natural images. the resulting pixelrnns are composed of up to twelve, fast twodimensional long shortterm memory (lstm) layers. these layers use lstm units in their state (hochreiter schmidhuber, 1997; graves schmidhuber, 2009) and adopt a convolution to compute at once all the states along one of the spatial dimensions of the data. we design two types of these layers. the first type is the row lstm layer where the convolution is applied along each row; a similar technique is described in (stollenga et al., 2015). the second type is the diagonal bilstm layer where the convolution is applied in a novel fashion along the diagonals of the image. the networks also incorporate residual connections (he et al., 2015) around lstm layers; we observe that this helps with training of the pixelrnn for up to twelve layers of depth.. we also consider a second, simplified architecture which shares the same core components as the pixelrnn. we observe that convolutional neural networks (cnn) can also be used as sequence model with a fixed dependency range, by using masked convolutions. the pixelcnn architecture is a fully convolutional network of fifteen layers that preserves the spatial resolution of its input throughout the layers and outputs a conditional distribution at each location.. both pixelrnn and pixelcnn capture the full generality of pixel interdependencies without introducing independence assumptions as in e.g., latent variable models. the dependencies are also maintained between the rgb color values within each individual pixel. furthermore, in contrast to previous approaches that model the pixels as continuous values (e.g., theis bethge (2015); gregor et al. (2014)), we model the pixels as discrete values using a multinomial distribution implemented with a simple softmax layer. we observe that this approach gives both representational and training advantages for our models.. the contributions of the paper are as follows. in section we design two types of pixelrnns corresponding to the two types of lstm layers; we describe the purely convolutional pixelcnn that is our fastest architecture; and we design a multiscale version of the pixelrnn. in section we show the relative benefits of using the discrete softmax distribution in our models and of adopting residual connections for the lstm layers. next we test the models on mnist and on cifar10 and show that they obtain loglikelihood scores that are considerably better than previous results. we also provide results for the largescale imagenet dataset resized to both and pixels; to our knowledge likelihood values from generative models have not previously been reported on this dataset. finally, we give a qualitative evaluation of the samples generated from the pixelrnns. our aim is to estimate a distribution over natural images that can be used to tractably compute the likelihood of images and to generate new ones. the network scans the image one row at a time and one pixel at a time within each row. for each pixel it predicts the conditional distribution over the possible pixel values given the scanned context. figure illustrates this process. the joint distribution over the image pixels is factorized into a product of conditional distributions. the parameters used in the predictions are shared across all pixel positions in the image.. to capture the generation process, theis bethge (2015) propose to use a twodimensional lstm network (graves schmidhuber, 2009) that starts at the top left pixel and proceeds towards the bottom right pixel. the advantage of the lstm network is that it effectively handles longrange dependencies that are central to object and scene understanding. the twodimensional structure ensures that the signals are well propagated both in the lefttoright and toptobottom directions.. in this section we first focus on the form of the distribution, whereas the next section will be devoted to describing the architectural innovations inside pixelrnn. the goal is to assign a probability p(x) to each image x formed of nn pixels. we can write the image x as a onedimensional sequence x1, ..., xn2 where pixels are taken from the image row by row. to estimate the joint distribution p(x) we write it as the product of the conditional distributions over the pixels:. the value p(xix1, ..., xi1) is the probability of the ith pixel xi given all the previous pixels x1, ..., xi1. the generation proceeds row by row and pixel by pixel. figure (left) illustrates the conditioning scheme.. each pixel xi is in turn jointly determined by three values, one for each of the color channels red, green and blue (rgb). we rewrite the distribution p(xixi) as the following product:. p(xi,rxi)p(xi,gxi, xi,r)p(xi,b xi, xi,r, xi,g) (2). each of the colors is thus conditioned on the other channels as well as on all the previously generated pixels.. note that during training and evaluation the distributions over the pixel values are computed in parallel, while the generation of an image is sequential. previous approaches use a continuous distribution for the values of the pixels in the image (e.g. by contrast we model p(x) as a discrete distribution, with every conditional distribution in equation being a multinomial that is modeled with a softmax layer. each channel variable xi, simply takes one of distinct values. the discrete distribution is representationally simple and has the advantage of being arbitrarily multimodal without prior on the shape (see fig. experimentally we also find the discrete distribution to be easy to learn and to produce better performance compared to a continuous distribution (section 5). in this section we describe the architectural components that compose the pixelrnn. in sections and 3.2, we describe the two types of lstm layers that use convolutions to compute at once the states along one of the spatial dimensions. in section we describe how to incorporate residual connections to improve the training of a pixelrnn with many lstm layers. in section we describe the softmax layer that computes the discrete joint distribution of the colors and the masking technique that ensures the proper conditioning scheme. in section we describe the pixelcnn architecture. finally in section we describe the multiscale architecture. the row lstm is a unidirectional layer that processes the image row by row from top to bottom computing features for a whole row at once; the computation is performed with a onedimensional convolution. for a pixel xi the layer captures a roughly triangular context above the pixel as shown in figure (center). dimensional convolution has size k where k 3; the larger the value of k the broader the context that is captured. the weight sharing in the convolution ensures translation invariance of the computed features along each row.. the computation proceeds as follows. an lstm layer has an inputtostate component and a recurrent statetostate component that together determine the four gates inside the lstm core. to enhance parallelization in the row lstm the inputtostate component is first computed for the entire twodimensional input map; for this a k convolution is used to follow the rowwise orientation of the lstm itself. the convolution is masked to include only the valid context (see section 3.4) and produces a tensor of size 4h n n, representing the four gate vectors for each position in the input map, where h is the number of output feature maps.. to compute one step of the statetostate component of the lstm layer, one is given the previous hidden and cell states hi1 and ci1, each of size h n the new hidden and cell states hi, ci are obtained as follows:. ci fi ci1 ii gi hi oi tanh(ci). where xi of size h n is row i of the input map, and represents the convolution operation and the elementwise multiplication. the weights kss and kis are the kernel weights for the statetostate and the inputtostate components, where the latter is precomputed as described above. in the case of the output, forget and input gates oi, fi and ii, the activation is the logistic sigmoid function, whereas for the content gate gi, is the tanh function. each step computes at once the new state for an entire row of the input map. because the row lstm has a triangular receptive field (figure 4), it is unable to capture the entire available context. the diagonal bilstm is designed to both parallelize the computation and to capture the entire available context for any image size. each of the two directions of the layer scans the image in a diagonal fashion starting from a corner at the top and reaching the opposite corner at the bottom. each step in the computation computes at once the lstm state along a diagonal in the image. figure (right) illustrates the computation and the resulting receptive field.. the diagonal computation proceeds as follows. we first skew the input map into a space that makes it easy to apply convolutions along diagonals. the skewing operation offsets each row of the input map by one position with respect to the previous row, as illustrated in figure 3; this results in a map of size n (2n 1). at this point we can compute the inputtostate and statetostate components of the diagonal bilstm. for each of the two directions, the inputtostate component is simply a convolutionkis that contributes to the four gates in the lstm core; the operation generates a 4h n n tensor. the statetostate recurrent component is then computed with a columnwise convolution kss that has a kernel of size the step takes the previous hidden and cell states, combines the contribution of the inputtostate component and produces the next hidden and cell states, as defined in equation the output feature map is then skewed back into an n n map by removing the offset positions. this computation is repeated for each of the two directions. given the two output maps, to prevent the layer from seeing future pixels, the right output map is then shifted down by one row and added to the left output map.. besides reaching the full dependency field, the diagonal bilstm has the additional advantage that it uses a convolutional kernel of size that processes a minimal amount of information at each step yielding a highly nonlinear computation. kernel sizes larger than are not particularly useful as they do not broaden the already global receptive field of the diagonal bilstm. we train pixelrnns of up to twelve layers of depth. as a means to both increase convergence speed and propagate signals more directly through the network, we deploy residual connections (he et al., 2015) from one lstm layer to the next. figure shows a diagram of the residual blocks. the input map to the pixelrnn lstm layer has 2h features. the inputtostate component reduces the number of features by producing h features per gate. after applying the recurrent layer, the output map is upsampled back to 2h features per position via a convolution and the input map is added to the output map. this method is related to previous approaches that use gating along the depth of the recurrent network (kalchbrenner et al., 2015; zhang et al., 2016), but has the advantage of not requiring additional gates. apart from residual connections, one can also use learnable skip connections from each layer to the output. in the experiments we evaluate the relative effectiveness of residual and layertooutput skip connections. the h features for each input position at every layer in the network are split into three parts, each corresponding to one of the rgb channels. when predicting the r channel for the current pixel xi, only the generated pixels left and above of xi can be used as context. when predicting the g channel, the value of the r channel can also be used as context in addition to the previously generated pixels. likewise, for the b channel, the values of both the r and g channels can be used. to restrict connections in the network to these dependencies, we apply a mask to the inputtostate convolutions and to other purely convolutional layers in a pixelrnn.. we use two types of masks that we indicate with mask a and mask b, as shown in figure (right). mask a is applied only to the first convolutional layer in a pixelrnn and restricts the connections to those neighboring pixels and to those colors in the current pixels that have already been predicted. on the other hand, mask b is applied to all the subsequent inputtostate convolutional transitions and relaxes the restrictions of mask a by also allowing the connection from a color to itself. the masks can be easily implemented by zeroing out the corresponding weights in the inputtostate convolutions after each update. pixelcnn row lstm diagonal bilstm. conv mask a multiple residual blocks: (see fig 5). lar masks have also been used in variational autoencoders (gregor et al., 2014; germain et al., 2015). the row and diagonal lstm layers have a potentially unbounded dependency range within their receptive field. this comes with a computational cost as each state needs to be computed sequentially. one simple workaround is to make the receptive field large, but not unbounded. we can use standard convolutional layers to capture a bounded receptive field and compute features for all pixel positions at once. the pixelcnn uses multiple convolutional layers that preserve the spatial resolution; pooling layers are not used. masks are adopted in the convolutions to avoid seeing the future context; masks have previously also been used in nonconvolutional models such as made (germain et al., 2015). note that the advantage of parallelization of the pixelcnn over the pixelrnn is only available during training or during evaluating of test images. the image generation process is sequential for both kinds of networks, as each sampled pixel needs to be given as input back into the network. the multiscale pixelrnn is composed of an unconditional pixelrnn and one or more conditional pixelrnns. the unconditional network first generates in the standard way a smaller ss image that is subsampled from the original image. the conditional network then takes the s s image as an additional input and generates a larger n n image, as shown in figure (middle).. the conditional network is similar to a standard pixelrnn, but each of its layers is biased with an upsampled version of the small s s image. the upsampling and biasing processes are defined as follows. in the upsampling process, one uses a convolutional network with deconvolutional layers to construct an enlarged feature map of size c n n, where c is the number of features in the output map of the upsampling network. then, in the biasing process, for each. layer in the conditional pixelrnn, one simply maps the c n n conditioning map into a 4h n n map that is added to the inputtostate map of the corresponding layer; this is performed using a unmasked convolution. the larger n n image is then generated as usual. in this section we give the specifications of the pixelrnns used in the experiments. we have four types of networks: the pixelrnn based on row lstm, the one based on diagonal bilstm, the fully convolutional one and the multiscale one.. table specifies each layer in the singlescale networks. the first layer is a convolution that uses the mask of type a. the two types of lstm networks then use a variable number of recurrent layers. the inputtostate convolution in this layer uses a mask of type b, whereas the statetostate convolution is not masked. the pixelcnn uses convolutions of size with a mask of type b. the top feature map is then passed through a couple of layers consisting of a rectified linear unit (relu) and a convolution. for the cifar10 and imagenet experiments, these layers have feature maps; for the mnist experiment, the layers have feature maps. residual and layertooutput connections are used across the layers of all three networks.. the networks used in the experiments have the following hyperparameters. for mnist we use a diagonal bilstm with layers and a value of h (section and figure right). for cifar10 the row and diagonal bilstms have layers and a number of h units. the pixelcnn has layers and h for imagenet we adopt a layer row lstm with h units and for imagenet we use a layer row lstm with h units; the latter model does not use residual connections. in this section we describe our experiments and results. we begin by describing the way we evaluate and compare our results. in section we give details about the training. then we give results on the relative effectiveness of architectural components and our best results on the mnist, cifar10 and imagenet datasets. all our models are trained and evaluated on the loglikelihood loss function coming from a discrete distribution. although natural image data is usually modeled with continuous distributions using density functions, we can compare our results with previous art in the following way.. in the literature it is currently best practice to add realvalued noise to the pixel values to dequantize the data when using density functions (uria et al., 2013). when uniform noise is added (with values in the interval [0, 1]), then the loglikelihoods of continuous and discrete models are directly comparable (theis et al., 2015). in our case, we can use the values from the discrete distribution as a piecewiseuniform continuous function that has a constant value for every interval [i, i 1], i 1, 2, this corresponding distribution will have the same loglikelihood (on data with added noise) as the original discrete distribution (on discrete data).. for mnist we report the negative loglikelihood in nats as it is common practice in literature. for cifar10 and imagenet we report negative loglikelihoods in bits per dimension. the total discrete loglikelihood is normalized by the dimensionality of the images (e.g., for cifar10). these numbers are interpretable as the number of bits that a compression scheme based on this model would need to compress every rgb color value (van den oord schrauwen, 2014b; theis et al., 2015); in practice there is also a small overhead due to arithmetic coding. our models are trained on gpus using the torch toolbox. from the different parameter update rules tried, rmsprop gives best convergence performance and is used for all experiments. the learning rate schedules were manually set for every dataset to the highest values that allowed fast convergence. the batch sizes also vary for different datasets. for smaller datasets such as mnist and cifar10 we use smaller batch sizes of images as this seems to regularize the models. for imagenet we use as large a batch size as allowed by the gpu memory; this corresponds to images/batch for imagenet, and images/batch for imagenet. apart from scaling and centering the images at the input of the network, we don\u2019t use any other preprocessing or augmentation. for the multinomial loss function we use the raw pixel color values as categories. for all the pixelrnn models, we learn the initial recurrent state of the network. apart from being intuitive and easy to implement, we find that using a softmax on discrete pixel values instead of a mixture density approach on continuous pixel values gives better results. for the row lstm model with a softmax output distribution we obtain bits/dim on the cifar10 validation set. for the same model with a mixture of conditional gaussian scale mixtures (mcgsm) (theis bethge, 2015) we obtain bits/dim.. in figure we show a few softmax activations from the model. although we don\u2019t embed prior information about the meaning or relations of the color categories, e.g. that pixel values and are neighbors, the distributions predicted by the model are meaningful and can be multimodal, skewed, peaked or long tailed. also note that values and often get a much higher probability as they are more frequent. another advantage of the discrete distribution is that we do not worry about parts of the distribution mass lying outside the interval [0, 255], which is something that typically happens with continuous distributions. another core component of the networks is residual connections. in table we show the results of having residual connections, having standard skip connections or having both, in the 12layer cifar10 row lstm model. we see that using residual connections is as effective as using skip connections; using both is also effective and preserves the advantage.. when using both the residual and skip connections, we see in table that performance of the row lstm improves with increased depth. this holds for up to the lstm layers that we tried. although the goal of our work was to model natural images on a large scale, we also tried our model on the binary version (salakhutdinov murray, 2008) of mnist (lecun et al., 1998) as it is a good sanity check and there is a lot of previous art on this dataset to compare with. in table we report the performance of the diagonal bilstm model and that of previous published results. to our knowledge this is the best reported result on mnist so far. next we test our models on the cifar10 dataset (krizhevsky, 2009). table lists the results of our models and that of previously published approaches. all our results were obtained without data augmentation. for the proposed networks, the diagonal bilstm has the best performance, followed by the row lstm and the pixelcnn. this coincides with the size of the respective receptive fields: the diagonal bilstm has a global view, the row lstm has a partially occluded view and the pixelcnn sees the fewest pixels in the context. this suggests that effectively capturing a large receptive field is important. figure (left) shows cifar10 samples generated. from the diagonal bilstm. although to our knowledge the are no published results on the ilsvrc imagenet dataset (russakovsky et al., 2015) that we can compare our models with, we give our ima. image size nll validation (train). genet loglikelihood performance in table (without data augmentation). on imagenet the current pixelrnns do not appear to overfit, as we saw that their validation performance improved with size and depth. the main constraint on model size are currently computation time and gpu memory.. note that the imagenet models are in general less compressible than the cifar10 images. imagenet has greater variety of images, and the cifar10 images were most. likely resized with a different algorithm than the one we used for imagenet images. the imagenet images are less blurry, which means neighboring pixels are less correlated to each other and thus less predictable. because the downsampling method can influence the compression performance, we have made the used downsampled images available1.. figure (right) shows samples drawn from our model trained on imagenet. figure shows samples from the same model with and without multiscale. 1http://imagenet.org/small/download.php. finally, we also show image completions sampled from the model in figure in this paper we significantly improve and build upon deep recurrent neural networks as generative models for natural images. we have described novel twodimensional lstm layers: the row lstm and the diagonal bilstm, that scale more easily to larger datasets. the models were trained to model the raw rgb pixel values. we treated the pixel values as discrete random variables by using a softmax layer in the conditional distributions. we employed masked convolutions to allow pixelrnns to model full dependencies between the color channels. we proposed and evaluated architectural improvements in these models resulting in pixelrnns with up to lstm layers.. we have shown that the pixelrnns significantly improve the state of the art on the mnist and cifar10 datasets. we also provide new benchmarks for generative image modeling on the imagenet dataset. based on the samples and completions drawn from the models we can conclude that the pixelrnns are able to model both spatially local and longrange correlations and are able to produce images that are sharp and coherent. given that these models improve as we make them larger and that there is practically unlimited data available to train on, more computation and larger models are likely to further improve the results.", "summary": "this paper explores the use of convolutional (pixelcnn) and recurrent units (pixelrnn) for modeling the distribution of images, in the framework of autoregression distribution estimation. in this framework, the input distribution p(x) is factorized into a product of conditionals \\pi p(xi xi1). previous work has shown that very good models can be obtained by using a neural network parametrization of the conditionals (e.g. see our work on nade \\citejournals/jmlr/larochellem11). moreover, unlike other approaches based on latent stochastic units that are directed or undirected, the autoregressive approach is able to compute logprobabilities tractably. so in this paper, by considering the specific case of x being an image, they exploit the topology of pixels and investigate appropriate architectures for this.. among the papers contributions are:. they propose diagonal bilstm units for the pixelrnn, which are efficient (thanks to the use of convolutions) while making it possible to, in effect, condition a pixels distribution on all the pixels above it (see figure for an illustration).. they demonstrate that the use of residual connections (a form of skip connections, from hidden layer i1 to layer i1) are very effective at learning very deep distribution estimators (they go as deep as layers).. they show that it is possible to successfully model the distribution over the pixel intensities (effectively an integer between and 255) using a softmax of units.. they propose a multiscale extension of their model, that they apply to larger 64x64 images.. the experiments show that the pixelrnn model based on diagonal bilstm units achieves stateoftheart performance on the binarized mnist benchmark, in terms of loglikelihood. they also report excellent loglikelihood on the cifar10 dataset, comparing to previous work based on realvalued density models. finally, they show that their model is able to generate high quality image samples."}]