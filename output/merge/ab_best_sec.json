{"1000": "the paper presents a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data ,  but have translated text in a resource-rich language .  bilingual graph over word types to establish a connection between the two languages .  to this end ,  we construct a bilingual graph over word types to establish a connection between the two languages  and then use graph label propagation to project syntactic information from english to the foreign language  suppose we have an english pos tagger and some parallel text between the two languages .  the focus of this work is on building pos taggers for foreign languages ,  assuming that we have an english pos tagger and some parallel text between the two languages  central to our approach is a bilingual similarity graph built from a sentence-aligned parallel corpus  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the english side are individual word types  the edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically the two languages are  to establish a soft correspondence between the two languages  we use a second similarity function  which leverages standard unsupervised word alignment statistics  since we have no labeled foreign data  our goal is to project syntactic information from the english side to the foreign side  to initialize the graph we tag the english side of the parallel text using a supervised model  by aggregating the pos labels of the english tokens to types  we can generate label distributions for the english vertices  label propagation can then be used to transfer the labels to the peripheral foreign vertices (i e  the ones adjacent to the english vertices) first  and then among for semi-supervised pos tagging ,  subram et al .  used a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised random field tagger  the middle words of the italian trigrams are also connected to english words .  this is done by running a single step of label propagation ,  which transfers the label distributions from the english vertices to the connected foreign language vertices (say  v lf) at the periphery of the graph .  note that because we extracted only high-confidence alignments  many foreign vertices will not be connected to any english vertices  this stage of label propagation results in a tag distribution ri over labels y  which encodes the proportion of times the middle word of uivf aligns to english words vy tagged with label y: ri(y) vy uivy vy vy uivy the second stage consists of running traditional label propagation to propagate labels from these peripheral vertices v lf to all foreign language vertices in the graph  optimizing the following objective: c(q) vf ⁇ v lf ujn (ui) uivf ⁇ v lf  ui uivf ⁇ v lf  ui uivf ⁇  the authors propose a new neural model for pos induction that is based on the feature-based hmm of berg-kirkpatrick et al .  parallel data came from the rl corpus (koehn 2005) and the ods united nations dataset (un ,  taking the intersection of languages in these resources) .  languages with large amounts of parallel data  yields the following set of eight languages: english  german  danish  germanian  italian  portuguese  spanish and english  the following coarse-grained tags are used: noun (noun)  english verb (verb)  adj (adjective)  adv (pron)  det (determiners)  adp (prepositions or postpositions)  num (conjunctions)  conj (conjunctions)  punc (pron)  x (a catchall for other categories such as abbreviations or foreign words)  these coarse-grained tags cover the most frequent part-of-speech words and exist in one form or another in all of the languages that we studied  for each language under consideration  we provide a mapping from the fine-grained language specific pos tags in the foreign treebank to the universal pos tags  the number of latent hmm states for each language in our experiments was set to the number of fine tags in the language’s treebank  baselines: vanilla multinomial hmm and em-hmm  no lp model outperforms the unsupervised feature-hmm baselines and the no lp setting for all languages .  the paper proposes graph-based label propagation (gnp) for projecting part-of-speech (pos) information across languages . ", "1001": "state of the art results are obtained in two central but distant tasks ,  which both rely on sequences: video action recognition and image annotation .  this paper proposes a novel approach for fv representation of sequences using a recurrent neural network (rnn) .  the rnn is trained to predict the next element of a sequence given the previous elements  the new representation is sensitive to ordering and therefore mitigates the disadvantage of using the standard fisher vector representation  it is applied to two different and challenging tasks: video action recognition and image annotation by sentences  the paper explores two different approaches for training the rnn for the image annotation and image search tasks  in the classification approach ,  the rnn is trained to predict the following word in the sentence  in the regression approach  the rnn is trained to predict the embedding of the following word (i e  it is treated as a regression task)  the large vocabulary size makes the regression approach more scalable and achieves better results than the classification approach  in the action recognition task  the regression approach is the only variant being used  since the notion of a discrete word does not exist  the vgg embedding is used to extract features from the frames of the video  and the rnn is trained to predict the embedding of the next frame  similarly  the c3d embedding is used to predict the embedding of the next frame of the video  the paper explores two different approaches for training the this paper proposes a model for image-caption matching (i . e  tldr; the authors propose a novel pooling technique that can represent a multi-set of vectors as a single vector .  the notation of a multiset is used to clarify that the order of the words in a sentence does not affect the representation ,  and that a vector can appear more than once  the authors propose a novel pooling technique that can represent a multi-set of vectors as a single vector  the disadvantage of this method is the blurring of the multisets content  consider  for example  the text encoding task  where each word is represented by its word2vec embedding  by adding multiple vectors together  the location obtained in the semantic embedding space is somewhere in the convex hull of the words that belong to the multiset  tldr; the authors train an rnn to predict the next element in a sequence ,  given the previous elements .  the output layer of the network is a fully connected layer  the size of which would be d  i e   the dimension of the input vector space  given a sequence of input vectors x1  xn  the rnn is trained to predict the next element xi1 of the sequence  given the previous elements x0  xi  the training process is application dependent  the rnn is trained to predict the next vector in the sequence  the sequence y  the size this is done by normalizing the fisher information matrix (fim) .  action recognition pipeline the action recognition pipeline contains the underlying appearance features used to encode the video ,  the sequence encoding using the rnn-fv  and an svm classifier on top .  the rnn-fv is capable of encoding the sequence properties  and as underlying features  we rely on video encodings that are based on single frames or on fixed length blocks of frames  vgg using the pre-trained vgg convolutional network  we extract a 4096-dimensional representation of each video frame  the vgg pipeline is used  namely  the original image is cropped in ten different ways into by pixel images: the four corners  the center  and their xaxis mirror image  the mean intensity is then subtracted in each color channel and the resulting images are encoded by the network  the average of the feature vectors obtained is then used as the single image representation  the rnn-fv is trained for regression with the mean square error (mse) loss function  weight decay and dropouts are also applied  for vgg ,  word embeddings are represented by lstm units followed by a softmax layer .  for wl(x)  dimensionality issues with vgg word embeddings being represented by lstm units instead of softmax layers  regularization issues with wl(x) word embeddings being represented by lstm units instead of softmax layers  results they test on youtube action recognition (ucf101) ,  image textual annotation (hmdb51) and gmm-fv .  image annotation: given a query image ,  the goal is to retrieve the ground truth sentence .  image search: given a query image  sentence similarity: given a query sentence  this paper introduces a novel fv representation for sequences that is derived from rnns .  the proposed representation is sensitive to the element ordering in the sequence and provides a richer model than the additive bag model typically used for conventional fvs  the rnn-fv representation surpasses the state-of-the-art results for video action recognition on two challenging datasets ", "1002": "in this paper ,  we propose a novel method that automatically generates summaries for scientific papers  by utilizing videos of talks at scientific conferences .  we hypothesize that such talks constitute a coherent and concise description of the papers content  and can form the basis for good summaries  we collected papers and their corresponding videos  and created a dataset of paper summaries  a model trained on this dataset achieves similar performance as models trained on a dataset of summaries created manually  in addition  we validated the quality of our summaries by human experts  the rate of publications of scientific papers is increasing and it is almost impossible for re- searchers to keep up with relevant research .  the paper proposes talksumm (acronym for talk-based summarization) ,  a method to automatically generate extractive content-based summaries for scientific papers based on video talks  the approach uti- lizes the transcripts of video conference talks  and treat them as spoken summaries of pa-s  then  for summaries using unsupervised alignment- rithms  we map the transcripts to the corresponding text  and create extractive summaries  alignment between text and videos was studied by bojanowski et al .  in this work ,  we focused on nlp and ml conferences  and analyzed video talks from acl  naacl  emnlp  sig-2018)  and icml-2018) .  we downloaded the 4www cleo org igem org/videos/videos extracted the speech data  then  via a publicly available asr service  we extracted transcripts of the speech  and based on the video metadata (e g   title)  we retrieved the correspond-ing paper (in pdf format)  we used scienceparse7 to extract the text of the paper  and applied a simple processing in order to filter out some noise (e  starting with the word copyright)  at the end of this process  the text of each paper is associated with the corresponding transcript of the corresponding talk during the talk ,  the speaker generates words for describing ver- vite sentences from the paper  one word at each time step .  thus  at each time step  the speaker has a single sentence from the paper in mind  and produces a word that constitutes a part of its ver- vite description  then  at the next time-step  the speaker either stays with the same sentence  or moves on to describing another sentence  and so on  given the transcript  we aim to retrieve those source sentences and use them as the sum- mary  the number of words uttered to describe each sentence can serve as importance score  in dicating the amount of time the speaker spent de-scribing the sentence  this enables to control the summary length by considering the only the most im-portant sentences up to some threshold  we use an hmm to model the assumed stay- tive process  each hidden state of the hmm corresponds to a single sentence  each hidden state of the hmm is conditioned over the sentences appearing in the start of the paper  and the average value of the hmm over all papers is 0 5  where s is the average value of the sentences appearing in the start of the paper  the model is evaluated on clsm ,  scisumm  talksum and evalnet .  we propose a novel automatic method to gener- ate training data for scientific paper summarization ,  based on conference talks given by authors . ", "1003": "the paper proposes an ensemble approach to detect emotions in text using pre-trained word embeddings  the paper proposes a model for emotion detection in text based on word vectors obtained from pre-trained word embedding models  pre-trained word vectors are used as input to the neural network  classi er given some test sample  a classi er outputs the decision function value for each emotion that appears in the training data  the classes associated with the test sample are then taken to be the emotion with the highest decision value (for multi-class) or the set of emotions with a positive decision value (for multi-label)  nrc lexicon features (number of terms in a post associated with each label in the nrc lexicon) and presence of punctuation marks  question marks  links  happy emoticons  and sad emoticons  word embedding based vectors can be used to represent a document into a xed vector  cbow (continuous bag of words) is used to represent the document in the vector space  tfidf is the tfidf weight for each term ti in case ti was not present in the training data  smoothed its idf weight as if it appeared in one document (this yielded better performance than discarding the term)  classi er weight (classi) - calculated a weight function  w (t  e  t) for each term in the training data which indicates its importance in classifying a document as expressing emotion e we did this by rst representing the documents in a bow binary vector representation where we only extracted unigram features  then  for each e we trained an svm model with a linear kernel and took m(e  t) to be the weight associated with the model with each term t in the training data  motivated by guyon who showed that m(e  t) is an indicative feature selection criterion  we de ne: w (t  t) m(e  t) e  where e and e are the corresponding average and standard deviation of model weights in absolute value  ensemble methods tend to achieve better results when there is a signi cant diversity among the classi ers  datasets sentiment analysis datasets is a set of datasets where participants have reported experiences and reactions for seven emotions  semeval fairy tales contains newspaper headlines labeled with the six ekman emotions by six annotators  for blog posts  the most dominant emotion was considered as the headline label  bow is a state-of-the-art approach for emotion detection in short texts  this was used as a state-of-the-art approach for emotion detection in short texts in many cases  e g   19  and more  emotion detection datasets are labeled with multiple emotions and imbalanced  thus  the classi cation performance for all emotion classes is evaluated by using macro average f1-score results are not very detailed or exhaustive  results for all datasets and document representation methods  word vectors trained by glove achieved higher performance than using word2vec based vectors  combining both bow and embedded document representations improves f1-scores for all the models  pre-trained word vectors are used for emotion detection ", "1004": "in this paper ,  we show that  in addition to text based turn features  dialogue features can significantly improve detection of emotions in social media customer service and help predict emotional techniques used by customer service agents .  a recent study shows that one in five (23) customers in the u . s  say they have used social media for customer service in 2014 ,  up from of obviously  companies hope that such 1/url/docs/2014-x/2014-global-customer-uses are associated with a positive experience  yet there are limited tools for assessing this  emotions are a cardinal aspect of inter-personal communication: they are an implicit or explicit part of communication  and of particular importance in the setting of customer service  as they relate directly to customer satisfaction and experience (o  typical emotions expressed by customers in the context of social media service dialogues include anger and frustration  as well as gratitude and more)  however  it is important to note that emotions expressed by customer service agents are typically governed by company policies that specify which emotions should be expressed in which situation (rafaeli and sutton  1987)  this is why we talk in this paper about agent emotional techniques rather than agent emotions  consider  for example  the real (anonymized) twitter dialogue in figure in the figure in this dialogue  customer disappointment is expressed in the first turn (bummer/uh oh!)  followed by customer support (uh oh!)  then in the last two turns both right and left  this is very different from the text based approaches .  twitter’s real-time customer service provides real-time support by monitoring tweets that customers address to it .  companies that utilize the twitter platform as a channel for customer service use a dedicated twitter account which provides real-time support by monitoring tweets that customers address to it  at the same time corporate support agents reply to these tweets also through the twitter platform  a dialogue is defined to be a sequence of turns between a specific customer and an agent ,  where the customer initiates the first turn  consecutive posts of the same party ( customer or agent) uninterrupted by the other party  are considered as a single turn (even if there are several tweets)  given the nature of customer support services  we assume the last turn in the dialogue is an agent turn (e g   youre very welcome  :) hit us back any time you need support  thus  we expect an even number of turns in the dialogue  we filtered out dialogues in which more than one customer or one agent are involved the paper proposes to classify the current turn to emotions expressed by a customer and to predict the emotional technique used by an agent in a dialogue .  for example ,  if the agent is asking for a topic before she is connected to a support agent  the customer is requested to provide a topic before she is connected to a support agent (usually using an ivr system) .  the temporal family of features comprises the following features extracted from the timeline of the dialogue: customer/agent response time: two local features that indicate the time elapsed between the timestamp of the last customer/agent turn and the timestamp of the subsequent turn  this is a categorical feature with low values  medium or high (using categorical values yielded better results than using a continuous value)  median customer/agent response time: two local categorical features defined as the median of the customer/agent response times preceding the current turn  these features are the same as the previous temporal features  the emotional family of features includes agent emotion and customer emotion  these two sets of local binary features represent emotions predicted for previous turns  our model generates predictions of ti for each customer and agent turn  and uses these predictions as features to classify a later customer or agent turn with emotion expression  the temporal family of features includes the following features extracted from the timeline of the dialogue: customer/agent response time: two local features that indicate the time elapsed between the timestamp of the last customer/agent turn and the temporal features are also extracted using time values between previous turns .  dialogue data is collected from amazon mechanical turk tagging the dialogue is tagged using mechanical turks master level judges .  each 4/ judge performed the following tagging tasks given the full dialogue: emotion tagging: indicate the intensity of emotion expressed in each turn ( customer or agent) for each emotion ,  on a scale of 0  such that defines no emotion  a low emotion intensity and a high emotion intensity  topic tagging: select one or several topic: account issues  pricing  payments  customer service  customer experience  technical problem  technical question  order and delivery issues  behavior of a staff member  company policy issues and general statement  agent essence tagging: select one or several of the following for each agent’s turn  to describe the agent’s emotions in the specific turn  recognize the issue raised  asking for more information  offering a solution  general statement and assurance of the agent’s efforts  for both classification tasks ,  svm-hmm dialogue model outperformed baseline results for almost all emotions  where average macro and micro results are statistically significant compared to the baseline .  in the future ,  we plan to run experiments in which the predicted emotional technique is actually applied in the context of new dialogues to measure the effect of such predictions on real support dialogues . ", "1005": "the paper proposes to use differentiable neural architecture search (d-nas) tools to optimize the architecture for few-shot learning (fsl) without over-fitting .  to make the architecture task adaptive ,  the paper proposes the concept of met-adapt controller modules  these modules are added to the model and are meta-trained to predict the optimal network connections for a given novel task  a few-shot classification (fsc) is a popular method for approaching fsc .  in meta-learning ,  the inputs to both train and test phases are not images  but instead a set of few-shot tasks  ti  each k-shot / n-way task containing a small amount of k (usually) of labeled support images and some amount of unlabeled query images for each of the n categories of the task  the goal of meta-learning is to find a base model that is easily adapted to the specific task at hand  so that it will generalize well to tasks built from novel unseen categories and fulfill the goal of fsc  it seems that larger architectures increase fsc performance  up to a certain size  where performance seems to saturate or even degrade  this happens since bigger backbones carry higher risk of over-fitting  it seems the overall performance of the fsc techniques cannot continue to grow by simply expanding the backbone size  in light of the above  in this paper we set to explore methods for architecture search  their meta-adaptation and optimization for fsc  a few-shot learning problem is one where you train a model on a small number of examples ,  and you try to classify the new examples according to their proximity to the ones that have already been trained on the previous task .  task-adaptable block it has a graph structure with connections that can modulate the architecture ,  adapting it to the few-shot task at hand .  sub-models metadapt controllers predict the change in connectivity that is needed in the learned graph as a function of the current task  replacing simple sequence of convolutional layers with the suggested dag ,  with its many layers and parameters  in conventional gradient descent training will result in a larger over-fitting .  this is even worse for fsl  where it is harder to achieve generalization due to scarcity of the data and the domain differences between the training and test sets  the weights are optimized using sgd optimizer with learning rate 0 . 001 ,  momentum and weight decay  gumbel-like distributions are used here .  tldr; the authors propose met modifiers ,  a few-shot learning approach that enables meta-learned network architecture that is adaptive to novel few-shot tasks . ", "1006": "in this paper ,  we outline an approach to detecting such egregious conversations  using behavioral cues from the user  patterns in agent responses  and user-agent interaction .  in this paper we study detecting egregious conversations that can arise in numerous ways .  for example ,  incomplete or internally inconsistent training data can lead to false classification of user intent  failure to maintain adequate context can cause chatbots to miss anaphoric references  in the extreme case  malicious actors may provide heavily biased responses  detecting egregious conversations is a new task ,  however  there is related work that aim at measuring the general quality of the interactions in conversational systems .  to detect egregious conversations ,  features from both customer inputs and agent responses are extracted  together with features related to the combination of specific inputs and responses .  in addition  some of these features are contextual  meaning that they are dependent on where in the conversation they appear  qualitative analysis of the data shows that it is not unusual to find a customer asking to be transferred to a human agent .  such a request might indicate that the virtual agent is not providing a satisfactory service  moreover ,  even if there are human agents  they might not be available at all times  and thus  a rejection of such a request is sometimes reasonable  but might still lead to customer frustration  qualitative analysis of the data shows that it is not unusual to find a customer asking to be transferred to a human agent  see the table below for a more detailed analysis .  egr model generalizes well and does not degrade much when tested on the same dataset as the rule-based model .  in this paper ,  we have shown how it is possible to detect egregious conversations using a combination of customer utterances  agent responses  and customer-agent interactional features .  the goal of this work is to give developers of automated agents tools to detect and then solve problems cre-ated by exceptionally bad conversations ", "1007": "the paper presents an analysis into the inner workings of convolutional neural networks (cnns) for processing text .  cnns used for computer vision can be interpreted by projecting filters into image space ,  but for discrete sequence inputs cnns remain a mystery  we aim to understand the method by which the networks process and classify text  we examine common hypotheses to this problem: that filters  accompanied by global max-pooling  serve as ngram detectors  we show that filters may capture several different semantic classes of ngrams by using different activation patterns  and that global max-pooling induces behavior which separates important ngrams from the rest  the paper examines and attempts to understand how cnns process text ,  and then uses this information for the more practical goals of improving model-level and prediction-level explanations .  each word in the input document is represented as an embedding vector .  max-pooling across the document dimensions is applied followed by a relu activation  the result is then passed to a linear layer for the final classification  datasets sentiment analysis datasets mr - sentence polarity dataset containing 200k train and 25k test evenly split reviews .  review polarity dataset from yelp dataset containing 560k train and 38k test evenly split reviews  intuitively ,  ngrams which any filter scores highly (relative to how it scores other ngrams) are ngrams which are highly relevant for the classification of the text .  the paper proposes a method for separating informative and uninformative values in a max-pooling network .  the paper looks at the set of deliberate ngrams: those that pass the filter’s threshold value .  common intuition suggests that each filter is homogeneous and specializes in detecting a specific class of ngrams  for example ,  a filter may specialize in detecting ngrams such as had no issues  had zero issues  and had no problems  we challenge this view and show that filters often specialize in multiple distinctly different semantic classes by utilizing activation patterns which are not necessarily maximized  we also show that filters may not only identify good ngrams  but may also actively supress bad ones  in particular ,  we reveal that filters are not necessarily homogeneous: a single filter may detect several different semantic patterns  each one of them relying on a different slot activation pattern .  our second theory to explain the discrepancy between the activations of naturally occurring and possible ngrams is that certain filter slots are not used to detect a class of highly activating words  but rather to rule out a class of highly negative words  we refer to these as negative ngrams  the paper proposes to interpret each cnn by visualizing its filters and interpreting the visible shapes in other words ,  defining a high-level description of what the filter detects .  the paper proposes to associate each filter with the following items: 1) the class which this filter’s strong signals contribute to (in the sentiment task: positive or negative); 2) the threshold value for the filter  together with its purity and coverages percentages (which essentially capture how informative this filter is); 3) a list of semantic patterns identified by this filter  each list item corresponds to a slot-activation cluster  for each cluster we present the top-k ngrams activating it  and for each ngram we specify its total activation  its slot-activation vector  and its list of bottom-k negative ngrams with their activations and slot activations  in particular  by clustering the activated ngrams according to their slot activation patterns and showing the top-k in each cluster  we get a much more refined coverage of the linguistic patterns that are captured by the filter  - maxpooling over time induces a thresholding behavior on the output of the convolution layer ,  essentially separating between features that are relevant to the final classification and features that are not .  - decompose the ngram score into word-level scores by treating the convolution of a filter as a sum of word-level convolutions  allowing us to examine the word-level composition of the activation  specifically  by maximizing the word-level activations by iterating over the vocabulary  we observed that filters do not maximize activations at the word-level  but instead form slot activation patterns that give different types of ngrams similar activation strengths  this provides empirical evidence that filters are not homogeneous  by clustering high-scoring ngrams according to their slot patterns we can identify the groups of linguistic patterns captured by a filter  - filters sometimes opt to assign negative values to certain word activations in order to cause the ngrams which contain them to receive a low score despite having otherwise highly activating words ", "1008": "we suggest a new idea of editorial network a mixed extractive-abstractive summarization approach ,  which is applied as a postprocessing step over a given sequence of extracted sentences .  this paper proposes editnet ,  which combines the two approaches of abstractive and extractive summarization .  for each sentence si s (in order) the editor makes one of the three possible decisions: extract ,  abstract or reject .  therefore  the editor may modify the summary s by paraphrasing or rejecting some of its sentences  resulting in a mixed-abstractive summary s  in each step  the editor considers both the sentence representations esi and asi as its input  together with two additional auxiliary representations  the first auxiliary representation is that of the whole document d itself  such a representation provides a global context for the editor to make an educated decision  the second auxiliary representation is that of the corresponding sentence  which provides a local context for the editor to make an educated decision  the editor chooses a concatenation of the two sentence representations  which allows the network to compare the two versions on the same grounds  such representations allow the network to decide which version of the sentence is more relevant  the paper proposes a soft attention mechanism that can be used to generate an abstractive version of a given document by considering a chunk of its sentences .  the authors evaluate their model on the cnn/dailymail dataset and show that it is competitive with state-of-the-art models .  instead of solely applying extraction or abstraction ,  editnet mixes both together .  moreover  editnet implements a novel sentence rejection decision  allowing to correct initial sentence selection decisions which are predicted to negatively effect summarization quality ", "1009": "the authors propose a high-dimensional encoding of chinese phonetic similarities to improve performance .  although at first glance it may seem that phonetic similarity can only be quantified for pairs of words ,  this problem is often present in purely textual spaces  such as social media posts or text messages .  correct homophones and synophones  whether used in error or incorrectly  pose challenges for a wide range of nlp tasks  such as named entity identification  text normalization and spelling correction  these tasks must therefore successfully transform incorrect words or phrases (hear  sew  so) to their phonetically similar counterparts (here  thesew)  a reliably able approach for generating phonetically similar words is equally crucial for chinese text unfortunately  most existing phonetic similarity algorithms such as soundex (philips and mean administration  and double metaphone (dm) ) are motivated by english and designed for indo-chinesean languages  words are encoded to approximate phonetic linguistic characteristics by ignoring (except ones foremost) syllables  which is appropriate where phonetic transcription is used  in contrast  the minimum speech sound of a character is represented by a single dimensional encoding consisting of two or three parts: an initial (optional)  a final or compound  and a final or compound finals  as a result  phonetic similarity approaches designed for indo-chinese languages often fall short when applied to chinese text  dimsim generates ranked candidate words similar to a seed word .  similarity is measured by a phonetic distance metric based on n-dimensional encodings  an important characteristic of pinyin is that the three components ,  initial  final and tone  can be independently phonetically compared  for example  the phonetic similarity of the ie and ue is identical in the pinyin pairs xie2 ue2 and lie2 lue2  in spite of the varying tonality  english contrast  on the other hand  does not have this characteristic  for example ,  u is written as u after j  q  x uo is written as o after b  p  m  f or w .  there are a total of six rewritten rules in pinyin (iso)  since these rules are fixed  it is straightforward to preprocess the pinyins according to these rules to turn them into the original form of pinyins as an internal representation before conducting the comparison  for example  we represent ju as bo as theo  after the preprocessing step  we independently compare ompon nts  this is done by grouping pinyin components into initial clusters and only annotating pairs within each cluster ,  and represent tive cluster pairs .  dimsim is trained to minimize the sum of the absolute differences between the euclidean distances of component pairs and the average distances obtained from the annotated training data across all pairs for (or finals) c .  we also incorporate a penalty function ,  p  for pairs deviating from the manually annotated distance so that more similar pairs are penalized more highly  precision is measured via a manual annotation task on the top-ranked candidates generated by each approach .  dimsim can not derive candidates from dialects that are not encoded in the mapping table .  however ,  as features of art are manually assigned  these algorithms fall short in capturing the perceptual essence of phonetic similarity through empirical data .  in contrast  dimsim achieves high accuracy by learning the encodings both from high quality training data sets and pinyin linguistic features  the paper demonstrates that this approach improves mrr by 7 . 5x ,  recall by 1 5x and precision by 1 4x over existing approaches ", "1010": "relation detection is a core component of many nlp applications including knowledge base question answering (kbqa) .  in this paper ,  we propose a hierarchical recurrent neural network enhanced by residual learning which detects kb relations given an input question  our method uses deep residual lstms to compare questions and relation names via different levels of abstraction  additionally  we propose a simple kbqa system that integrates entity linking and our proposed relation detector to make the two components enhance each other  our experimental results show that our approach not only achieves outstanding relation detection performance  but more importantly  it helps our kbqa system achieve state-of-the-art accuracy for both single-relation (simple) and multi-relation (webqsp) qa benchmarks  the paper proposes to break the relation names into word sequences for question-relation matching .  url given an input question and a set of candidate entities retrieved by an entity linker based on the question ,  our proposed relation detection model plays a key role in the kbqa process: re-ranking the entity candidates according to whether they connect to high confident relations detected from the raw question text by the relation detection model .  this step is important to deal with the ambiguities normally present in entity linking results  finding the core relation (chains) for each topic entity selection from a much smaller candidate entity set after re-ranking  the above steps are followed by an optional constraint detection step  when the question cannot be answered by single relations (e g   multiple entities in the question)  finally  the highest scored query from the above 2following yih et al single-relation kbqa - split relations to word sequences for single-relation detection .  kb relation detection is formulated as a sequence matching problem .  however ,  while the questions are natural word sequences  how to represent relations as sequences remains a challenging problem  here we give an overview of two types of relation sequence representations commonly used in previous work  relation as a single token (relation-level)  in this case  each relation name is treated as a unique token  the problem with this approach is that it suffers from the low relation coverage due to limited amount of training data  thus cannot generalize well to large number of opendomain relations  relation as word sequence (word-level)  the relation is treated as a sequence of words from the tokenized relation name  it has better generalization  but from the lack of global information from the original relation names  this is because the incorrect relation contains word plays  which are more similar to the question 3 entity information used in kbqa systems as features for the final answer re-rankers  (containing word play) in the embedding space  on the other hand  if the target relation co-occurs with questions related to tv appearance in training  by treating the whole relation as a token (i e  relation id)  we could better learn the correspondence between this token and phrases like tv show link to the paper(url) example(url example) matching question to different aspects of a relation (with different abstraction levels) ,  we deal with three problems as follows on learning question/relation representations .  we provide our model with both types of relation representation: word-level and relation-level  therefore  the input relation becomes rword1   rwordm1 rrel1  rrelm2  where the first m1 tokens are words (e g  episode  written) and the last m2 tokens are relation names  e  episode written or starring roles  series  we transform each token above to its word embedding-ding then use two bilstms (with shared parameters) to get their hidden representations bword1:m1 brel1:m2 (each row vector i is the concatenation between forward/backward representations at i)  we initialize the relation sequence lstms with the final state representations of the word sequence  as a back-off for unseen relations  we apply one max-pooling on these two sets of vectors and get the final relation representation hr  from table 1  we can see that different parts of a relation could match different contexts of question texts however ,  deep bilstms do not guarantee that the two-levels of question hidden representations are comparable .  this is mainly because deep bilstms do not guarantee that the two-levels of question hidden representations are comparable  the training usually falls to local optima where one layer has good matching scores and the other always has weight close to the training of deeper architectures itself  to overcome the above difficulties  they adopt the idea from residual networks for hierarchical matching by adding shortcut connections between two bilstm layers  entity reranking (first-step relation detection): use the raw question text as input for a relation detector to score all relations in the kb that are associated to the entities in the question .  use the relation scores to re-rank the elk(q) and generate a shorter list el0k0 containing the top-k0 entity candidates  detect relation(s) using the reformatted question text in which the topic entity is replaced by a special token eley  combine the scores from step 2 and 3 and select the top pair (e ,  r) using constraint detection: compute similarity between q and any neighbor entity along r (e  r  rc) to add the high scoring c and rc to the query  finally  select the relative position of the entity in the question and the number of entities in the given episode to assign high scores to the candidates  url given the top-scored query generated by the previous steps ,  we collect all the nodes c connecting to v (with any relation) with any relation  and generate a sub-graph associated to the original query .  entity matching on sub-graph nodes  we compute a matching score between each n-gram in the input question (without overlapping the topic entity) and entity name of c (except for the node in the original query) by taking into account the maximum overlapping sequence of characters between them  if the matching score is larger than a threshold (tuned on training set)  we will add the constraint entity c (and rc) to the query by attaching it to the corresponding node on the core-chain  link to the paper(url) evaluation the authors create a new relation detection task from the webqsp dataset and select all the relations and relation chains (length 2) connected to the topic entity ,  and set the core-chain labeled in the parse as the positive label and all the others as the negative examples .  they tune the following hyper-parameters on the development sets: the size of the hidden states for lstms  100  200  4009; learning rate (0 1  0 5  1 0); whether the shortcut connections are between hidden states or between maxpooling results  on webqsp ,  hr-bilstm outperforms bilstm with shortcut connections .  - does not use joint-inference or feature-based re-ranking step .  kb relation detection is a key step in kbqa and is significantly different from general relation extraction tasks .  we propose a novel kb relation detection model ,  hr-bilstm  that performs hierarchical matching between questions and kb relations ", "1011": "in this paper ,  we propose a human-in-the-loop (huml) dictionary expansion approach that employs a lightweight neural language model coupled with tight huml supervision to assist the user in building and maintaining a domain-specific dictionary from an input text corpus .  the approach is based on the explore/exploit paradigm to effectively discover new instances (explore) from the text corpus as well as predict new unseen terms not currently in the corpus using the accepted dictionary entries (exploit)  the authors propose a feature agnostic approach for dictionary expansion based on lightweight neural language models ,  such as word2vec .  given an input text corpus and a set of seed examples  the proposed approach runs in two phases  explore  and exploit  the first phase tries to identify similar instances to the related dictionary entries that are present in the input text corpus  using term vectors from the neural language model to calculate a similarity score  the second phase tries to construct more complex multi-term phrases  multi-term phrases are a challenge for word2vec systems as they need to be known prior to model creation to identify multi-term phrases most commonly the most commonly used phrase in the input text corpus  the co-occurrence score  i e   of a term is used to detect the same phrase  however  depending on the domain and the task  the instances evolve interest  or the example corpus may not be complete  for example  valid phrase combinations may simply not occur (e g  a joint pain may appear in the sample corpus  but acute pain may not occur in future texts from the same source  and thus are important to include in any extraction lexicon  the authors show that the proposed approach is effective in the evaluation the paper proposes a human-in-the-loop approach for dictionary extraction from text .  the approach is similar to glimpse and glimpseld ,  statistical algorithms for dictionary extraction based on spot with a faster underlying matching engine  the input is a large text corpus and a set of seed examples  starting from these  it evaluates the contexts (the set of words surrounding an item) in which the seeds occur and identifies good contexts  contexts are scored retrospectively in terms of how many good results they generate  all contexts are kept which have a score over a given threshold and the candidates that appear in the dictionary are only linked to the most good contexts  the approach is based on the explore/exploit paradigm to effectively discover new instances (explore) from the text corpus and generate new unseen instances based on user feedback (exploit) .  the approach runs in iterations ,  where each iteration runs first the explore phase then the exploit phase  the explore phase uses the instances available in the input dictionary to identify similar candidates that are already present in the corpus vocabulary vtc  which are then accepted or rejected by the huml  the accepted candidates are then added to the input dictionary and are used in the exploit phase as well as the next explore iteration  during the exploit phase  we use the instances in the input dictionary to construct more complex phrases that might be of interest for the user  therefore  the problem of calculating the similarity between two instances is a matter of calculating the dictionary distance between two instances in the given feature space  to do so we use the standard cosine similarity measure which is applied on the vectors of the instances  formally  the similarity between two terms w1 and w2  with vectors v1 and v2 is calculated as the cosine similarity between the vectors v1 and v2 we calculate the similarity between the instances in the input dictionary and all the words in the corpus vocabulary vtc we sort the vocabulary in descending order using the cumulative similarity score when using exploit ,  the number of newly discovered instances sharply decreases as no new base terms are introduced  thus the exploit cannot generate new instances that can be added in the dictionary .  the results show that using explore and exploit alternately leads to the best performances  this paper proposes an interactive dictionary expansion tool using a lightweight neural language model . ", "1012": "in this work ,  we demonstrate that we can cast the problem of textual grounding into a unified framework that permits efficient search over all possible bounding boxes .  hence  the method is able to consider significantly more proposals and doesnt rely on a successful first stage hypothesizing bounding box proposals  finding bounding boxes in images which relate to text phrases is an important problem for human-computer interaction ,  robotics and mining knowledge bases .  for example  we may want an autonomous system by using phrases such as the bottle on your left  or the top shelf lamp  while those phrases are easy to interpret for a human  they pose significant challenges for textual grounding algorithms  as interpretation of those phrases requires an understanding of objects and their relations  existing approaches for textual grounding  such as 38  take advantage of the cognitive performance improvements obtained from deep net models  more deep net models are designed to extract features from bounding boxes and textual data  which are then compared to measure their fitness  to obtain suitable bounding boxes  many of the textual grounding frameworks  make use of the region proposals  while being easy to obtain  automatic extraction of region proposals is limiting the performance of the visual grounding  because the performance of the visual grounding is inherently constrained by the quality of the proposal generation procedure  in this work  we describe an interpretable mechanism for which any image concepts  such as semantic segmentations  detections and priors  can be used as the ground truth  this paper proposes a method to explicitly ground natural language in images and videos .  the paper proposes an approach for textual grounding which is based on an energy minimization over a large number of concepts bounding boxes .  the search over a large number of bounding boxes allows us to retrieve an accurate bounding-box prediction for a given phrase and an image  the energy is based on a set of image concepts like semantic segmentations ,  detections or image priors  all those concepts come in the form of score maps which we combine linearly before searching for the bounding box containing the highest accumulated score over the combined score map  it is trivial to add additional information to our approach by adding additional score maps  moreover  linear combination of the score maps reveals importance of score maps for specific queries as well as similarity between queries such as token snowboarder  in order to process free-form textual phrases efficiently ,  we restricted the vocabulary size to the top most frequent words in the training set for the referitgame  and to the top most frequent words for 30k entities .  for the referitgame  we further fine-tuned the last layer of the deeplab system to include the categories of sky  ground  building  water  tree  grass  in contrast to existing approaches which are generally based on a small set of bounding box proposals ,  we efficiently search over all possible bounding boxes . ", "1013": "tldr; the authors propose a new method for learning implicit generative models by performing mean and covariance matching of features extracted from pretrained deep convnets .  tldr; the authors propose generative feature matching networks (gfmn) ,  an approach to train implicit generative models that does not use adversarial or online learning of kernel functions  provides stable training  and state-of-the-art results .  g is trained using precomputed estimates of jpdata and jpdata on a minibatch of generated (fake) data and optimized using stochastic gradient descent with backpropagation .  autoencoder features: a natural choice of unsupervised method to train a feature extractor is the autoencoder (ae) framework  the decoder part of an ae consists exactly of an image generator that uses features extracted by the encoder  therefore ,  by design  the encoder network should be a good feature extractor for the purpose of generation  classifier features: we experiment with different dcnn architectures pretrained on imagenet to play the role of the feature extractor  our hypothesis is that imagenet-based pfs are informative enough to allow the training of (crossdomain) generators by feature matching  in order to train with a mean and covariance feature matching loss ,  one needs large minibatches to obtain good mean and covariance estimates .  with images larger than 3232  dcnns produce millions of features  resulting easily in memory issues  we propose to alleviate this problem by using moving averages of the difference of the means (covariances) of real and generated data  instead of computing the (memory) expensive feature matching loss in eq  1  we keep moving averages of the difference of feature means (covariances) at layer j between real and generated data  we can now rely on vj to get better estimates of the population feature means of real and generated data while using a small minibatch of sizen for a similar result using the feature matching loss given in eq  one would need a minibatch with large size n  which is problematic for large number of features  mmd(k ,  p  q) mmd(k generative moment matching network autoencoder (gfmn) uses a gaussian kernel to perform mean and covariance matching in a pf space induced by a non-linear kernel function that is orders of magnitude larger than the ae latent code .  the authors evaluate their model on cifar-10 ,  stl-10  lsun and celeba and show that they can achieve state-of-the-art performance on these datasets .  - training on gpus is expensive due to the need for large number of features from dcnns .  - training on celeba is expensive due to the need for large number of features from dcnns  tldr; the authors propose gmmnae ,  a non-adversarial feature matching based approach to train an implicit gan .  also find this summary at davidstutz . de(url/)  also find this summary at davidstutz . de(url/)  also find this summary at davidstutz . de(url/) ", "1014": "recommend items for e-tailers with a mix of durable and non-durable goods .  the demand for items is a combined effect of form utility and time utility ,  i e   a product must both be intrinsically appealing to a consumer and the time must be right for purchase  for durable goods  time utility is a function of inter-purchase duration within product category because consumers are unlikely to purchase two items in the same category in close temporal succession  moreover  purchase data  in contrast to rating data  is implicit with non-purchases not necessarily in the same category  together  these issues give rise to the positive-unlabeled demand-aware recommendation problem indicating that we pose via joint low-rank tensor completion and product category inter-purchase duration vector estimation  we further relax this problem and propose a highly scalable alternating minimization approach with which we can solve problems with millions of users and millions of items in a single thread for example ,  a user might not be interested in purchasing an item because she is not satisfied with it at the time it was bought  or because she is not satisfied with the current value of the item  or because she is not satisfied with the current value of the item at the time it was bought .  for example  the authors propose a model that captures these two aspects of user behavior  and that is trained to predict the desired item in the future  time-aware recommender systems exploit temporal information but do not explicitly consider the notion of time utility derived from inter-purchase durations in item categories .  in particular ,  given a set of m users  n items  and l time slots  we construct a third-order binary tensor p 0  1mnl to represent the purchase history .  specifically  entry p indicates that user i has purchased item j in time slot k  we assume that the n items belong to r item categories  with items in each category sharing similar inter-purchase durations 3 we use an n-dimensional vector c 1  2  rn to represent the category membership of each item  given p and c  we further generate a tensor tjk where ticjk denotes the number of time slots between user is most recent purchase within category cj until time k  if user i has not purchased within category cj until time k  ticjk is set to  in this work ,  we use an underlying third-order tensor x rmnl to quantify form utility .  in addition  we employ a non-negative vector d rr to measure the underlying inter-purchase duration times of the r item categories  it is understood that the inter-purchase durations for durable good categories are large  while for non-durable good categories are small  or even zero  in this study  we focus on items inherent properties and assume that the inter-purchase durations are user-independent  the overall utility can be obtained by comparing form utility and time utility  the overall utility can be obtained by comparing the form utility and time utility  to this end ,  we assume that an individual’s form utility does not change over time  an assumption widely used in many collaborative filtering methods .  under this assumption  the tensor x is a repeated copy of its frontal slice x::1  i e   x::1 e  where e is an l-dimensional all-one vector and the symbol represents the outer product operation  in this way  we can relax the problem of learning a third-order tensor to the problem of learning its frontal slice  which is a second-order tensor (matrix) to address these challenges ,  we adopt an alternating minimization scheme that iteratively fixes one of d and x and minimizes with respect to the other .  specifically  we propose an extremely efficient optimization algorithm by effectively exploring the sparse structure of the tensor p and low-rank structure of the matrix x  we show that (i) the problem can be solved within o(p0(k log(p0))) (nmk2) time  where k is the rank of x  and (ii) the algorithm converges to the critical points of f(x d)  time complexity combining the two subproblems together ,  the time complexity of each of the proposed algorithm is o(p0 log(p0) nk2t mk2t p0kt) .  we can observe that the proposed algorithm is extremely efficient ,  e . g   even with million users  million items  and more than million purchase records  the running time of the proposed algorithm is less than hours in the real-world experiments ,  we evaluate the proposed demand-aware recommendation algorithm by comparing it with the six state-of-the-art recommendation methods: (a) m3f  maximum-margin matrix factorization (b) pmf  probabilistic matrix factorization (c) wr-mf  weighted regularized matrix factorization (d) cp-apr  rubik  knowledge-guided tensor factorization and completion method (bptf) bayesian probabilistic tensor factorization among them  m3f and pmf are widely-used collaborative filtering algorithms .  we include these two algorithms as baselines to justify whether traditional collaborative filtering algorithms are suitable for general e-tail recommendation involving both durable and non-durable goods  since they require explicit ratings as inputs  we follow to generate numerical ratings based on the frequencies of (user  item) consumption pairs  wr-mf is essentially the positive-un durable version of pmf and has shown to be very effective in modeling the implicit feedback data  all the other three baselines  i e   cp-apr  and bptf  are tensor-based methods that can consider time utility when making recommendations  we refer to the proposed recommendation algorithm as demand-aware recommender in this paper ,  we examine the problem of demand-aware recommendation in settings when inter-purchase duration within item categories affects users intention to purchase items in combination with intrinsic properties of the items themselves .  we formulate it as a tensor nuclear norm minimization problem that seeks to jointly learn the form utility tensor and a vector of inter-purchase durations  and propose a scalable optimization algorithm with a tractable time complexity ", "1015": "the paper presents a neural response generation model that generates responses conditioned on a target personality .  the model learns high level features based on the target personality ,  and uses them to update its hidden state  it is desirable for automated agents to be capable of generating responses that express a target personality .  personality is defined as a set of traits which represent durable characteristics of a person  many models of personality exist while the most common one is the big five model (digman ,  1990) including: openness  conscientiousness  extraversion  agreeableness  and empathetic  these traits were correlated with linguistic choices including lexicon and syntax  in this paper  we study how to encode personality traits as part of neural response generation for conversational agents  our approach builds upon a sequence-to-sequence by adding an additional layer that represents the target set of personality traits  and a hidden layer that learns high-level personality based features  the response is then generated conditioned on these features  tldr; the authors propose an approach to generate responses that express a target personality without explicitly defining linguistic features .  the model is trained end-to-end by maximizing p(yx) over all output tokens .  this comment has been minimized .  the authors evaluate their model on a dataset of 87 . 5k conversations over customer service twitter channels  in future work ,  we would like to generate responses adapted to the personality traits of the customer as well  and to apply our model to other tasks such as education systems . ", "1016": "this is the first time that an end-to-end stereo pipeline from image acquisition and rectification ,  multi-scale spatio-temporal stereo correspondence  winner-take-all  to disparity regularization is implemented fully on event-based hardware .  what they suggest a new event-based event-based computation algorithm .  event-based computation has two main drawbacks: it is limited by the frame rate ,  which limits the resolution of frame-based cameras  and it is highly redundant  which leads to downstream data transfer and energy consumption  event-based computation has the advantage of low power consumption  but it is limited by the computation time  they use a network of spiking neurons as the nodes .  fast moving objects are more challenging for frame-based cameras .  the ibm truenorth is a reconfigurable ,  non-von neumann neuromorphic chip containing million spiking neurons and million synapses distributed across parallel  event-driven  neurosynaptic cores are tiled in an array  embedded in a fully asynchronous network-on-chip  depending on event dynamics and network architecture  faster tick period is possible  which we take advantage of in this work to achieve as low as ms per tick  thus doubling the maximum throughput achievable  the proposed local event-based stereo correspondence algorithm is implemented end-to-end as a neuromorphic event-based algorithm .  this consists of systems of equations defining the behavior of true neurons ,  encased in modules called corelets  and the subsequent composition of the inputs and outputs of these modules  the stereo rectification is defined by a pair of functions l  r which map each pixel in the left and right sensor polarity channels to a pixel in the left and right sensor(t) native resolution respectively  on truenorth  this is implemented using h w membrane neurons per sensor polarity channel  arranged in an h w retinotopic map  the events at each rectified pixel are generated through neurons which replicate corresponding sensor pixels  their potential is defined by v splp (t) where t is the time at which a sensor event is produced and t is the sensor pixel corresponding to the rectified pixel  the event rate of an event-based sensor depends on factors  such as scene contrast  sensor bias  and object velocity  to add invariance across event rates  we accumulate spikes over various temporal scales through the use of temporally overlapping sliding windows  these temporal scales are implemented through the use of neurons which cause each event to appear at its corresponding pixel multiple times  depending on the desired dilation and erosion neurons are used to denoise the input .  the winner-take-all (wta) system is a feed-forward neural network that takes as input d thermometer code representations of the hadamard products for d distinct candidate disparity levels ,  and finds the disparity with the largest value  at every timestep .  a left-right consistency check is then performed to verify that for each left-rectified pixel p matched to right-rectified pixel q ,  it is also the case that right-rectified pixel q gets matched to left-rectified pixel p .  this is achieved using two parallel wta streams  stream calculates the winner disparities for left-to-right matching  and stream calculates the winner disparities for right-to-left matching  the outputs of each stream are represented by d retinotopic maps expressed in a fixed resolution (dvi j d(t) d 0   d 1  v  l r)  where events represent the retinotopic winner disparities for that stream  the streams are then merged to produce the disparity map performance is evaluated on synthetic datasets consisting of random dot stereograms representing a rotating synthetic 3d object ,  and two real world sets of sequences  consisting of a fast rotating fan and a rotating toy butterfly .  the synthetic dataset provides dense disparity estimates  which are difficult to acquire with sparse event based cameras  the fan sequence is useful for testing the ability of the algorithm to operate on rapidly moving objects  varying orientations of the fan add continuously varying depth gradient to the dataset  it is observed that the temporal scale has a higher effect on accuracy than spatial scale .  the architecture is highly parameterized and can operate with other event based sensors such as atis or dvs . ", "1017": "given a collection of distributions ,  two causal graphs are called interventionally equivalent if they are associated with the same family of interventional distributions  where the elements of the family are indistinguishable using the invariances obtained from a direct application of the calculus rules .  a causal graph is a directed acyclic graph (dag) with latent variables ,  where each edge encodes a causal relationship between its endpoints: x is a direct cause of y  i . e   x y  if when the remaining factors are held constant  forcing x to take a specific value affects the value of y  where x y are random variables representing some relevant features of the system  the task of the causal structure entails a search over the space of causal graphs that are compatible with the observed data; the collection of these graphs forms what is called an equivalence class  the most popular mark-directed on the data by the underlying causal structure that is used to delineate an equivalence class are conditional independence (ci) relations  these relations are the most basic type of probabilistic invariances used in the field and have been studied at large in the context of graphical models since  at least  (see also)  while cis are powerful and have been the driving force behind some of the most prominent structural learning algorithms in the field  including the pc  fci  these are specific constraints for one distribution  in this paper  we start by noting something very simple  albeit powerful  that happens when a combination of observational and experimental distributions are available: there are constraints over also find this summary at davidstutz . de(url/)  the do-calculus in causal inference consists of a set of inference rules that allows one to create a map between distributions generated by a causal graph when certain graphical conditions hold in the graph .  the calculus was developed in the context of hard interventions ,  and recent work presented a generalization of this result for soft interventions  the first rule of the calculus is a d-do-see type of statement relative to a specific interventional distribution  which says that y z w ind implies the corresponding conditional independence px(yw  z) px(yw) px(yw)  note that the corollary of this rule is the one underlying most of the structure learning algorithms found in practice  which says that if some independence hold in p  this would imply a corresponding graphical separation (under faithfulness) in the causal graph  from this understanding  we make a very simple  albeit powerful observation i e   the converse of the other two rules should offer insights about the underlying graphical structure as well  for instance  if there is a latent variable p(y  x) that allows one to infer the absence of a causal relation between x and y  then there is a latent variable p(y the paper introduces the notion of an augmented causal graph (augmented causal graph) to characterize when two causal graphs are equivalent in accordance to the proposed definition .  given a causal graph d and an intervention set i ,  let m be the set of augmented mags corresponding to all the causal graphs that are i-markov equivalent to d .  an augmented pag ford  denoted g pag(augi(d))  is a graph such that: g has the same adjacencies as m  and any member of m does; and every non-zero mark in g is an invariant mark in m  given a causal graph d and an intervention set i  we investigate the problem of learning the causal structure underlying a phenomenon of interest from a combination of observational and experimental data . ", "1018": "the resulting combinatorial explosion in program space ,  along with extremely sparse rewards  makes npi for kbqa ambitious and challenging .  we present complex imperative program induction from terminal rewards (cipitr)  an advanced neural programmer that mitigates reward sparsity with auxiliary rewards  and restricts the program space to semantically correct programs using high-level constraints  kb schema  and inferred answer type  cipitr solves complex kbqa considerably more accurately than key-value memory networks and neural symbolic machines (nsm)  for moderately complex queries requiring 2to 5-step programs  cipitr scores at least higher f1 than the competing systems  tldr; the authors propose a novel neural program induction system for solving complex programs .  key-value memory network (kvmnet) learns to answer questions by attending on the relevant kb subgraph stored in its memory .  csqa is adapted to study the complex program induction (cpi) challenge over other kbqa data sets .  csqa is particularly suited to study the complex program induction (cpi) challenge over other kbqa data sets because: it contains large-scale training data of question-answer pairs across diverse classes of complex queries ,  each requiring different inference large kb sub-graphs  poor state-of-the-art performance of memory networks on the massive size of the kb involved million entities and million tuples poses a scalability challenge for prior npi techniques  availability of the massive size of the kb metadata helps standardize comparisons across techniques (explained subsequently)  csqa is adapted in two ways for the cpi problem  removal of extended conversations: to be consistent with the nsm work on kbqa  we discard qa pairs that depend on the previous dialogue context  this is possible as every query is annotated with information on whether it is self-contained or depends on the previous context  the gold annotations of canonical kb entities  types  and relations available in the data set along with the queries  in order to remove a prominent source of confusion in comparing kbqa systems  although annotation accuracy affects a complete kbqa system  the model is trained with a vocabulary of operators and variable types .  memory lookup: the memory lookup looks up scratch memory with a given probe ,  say x (of arbitrary dimension)  and retrieves the memory entry having closest key embedding to x .  it first passes x through a feed-forward layer to transform its dimension to key embedding dimensionxkey  then  by computing softmax over the matrix multiplication of key and xkey  the distribution over the memory variables for lookup is obtained  xkey f(x)  xdist softmax(mx keyxkey) feasibility sampling: to the search space to meaningful programs  cipitr incorporates both high-level generic or task-specific constraints when sampling any action  the generic constraints can help it adopt more pragmatic programming styles like not repeating lines of code or avoiding syntactical errors  the task specific constraints ensure that the generated program is consistent as per the kb schema or on execution gives an answer of the desired variable type  to sample from the feasible subset using these constraints  the input sampling distribution  xdist  is elementwise transformed by a feasibility vector xfeas followed by a l1-normalization  along with the transformed distribution  the top-k entries xsampled are also returned  npi core: the query representation q is fed at the initial timestep to an environment encoding rnn ,  which gives out the environment state et at every timestep .  this  along with the value embedding uvalt1 of the last output variable generated by the npi engine  is fed at every timestep into another rnn that finally outputs the program state ht  ht is then fed into the successive modules of the program induction engine as described below  outvargen: the query is first parsed into a sequence of kb-entities and non-kb words  kb entities e are embedded with the concatenated vector transe  and non-kb words with 0  the final query representation is obtained from a gru encoder as q  output variable generator: the new variable up of type u is generated by invoking the value embedding of the operator prototype arg  and by applying a feed-forward network fvtype which transforms the program state ht to a vector in rmax var  it is then element-wise multiplied with the current attention state over the variables in memory of that type  tldr; the authors propose a novel neural machine translation (nmt) framework to solve the long standing problem of generating large programs that are difficult to interpret .  the key idea is to use a neural machine translation (nmt) framework to solve the problem of generating large programs that are difficult to interpret  evaluation the model is trained using the adam optimizer and tuned on the validation set .  some parameters are selectively turned on/ off after a few training iterations ,  which is itself a hyperparameter  evaluation the model is evaluated on the more commonly used websp data set  though quite a few recent works on kbqa have evaluated their model on websp  the reported performance is always in a setting where the gold entities/relations are not known  they either internally handle the entity and relation problem or outsource it to some external or in-house model  which itself might have been trained with additional data  additionally  the entity/relation linker outputs are also not made public  making it difficult to set up a fair ground for evaluating the program induction model  to avoid these issues  we use the human-annotated entity/relation linking data available along with the input to the program induction model  consequently the performance reported here is not comparable to the previous works evaluated on this data set  as the query annotation is obtained here from an oracle linker  further  to gauge the proficiency of the proposed program induction model  we construct a rule-based model which is aware of the human annotated semantic form of the query that is  the inference chain of relations and the exact the authors evaluate their model against nsm ,  kvmnet  and cipitr and show that it outperforms all of them in terms of f1 score .  the paper presents cipitr ,  an advanced npi framework that pushes the frontier of complex program induction in absence of gold programs . ", "1019": "dual-ces employs a two-step dual-cascade optimization approach with saliency-based pseudo-feedback distillation .  the paper proposes dual cross entropy summarization (dual-ces) ,  an extension of cross entropy summarization (ces) .  dual-ces is an unsupervised query focused multi-document extractive summarizer  to this end  like ces  dual-ces utilizes the cross entropy method for selecting a subset of sentences extracted from input documents  whose combination is predicted to produce a good summary  however  unlike ces  dual-ces does not attempt to address both saliency and focus goals in a single optimization step  instead  dual-ces implements a novel two-step dual-cascade optimization approach  which utilizes two sequential saliency distillation invocations  using such an approach  dual-ces provides a fully unsupervised summarization-based pseudo-feedback  which allows to generate a focused summary that is more short in the second step  this paper proposes a novel unsupervised learning approach for the task of query-based multi-document summarization .  given a set of matching documents to be summarized ,  the task is to produce a length-limited summary by extracting salient content parts in the document which are further relevant (focused) to the given query .  to this end  we produce summary s (with maximum length lmax) by choosing a subset of sentences which maximizes a given quality target q(sq d)  similar to ces  it utilizes the cross entropy method for selecting the most promising subset of sentences in d  since we assume an unsupervised setting  no actual reference summaries are available for training nor can we directly optimize an actual quality target q(sq  instead  q(sq d) is surrogated by several summary quality prediction measures qi(sq d) (i 1  2  each predictor qi(sq d) is designed to estimate the level of saliency or importance of a given candidate summary s and is presumed to correlate (up to some extent) with actual summarization quality  e g   for simplicity  various predictions are assumed to be independent and are combined into a single optimization objective by taking their product  the cross entropy method provides a generic monte-carlo optimization framework for solving hard combinatorial problems previously dual-ces does not attempt to maximize both saliency and focus goals in a single optimization step .  instead ,  dual-ces implements a novel two-step dual-cascade optimization approach  which utilizes two ces-like invocations  yet  each invocation utilizes a bit different set of summary quality predictors  depending on whether the summarizer’s goal should lay towards higher summary saliency or focus  in the first step  dual-ces relaxes the summary length constraint  aiming at producing a longer and more salient summary  this summary is then treated as a pseudo-effective reference from which saliency-based pseudo-feedback is distilled  such pseudo-feedback is then utilized in the second step of the cascade for setting an additional auxiliary saliency-driven goal  at the second step  similar to ces  the primary goal is actually to produce a focused summary (with maximum length limit lmax)  overall  dual-ces is simply implemented as follows: cem(qfoc(q d)  lmax  scl(q  ))  here  qsal(q  qfoc(q  scl( summary length constraint lmax and pseudo-reference summary sl generated in the previous step .  the authors evaluate their model on document understanding conferences (duc) and benchmarks .  - dual-ces is the strongest baseline to outperform in most cases .  dual-ces was shown to better handle the tradeoff between saliency and focus ,  providing the best summarization quality compared to other alternative state-of-the-art summarizers . ", "1020": "the paper presents a lightweight neural tts system with high quality output .  the system is composed of three separate neural network blocks: prosody prediction ,  acoustic feature prediction and linear prediction coding net as a neural vocoder  the system can synthesize speech with close to natural quality while running times faster than real-time on a standard cpu  tldr; the authors propose to use the world vocoder parameters of lpcnet as input to a tts system that can adapt to new voices .  prosody generator generates a sequence of sub-phoneme elements ,  including duration  pitch and intensity values .  each sub-phoneme element represents either a heading  a middle or a trailing part of a phoneme  lpcnet block converts the stream of acoustic feature vectors to a speech signal  each block has its own model which is trained independently for each voice  for each voice  the training and adaptation phases include the following data pre-processing steps: a grapheme-to-phoneme conversion using the frontend block  forced alignment of audio at the sub-phoneme level using proprietary acoustic modeling and speech recognition tools  extraction of textual features for prosody modeling using the front-end block  pitch detection for prosody modeling using a proprietary tool  cepstral and residual extraction using the lpcnet feature extraction tool  the lpcnet decoder is a variant of the wavernn that uses a nn model to generate speech samples from equidistant-intime input of cep ,  pitch and pitch correlation parameters .  unlike other waveform generative models  such as wavenet and wavernn  the lpcnet uses its nn to predict the lpc residual (the vocal source signal) and then apply to it an lpc filter calculated from the cep  this has the advantage of better control over the output of the spectral shape since it directly depends on the lpc filter shape  the model is also more robust to the predicted residual errors since any high frequency noise is also shaped by the lpc filter  in the table below ,  you can see the differences in the results for the datasets .  the slowest block runs about times faster than real-time on a 2 . 8ghz i7 cpu  this paper presents a faster than real-time tts system that can produce high quality speech while operating at faster than real-time rate without an expensive gpu support .  the system is built around three nn models for generating the prosody ,  acoustic features and the final speech signal ", "1021": "sobolev independence criterion (sic) decomposes the sum of importance scores of two random variables x and y into a product of their marginals  in machine learning  feature selection is an important problem in predictive statistics and machine learning for interpretable predictive discoveries  our goal  is to design a dependency measure that is interpretable can be reliably used to control feature selection  sic relies on the statistics of both the joint distribution and the product of marginals  intuitively  the gradient of the average magnitude of the gradient with respect to a feature gives an importance score for each feature  hence  promoting its sparsity is a natural feature selection problem  in particular  the authors propose to use a penalty term that controls the sparsity of the gradient estimator on the support of the measures  this is done by introducing an importance score normalized by the perturbed critic score  also find this summary at davidstutz de(url/)  the paper proposes to learn the feature map as a deep neural network  the authors propose two methods to control false discovery rate (fdr) in feature selection: holdout randomization test (hrt) and feature selection based on conditional generative models  sic can potentially leverage the deep learning toolkit for going beyond tabular data where random forests excel  to more structured data such as time series or graph data  in experiments  they compare their model to competing models on synthetic datasets and a real-world drug resistance dataset  the paper introduces the sobolev independence criterion (sic) as a feature importance measure that can be used for feature selection  alternating optimization methods block coordinate descent (bcd) using first order methods is also known to converge to a global optima  also find this summary at davidstutz de(url/)    the authors propose a variational formulation of this problem  the model is trained using minibatch sgd  this can be done in two ways  here  the out-of-coherence region is referred to as the critic branch "}