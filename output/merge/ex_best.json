{"1000": "supervised learning approaches have advanced the state-of-the-art on a variety of tasks in natural language processing, resulting in highly accurate systems. unsupervised learning approaches appear to be a natural solution to this problem, as they require only unannotated text for trainthis research was carried out during an internship at google research. unfortunately, the best completely unsupervised english pos tagger , making its practical usability questionable at best. to bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language study related but different multilingual grammar and tagger induction tasks, where it is assumed that no labeled data at all is available. our work is closest to that of yarowsky and ngai (2001), but differs in two important ways. first, we use a novel graph-based framework for projecting syntactic information across language boundaries. to this end, we construct a bilingual graph over word types to establish a connection between the two languages (3), and then use graph label propagation to project syntactic information from english to the foreign language (4). second, we treat the projected labels as features in an unsuper- 1for simplicity of exposition we refer to the resource-poor language as the foreign language. syntactic universals are a well studied concept in linguistics for multilingual grammar induction. because there might be some controversy about the exact definitions of such universals, this set of coarse-grained pos categories is defined operationally, by collapsing language (or treebank) specific distinctions to a set of categories that exists across all languages. these universal pos categories not only facilitate the transfer of pos information from one language to another, but also relieve us from using controversial evaluation metrics,2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed english labels. as discussed in more detail in 3, we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types, while the vertices on the english side are individual word types. algorithm bilingual pos induction require: parallel english and foreign language data de and df , unlabeled foreign training data f ; english tagger. after running label propagation (lp), we compute tag probabilities for foreign word types x by marginalizing the pos tag distributions of foreign trigrams ui x x x over the left and right context words: p(yx) x,x qi(y) x,x,y qi(y ) (6) we then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value : tx(y) if p(yx) otherwise (7) we describe how we choose in this vector tx is constructed for every word in the foreign vocabulary and will be used to provide features for the unsupervised foreign language pos tagger. (2011) in our experiments.10 this set c consists of the following coarse-grained tags: noun (nouns), verb (verbs), adj (adjectives), adv (adverbs), pron (pronouns), det (determiners), adp (prepositions or postpositions), num (numerals), conj (conjunctions), prt (particles), punc 9we extracted only the words and their pos tags from the treebanks. as indicated by bolding, for seven out of eight languages the improvements of the with lp setting are statistically significant with respect to the other models, including the no lp setting.11 overall, it performs better than the hitherto state-of-the-art feature-hmm baseline, and better than direct projection, when we macro-average the accuracy over all languages. our full model outperforms the no lp setting because it has better vocabulary coverage and allows the extraction of a larger set of constraint features. figure shows an excerpt of a sentence from the italian test set and the tags assigned by four different models, as well as the gold tags. while the first three models get three to four tags wrong, our best model gets only one word wrong and is the most accurate among the four models for this example. as a result, its pos tag needs to be induced in the no lp case, while the 11a word level paired-t-test is significant at p for danish, greek, italian, portuguese, spanish and swedish, and p for dutch. gold: si trovava in un parco con il fidanzato paolo f. , anni , rappresentante em-hmm: feature-hmm: no lp: with lp: conj noun det det noun adp det noun noun pron verb adp det noun conj det noun noun noun verb pron verb adp det noun adp det noun noun noun noun verb verb adp det noun adp det adj noun adj noun verb verb adp det noun adp det noun noun noun noun figure 2: tags produced by the different models along with the reference set of tags for a part of a sentence from the italian test set. we have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages. because we are interested in applying our techniques to languages for which no labeled resources are available, we paid particular attention to minimize the number of free parameters and used the same hyperparameters for all language pairs. our results suggest that it is possible to learn accurate pos taggers for languages which do not have any annotated data, but have translations into a resource-rich language. our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised pos tagging models.", "1001": "fisher vectors have been shown to provide a significant performance gain on many different applications in the domain of computer vision 39, 33, 2, in the domain of video action recognition, fisher vectors and stacked fisher vectors have recently outperformed state-of-theart methods on multiple datasets 33, fisher vectors (fv) have also recently been applied to word embedding (e.g. word2vec 30) and have been shown to provide state of the art results on a variety of nlp tasks 24, as well as on image annotation and image search tasks in all of these contributions, the fv of a set of local descriptors is obtained as a sum of gradients of the loglikelihood of the descriptors in the set with respect to the parameters of a probabilistic mixture model that was fitted on a training set in an unsupervised manner. in spite of being richer than the mean vector pooling method, fisher vectors based on a probabilistic mixture model are invariant to order. this makes them less appealing for annotating, for example, video, in which the sequence of events determines much of the meaning. this work presents a novel approach for fv representation of sequences using a recurrent neural network (rnn). the rnn is trained to predict the next element of a sequence given the previous elements. it is applied to two different and challenging tasks: video action recognition and image annotation by sentences. several recent works have proposed to use an rnn for sentence representation 44, 1, 31, the recurrent neural network fisher vector (rnn-fv) method differs from these works in that a sequence is represented by using derived gradient from the rnn as features, instead of using a hidden or an output layer of the rnn. in the classification approach, the rnn is trained to predict the following word in the sentence. the regression approach tries to predict the embedding of the following word (i.e. the vgg convolutional neural network (cnn) is used to extract features from the frames of the video and the rnn is trained to predict the embedding of the next frame given the previous ones. the classification application is applicable for predicting a sequence of symbols w1,w2,...,wn that have matching vector representations r(w1) x1, r(w2) x2, ..., r(wn ) xn the rnn predicts the sequence u (w1, w2, , wn ) from the sequence x (x0, x1, denote by m the size of our symbol alphabet, i.e., the number of unique symbols in the input sequences. for flickr30k and coco, no training splits are given, and we use the same splits used by there are three tasks in this benchmark: image annotation, in which the goal is to retrieve, given a query image, the five ground truth sentences; image search, in which, given a query sentence, the goal is to retrieve the ground truth image; and sentence similarity, in which the goal is, given a sentence, to retrieve the other four sentences describing the same image. sec- ond, we notice the competitive performance of the model trained on wikipedia sentences, which demonstrates the generalization power of the rnn-fv, being able to perform well on data different than the one which the rnn was trained on. finally, the combination of rnn-fv with the best model (gmmhglmm) of outperforms the current state of the art on flickr8k, and is competitive on the other datasets. this paper introduces a novel fv representation for sequences that is derived from rnns. the proposed representation is sensitive to the element ordering in the sequence and provides a richer model than the additive bag model typically used for conventional fvs. the rnn-fv representation surpasses the state-of-theart results for video action recognition on two challenging datasets. when used for representing sentences, the rnnfv representation achieves state-of-the-art or competitive results on image annotation and image search tasks. since the length of the sentences in these tasks is usually short and, therefore, the ordering is less crucial, we believe that using the rnn-fv representation for tasks that use longer text will provide an even larger gap between the conventional fv and the rnn-fv. a transfer learning result from the image annotation task to the video action recognition task was shown. the con- ceptual distance between these two tasks makes this result both interesting and surprising. it supports a human development-like way of training, in which visual labeling is learned through natural language, as opposed to, e.g., associating bounding boxes with nouns. while such training was used in computer vision to learn related image to text tasks, and while recently zero-shot action recognition was shown 11, 55, nlp to video action recognition transfer was never shown to be as general as presented here.", "1002": "the rate of publications of scientific papers is increasing and it is almost impossible for re- searchers to keep up with relevant research. in general, there are two com- mon approaches to summarizing scientific papers: citations-based, based on a set of citation sen- tences (nakov et al., 2004; abu-jbara and radev, 2011; yasunaga et al., 2019), and content-based, based on the paper itself (collins et al., 2017; nikola nikolov and hahnloser, 2018). automatic summarization is studied exhaustively for the news domain (cheng and lapata, 2016; see et al., 2017), while summarization of scientific papers is less studied, mainly due to the lack of large-scale training data. in such talks, the presenter (usually a co-author) must describe their paper coherently and concisely (since there is a time limit), provid- ing a good basis for generating summaries. based on this idea, in this paper, we propose a new method, named talksumm (acronym for talk- based summarization), to automatically generate extractive content-based summaries for scientific papers based on video talks. our approach uti- lizes the transcripts of video content of conference talks, and treat them as spoken summaries of pa- pers. then, using unsupervised alignment algo- rithms, we map the transcripts to the correspond- ing papers’ text, and create extractive summaries. table gives an example of an alignment between 1vimeo.com/aclweb icml.cc/conferences/2017/videos a paper and its talk transcript (see table in the appendix for a complete example). summaries generated with our approach can then be used to train more complex and data- demanding summarization models. although our summaries may be noisy (as they are created auto- matically from transcripts), our dataset can easily grow in size as more conference videos are aggre- gated. our main contributions are as follows: (1) we propose a new approach to automatically gener- ate summaries for scientific papers based on video talks; (2) we create a new dataset, that contains summaries for papers from several computer science conferences, that can be used as training data; (3) we show both automatic and human eval- uations for our approach. to our knowl- edge, this is the first approach to automatically cre- ate extractive summaries for scientific papers by utilizing the videos of conference talks. then, via a publicly available asr service6, we extracted transcripts of the speech, and based on the video metadata (e.g., title), we retrieved the correspond- ing paper (in pdf format). we used scienceparse7 to extract the text of the paper, and applied a simple processing in order to filter-out some noise (e.g. the transcript itself cannot serve as a good sum- mary for the corresponding paper, as it constitutes only one modality of the talk (which also consists of slides, for example), and hence cannot stand by itself and form a coherent written text. thus, to create an extractive paper summary based on the transcript, we model the alignment between spo- ken words and sentences in the paper, assuming the following generative process: during the talk, the speaker generates words for describing ver- bally sentences from the paper, one word at each time step. thus, at each time step, the speaker has a single sentence from the paper in mind, and produces a word that constitutes a part of its ver- bal description. the number of words uttered to describe each sentence can serve as importance score, in- dicating the amount of time the speaker spent de- scribing the sentence. data for evaluation we evaluate the quality of our dataset generation method by training an extractive summarization model, and evaluating this model on a human-generated dataset of sci- entific paper summaries. training data using the hmm importance scores, we create four training sets, two with fixed-length summaries (150 and words), and two with fixed ratio between summary and paper lengths (0.3 and 0.4). automatic evaluation table summarizes the results: both gcn cited text spans and talksumm-only models, are not able to obtain better performance than abstract8 however, for the hybrid approach, where the abstract is aug- mented with sentences from the summaries emit- ted by the models, our talksumm-hybrid out- performs both gcn hybrid and abstract. as our goal is to test more comprehensive summaries, we generated summaries composed of sentences (approximately of a long paper). we randomly selected presenters from our corpus and asked them to perform two tasks, given the gen- erated summary of their paper: (1) for each sen- tence in the summary, we asked them to indicate whether they considered it when preparing the talk (yes/no question); (2) we asked them to globally evaluate the quality of the summary (1-5 scale, ranging from very bad to excellent, means good). as for the global task (2), the quality of the sum- maries was on average, with standard deviation of these results validate the quality of our generation method. we propose a novel automatic method to gener- ate training data for scientific papers summariza- tion, based on conference talks given by authors. we show that the a model trained on our dataset achieves competitive results compared to models trained on human generated summaries, and that the dataset quality satisfies human experts. in the future, we plan to study the effect of other video modalities on the alignment algorithm. we hope our method and dataset will unlock new opportu- nities for scientific paper summarization.", "1003": "emotions are an important element of human nature and detecting them in the textual messages written by users has many applications in information retrieval and human-computer interaction a common approach to emotion analysis and modeling is categorization, e.g., according to ekman’s basic emotions; namely, anger, disgust, fear, happiness, sadness, and surprise approaches to categorical emotion classi cation based on text often employ supervised machine learning classi ers, which require labeled training data. currently, two types of datasets labeled with emotions are publicly available: manually labeled, and pseudo-labeled. manual annotation requires high cognitive capabilities of multiple human annotators per sample. however, the task is tedious, time-consuming, and expensive 4, and thus, these datasets are usually small (in the order of thousands of annotated samples). manual annotations are usually applied to domain speci c datasets (e.g., news headlines). to overcome these limitations, pseudo-labeled datasets are gathered from social media platforms where social media posts are explicitly tagged by the author by using the hashtag symbol () or by adding emoticons this tagged data can be used to create large-scale training data labeled with emotions in a non-speci c domain as in given such a dataset (manually or pseudo labeled), it is then common to train a linear classi er based on bag-of-words (bow) 1url/ permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. doi: 10.1145/3121050.3121093 representation: representing text samples as sparse vectors, where each vector entry corresponds to the presence of a speci c feature (such as n-grams, punctuation and other) as reported recently by 11, deep learning is a promising approach for solving nlp tasks including text classi cation. while the aforementioned approach utilizes bow representation and linear classi ers, neural network methods are based on dense vector representations of text samples (word embedding) and are nonlinear. such word embedding representation captures syntactic and semantic knowledge, which can improve the emotion detection task. classi er weights (class): in this approach we calculated a weight function, w (t , e ) for each term t in the training data which indicates its importance in classifying a document as expressing emotion e we did this by rst representing the documents in a bow binary vector representation where we only extracted unigram features. we have experimented with the following weighed average probabilities methods: equal weights ( 0.5), stacking ( is learned by an additional classi er) and precision-based weighting ( re ects the ratio between the macro precision scores for the two classi ers over the training data). for our experiments, we considered the most dominant emotion as the headline label as in fairy tales includes sentences from fairy tales, labeled with ve emotions by six annotators. for our experiments, we used only sentences with high annotation agreement of four identical emotion labels, as in blog posts consists of emotion-rich sentences collected from blogs labeled with emotions by four annotators. we considered only sentences for which the annotators agreed on the emotion category, as in customer support dialogs in twitter consists of customer turns from customer support dialogs in twitter, labeled by ve annotators with nine emotions relevant to customer care. we compared the quality of two publicly available pre-trained word vector sources, based on glove4 and word2vec (googlenews)5, in terms of emotion detection performance. table depicts the macro f1-scores for each dataset and each document representation method that is based on word vectors, as detailed in section results show that for all datasets and representation methods, word vectors trained by glove achieved higher performance than using word2vec based vectors. table depicts the macro f1-scores for each dataset, and for the di erent models: bow is our baseline, presented in section en-cbow, en-tfidf and en-class are the ensemble models of bow and the corresponding embedded representations presented in section our ensemble methods outperformed the baseline for each document representation method. the best result, which is also signi - cantly better for each dataset, is of en-class model that achieved an average relative improvement of in f1-score over all datasets. these results indicate the advantage in combining both bow and embedded document representations for emotion detection from text. this work studied the use of pre-trained word vectors for emotion detection. we presented class, a novel method for representing a document as a dense vector based on the importance of the document’s terms in respect to emotion classi cation. our results show that an ensemble that combines bow and embedded representations using our class method, outperforms previous approaches for domain-speci c datasets. in comparison to other deep-learning methods, our approach ts a small number of model parameters and requires little computing power. for future work we plan to investigate the use of deep learning models trained on domain adapted pseudo-labeled large-scale datasets. we also plan to investigate transfer learning for multidomain emotion detection.", "1004": "an interesting use case for social media is customer support that can now take place over public social media channels. using this medium has its advantages as described, for example, in (demers, 2014): customers appreciate the simplicity and immediacy of social media conversations, the ability to reach real human beings, the transparency, and the feeling that someone listens to them. a recent study shows that one in five (23) customers in the u.s. say they have used social media for customer service in 2014, up from in obviously, companies hope that such 1url/ news/docs/2014x/2014-global-customer- uses are associated with a positive experience. in this paper, we analyze customer support dialogues using the twitter platform and show the utility of such analyses. emotions are a cardinal aspect of inter-personal communication: they are an implicit or explicit part of essentially any communication, and of particular importance in the setting of customer service, as they relate directly to customer satisfaction and experience (oliver, 2014). typical emotions expressed by customers in the context of social media service dialogues include anger and frustration, as well as gratitude and more (gelbrich, 2010). the analysis of emotions being expressed in customer support conversations can take two applications: (1) to discern and compute quality of service indicators and (2) to provide real-time clues to customer service agents regarding the cus- service-barometer-us.pdf tomer emotion expressed in a conversation. a possible application here is recommending to customer service agents what should be their emotional response (for example, in each situation, should they apologize, should they thank the customer, etc.) another interesting trend in customer service, in addition to the use of social media described above, is the automation of various functions of customer interaction. several companies are developing text-based chat agents, typically accessible through corporate web sites, and partially automatized: in these platforms, a computer program handles simple conversations with customers, and more complicated dialogues are transferred to a human agent. the automation in such systems helps save human resources and, with further development based on artificial intelligence, more automation in customer service chats is likely to appear. given the importance of emotions in service dialogues, such systems will benefit from the ability to detect (customer) emotions and will need to guide employees (and machines) regarding the right emotional technique in various situations (e.g., apologizing at the right point). thus, our goal, in this paper, is to show that the functionality of guiding employees regarding appropriate responses can be developed based on the analysis of textual dialogue data. we show first that it is possible to automatically detect emotions being expressed and, second that it is possible to predict the emotional technique that is likely to be used by a human agent in a given situation. this analysis reflects our ultimate goal: to enable a computer system to discern the emotions expressed by human customers, and to develop computerized tools that mimic the emotional technique used by a human customer service agent in a particular situation. we see the main contributions of this paper as follows: (1) to our knowledge, this is the first research focusing on automatic analysis of emotions expressed in customer service provided through social media. (2) this is the first research using unique dialogue features (e.g., emotions expressed in previous dialogue turns by the agent and customer, time between dialogue turns) to improve emotion detection. each turn ti is a tuple consisting of turn number, timestamp, content where turn number represents the sequential position of the turn in the dialogue, timestamp captures the time the message was published on twitter, and content is the textual message. we extracted this data from december until june specifically, for each customer that posted a tweet to the customer support accounts, we searched for the previous, if any, turn to which it replied. this difference stems from the fact that in order to train an automated service agent to respond based on customer input, the agent’s emotional technique needs to be computed before the agent generates its response sentence. based on the families of feature sets that we defined in the methodology section, we tested the performance of different feature set combinations in our models, added in the following order: baseline (textual features), emotional, temporal and integral. in this work we studied emotions being expressed in customer service dialogues in the social media. specifically, we described two classification tasks, one for detecting customer emotions and the other for predicting the emotional technique used by support service agent. we have proposed two different models (svm dialogue and svm-hmm dialogue models) for these tasks. we studied the impact of dialogue features and dialogue history on the quality of the classification and showed improvement in performance for both models and both classification tasks. we also showed the robustness of our models across different data sources. as for future work we plan to work on several aspects: (1) in this work, we showed that it is possible to predict the emotional technique. in the future, we plan to run experiments in which the predicted emotional technique is actually applied in the context of new dialogues to measure the effect of such predictions on real support dialogues. (2) distinguish between dialogues that have positive outcomes (e.g., high customer satisfaction) and others.", "1005": "recently, there has been a lot of exciting progress in the field of few-shot learning in general, and in few-shot classification (fsc) in particular. a popular method for approaching fsc is meta-learning, or learning-to-learn. in meta-learning, the inputs to both train and test phases are not images, but instead a set of few-shot tasks, ti, each k-shot / n -way task containing a small amount k (usually 1-5) of labeled support images and some amount of unlabeled query images for each of the n categories of the task. the goal of meta-learning is to find a base model that is easily adapted to the specific task at hand, so that it will generalize well to tasks built from novel unseen categories and fulfill the goal of fsc (see section for further review). equal contributors corresponding authors: sivan doveh sivan.dovehibm.com and leonid karlinsky leonidkail.ibm.com ar x iv :1 2v cs .c v m ar many successful meta-learning based approaches have been developed for fsc 60,55,13,39,51,41,29 advancing its state-of-the-art. one of such major factors is the cnn backbone architecture at the basis of all the modern fsc methods. carefully reviewing and placing on a single chart the test accuracies of top-performing fsc approaches w.r.t. in light of the above, in this paper we set to explore methods for architecture search, their meta-adaptation and optimization for fsc. neural architecture search (nas) is a very active research field that has contributed significantly to overall improvement of the state of the art in supervised classification. to summarize, our contributions in this work are as follows: (1) we show that darts-like bi-level iterative optimization of layer weights and network connections performs well for few-shot classification without suffering from overfitting due to over-parameterization; (2) we show that adding small neural networks, metadapt controllers, that adapt the connections in the main network according to the given task further (and significantly) improves performance; (3) using the proposed method, we obtain improvements over fsc state-of-the-art on two popular fsc benchmarks: miniimagenet and fc100 these approaches include: (i) semi-supervised approaches using additional unlabeled data 9,14; (ii) fine tuning from pre-trained models 31,62,63; (iii) applying domain transfer by borrowing examples from relevant categories or using semantic vocabularies 3,15; (iv) rendering synthetic examples 42,10,56; (v) augmenting the training examples using geometric and photometric transformations or learning adaptive augmentation strategies 21; (vi) example synthesis using generative adversarial networks (gans) 69,25,20,48,45,35,11,23,2. in 22,54 additional examples are synthesized via extracting, encoding, and transferring to the novel category instances, of the intra-class relations between pairs of instances of reference categories. we introduce the task-adaptable block, it has a graph structure with adaptable connections that can modulate the architecture, adapting it to the few-shot task at hand. we then describe the sub-models, metadapt controllers, that predict the change in connectivity that is needed in the learned graph as a function of the current task. the list of search space operations used in our experiments is provided in table this list includes the zero-operation and identity-operation that can fully or partially (depending on the corresponding (i,j) o ) cut the connection or make it a residual one (skip-connection). darts, at search time the training is done on the full model at each iteration where each edge is a weighted-sum of its operations according to i,j contrarily, in snas i,j are treated as probabilities of a multinomial distribution and at each iteration a single operation is sampled accordingly. so at each iteration only a single operation per edge affects the classification outcome and only this operation is be updated in the gradient descent backward step. at training time, instead of the mixed operation defined in equation 1, we define the mixed operation to be: oi,j(x) oo zi,jo o(x) (8) where z(i,j) is a continuous approximation of a one-hot vector sampled from a gumbel distribution: zi,j gumbel(i,j). (9) here i,j are after softmax normalization and summed to at test time, rather than the one-hot approximation, we use the operation with the top probability zi,jk 1, if k argmax(i,j) 0, otherwise (10) using this method we get better results for fc100 1-shot and comparable results for 5-shot, compared to vanilla metaoptnet. however, it does not perform as well as the non-stochastic version of metadapt. in this work we have proposed metadapt, a few-shot learning approach that enables meta-learned network architecture that is adaptive to novel few-shot tasks. the proposed approach effectively applies tools from the neural architecture search (nas) literature, extended with the concept of metadapt controllers’, in order to learn adaptive architectures. these tools help mitigate over-fitting to the extremely small data of the few-shot tasks and domain shift between the training set and the test set. we demonstrate that the proposed approach successfully improves state-of-the-art results on two popular few-shot benchmarks, miniimagenet and fc100, and carefully ablate the different optimization steps and design choices of the proposed approach. some interesting future work directions include extending the proposed approach to progressively searching the full network architecture (instead of just the last block), applying the approach to other few-shot tasks such as detection and segmentation, and researching into different variants of task-adaptivity including global connections modifiers and inter block adaptive wiring.", "1006": "automated conversational agents (chatbots) are becoming widely used for various tasks such as personal assistants or as customer service agents. recent studies project that of businesses plan to use chatbots by 20201, and that chatbots will power of customer service interactions by the year this increasing usage is mainly due to advances in artificial intelligence and natural language processing (hirschberg and manning, 2015) 1url 2url along with increasingly capable chat development environments, leading to improvements in conversational richness and robustness. still, chatbots may behave extremely badly, leading to conversations so off-the-mark that only a human agent could step in and salvage them. for example, incomplete or internally inconsistent training data can lead to false classification of user intent. consider, for example, the anonymized but representative conversation depicted in figure here the customer aims to understand the details of a flight ticket. in the first two turns, the chatbot misses the customer’s intentions, which leads to the customer asking are you a real person?. the customer then tries to explain what went wrong, but the chatbot has insufficient exposure to this sort of utterance to provide anything but the default response (i’m not trained on that). the response seems to upset the customer and leads to a request for a human agent, which is rejected by the system (we don’t currently have live agents). being able to automatically detect such conversations, either in real time or through log analysis, could help to improve chatbot quality. as an aid to chatbot improvement, analysis of egregious conversations can often point to problems in training data or system logic that can be repaired. specifically, we consider customer inputs throughout a whole conversation, and detect cues such as rephrasing, the presence of heightened emotions, and queries about whether the chatbot is a human or requests to speak to an actual human. we focused on negative emotions (denoted as neg emo) to identify turns with a negative emotional peak (i.e., single utterances that carried high negative emotional state), as well as to estimate the aggregated negative emotion throughout the conversation (i.e., the averaged negative emotion intensity). given the full conversation, each judge tagged whether the conversation was egregious or not following this guideline: conversations which are extraordinarily bad in some way, those conversations where you’d like to see a human jump in and save the conversation. we also studied how robust our features were: if our features generalize well, performance should not drop much when testing company b with the classifier trained exclusively on the data from company a. although company a and company b share similar conversation engine platforms, they are completely different in terms of objectives, domain, terminology, etc. specifically, in our setting, the relevant motivations are12: (1) natural language understanding (nlu) error - the agent’s intent detection is wrong, and thus the agent’s response is semantically far from the customer’s turn; (2) language generation (lg) limitation - the intent is detected correctly, but the customer is not satisfied by the response (for example, the response was too generic); (3) unsupported intent error - the customer’s intent is not supported by the agent. in order to detect nlu errors, we measured the similarity between the first customer turn claiming that the best answer given by the system has the highest similarity value between the customer turn and the agent answer. to detect unsupported intent error we used the approach described in section as reported in table 4, rephrasing due to an unsupported intent is more common in egregious conversations (18 vs. 14), whereas, rephrasing due to generation limitations (lg limitation) is more common in 12we did not consider other motivations like automatic speech recognition (asr) errors, fallback to search, and backend failure as they are not relevant to our setting. we further investigated why the egr model was better at identifying egregious conversations (i.e., its recall was higher compared to the baseline models). we manually examined egregious conversations that were identified justly so by the egr model, but misclassified by the other models. those conversations were particularly prevalent with the agent’s difficulty to identify correctly the user’s intent due to nlu errors or lg limitation. we did not encounter any unsupported intent errors leading to customer rephrasing, which affected the ability of the rule-based model to classify those conversations as egregious. in addition, the customer intents that appeared in those conversations were very diverse. while customer rephrasing was captured by the egr model, for the text-based model some of the intents were new (did not appear in the training data) and thus were difficult for the model to capture. in this paper, we have shown how it is possible to detect egregious conversations using a combination of customer utterances, agent responses, and customer-agent interactional features. as explained, the goal of this work is to give developers of automated agents tools to detect and then solve problems cre- ated by exceptionally bad conversations. in this context, future work includes collecting more data and using neural approaches (e.g., rnn, cnn) for analysis, validating our models on a range of domains beyond the two explored here. we also plan to extend the work to detect egregious conversations in real time (e.g., for escalating to a human operators), and create log analysis tools to analyze the root causes of egregious conversations and suggest possible remedies.", "1007": "convolutional neural networks as well as other traditional natural language processing , even when considering relatively simple one-layer models (kim, 2014). the ability to interpret neural models can be used to increase trust in model predictions, analyze errors or improve the model the problem of interpretability in machine learning can be divided into two concrete tasks: given a trained model, model interpretability aims to supply a structured explanation which captures what the model has learned. given a trained model and a single example, prediction interpretability aims to explain how the model arrived at its prediction. accompanying their rising popularity, cnns have seen multiple advances in interpretability when used for computer vision tasks , which is likely different than the role it has when processing text. in this work, we examine and attempt to understand how cnns process text, and then use this information for the more practical goals of improving model-level and prediction-level explanations. we identify and refine current intuitions as to how cnns work. specifically, current common wisdom suggests that cnns classify text by working through the following steps (goldberg, 2016): 1) 1-dimensional convolving filters are used as ngram detectors, each filter specializing in a closely-related family of ngrams. for our empirical experiments and results presented in this work we use three text classification datasets for sentiment analysis, which involves classifying the input text (user reviews in all cases) between positive and negative. however, as we cannot measure directly which values pj influence the final decision, we opt instead for measuring correlation between pj values and the predicted label for the vector p. the linearity of the decision function wp allows to measure exactly how much pj is weighted for the logit of label class k. the class which filter fj contributes to is cj argmaxk wkj we refer to class cj as the class identity of filter fj by assigning each filter a class identity cj and comparing it to the predicted label we arrive at a correlation labelwhether the filter’s identity class matches the final decision by the network. we consider two hypotheses to explain this behavior: (i) each filter captures multiple semantic classes of ngrams, and each class has some dominating slots and some non-dominating slots (which we define as a slot activation pattern). in order to identify case negative ngrams, we heuristically test whether the changed words’ scores directly influence the status of the activation relative to the threshold: given an already identified negative ngram, if the ngram scoresans the bottom-k negative slot activations (considering a hamming distance of k and given that there are k negative slot activations)passes the threshold, yet it does not pass the threshold by including the negative slot activations, then the ngram is considered a case negative ngram. in this section we show two practical implications of the findings above: improvements in both model-level and prediction-level interpretability of 1d cnns for text classification. as in computer vision, we can now interpret a trained cnn model by visualizing its filters and interpreting the visible shapesin other words, defining a high-level description of what the filter detects. we propose to associate each filter with the following items: 1) the class which this filter’s strong signals contribute to (in the sentiment task: positive or negative); 2) the threshold value for the filter, together with its purity and coverages percentages (which essentially capture how informative this filter is); 3) a list of semantic patterns identified by this filter. for each cluster we present the top-k ngrams activating it, and for each ngram we specify its total activation, its slot-activation vector, and its list of bottom-k negative ngrams with their activations and slot activations. in particular, by clustering the activated ngrams according to their slot activation patterns and showing the top-k in each clusters, we get a much more refined coverage of the linguistic patterns that are captured by the filter. finally, we can also mark cases of negative-ngrams (section 5.4), where an ngram has high slot activations for some words, but these are negated by a highly-negative slot and as a consequence are not selected by max-pooling, or are selected but do not pass the filter’s threshold. first, we have shown that maxpooling over time induces a thresholding behavior on the convolution layer’s output, essentially separating between features that are relevant to the final classification and features that are not. we also associate each filter with the class it contributes to. we decompose the ngram score into word-level scores by treating the convolution of a filter as a sum of word-level convolutions, allowing us to examine the word-level composition of the activation. specifically, by maximizing the word-level activations by iterating over the vocabulary, we observed that filters do not maximize activations at the word-level, but instead form slot activation patterns that give different types of ngrams similar activation strengths. this provides empirical evidence that filters are not homogeneous. by clustering high-scoring ngrams according to their slotactivation patterns we can identify the groups of linguistic patterns captured by a filter. we also show that filters sometimes opt to assign negative values to certain word activations in order to cause the ngrams which contain them to receive a low score despite having otherwise highly activating words. finally, we use these findings to suggest improvements to model-based and predictionbased interpretability of cnns for text.", "1008": "automatic text summarizers condense a given piece of text into a shorter version (the summary). this is done while trying to preserve the main essence of the original text and keeping the generated summary as readable as possible. existing summarization methods can be classified into two main types, either extractive or abstractive. extractive methods select and order text fragments (e.g., sentences) from the original text source. such methods are relatively simpler to develop and keep the extracted fragments untouched, allowing to preserve important parts, e.g., keyphrases, facts, opinions, etc. abstractive methods apply natural language paraphrasing and/or compression on a given text. a common approach is based on the encoder-decoder , with the original text sequence being encoded while the summary is the decoded sequence. work was done during a summer internship in ibm research ai while such methods usually generate summaries with better readability, their quality declines over longer textual inputs, which may lead to a higher redundancy moreover, such methods are sensitive to vocabulary size, making them more difficult to train and generalize a common approach for handling long text sequences in abstractive settings is through attention mechanisms, which aim to imitate the attentive reading behaviour of humans two main types of attention methods may be utilized, either soft or hard. soft attention methods first locate salient text regions within the input text and then bias the abstraction process to prefer such regions during decoding on the other hand, hard attention methods perform abstraction only on text regions that were initially selected by some extraction process compared to previous works, whose final summary is either entirely extracted or generated using an abstractive process, in this work, we suggest a new idea of editorial network (editnet) a mixed extractive-abstractive summarization approach. using the cnn/dailymail dataset we demonstrate that, editnet’s summarization quality is highly competitive to that obtained by both state-of-the-art abstractive-only and extractive-only baselines. editnet is applied as a post-processing step over a given input summary whose sentences were initially selected by some extractor. let s denote a summary which was extracted from a given text (document) d. the editorial process is implemented by iterating over sentences in s according to the selection order of the extractor. the first decision is to keep the extracted sentence untouched (represented by label e in figure 1). as another example, based on the interaction between both sentence versions with either of the local or global contexts (and possibly among the last two), the network may learn that both sentence versions may only add superfluous or redundant information to the summary, and therefore, decide to reject both. the second is a sentence selector using a pointernetwork for the latter, let p (s) be the selection likelihood of sentence s. the abstractor of yet, instead of applying it directly only on a single given extracted sentence sei s, we apply it on a chunk of three consecutive sentences2 (se, s e i , s e ), where s e and s e denote the sentence that precedes and succeeds sei in d, respectively. we trained, validated and tested our approach using the non-annonymized version of the cnn/dailymail dataset following , we used the story highlights associated with each article as its ground truth summary. this includes the extractor (rnn-ext-rl) and abstractor (rnn-ext-absrl) components of (chen and bansal, 2018) that we utilized for implementing editnet we further report the quality of editnet when it was being enforced to take an extract-only or abstract-only decision, denoted hereinafter as editnete and editneta, respectively. overall, editnet provides a highly competitive summary quality, where it outperforms most baselines. interestingly, editnet’s summarization quality is quite similar to that of neusum yet, while neusum applies an extraction-only approach, summaries generated by editnet include a mixture of sentences that have been either extracted or abstracted. two models outperform editnet, bertsum the bertsum model gains an impressive accuracy, yet it is an extractive model that utilizes many attention layers running in parallel with millions of parameters dca gains a comparable quality to editnet, it outperforms on r-2 and slightly on r-1. the contextual encoder of dca is comprised of several lstm layers one on top of the other with varied number of agents (hyper-tuned) that transmit messages to each other. considering the complexity of these models, and the slow down that can incur during training and inference, we think that editnet still provides a useful, high quality and relatively simple extension on top of standard encoder aligned decoder architectures. on average, and of editnet’s decisions were to abstract (a) or reject (r), respectively. moreover, on average, per summary, editnet keeps only of the original (extracted) sentences, while the rest (67) are abstracted ones. this demonstrates that, editnet has a high capability of utilizing abstraction, while being also able to maintain or reject the original extracted text whenever it is estimated to provide the best benefit for the summary’s quality. we have proposed editnet a novel alternative summarization approach that instead of solely applying extraction or abstraction, mixes both together. moreover, editnet implements a novel sentence rejection decision, allowing to correct initial sentence selection decisions which are predicted to negatively effect summarization quality. as future work, we plan to evaluate other alternative extractor-abstractor configurations and try to train the network end-to-end. we further plan to explore reinforcement learning (rl) as an alternative decision making approach.", "1009": "performing the mental gymnastics of transforming i’m hear’ to i’m here,’ or, i can’t so buttons’ to i can’t sew buttons,’ is familiar to anyone who has encountered autocorrected text messages, punny social media posts, or just friends with bad grammar. although at first glance it may seem that phonetic similarity can only be quantified for audible words, this problem is often present in purely textual spaces, such as social media posts or text messages. incorrect homophones and synophones, whether used in error or in jest, pose challenges for a wide range of nlp tasks, such as named entity identification, text normalization and spelling correction these tasks must therefore successfully transform incorrect words or phrases (hear’,’so’) to their phonetically similar correct counterparts (’here’,’sew’), which in turn requires a robust representation of phonetic similarity between word pairs. a reli- able approach for generating phonetically similar words is equally crucial for chinese text unfortunately, most existing phonetic similarity algorithms such as soundex (archives and administration, 2007) and double metaphone (dm) philips (2000) are motivated by english and designed for indo-european languages. words are encoded to approximate phonetic presentations by ignoring vowels (except foremost ones), which is appropriate where phonetic transcription consists of a sequence of phonemes, such as for english. in contrast, the speech sound of a chinese character is represented by a single syllable in pinyin consisting of two or three parts: an initial (optional), a final or compound finals, and tone (table 1). as a result, phonetic similarity approaches designed for indo-european languages often fall short when applied to chinese text. note that we use pinyin as the phonetic representation because it is a widely accepted romanization system (san, 2007; iso, 2015) of chinese syllables, used to teach pronunciation of standard chinese. table shows two sentences from chinese microblogs, containing informal words derived from phonetic transcription. near-homonyms of from table are shown in table since both dm and soundex ignore vowels and tones, words with dissimilar pronunciations are incorrectly assigned to the same encoding (e.g. and ), while true nearhomonyms are encoded much further apart (e.g. on the other hand, additional candidates with similar phonetic distances such as xin1fan2xi1fang1 for should be generated, for consumption by downstream applications such as text normalization. this paper presents dimsim, a learned ndimensional phonetic encoding for chinese along with a phonetic similarity algorithm, which uses the encoding to generate and rank phonetically similar words. to address the complexity of relative phonetic similarities in pinyin components, we propose a supervised learning approach to learn n dimensional encodings for finals and initials where n can be easily extended from one to two or higher dimensions. the learning model derives accurate encodings by jointly considering pinyin linguistic characteristics, such as place of articulation and pronunciation methods, as well as high quality annotated training data sets. we compare dimsim to double metaphone(dm), minimum edit distance(med) and aline demonstrating that dimsim outperforms these algorithms by 7.5x on mean reciprocal rank, 1.4x on precision and 1.5x on recall on a real-world dataset. we t n use ed t distance and common sequence length con traints to g ide the pair generation; specifically, we compare a pair of finals if the edit distance between them is or since the length of finals on average is two, an edit distance of three means a complete change to the final, resulting in pairs with the lowest similarity. for each word pair, the annotators give a label on a point scale representing their agreement, where the labels range from ’completely disagree’ (1) to ’completely agree’ (7). the distance sp of a pair p of points (x1, x2, ..., xn), (y1, y2, ..., yn) is calculated using euclidean distance as shown in equation sp 1in (xi yi)2 (3) the model aims to minimize the sum of the absolute differences between the euclidean distances of component pairs and the average distances obtained from the annotated training data across all pairs for initials (or finals) c. we also incorporate a penalty function, p, for pairs deviating from the manually annotated distance so that more phonetically similar pairs are penalized more highly (we discuss further in section 3.2). having determined the phonetic encodings and the mechanism to compute the phonetic similarity using learned phonetic encodings, we now describe how to generate and rank similar candidates in algorithm given a word w, a similarity threshold th, and a chinese pinyin dictionary dict, we retrieve the pinyin py of w from dict. motivated by phonetic transcription as a widely observed phenomenon in chinese social media and informal language, we have designed an accurate phonetic similarity algorithm. dimsim generates phonetically similar candidate words based on learned encodings that capture the pronunciation characteristics of pinyin initial, final, and tone components. using a real world dataset, we demonstrate that dimsim effectively improves mrr by 7.5x , recall by 1.5x and precision by 1.4x over existing approaches. the original motivation for this work was to improve the quality of downstream nlp tasks, such as named entity identification, text normalization and spelling correction. these tasks all share a dependency on reliable phonetic similarity as an intermediate step, especially for languages such as chinese where incorrect homophones and synophones abound. we therefore plan to extend this line of work by applying dimsim to downstream applications, such as text normalization.", "1010": "kb query, which can be executed to retrieve the answers from a kb. figure illustrates the process used to parse two sample questions in a kbqa system: ; and (b) a more complex case, where some constraints need to be handled for multiple entities in the question. the kbqa system in the figure performs two key tasks: (1) entity linking, which links n-grams in questions to kb entities, and (2) relation detection, which identifies the kb relation(s) a question refers to. the main focus of this work is to improve the relation detection subtask and further explore how it can contribute to the kbqa system. first, in most general relation detection tasks, the number of target relations is limited, normally smaller than in contrast, in kbqa even a small kb, like freebase2m , contains more than 6,000 relation types. given an input question and a set of candidate entities retrieved by an entity linker based on the question, our proposed relation detection model plays a key role in the kbqa process: (1) re-ranking the entity candidates according to whether they connect to high confident relations detected from the raw question text by the relation detection model. the first-layer of bilstm works on the word embeddings of question words q q1, , qn and gets hidden representations (1) 1:n (1) ; ; (1) n the second-layer bilstm works on (1)1:n to get the second set of hidden representations (2)1:n since the second bilstm starts with the hidden vectors from the first layer, intuitively it could learn more general and abstract information compared to the first layer. now we have question contexts of different lengths encoded in (1)1:n and (2) 1:n unlike the standard usage of deep bilstms that employs the representations in the final layer for prediction, here we expect that two layers of question representations can be complementary to each other and both should be compared to the relation representation space (hierarchical matching). then we generate the kb queries for q following the four steps illustrated in algorithm algorithm 1: kbqa with two-step relation detection input : question q, knowledge base kb, the initial top-k entity candidates elk(q) output: top query tuple (e, r, (c, rc)) entity re-ranking (first-step relation detection): use the raw question text as input for a relation detector to score all relations in the kb that are associated to the entities in elk(q); use the relation scores to re-rank elk(q) and generate a shorter list el0k0(q) containing the top-k0 entity candidates (section 5.1) relation detection: detect relation(s) using the reformatted question text in which the topic entity is replaced by a special token e (section 5.2) query generation: combine the scores from step and 2, and select the top pair (e, r) (section 5.3) constraint detection (optional): compute similarity between q and any neighbor entity c of the entities along r (connecting by a relation rc) , add the high scoring c and rc to the query (section 5.4). , we use s-mart (yang and chang, 2015) entity-linking outputs.7 in order to evaluate the relation detection models, we create a new relation detection task from the webqsp data set.8 for each question and its labeled semantic parse: (1) we first select the topic entity from the parse; and then (2) select all the relations and relation chains (length 2) connected to the topic entity, and set the corechain labeled in the parse as the positive label and all the others as the negative examples. in order to highlight the effect of different relation detection models on the kbqa end-task, we also implemented another baseline that uses our kbqa system but replaces hr-bilstm with our implementation of ampcnn (for simplequestions) or the char-3-gram bicnn (for webqsp) relation detectors (second block in table 3). compared to the baseline relation detector (3rd row of results), our method, which includes an improved relation detector (hr-bilstm), improves the kbqa end task by 2-3 (4th row). since the reranking step relies on the relation detection models, this shows that our hr-bilstm model contributes to the good performance in multiple ways. this is probably because our joint performance on topic entity and core-chain detection is more accurate (77.5 top-1 accuracy), leaving a huge potential (77.5 vs. 58.0) for the constraint detection module to improve. finally, like stagg, which uses multiple relation detectors for the three models used), we also try to use the top-3 relation detectors from section as shown on the last row of table 3, this gives a significant performance boost, resulting in a new state-of-the-art result on simplequestions and a result comparable to the state-of-the-art on webqsp. kb relation detection is a key step in kbqa and is significantly different from general relation extraction tasks. we propose a novel kb relation detection model, hr-bilstm, that performs hierarchical matching between questions and kb relations. our model outperforms the previous methods on kb relation detection tasks and allows our kbqa system to achieve state-of-the-arts. for future work, we will investigate the integration of our hr-bilstm into end-to-end systems. for example, our model could be integrated into the decoder in , to provide better sequence prediction. we will also investigate new emerging datasets like graphquestions (su et al.,", "1011": "dictionary expansion is one area where close integration of humans into the discovery loop has been shown to enhance task performance substantially over more traditional post-adjudication. this is not surprising, as dictionary membership is often a fairly subjective judgment (e.g., should a fruit dictionary include tomatoes?) thus even with a system which finds similar terms (e.g., word2vec) guidance is important to keep the system focused on the subject matter expert’s notion of lexicon. in this work we propose a feature agnostic approach for dictionary expansion based on lightweight neural language models, such as word2vec to prevent semantic drift during the dictionary expansion, we effectively include humanin-the-loop (huml). given an input text corpus and a set of seed examples, the proposed approach runs in two phases, explore and exploit, to identify new potential dictionary entries. the explore phase tries to identify similar instances to the dictionary entries that are present in the input text corpus, using term vectors from the neural language model to calculate a similarity score. the exploit phase tries to construct more complex multi-term phrases based on the instances already in the input dictionary. multi-term phrases are a challenge for word2vec style systems as they need to be known prior to model creation. to identify multi-term phrases, most commonly a simple phrase detection model is used, which is based on a term’s co-occurrence score, i.e., terms that often appear together probably are part of the same phrase the phrase detection must be done before the model is built, and they remain unchanged after the model is built. for example, valid phrase combinations may simply not occur (e.g., acute joint pain may appear in the sample corpus, but for some reason chronic hip pain may not). however, these phrases are likely to occur in future texts from the same source, and thus are important to include in any entity extraction lexicon. in the exploit phase, the approach generates new phrases by analyzing the single terms of the instances in the input dictionary. we use two phrase generation algorithms: (i) modify the phrases by replacing single terms with similar terms from the text corpus, e.g., abnormal behavior can be modified to strange behavior; (ii) extend the instances with terms from the text corpus that are related to the terms in the instance, e.g., abnormal blood clotting problems is a an adverse drug reaction, which doesn’t appear as such in a large text corpus, however the instances abnormal blood count, blood clotting and clotting problems appear several times in the corpus, which can be used to build the more complex instance. formally, the similarity between two terms w1 and w2, with vectors v1 and v2, is calculated as the cosine similarity between the vectors v1 and v2: sim(w1, w2) v1 v2 v1 v2 (1) we calculate the similarity between the instances in the input dictionary and all the words in the corpus vocabulary vtc we sort the vocabulary in descending order using the cumulative similarity score, and choose the top-n candidates to present to the huml. in the first approach, we first break each instance in to a set of single terms t t1, t2, ..., tn, then for each term ti in t we identify a set of similar terms tsti ts1, ts2, ..., tss in the vocabulary vtc using equation in the next step, we build new phrases by replacing ti with a term tsi from tsti the new phrases are sorted based on the similarity score and the top-n are selected as candidates. as before, we first break each instance in to a set of single terms t t1, t2, ..., tn, then for each term ti in t we identify a set of similar terms trti tr1, tr2, ..., trr in the vocabulary vtc using equation in the next step, we build new phrases by appending a term tri from trti to each term ti from t the new phrases are sorted based on the relatedness score and the top-n are selected as candidates. in this experiment we show the importance of the promptness of the huml on the number of newly discovered instances, i.e., we evaluate if the user gives their feedback to the system sooner it will improve the performance of the system. the feedback interval indicates how many candidates the system needs to identify before the user gives their feedback to the system. after iterations the system discovered new dictionary entries, compared to only new entries when using examples feedback interval. that yields improvement in effectivness of the system. this paper proposes an interactive dictionary expansion tool using a lightweight neural language model. our algorithm is iterative and purely statistical, hence does not require any feature extraction beyond tokenization. it incorporates human feedback to improve performance and control semantic drift at every iteration cycle. the experiments showed high importance of tight huml integration on discovery efficiency. in this work, we have considered only lightweight language models, which can be efficiently built and updated on large text corpora. in future work, we will analyze more complex language neural network models, such as recurrent neural networks (rnn), long short term memory networks (lstm), and bidirectional lstm, which might improve the search for similar and related terms, at the expense of higher training time. furthermore, future work will include an evaluation of the approach on multiple datasets covering different domains.", "1012": "grounding of textual phrases, i.e., finding bounding boxes in images which relate to textual phrases, is an important problem for human-computer interaction, robotics and mining of knowledge bases, three applications that are of increasing importance when considering autonomous systems, augmented and virtual reality environments. for example, we may want to guide an autonomous system by using phrases such as the bottle on your left,’ or the plate in the top shelf.’ while those phrases are easy to interpret for a human, they pose significant challenges for present day textual grounding algorithms, as interpretation of those phrases requires an understanding of objects and their relations. more specifically, deep net models are designed to extract features from given bounding boxes and textual data, which are then compared to measure their fitness. while being easy to obtain, automatic extraction of region proposals is limiting, because the performance of the visual grounding is inherently constrained by the quality of the proposal generation procedure. in this work we describe an interpretable mechanism which additionally alleviates any issues arising due to a limited number of region proposals. our approach is based on a number of image concepts’ such as semantic segmentations, detections and priors for any number of objects of interest. second bike from right in front painting next to the two on theleft person all the way to the right figure 1: results on the test set for grounding of textual phrases using our branch and bound based algorithm. moreover, linear combination of score maps reveals importance of score maps for specific queries as well as similarity between queries such as skier’ and snowboarder.’ hence the framework that we discuss in the following is easy to interpret and extend to other settings. general problem formulation: for simplicity we use x to refer to both given input data modalities, i.e., x (q, i), with query text, q, and image, i we will differentiate them in the narrative. in addition, we define a bounding box y via its top left corner (y1, y2) and its bottom right corner (y3, y4) and subsume the four variables of interest in the tuple y (y1, every integral coordinate yi, i 1, , yi,max, and y denotes the product space of all four coordinates. for notational simplicity only, we assume all images to be scaled to identical dimensions, i.e., yi,max is not dependent on the input data x. we obtain a bounding box prediction y given our data x, by solving the energy minimization y arg min yy e(x, y, w), (1) to global optimality. energy function details: our energy function e(x, y, w) is based on a set of image concepts,’ such as semantic segmentation of object categories, detections, or word priors, all of which we subsume in the set c. importantly, all image concepts c c are attached a parametric score map c(x,wr) rwh following the image width w and height h note that those parametric score maps may depend nonlinearly on some parameters wr. language processing: in order to process free-form textual phrases efficiently, we restricted the vocabulary size to the top most frequent words in the training set for the referitgame, and to the top most frequent training set words for flickr 30k entities; both choices cover about of all phrases in the training set. for detection, we use the yolo object detection system 37, to extract categories, trained on pascal voc-2012, and trained on mscoco for pose estimation, we use the system from to extract the body part location, then post-process to get the head, upper body, lower body, and hand regions. expected groups of words form, for example (bicycle, bike), (camera, cellphone), (coffee, cup, drink), (man woman), (snowboarder, skier). the word vectors capture image-spatial relationship of the words, meaning items that can be replaced in an image are similar; (e.g., a snowboarder can be replaced with a skier and the overall image would still be reasonable). the inference speed can be divided into three main parts, (1) extracting image features, (2) extracting language features, and (3) computing scores. for extracting image features, grounder requires a forward pass on vgg16 for each image region, where cca and our approach requires a single forward pass which can be done in ms. for extracting language features, our method requires index lookups, which takes negligible amount of time (less than 1e-6 ms). cca, uses word2vec for processing the text, which takes ms. grounder uses a long-short-term memory net, which takes ms. computing the scores with our c implementation takes 1.05ms on a cpu. cca needs to compare projections of the text and image features, which takes 13.41ms on a gpu and 609ms on a cpu. grounder uses a single fully connected layer, which takes ms on a gpu. we demonstrated a mechanism for grounding of textual phrases which provides interpretability, is easy to extend, and permits globally optimal inference. in contrast to existing approaches which are generally based on a small set of bounding box proposals, we efficiently search over all possible bounding boxes. we think interpretability, i.e., linking of word and image concepts, is an important concept, particularly for textual grounding, which deserves more attention.", "1013": "the use of features from deep convolutional neural networks (dcnns) pretrained on imagenet has led to important advances in computer vision. dcnn features, usually called perceptual features (pfs), have been used in tasks such as transfer learning 40, 16, style transfer and super-resolution while there have been previous works on the use of pfs in the context of image generation and transformation 7, 17, exploration of pfs as key source of information for learning generative models is not well studied. moment matching approaches for generative modeling are based on the assumption that one can learn the data distribution by matching the moments of the model distribution to the empirical data distribution. more specifically, we propose a simple but effective moment matching method that: (1) breaks away from the problematic min/max game completely; (2) does not use online learning of kernel functions; and (3) is very efficient with regard to both number of used moments and required minibatch size. our proposed approach, named generative feature matching networks (gfmn), learns implicit generative models by performing mean and covariance matching of features extracted from all convolutional layers of pretrained deep convnets. the main contributions of this work can be summarized as follows: (1) we propose a new effective moment matchingbased approach to train implicit generative models that does not use adversarial or online learning of kernel functions, provides stable training, and achieves state-of-the-art results; (2) we show theoretical results that demonstrate gfmn convergence under the assumption of the universality of perceptual features; (3) we propose an adam-based moving average method that allows effective training with small minibatches; (4) our extensive quantitative and qualitative experimental results demonstrate that pretrained autoencoders and dcnn classifiers can be effectively used as (cross-domain) feature extractors for gfmn training. let g be the generator implemented as a neural network with parameters , and let e be a pretrained neural network with l hidden layers. our proposed approach consists in training g by minimizing the following loss function: min m j1 jpdata jpg()2 jpdata jpg()2 (1) where: jpdata expdataej(x) rdj jpg() ezn (0,inz )ej(g(z; )) r dj jpdata, expdataej,(x) j,pdata 2, dj and is the l2 loss; x is a real data point sampled from the data generating distribution pdata; z rnz is a noise vector sampled from the normal distribution n (0, inz ); ej(x), denotes the output vector/feature map of the hidden layer j from e; m l is the number of hidden layers used to perform feature matching. the main advantage of ama over simple moving average (ma) is in its adaptive first and second order moments that ensure stable estimation of the moving averages vj in fact, this is a non-stationary estimation since the mean of the generated data changes in the training, and it is well known that adam works well for such online non-stationary losses in section we provide experimental results supporting: (1) the memory advantage that the ama formulation of feature matching offers over the naive implementation; (2) the stability advantage and improved generation results that ama allows compared to the naive implementation. gfmn is related to the recent body of work on mmd and moment matching based generative models 22, 8, 21, 3, the closest to our method is the generative moment matching network autoencoder (gmmnae) proposed in in gmmnae, the objective is to train a generator g that maps from a prior uniform distribution to the latent code learned by a pretrained ae, and then uses the frozen pretrained decoder to map back to image space. presents samples from gfmnvgg19 trained with celeba dataset with resolution 128128, which shows that gfmn can achieve good performance with image resolutions larger than these results also demonstrate that: (1) the same classifier (vgg19 trained on imagenet) can be successfully applied to train gfmn models across different domains; (2) perceptual features from dcnns encapsulate enough statistics to allow the learning of good generative models through moment matching. we achieve successful non-adversarial training of implicit generative models by introducing different key ingredients: (1) moment matching on perceptual features from all layers of pretrained neural networks; (2) a more robust way to compute the moving average of the mean features by using adam optimizer, which allows us to use small minibatches; and (3) the use of perceptual features from multiple neural networks at the same time (vgg19 resnet18). in the experiments, we used a wgan-gp architecture where: (1) the discriminator is a vgg19 or a resnet18; (2) the discriminator is pretrained on imagenet; (3) the generator is pretrained on cifar10 through autoencoding. if a discriminator can distinguish perfectly between real and fake early on, the generator cannot learn properly and the min/max game becomes unbalanced, having no good discriminator gradients for the generator to learn from, producing degenerate models. in this appendix, we present a comparison between the simple moving average (ma) and adam moving average (ama) for the case where vgg19 imagenet classifier is used as a feature extractor. this experiment uses a minibatch size of that ama has a very positive effect in the quality of generated images. gfmn trained with ma produces various images with some sort of crossing line artifacts. 10a and 10b) and generative moment matching networks (gmmn) (figs.", "1014": "e-commerce recommender systems aim to present items with high utility to the consumers utility may be decomposed into form utility: the item is desired as it is manifested, and time utility: the item is desired at the given point in time 28; recommender systems should take both types of utility into account. economists define items to be either durable goods or nondurable goods based on how long they are intended to last before being replaced a key characteristic of durable goods is the long duration of time between successive purchases within item categories whereas this duration for nondurable goods is much shorter, or even negligible. although we have witnessed great success of collaborative filtering in media recommendation, we should be careful when expanding its application to general e-commerce recommendation involving both durable and nondurable goods due to the following reasons: since media such as movies and music are nondurable goods, most users are quite receptive to buying or renting them in rapid succession. therefore, recommending an item for which a user has no immediate demand can hurt user experience and waste an opportunity to drive sales. a key assumption made by matrix factorization- and completion-based collaborative filtering algorithms is that the underlying rating matrix is of low-rank since only a few factors typically contribute to an individual’s form utility however, a user’s demand is not only driven by form utility, but is the combined effect of both form utility and time utility. an additional challenge faced by many real-world recommender systems is the one-sided sampling of implicit feedback 15, unlike the netflix-like setting that provides both positive and negative feedback (high and low ratings), no negative feedback is available in many e-commerce systems. for example, a user might not purchase an item because she does not derive utility from it, or just because she was simply unaware of it or plans to buy it in the future. in this sense, the labeled training data only draws from the positive class, and the unlabeled data is a mixture of positive and negative samples, a problem usually referred to as positive-unlabeled (pu) learning to address these issues, we study the problem of demand-aware recommendation. given purchase triplets (user, item, time) and item categories, the objective is to make recommendations based on users’ overall predicted combination of form utility and time utility. specifically, we model a user’s time utility for an item by comparing the time t since her most recent purchase within the item’s category and the item category’s underlying inter-purchase duration d; the larger the value of d t, the less likely she needs this item. compared to existing recommender systems, our work has the following contributions and advantages: (i) to the best of our knowledge, this is the first work that makes demand-aware recommendation by considering inter-purchase durations for durable and nondurable goods; (ii) the proposed algorithm is able to simultaneously infer items’ inter-purchase durations and users’ real-time purchase intentions, which can help e-retailers make more informed decisions on inventory planning and marketing strategy; (iii) by effectively exploiting sparsity, the proposed algorithm is extremely efficient and able to handle large-scale recommendation problems. in the first task, we record the highest ranking of items that are within item i’s category among all items at time t. since a purchase record (u, i, t) may suggest that in time slot t, user u needed an item that share similar functionalities with item i, category prediction essentially checks whether the recommendation algorithms recognize this need. although they may not perfectly reflect the true inter-purchase durations, the estimated durations clearly distinguish between durable good categories, e.g., automotive, musical instruments, and non-durable good categories, e.g., instant video, apps, and food. in this study, we replace category prediction with a more strict evaluation metric item prediction 8, which indicates the predicted ranking of item i among all items at time t for each purchase record (u, i, t) in the test set. on the other hand, this result verifies the effectiveness of the pu formulation even if the durations are underestimated, our algorithm still outperforms the competitors by a considerable margin. as a final note, we want to point out that tmall and amazon review may not take full advantage of the proposed algorithm, since (i) their categories are relatively coarse and may contain multiple sub-categories with different durations, and (ii) the time stamps of amazon review reflect the review time instead of purchase time, and inter-review durations could be different from inter-purchase durations. by choosing a purchase history dataset with a more appropriate category granularity, we may obtain more accurate duration estimations and also a better recommendation performance. in this paper, we examine the problem of demand-aware recommendation in settings when interpurchase duration within item categories affects users’ purchase intention in combination with intrinsic properties of the items themselves. we formulate it as a tensor nuclear norm minimization problem that seeks to jointly learn the form utility tensor and a vector of inter-purchase durations, and propose a scalable optimization algorithm with a tractable time complexity. our empirical studies show that the proposed approach can yield perfect recovery of duration vectors in noiseless settings; it is robust to noise and scalable as analyzed theoretically. on two real-world datasets, tmall and amazon review, we show that our algorithm outperforms six state-of-the-art recommendation algorithms on the tasks of category, item, and purchase time predictions.", "1015": "automated conversational agents are becoming popular for various tasks, such as personal assistants, shopping assistants, or as customer service agents. automated agents benefit from adapting their personality according to the task at hand thus, it is desirable for automated agents to be capable of generating responses that express a target personality. personality is defined as a set of traits which represent durable characteristics of a person. many models of personality exist while the most common one is the big five model (digman, 1990) , including: openness, conscientiousness, extraversion, agreeableness, and neuroticism. these traits were correlated with linguistic choices including lexicon and syntax (mairesse and walker, 2007). in this paper we study how to encode personality traits as part of neural response generation for conversational agents. our approach builds upon a sequence-to-sequence by adding an additional layer that represents the target set of personality traits, and a hidden layer that learns high-level personality based features. specifically, we focus on conversational agents for customer service; in this context, many studies examined the effect of specific personality traits of human agents on service performance. the first response (in each example), is generated by a standard seq2seq response generation system that ignores personality modeling and in effect generates the consensus response of the humans represented in the training data. the second response is generated by our system, and is aimed to generate data for an agent that expresses a high level of a specific trait. in example 1, the agreeableness-agent is more compassionate (expresses empathy) and is more cooperative (asks questions). we experimented with a dataset of 87.5k real customer-agent utterance pairs from social media. neural response generation models are based on a seq2seq architecture and employ an encoder to represent the user utterance and an attention-based decoder that generates the agent response one token at a time. neural response generation can be viewed as a sequence-to-sequence problem , where a sequence of input language tokens x x1, , xm , describing the user utterance, is mapped to a sequence of output language tokens y1, , yn , describing the agent response. at each time step j, it generates yj based on the current hidden state sj , then updates the hidden state sj1 based on sj and yj formally, the decoder is defined by the following equations: s1 tanh(w (s)bm), (1) p(yj w x, y1:j1) exp(u sj , cj ), (2) sj1 lstm((out)(yj), cj , sj), (3) where i 1, , n and the context vector, cj , is the result of global attention ). in this section we present our personality-based model (figure 2) which generates responses conditioned on a target set of personality traits values which the responses should express. now, at each token generation, the decoder updates the hidden state conditioned on the personality traits features hp, as well as on the previous hidden state, the output token and the context. cold start: in our second experiment, we split the dataset such that of the agents only formed the validation and test sets (half of each agent’s examples for each set). note that, we extracted target personality traits for agents in the training set using their training data, or, for agents in the test set, using validation data. table shows that, in this setting, we get better performance by utilizing personality based representation: our model achieves a relative decrease in perplexity, and a relative improvement in bleu score. results from both experiments demonstrate that we can better model the linguistic variation in agent responses by conditioning on target personality traits. we conducted a human evaluation of our personality-based model using a crowd-sourcing service. this evaluation measures whether the responses generated by our model are correlated with the target personality traits. we focused on two personality traits from the big five model that are important to customer service: agreeableness and conscientiousness we extracted customer utterances from the validation set of the cold start setting described above. we selected customer utterances that convey a negative sentiment, since re- sponses to this kind of utterances vary much. we generated a high-trait target personality distribution (trait was either agreeableness or conscientiousness), where trait was set to a value of 0.9, and all other traits to similarly, we created a low-trait version where trait was set to for each trait and customer utterance we generated a response for the high-trait and low-trait versions. each triplet methodology, the two responses were presented in a random order, and judged on a 5-point zero-sum scale. a score of (2) was assigned if one response was judged to express the trait more (less) than the other response, and (1) if one response expressed the trait somewhat more (less) than the other. the judges rated each pair, and their scores were averaged and mapped into equal-width bins. after discarding ties, we found that the high-trait responses generated by our personality-based model were judged either more expressive or somewhat more expressive than the low-trait corresponding responses in of cases. if we ignore the somewhat more expressive judgments, the high-trait responses win in of cases. we have presented a personality-based response generation model and tested it in customer care tasks, outperforming baseline seq2seq model. in future work, we would like to generate responses adapted to the personality traits of the customer as well, and to apply our model to other tasks such as education systems.", "1016": "recently developed event-based cameras such as dynamic vision sensor (dvs) 37, and atis 50, inspired by the biological retina, encode pixel illumination changes as events. a live-feed version of the system running on nine truenorth chips is shown to calculate disparity maps per second, and the ability to increase this up to 2,000 disparities per second (subject to certain trade-offs) is demonstrated, for use with high speed event cameras, such as dvs. the main advantages of the proposed method, compared to the related work 17, 49, 45, 52, 57, are simultaneous end-to-end neuromorphic disparity calculation, low power, high throughput, low latency (9-11 ms), and linear scalability to multiple neuromorphic processors for larger input sizes. when the data in a cycle is sparse, as is the case with a dvs sensor, most neurons would not compute for most of the time, resulting in low active power this processing differs from traditional architectures that use frame-buffers and other conventional data structures; where same memory fetching and computation is repeated for each pixel every frame, independent of scene activity. the left and right sensor’s positive () or negative () polarity channel.2 given a pair of spatiotemporal coordinate tensors xl,p, xr,q centered at coordinates p, q in the left and right rectified image respectively and representing k coordinates each, we calculate the binary hadamard product fl(p, t) fr(q, t) associated with the corresponding patches at time t, where fl(p, t) ia1(v l x (i) l,p (t)) 0, 1k and fr(q, t) ia1(v r x (i) r,q (t)) 0, 1k the product is calculated in parallel across multiple neurons, as k pairwise logical and operations of corresponding feature vector entries, resulting in (a1(v dot p,q,1), ...,a1(v dot p,q,k)) where v dotp,q,i(t) t a1(v l x (i) l,p (t 1)) a1(v r x (i) r,q (t 1)) the population code representation of the hadamard product output is converted to a thermometer code3, which is passed to the winner-take-all circuit described below. the winner-take-all (wta) system is a feed-forward neural network that takes as input d thermometer code representations of the hadamard products for d distinct candidate disparity levels, and finds the disparity with the largest value, at every tick. as an illustration, winner is computed from the example set of numbers in table and the winner selection process is shown in table a left-right consistency check is then performed to verify that for each left-rectified pixel p matched to right-rectified pixel q, it is also the case that right-rectified pixel q gets matched to left-rectified pixel p. this is achieved using two parallel wta streams. total chip power is the sum of passive power, computed by multiplying the idle power by the fraction of the chip’s cores under use, and active power computed by subtracting idle power from the total power measured when the system is accepting input events the rds is tested on a model using spatial windows, left-right consistency constraints, no morphological erosion/dilation after rectification, and disparity levels (0-30) plus a no-disparity’ indicator (often occurring due to self-occlusions). the models that run on live davis input are operated at spike injection rate of up to 2,000hz (a new input every 1/2,000 seconds) and disparity map throughput of 400hz at a 0.5ms tick period (400 distinct disparity maps produced every second) across a cluster of truenorth chips. by adding a multiplexing spiking network to the network, we are able to reuse each feature-extraction/wta circuit to process the disparities for different pixels, effectively decreasing the maximum disparity map throughput from 2,000hz to 400hz, requiring fewer chips to process the full image (9 truenorth chips). we tested the maximum disparity map throughput achievable, by executing a one-chip model on a cropped input, with no multiplexing (one disparity map ejected per tick) at a 0.5ms tick period, achieving the 2,000hz disparity map throughput. we also observe qualitatively good performance (fig. it is observed that the temporal scale has a higher effect on accuracy than spatial scale. left-right consistency constraints are typically present in the best performing fan-sequence models, but not so in the butterfly sequences. distance and orientation do not have a significant effect on performance. we have introduced an advanced neuromorphic 3d vision system uniting a pair of davis cameras with multiple truenorth processors, to create an end-to-end, scalable, event-based stereo system. by using a spiking neural network, with low-precision weights, we have shown that the system is capable of injecting event streams and ejecting disparity maps at high throughputs, low latencies, and low power. the system is highly parameterized and can operate with other event based sensors such as atis or dvs table compares our approach with the literature on event based disparity. comparative advantages are low power, multi-resolution disparity calculation, scalability to live sensor feed with large input sizes, and evaluation using synthetic as well as real world fast movements and depth gradients, in neuromorphic, non von-neumann hardware. the implemented neuromorphic stereo disparity system achieves these advantages, while consuming less power per pixel per disparity map compared to the stateof-the-art the homogeneous computational substrate provides the first example of a fully end-to-end low-power, high throughput fully event-based neuromorphic stereo system capable of running on live input event streams, using a fully graph-based computation model, where no frames, arrays or other such data-structures are used.", "1017": "explaining a complex system through their cause and effect relations is one of the fundamental challenges in science. data is collected and experiments are performed with the intent of understanding how a certain phenomenon comes about, or how the underlying system works, which could be social, biological, artificial, among others. the latter, inference, attempts to leverage the causal structure to compute quantitative claims about the effect of interventions and retrospective counterfactuals, which are critical to assign credit, understand blame and responsibility, and perform judgement about fairness in decision-making. one of the most popular languages used to encode the invariances needed to reason about causal relations, for both learning and inference, is based on graphical models, and appears under the rubric of causal graphs 16, 21, a causal graph is a directed acyclic graph (dag) with latent variables, where each edge encodes a causal relationship between its endpoints: x is a direct cause of y , i.e., x y , if, when the remaining factors are held constant, forcing x to take a specific value affects the realization of y , where x,y are random variables representing some relevant features of the system. the task of learning the causal structure entails a search over the space of causal graphs that are compatible with the observed data; the collection of these graphs forms what is called an equivalence class. the most popular mark imprinted on the data by the underlying causal structure that is used to delineate an equivalence class are conditional independence (ci) relations. based on the second rule of do-calculus, one can infer that there is an open backdoor path from x to y , where the edge adjacent to x on this path has an arrowhead into x. in our setting, we do not have access to the true graph, but we leverage this and the other do-constraints to reverse engineer the process and try to learn the structure. for our characterization, we utilize an extension of the causal calculus to soft interventions introduced in under soft-interventions, the do-see test can be written as checking if px(yx) p(yx), where px is the distribution obtained after a soft intervention on x. the second observation leveraged here follows from another realization by pearl that interventions can be represented explicitly in the graphical model he then introduced what we call f-nodes, which graphically encode the changes due to an intervention and the corresponding parametrization (see also 16, sec. ,w,z,y, is a discriminating path for z if (1) p includes at least three edges; (2) z is a non-endpoint node on p, and is adjacent to y on p; and (3) x is not adjacent to y , and every node between x and z is a collider on p and is a parent of y two mags are markov equivalent if and only if (1) they have the same adjacencies; (2) they have the same unshielded colliders; and (3) if a path p is a discriminating path for a vertex z in both graphs, then z is a collider on the path in one graph if and only if it is a collider on the path in the other. a pag, which represents a markov equivalence class of a mag, is learnable from the independence model over the observed variables, and the fci algorithm is a standard sound and complete method to learn such an object related work: learning causal graphs from a combination of observational and interventional data has been studied in the literature 3, 11, 7, 20, 8, 12, for causally sufficient systems, the notion and characterization of interventional markov equivalence has been introduced in 9, more recently, showed that the same characterization can be used for both hard and soft interventions. rule (inducing paths): if fk f is adjacent to a node y sk and sk 1, e.g., sk x, then orient x y out of x, i.e., x y the intuition for this rule is as follows: if fk is adjacent to a node y sk in g, then there is an inducing path p between fk and y in augi(d), where d is any causal graph in the equivalence class. hence, the edge between x and y is out of x and into y in mag(augi (d)) and consequently in g. we give an example to illustrate the steps of the algorithm in figure 3, where i , x. figure 3a shows the augmented causal graph, i.e., augi (d), and figure 3b shows the corresponding augmented mag, i.e., mag(augi (d)). consider a set of interventional distributions (pi)ii c-faithful to a causal graph d (v l), where i is a set of controlled experiments. we investigate the problem of learning the causal structure underlying a phenomenon of interest from a combination of observational and experimental data. we pursue this endeavor by noting that a generalization of the converse of pearl’s do-calculus (thm. 1) leads to new tests that can be evaluated against data. these tests, in turn, translate into constraints over the structure itself. we then define an interventional equivalence class based on such criteria (def. 1), and then derive a graphical characterization for the equivalence of two causal graphs (thm. finally, we develop an algorithm to learn an interventional equivalence class from data, which includes new orientation rules.", "1018": "structured knowledge bases , or multi-hop , or complex queries such as how many countries have more rivers and lakes than brazil?’’ complex queries require a proper assembly of selected operators from a library of graph, set, logical, and arithmetic operations into a complex procedure, and is the subject of this paper. more complex queries need to be evaluated as an acyclic expression graph over nodes representing kb access, set, logical, and arithmetic operators a practical alternative to inferring a stateless expression graph is to generate an imperative sequential program to solve the query. each step of the program selects an atomic operator and a set of previously defined variables as arguments and writes the result to scratch memory, which can then be used in subsequent steps. with this approach it is now possible to first train separate modules for each of the atomic operations involved and then train a program induction model that learns to use these separately trained models and invoke the sub-modules in the correct fashion to solve the task. as well as for manipulating a physical environment program induction has also seen initial promise in translating simple natural language queries into programs executable in one or two hops over a kb to obtain answers in contrast, many of the complex queries from saha et al. sample operations include genset: collecting t : (h, r, t) kb, computing setunion, counting set sizes (setcount), comparing numbers or sets, and so forth. main contributions we present complex imperative program induction from terminal rewards’’ (cipitr),2 an advanced neural program induction (npi) system that is able to answer complex logical, quantitative, and comparative queries by inducing programs of length up to 7, using atomic operators and variable types. relevant statistics of the resulting data set are presented in table use of gold entity, type, and relation annotations to standardize comparisons: our focus being on the reasoning aspect of the kbqa problem, we use the gold annotations of canonical kb entities, types, and relations available in the data set along with the the queries, in order to remove a prominent source of confusion in comparing kbqa systems (i.e., all systems take as inputs the natural language query, with spans identified with kb ids of entities, types, relations, and integers). memory lookup: the memory lookup looks up scratch memory with a given probe, say x (of arbitrary dimension), and retrieves the memory entry having closest key embedding to x. it first passes x through a feed-forward layer to transform its dimension to key embedding dimensionxkey. on the other hand, training the kvmnet model on the balanced data helps showcase the real performance of the model, where cipitr outperforms kvmnet significantly on most of the harder query classes. to summarize, cipitr has the following advantages, inducing programs more efficiently and pragmatically, as illustrated by the sample outputs in table 5: generating syntactically correct programs: because of the token-by-token decoding of the program, nsm cannot restrict its search to only syntactically correct programs, but rather only resorts to a post-filtering step during training. however, at test time, it could still generate programs with wrong syntax, as shown in table for example, for the logical question, it invokes a genset with a wrong argument type none and for the quantitative count question, it invokes the setunion operator on a non-set argument. on the other hand, cipitr, by design, can never generate a syntactically incorrect program because at every step it implicitly samples only feasible actions. generating semantically correct programs: cipitr is capable of incorporating different generic programming styles as well as problemspecific constraints, restricting its search space to only semantically correct programs. on the other hand the nsmgenerated programs are often semantically wrong, for instance, both in the quantitative and quantitative count based questions, the type of the answer is itself wrong, rendering the program meaningless. this arises once again, owing to the token-by-token decoding of the program by nsm which makes it hard to incorporate high level rules to guide or constrain the search. efficient search-space exploration: owing to the different strategies used to explore the program space more intelligently, cipitr scales better to a wide variety of complex queries by using less than half of nsm’s beam size. we experimentally established that for programs of length these various techniques reduced the average program space from to 2,998 programs. we presented cipitr, an advanced npi framework that significantly pushes the frontier of complex program induction in absence of gold programs. cipitr uses auxiliary rewarding techniques to mitigate the extreme reward sparsity and incorporates generic pragmatic programming styles to constrain the combinatorial program space to only semantically correct programs. as future directions of work, cipitr can be further improved to handle the hardest question types by making the search more strategic, and can be further generalized to a diverse set of goals when training on all question categories together. other potential directions of research could be toward learning to discover sub-goals to further decompose the most complex classes beyond just the two-level phase transition proposed here. additionally, further improvements are required to induce complex programs without availability of gold program input variables.", "1019": "the vast amounts of textual data end users need to consume motivates the need for automatic summarization an automatic summarizer gets as an input one or more documents and possibly also a limit on summary length (e.g., maximum number of words). the summarizer then needs to produce a textual summary that captures the most salient (general and informative) content parts within input documents. moreover, the summarizer may also be required to satisfy a specific user information need, expressed by one or more queries. therefore, the summarizer will need to produce a focused summary which includes the most relevant information to that need. while both saliency and focus goals should be considered within a query-focused summarization setting, these goals may be actually conflicting with each other higher saliency usually comes at the expense of lower focus and vice-versa. contact author: haggaiil.ibm.com to illustrate the effect of summary length on this tradeoff, using the duc dataset, figure reports the summarization quality which was obtained by the cross entropy summarizer (ces) a state of the art unsupervised query-focused multidocument extractive summarizer saliency was measured according to cosine similarity between the summary’s bigram representation and that of the input documents. focus was further measured relatively to how much the summary’s induced unigram model is concentrated around query-related words. aiming at better handling the saliency versus focus tradeoff, in this work, we propose dual-ces an extended ces summarizer similar to ces, dual-ces is an unsupervised query-focused, multi-document, extractive summarizer. to this end, like ces, dual-ces utilizes the cross entropy method for selecting a subset of sentences extracted from input documents, whose combination is predicted to produce a good summary. using such an approach, dual-ces tries to handle the tradeoff by gradually shifting from generating a long summary that is more salient in the first step to generating a short summary that is more focused in the second step. similar to ces, it utilizes the cross entropy method for selecting the most promising subset of sentences in d. since we assume an unsupervised setting, no actual reference summaries are available for training nor can we directly optimize an actual quality target q(sq,d). instead, following 6, q(sq,d) is surrogated by several summary quality prediction measures qi(sq,d) (i 1, 2, each predictor qi(sq,d) is designed to estimate the level of saliency or focus of a given candidate summary s and is presumed to correlate (up to some extent) with actual summarization quality, e.g., rouge for simplicity, similar to ces, various predictions are assumed to be independent and are combined into a single optimization objective by taking their product, i.e. the ce-method provides a generic monte-carlo optimization framework for solving hard combinatorial problems previously, it was utilized for solving the sentence subset selection problem to this end, the ce-method gets as an input q(q,d), a constraint on maximum summary length l and an optional pseudo-reference summary sl, whose usage will be explained later on. for a given sentence s d, let (s) denote the likelihood that it should be included in summary s. starting with a selection policy with the highest entropy (i.e. to this end, () is incrementally learned using an importance sampling approach ., a sample of n sentence-subsets sj is generated according to the selection policy t1() which was learned in the previous iteration t the likelihood of picking a sentence s d at iteration t is estimated (via cross-entropy minimization) as follows: t(s) def n j1 q(sj q,d)tssj n j1 q(sj q,d)t (1) here, denotes the kronecker-delta (indicator) function and t denotes the (1 )-quantile ( (0, 1)) of the sample performances q(sj q,d) (j 1, 2, therefore, the likelihood of picking a sentence s d will increase when it is being included in more (subset) samples whose performance is above the current minimum required quality target value t. we further smooth t() as follows: t() t1() (1 )t(); with 0, upon its termination, the ce-method is expected to converge to the global optimal selection policy () we then produce a single summary s (). to this end, we add a predictor: qqf (sq,d) def wq p(ws), which acts as a query-anchor and measures to what extent summary s’s unigram model is devoted to the information need q. the input to the second step of the cascade consists of the same set of documents d, summary length constraint lmax and the pseudo-reference summary sl that was generated in the previous step. dual-ces then utilizes such salient words for better selection of salient sentences within its second step of focused summary production. figure illustrates the (average) learning curve of its adaptive-length parameter lt. overall, dual-ces’s summarization quality remains quite stable, exhibiting low sensitivity to l. similar stability was further observed for the two other duc benchmarks. in addition, figure depicts an interesting empirical outcome: dual-ces-a converges (more or less) to the best hyperparameter l value (i.e., l in table 3). dual-ces-a, therefore, serves as a robust alternative for flexibly estimating such hyperparameter value during runtime. dual-ces-a can provide similar quality and may outperform dual-ces. we proposed dual-ces, an unsupervised, query-focused, extractive multi-document summarizer. dual-ces was shown to better handle the tradeoff between saliency and focus, providing the best summarization quality compared to other alternative stateof-the-art unsupervised summarizers. moreover, in many cases, dual-ces even outperforms state-of-the-art supervised summarizers. as a future work, we would like to learn to distill from additional pseudo-feedback sources.", "1020": "in recent years we are experiencing a dramatic improvement of the synthesized speech quality in tts systems, with the introduction of systems that are based on neural networks (nn). a major improvement in quality was achieved by using attention based models such as tacotron and by replacing vocoders with a nn based waveform generators such as wavenet a useful feature of systems with trainable models is the ability to adapt the tts to an unseen voice using a small amount of training data (from a few seconds to an hour of speech). this is usually done by training the system on a large number of speakers, and providing a speaker embedding vector as one of the system’s inputs. using this approach allows later retraining of only a subsets of the model parameters or prediction of the speaker embedding vector 3, 4, the drawback of this approach is that the resulting systems use large nn models. furthermore, a multi-speaker model usually needs much more trainable parameters than a single speaker model. such requirements pose a severe problem for practical tts system that require very low latency for a dialog with a human. in our previous paper we introduced a nn based tts system with two trainable modules for prosody prediction and acoustic features prediction. this was carried out by retraining nn models that had already been trained using a large highquality voice, on a small amount of data from the new voice. in this paper we show that we can get a considerable quality improvement by modifying a tts system that produced the world vocoder parameters to predict parameters for lpcnet as in the previous work 6, we conduct multiple adaptation experiments, applied on multiple vctk voices and show that the new system has much better quality and similarity to the target voices but can still run much faster than real-time in a single-cpu mode. we adopted the front-end block which is used in the ibm watson tts engine and is described in detail in the front-end performs a grapheme-to-phoneme conversion, represents each word with a set of positional and categorical linguistic features and associates the features with the phonemes contained within the word. each sub-phoneme element represents either a heading, a middle or a trailing part of a phoneme. forced alignment of audio at the sub-phoneme level using proprietary acoustic modeling and speech recognition tools. the input features, derived from the tts front end, are comprised of 1-hot coded categorical features and standard positional features in this architecture the prosody adaptation to unseen speaker is based on a variational auto encoder (vae) utterance prosody embedding, averaged over all the speaker utterances 6, as presented on figure in the current work we used multi-speaker baseline models for prosody adaptation to unseen voices, as it resulted in better quality than the single speaker models. to adapt the model to a smaller unseen voice, we first initialize the training with the weights of the base model of the same gender. the subject is asked to rate their voice similarity, using a 4-point scale adopted from the voice conversion challenge (vcc) 16, and utilized in our previous experiments we performed two tests: one with only male voices and the second with only female voices. for reference, in each test we also checked the similarity of pairs of natural speech samples from the same speaker and of pairs of natural speech from different speakers of the same gender. we can compare these results to those of the vcc hub task although the task setup and the listening tests conditions are a bit different we can see that our system mos and similarity score are comparable to those of the best vcc system (n10 with quality of and similarity of where the corresponding scores for the original speech are and 95). to compensate for the variability between the voices, we have normalize the mos scores for each voice in the range from to the score of natural samples of this voice. the similarity scores for each voice, were normalized to the range between the scores of natural samples from different and same speakers. when adding the rest of the blocks we found that we can synthesize about time faster than real-time on a cpu. we have presented in this article a new tts system that addresses the challenging goals of producing high quality speech while operating at faster than real-time rate without an expensive gpu support. the system is built around three nn models for generating the prosody, acoustic features and the final speech signal. we tested this system using two proprietary tts voice datasets and demonstrated that our system produces high quality speech that is comparable to larger and much slower tacotron2 wavenet systems. the task of creating a high-quality tts system out of a smaller set of audio data is even more challenging. we have shown that our system can perform well even with datasets as small as 5-20 minutes of audio. we demonstrated that when we reduce the size of the training data, there is some graceful degradation to the quality, but we are still able to maintain good similarity to the original speaker. for future work, we plan to allow voice modifications by adding control over voice parameters such as pitch, breathiness and vocal tract.", "1021": "feature selection is an important problem in statistics and machine learning for interpretable predictive modeling and scientific discoveries. our goal in this paper is to design a dependency measure that is interpretable and can be reliably used to control the false discovery rate in feature selection. the mutual information between two random variables x and y is the most commonly used dependency measure. the mutual information i(x;y ) is defined as the kullback-leibler divergence between the joint distribution pxy of x,y and the product of their marginals pxpy, i(x;y ) kl(pxy, pxpy). for instance, the hilbert-schmidt independence criterion (hsic) uses the maximum mean discrepancy (mmd) to assess the dependency between two variables, i.e. hsic(x,y ) mmd(pxy, pxpy), which can be easily estimated from samples via kernel mean embeddings in a reproducing kernel hilbert space (rkhs) in this paper we introduce the sobolev independence criterion (sic), a form of gradient regularized integral probability metric (ipm) between the joint distribution and the product of marginals. in section we show how sic and conditional generative models can be used to control the false discovery rate using the recently introduced holdout randomization test and knockoffs we validate sic and its fdr control on synthetic and real datasets in section we start by motivating gradient-sparsity regularization in sic as a mean of selecting the features that maintain maximum dependency between two randoms variable x (the input) and y (the response) defined on two spaces x rdx and y rdy (in the simplest case dy 1). instead of the usual kl divergence, the metric d with its witness function, or critic, f(x, y) measures the distance between the joint pxy and the product of marginals pxpy with this generalized definition of mutual information, the feature selection problem can be formalized as finding a sparse selector or gate w rdx such thatd(pw x,y, pw xpy) is maximal , i.e. we experiment with two datasets: a) complex multivariate synthetic data (sinexp), which is generated from a complex multivariate model proposed in sec 5.3, where ground truth features xi out of generate the output y through a non-linearity involving the product and composition of the cos, sin and exp functions (see appendix f.1). to increase the difficulty even further, we introduce a pairwise correlation between all features of we show results for datasets of and samples repeated times comparing performance of our models with the one of two baselines: elastic net (en) and random forest (rf). tpr top fdr top tpr hrt fdr hrt tpr top fdr top tpr hrt fdr hrt po we r a nd f dr elastic net random forest tpr top fdr top tpr hrt fdr hrt tpr top fdr top tpr hrt fdr hrt mse sobolev penalty sic dataset sinexp, n125 samples dataset sinexp, n500 samples feature selection on drug response dataset. we study the result of using the normalized importance scores j from sic for (heuristic) feature selection, against features selected by elastic net. table shows the heldout mse of a predictor trained on selected features, averaged over runs (each run: new randomized 90/10 data split, nn initialization). the sic critic and regressor nn were respectively the bigcritic and regressornn described with training details in appendix f.3, while the random forest is trained with default hyper parameters from scikit-learn we can see that, with just j , informative features are selected for the downstream regression task, with performance comparable to those selected by elasticnet, which was trained explicitly for this task. the features selected with high j values and their overlap with the features selected by elasticnet are listed in appendix f.2 table hiv-1 drug resistance with knockoffs-sic. the second real-world dataset that we analyze is the hiv-1 drug resistance38, which consists in detecting mutations associated with resistance to a drug type. for our experiments we use all the three classes of drugs: protease inhibitors (pis), nucleoside reverse transcriptase inhibitors (nrtis), and non-nucleoside reverse transcriptase inhibitors (nnrtis). concretely, we construct a dataset (x, x) of the concatenation of the real data and gaussian knockoffs 9, and fit sic(x, x, y ). as explained in section 6, we use in the knockoff filter the statistics wj j jdx , i.e. the difference of sic importance scores between each feature and its corresponding knockoff. for sic experiments, we use smallcritic architecture (see appendix f.3 for training details). we use boosted sic, by varying the batch sizes in n 10, 30, 50, and computing the geometric mean of produced by those three setups as the feature importance needed for knockoffs. we introduced in this paper the sobolev independence criterion (sic), a dependency measure that gives rise to feature importance which can be used for feature selection and interpretable decision making. we laid down the theoretical foundations of sic and showed how it can be used in conjunction with the holdout randomization test and knockoffs to control the fdr, enabling reliable discoveries. we demonstrated the merits of sic for feature selection in extensive synthetic and real-world experiments with controlled fdr."}