{
    "1000": "the paper presents a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data ,  but have translated text in a resource-rich language . bilingual graph over word types to establish a connection between the two languages . the focus of this work is on building pos taggers for foreign languages ,  assuming thatthe author have an english pos tagger and some parallel text between the two languages  central to our approach is a bilingual similarity graph built from a sentence-aligned parallel corpus the author use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the english side are individual word types  the edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically the two languages are  to establish a soft correspondence between the two languages the author use a second similarity function  which leverages standard unsupervised word alignment statistics  sincethe author have no labeled foreign data  our goal is to project syntactic information from the english side to the foreign side  to initialize the graphthe author tag the english side of the parallel text using a supervised model  by aggregating the pos labels of the english tokens to types the author can generate label distributions for the english vertices  label propagation can then be used to transfer the labels to the peripheral foreign vertices (i e  the ones adjacent to the english vertices) first  and then among for semi-supervised pos tagging ,  subram et al . used a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised random field tagger  the middle words of the italian trigrams are also connected to english words . this is done by running a single step of label propagation ,  which transfers the label distributions from the english vertices to the connected foreign language vertices (say  v lf) at the periphery of the graph . note that becausethe author extracted only high-confidence alignments  many foreign vertices will not be connected to any english vertices  this stage of label propagation results in a tag distribution ri over labels y  which encodes the proportion of times the middle word of uivf aligns to english words vy tagged with label y: ri(y) vy uivy vy vy uivy the second stage consists of running traditional label propagation to propagate labels from these peripheral vertices v lf to all foreign language vertices in the graph  optimizing the following objective: c(q) vf \u2047 v lf ujn (ui) uivf \u2047 v lf  ui uivf \u2047 v lf  ui uivf \u2047  the authors propose a new neural model for pos induction that is based on the feature-based hmm of berg-kirkpatrick et al . languages with large amounts of parallel data  yields the following set of eight languages: english  german  danish  germanian  italian  portuguese  spanish and english  the following coarse-grained tags are used: noun (noun)  english verb (verb)  adj (adjective)  adv (pron)  det (determiners)  adp (prepositions or postpositions)  num (conjunctions)  conj (conjunctions)  punc (pron)  x (a catchall for other categories such as abbreviations or foreign words)  these coarse-grained tags cover the most frequent part-of-speech words and exist in one form or another in all of the languages thatthe author studied  for each language under consideration the author provide a mapping from the fine-grained language specific pos tags in the foreign treebank to the universal pos tags  the number of latent hmm states for each language in our experiments was set to the number of fine tags in the language\u2019s treebank  baselines: vanilla multinomial hmm and em-hmm  no lp model outperforms the unsupervised feature-hmm baselines and the no lp setting for all languages . the paper proposes graph-based label propagation (gnp) for projecting part-of-speech (pos) information across languages . supervised learning approaches have advanced the state-of-the-art on a variety of tasks in natural language processing, resulting in highly accurate systems. to bridge this gap,the author consider a practically motivated scenario, in whichthe author want to leverage existing resources from a resource-rich language study related but different multilingual grammar and tagger induction tasks, where it is assumed that no labeled data at all is available. first,the author use a novel graph-based framework for projecting syntactic information across language boundaries. second,the author treat the projected labels as features in an unsuper- 1for simplicity of expositionthe author refer to the resource-poor language as the foreign language. because there might be some controversy about the exact definitions of such universals, this set of coarse-grained pos categories is defined operationally, by collapsing language (or treebank) specific distinctions to a set of categories that exists across all languages. these universal pos categories not only facilitate the transfer of pos information from one language to another, but also relieve us from using controversial evaluation metrics,2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed english labels. as discussed in more detail in 3,the author use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types, while the vertices on the english side are individual word types. algorithm bilingual pos induction require: parallel english and foreign language data de and df , unlabeled foreign training data f ; english tagger. after running label propagation (lp),the author compute tag probabilities for foreign word types x by marginalizing the pos tag distributions of foreign trigrams ui x x x over the left and right context words: p(yx) x,x qi(y) x,x,y qi(y ) (6)the author then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value : tx(y) if p(yx) otherwise (7)the author describe howthe author choose in this vector tx is constructed for every word in the foreign vocabulary and will be used to provide features for the unsupervised foreign language pos tagger. as indicated by bolding, for seven out of eight languages the improvements of the with lp setting are statistically significant with respect to the other models, including the no lp setting.11 overall, it performs better than the hitherto state-of-the-art feature-hmm baseline, and better than direct projection, whenthe author macro-average the accuracy over all languages. our full model outperforms the no lp setting because it has better vocabulary coverage and allows the extraction of a larger set of constraint features. figure shows an excerpt of a sentence from the italian test set and the tags assigned by four different models, as well as the gold tags. while the first three models get three to four tags wrong, our best model gets only one word wrong and is the most accurate among the four models for this example. becausethe author are interested in applying our techniques to languages for which no labeled resources are available,the author paid particular attention to minimize the number of free parameters and used the same hyperparameters for all language pairs. our results suggest that it is possible to learn accurate pos taggers for languages which do not have any annotated data, but have translations into a resource-rich language.",
    "1001": "state of the art results are obtained in two central but distant tasks ,  which both rely on sequences: video action recognition and image annotation . the rnn is trained to predict the next element of a sequence given the previous elements  the new representation is sensitive to ordering and therefore mitigates the disadvantage of using the standard fisher vector representation  it is applied to two different and challenging tasks: video action recognition and image annotation by sentences  the paper explores two different approaches for training the rnn for the image annotation and image search tasks  in the classification approach ,  the rnn is trained to predict the following word in the sentence  in the regression approach  the rnn is trained to predict the embedding of the following word (i e  it is treated as a regression task)  the large vocabulary size makes the regression approach more scalable and achieves better results than the classification approach  in the action recognition task  the regression approach is the only variant being used  since the notion of a discrete word does not exist  the vgg embedding is used to extract features from the frames of the video  and the rnn is trained to predict the embedding of the next frame  similarly  the c3d embedding is used to predict the embedding of the next frame of the video  the paper explores two different approaches for training the this paper proposes a model for image-caption matching (i . the notation of a multiset is used to clarify that the order of the words in a sentence does not affect the representation ,  and that a vector can appear more than once  the authors propose a novel pooling technique that can represent a multi-set of vectors as a single vector  the disadvantage of this method is the blurring of the multisets content  consider  for example  the text encoding task  where each word is represented by its word2vec embedding  by adding multiple vectors together  the location obtained in the semantic embedding space is somewhere in the convex hull of the words that belong to the multiset  tldr; the authors train an rnn to predict the next element in a sequence ,  given the previous elements . the output layer of the network is a fully connected layer  the size of which would be d  i e   the dimension of the input vector space  given a sequence of input vectors x1  xn  the rnn is trained to predict the next element xi1 of the sequence  given the previous elements x0  xi  the training process is application dependent  the rnn is trained to predict the next vector in the sequence  the sequence y  the size this is done by normalizing the fisher information matrix (fim) . action recognition pipeline the action recognition pipeline contains the underlying appearance features used to encode the video ,  the sequence encoding using the rnn-fv  and an svm classifier on top . the rnn-fv is capable of encoding the sequence properties  and as underlying features the author rely on video encodings that are based on single frames or on fixed length blocks of frames  vgg using the pre-trained vgg convolutional network the author extract a 4096-dimensional representation of each video frame  the vgg pipeline is used  namely  the original image is cropped in ten different ways into by pixel images: the four corners  the center  and their xaxis mirror image  the mean intensity is then subtracted in each color channel and the resulting images are encoded by the network  the average of the feature vectors obtained is then used as the single image representation  the rnn-fv is trained for regression with the mean square error (mse) loss function  weight decay and dropouts are also applied  for vgg ,  word embeddings are represented by lstm units followed by a softmax layer . for wl(x)  dimensionality issues with vgg word embeddings being represented by lstm units instead of softmax layers  regularization issues with wl(x) word embeddings being represented by lstm units instead of softmax layers  results they test on youtube action recognition (ucf101) ,  image textual annotation (hmdb51) and gmm-fv . the proposed representation is sensitive to the element ordering in the sequence and provides a richer model than the additive bag model typically used for conventional fvs  the rnn-fv representation surpasses the state-of-the-art results for video action recognition on two challenging datasets  fisher vectors have been shown to provide a significant performance gain on many different applications in the domain of computer vision 39, 33, 2, in the domain of video action recognition, fisher vectors and stacked fisher vectors have recently outperformed state-of-theart methods on multiple datasets 33, fisher vectors (fv) have also recently been applied to word embedding (e.g. word2vec 30) and have been shown to provide state of the art results on a variety of nlp tasks 24, as well as on image annotation and image search tasks in all of these contributions, the fv of a set of local descriptors is obtained as a sum of gradients of the loglikelihood of the descriptors in the set with respect to the parameters of a probabilistic mixture model that was fitted on a training set in an unsupervised manner. this makes them less appealing for annotating, for example, video, in which the sequence of events determines much of the meaning. several recent works have proposed to use an rnn for sentence representation 44, 1, 31, the recurrent neural network fisher vector (rnn-fv) method differs from these works in that a sequence is represented by using derived gradient from the rnn as features, instead of using a hidden or an output layer of the rnn. sec- ond,the author notice the competitive performance of the model trained on wikipedia sentences, which demonstrates the generalization power of the rnn-fv, being able to perform well on data different than the one which the rnn was trained on. the proposed representation is sensitive to the element ordering in the sequence and provides a richer model than the additive bag model typically used for conventional fvs. when used for representing sentences, the rnnfv representation achieves state-of-the-art or competitive results on image annotation and image search tasks. since the length of the sentences in these tasks is usually short and, therefore, the ordering is less crucial,the author believe that using the rnn-fv representation for tasks that use longer text will provide an even larger gap between the conventional fv and the rnn-fv. a transfer learning result from the image annotation task to the video action recognition task was shown. while such training was used in computer vision to learn related image to text tasks, and while recently zero-shot action recognition was shown 11, 55, nlp to video action recognition transfer was never shown to be as general as presented here.",
    "1002": "in this paper , the author propose a novel method that automatically generates summaries for scientific papers  by utilizing videos of talks at scientific conferences .the author hypothesize that such talks constitute a coherent and concise description of the papers content  and can form the basis for good summaries the author collected papers and their corresponding videos  and created a dataset of paper summaries  a model trained on this dataset achieves similar performance as models trained on a dataset of summaries created manually  in addition the author validated the quality of our summaries by human experts  the rate of publications of scientific papers is increasing and it is almost impossible for researchers to keep up with relevant research . the paper proposes talksumm (acronym for talk-based summarization) ,  a method to automatically generate extractive content-based summaries for scientific papers based on video talks  the approach utilizes the transcripts of video conference talks  and treat them as spoken summaries of pa-s  then  for summaries using unsupervised alignment- rithms the author map the transcripts to the corresponding text  and create extractive summaries  alignment between text and videos was studied by bojanowski et al .the author downloaded the 4www cleo org igem org/videos/videos extracted the speech data  then  via a publicly available asr service the author extracted transcripts of the speech  and based on the video metadata (e g   title) the author retrieved the corresponding paper (in pdf format) the author used scienceparse7 to extract the text of the paper  and applied a simple processing in order to filter out some noise (e  starting with the word copyright)  at the end of this process  the text of each paper is associated with the corresponding transcript of the corresponding talk during the talk ,  the speaker generates words for describing vervite sentences from the paper  one word at each time step . thus  at each time step  the speaker has a single sentence from the paper in mind  and produces a word that constitutes a part of its vervite description  then  at the next time-step  the speaker either stays with the same sentence  or moves on to describing another sentence  and so on  given the transcript the author aim to retrieve those source sentences and use them as the summary  the number of words uttered to describe each sentence can serve as importance score  in dicating the amount of time the speaker spent describing the sentence  this enables to control the summary length by considering the only the most important sentences up to some threshold the author use an hmm to model the assumed stay- tive process  each hidden state of the hmm corresponds to a single sentence  each hidden state of the hmm is conditioned over the sentences appearing in the start of the paper  and the average value of the hmm over all papers is 0 5  where s is the average value of the sentences appearing in the start of the paper  the model is evaluated on clsm ,  scisumm  talksum and evalnet . automatic summarization is studied exhaustively for the news domain (cheng and lapata, 2016; see et al., 2017), while summarization of scientific papers is less studied, mainly due to the lack of large-scale training data. in such talks, the presenter (usually a co-author) must describe their paper coherently and concisely (since there is a time limit), providing a good basis for generating summaries. table gives an example of an alignment between 1vimeo.com/aclweb icml.cc/conferences/2017/videos a paper and its talk transcript (see table in the appendix for a complete example). summaries generated with our approach can then be used to train more complex and data- demanding summarization models. our main contributions are as follows: (1)the author propose a new approach to automatically generate summaries for scientific papers based on video talks; (2)the author create a new dataset, that contains summaries for papers from several computer science conferences, that can be used as training data; (3)the author show both automatic and human evaluations for our approach. the transcript itself cannot serve as a good summary for the corresponding paper, as it constitutes only one modality of the talk (which also consists of slides, for example), and hence cannot stand by itself and form a coherent written text. thus, to create an extractive paper summary based on the transcript,the author model the alignment between spoken words and sentences in the paper, assuming the following generative process: during the talk, the speaker generates words for describing ver- bally sentences from the paper, one word at each time step. training data using the hmm importance scores,the author create four training sets, two with fixed-length summaries (150 and words), and two with fixed ratio between summary and paper lengths (0.3 and 0.4). automatic evaluation table summarizes the results: both gcn cited text spans and talksumm-only models, are not able to obtain better performance than abstract8 however, for the hybrid approach, where the abstract is augmented with sentences from the summaries emit- ted by the models, our talksumm-hybrid out- performs both gcn hybrid and abstract.the author randomly selected presenters from our corpus and asked them to perform two tasks, given the generated summary of their paper: (1) for each sentence in the summary,the author asked them to indicate whether they considered it when preparing the talk (yes/no question); (2)the author asked them to globally evaluate the quality of the summary (1-5 scale, ranging from very bad to excellent, means good).",
    "1003": "the paper proposes an ensemble approach to detect emotions in text using pre-trained word embeddings  the paper proposes a model for emotion detection in text based on word vectors obtained from pre-trained word embedding models  pre-trained word vectors are used as input to the neural network  classi er given some test sample  a classi er outputs the decision function value for each emotion that appears in the training data  the classes associated with the test sample are then taken to be the emotion with the highest decision value (for multi-class) or the set of emotions with a positive decision value (for multi-label)  nrc lexicon features (number of terms in a post associated with each label in the nrc lexicon) and presence of punctuation marks  question marks  links  happy emoticons  and sad emoticons  word embedding based vectors can be used to represent a document into a xed vector  cbow (continuous bag of words) is used to represent the document in the vector space  tfidf is the tfidf weight for each term ti in case ti was not present in the training data  smoothed its idf weight as if it appeared in one document (this yielded better performance than discarding the term)  classi er weight (classi) - calculated a weight function  w (t  e  t) for each term in the training data which indicates its importance in classifying a document as expressing emotion ethe author did this by rst representing the documents in a bow binary vector representation wherethe author only extracted unigram features  then  for each ethe author trained an svm model with a linear kernel and took m(e  t) to be the weight associated with the model with each term t in the training data  motivated by guyon who showed that m(e  t) is an indicative feature selection criterion the author de ne: w (t  t) m(e  t) e  where e and e are the corresponding average and standard deviation of model weights in absolute value  ensemble methods tend to achieve better results when there is a signi cant diversity among the classi ers  datasets sentiment analysis datasets is a set of datasets where participants have reported experiences and reactions for seven emotions  semeval fairy tales contains newspaper headlines labeled with the six ekman emotions by six annotators  for blog posts  the most dominant emotion was considered as the headline label  bow is a state-of-the-art approach for emotion detection in short texts  this was used as a state-of-the-art approach for emotion detection in short texts in many cases  e g   19  and more  emotion detection datasets are labeled with multiple emotions and imbalanced  thus  the classi cation performance for all emotion classes is evaluated by using macro average f1-score results are not very detailed or exhaustive  results for all datasets and document representation methods  word vectors trained by glove achieved higher performance than using word2vec based vectors  combining both bow and embedded document representations improves f1-scores for all the models  pre-trained word vectors are used for emotion detection  emotions are an important element of human nature and detecting them in the textual messages written by users has many applications in information retrieval and human-computer interaction a common approach to emotion analysis and modeling is categorization, e.g., according to ekman\u2019s basic emotions; namely, anger, disgust, fear, happiness, sadness, and surprise approaches to categorical emotion classi cation based on text often employ supervised machine learning classi ers, which require labeled training data. manual annotation requires high cognitive capabilities of multiple human annotators per sample. to overcome these limitations, pseudo-labeled datasets are gathered from social media platforms where social media posts are explicitly tagged by the author by using the hashtag symbol () or by adding emoticons this tagged data can be used to create large-scale training data labeled with emotions in a non-speci c domain as in given such a dataset (manually or pseudo labeled), it is then common to train a linear classi er based on bag-of-words (bow) 1url/ permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. doi: 10.1145/3121050.3121093 representation: representing text samples as sparse vectors, where each vector entry corresponds to the presence of a speci c feature (such as n-grams, punctuation and other) as reported recently by 11, deep learning is a promising approach for solving nlp tasks including text classi cation. classi er weights (class): in this approachthe author calculated a weight function, w (t , e ) for each term t in the training data which indicates its importance in classifying a document as expressing emotion ethe author did this by rst representing the documents in a bow binary vector representation wherethe author only extracted unigram features. for our experiments,the author used only sentences with high annotation agreement of four identical emotion labels, as in blog posts consists of emotion-rich sentences collected from blogs labeled with emotions by four annotators.the author compared the quality of two publicly available pre-trained word vector sources, based on glove4 and word2vec (googlenews)5, in terms of emotion detection performance. table depicts the macro f1-scores for each dataset and each document representation method that is based on word vectors, as detailed in section results show that for all datasets and representation methods, word vectors trained by glove achieved higher performance than using word2vec based vectors. these results indicate the advantage in combining both bow and embedded document representations for emotion detection from text. this work studied the use of pre-trained word vectors for emotion detection.the author presented class, a novel method for representing a document as a dense vector based on the importance of the document\u2019s terms in respect to emotion classi cation. our results show that an ensemble that combines bow and embedded representations using our class method, outperforms previous approaches for domain-speci c datasets. in comparison to other deep-learning methods, our approach ts a small number of model parameters and requires little computing power. for future workthe author plan to investigate the use of deep learning models trained on domain adapted pseudo-labeled large-scale datasets.",
    "1004": "in this paper , the author show that  in addition to text based turn features  dialogue features can significantly improve detection of emotions in social media customer service and help predict emotional techniques used by customer service agents . s  say they have used social media for customer service in 2014 ,  up from of obviously  companies hope that such 1/url/docs/2014-x/2014-global-customer-uses are associated with a positive experience  yet there are limited tools for assessing this  emotions are a cardinal aspect of inter-personal communication: they are an implicit or explicit part of communication  and of particular importance in the setting of customer service  as they relate directly to customer satisfaction and experience (o  typical emotions expressed by customers in the context of social media service dialogues include anger and frustration  as well as gratitude and more)  however  it is important to note that emotions expressed by customer service agents are typically governed by company policies that specify which emotions should be expressed in which situation (rafaeli and sutton  1987)  this is whythe author talk in this paper about agent emotional techniques rather than agent emotions  consider  for example  the real (anonymized) twitter dialogue in figure in the figure in this dialogue  customer disappointment is expressed in the first turn (bummer/uh oh!) twitter\u2019s real-time customer service provides real-time support by monitoring tweets that customers address to it . companies that utilize the twitter platform as a channel for customer service use a dedicated twitter account which provides real-time support by monitoring tweets that customers address to it  at the same time corporate support agents reply to these tweets also through the twitter platform  a dialogue is defined to be a sequence of turns between a specific customer and an agent ,  where the customer initiates the first turn  consecutive posts of the same party ( customer or agent) uninterrupted by the other party  are considered as a single turn (even if there are several tweets)  given the nature of customer support services the author assume the last turn in the dialogue is an agent turn (e g   youre very welcome  :) hit us back any time you need support  thus the author expect an even number of turns in the dialogue the author filtered out dialogues in which more than one customer or one agent are involved the paper proposes to classify the current turn to emotions expressed by a customer and to predict the emotional technique used by an agent in a dialogue . for example ,  if the agent is asking for a topic before she is connected to a support agent  the customer is requested to provide a topic before she is connected to a support agent (usually using an ivr system) . the temporal family of features comprises the following features extracted from the timeline of the dialogue: customer/agent response time: two local features that indicate the time elapsed between the timestamp of the last customer/agent turn and the timestamp of the subsequent turn  this is a categorical feature with low values  medium or high (using categorical values yielded better results than using a continuous value)  median customer/agent response time: two local categorical features defined as the median of the customer/agent response times preceding the current turn  these features are the same as the previous temporal features  the emotional family of features includes agent emotion and customer emotion  these two sets of local binary features represent emotions predicted for previous turns  our model generates predictions of ti for each customer and agent turn  and uses these predictions as features to classify a later customer or agent turn with emotion expression  the temporal family of features includes the following features extracted from the timeline of the dialogue: customer/agent response time: two local features that indicate the time elapsed between the timestamp of the last customer/agent turn and the temporal features are also extracted using time values between previous turns . each 4/ judge performed the following tagging tasks given the full dialogue: emotion tagging: indicate the intensity of emotion expressed in each turn ( customer or agent) for each emotion ,  on a scale of 0  such that defines no emotion  a low emotion intensity and a high emotion intensity  topic tagging: select one or several topic: account issues  pricing  payments  customer service  customer experience  technical problem  technical question  order and delivery issues  behavior of a staff member  company policy issues and general statement  agent essence tagging: select one or several of the following for each agent\u2019s turn  to describe the agent\u2019s emotions in the specific turn  recognize the issue raised  asking for more information  offering a solution  general statement and assurance of the agent\u2019s efforts  for both classification tasks ,  svm-hmm dialogue model outperformed baseline results for almost all emotions  where average macro and micro results are statistically significant compared to the baseline . in the future , the author plan to run experiments in which the predicted emotional technique is actually applied in the context of new dialogues to measure the effect of such predictions on real support dialogues . an interesting use case for social media is customer support that can now take place over public social media channels. emotions are a cardinal aspect of inter-personal communication: they are an implicit or explicit part of essentially any communication, and of particular importance in the setting of customer service, as they relate directly to customer satisfaction and experience (oliver, 2014). the analysis of emotions being expressed in customer support conversations can take two applications: (1) to discern and compute quality of service indicators and (2) to provide real-time clues to customer service agents regarding the cus- service-barometer-us.pdf tomer emotion expressed in a conversation. another interesting trend in customer service, in addition to the use of social media described above, is the automation of various functions of customer interaction. several companies are developing text-based chat agents, typically accessible through corporate web sites, and partially automatized: in these platforms, a computer program handles simple conversations with customers, and more complicated dialogues are transferred to a human agent. given the importance of emotions in service dialogues, such systems will benefit from the ability to detect (customer) emotions and will need to guide employees (and machines) regarding the right emotional technique in various situations (e.g., apologizing at the right point). thus, our goal, in this paper, is to show that the functionality of guiding employees regarding appropriate responses can be developed based on the analysis of textual dialogue data.the author show first that it is possible to automatically detect emotions being expressed and, second that it is possible to predict the emotional technique that is likely to be used by a human agent in a given situation. this analysis reflects our ultimate goal: to enable a computer system to discern the emotions expressed by human customers, and to develop computerized tools that mimic the emotional technique used by a human customer service agent in a particular situation.the author see the main contributions of this paper as follows: (1) to our knowledge, this is the first research focusing on automatic analysis of emotions expressed in customer service provided through social media. (2) this is the first research using unique dialogue features (e.g., emotions expressed in previous dialogue turns by the agent and customer, time between dialogue turns) to improve emotion detection. this difference stems from the fact that in order to train an automated service agent to respond based on customer input, the agent\u2019s emotional technique needs to be computed before the agent generates its response sentence. specifically,the author described two classification tasks, one for detecting customer emotions and the other for predicting the emotional technique used by support service agent. as for future workthe author plan to work on several aspects: (1) in this work,the author showed that it is possible to predict the emotional technique.",
    "1005": "to make the architecture task adaptive ,  the paper proposes the concept of met-adapt controller modules  these modules are added to the model and are meta-trained to predict the optimal network connections for a given novel task  a few-shot classification (fsc) is a popular method for approaching fsc . in meta-learning ,  the inputs to both train and test phases are not images  but instead a set of few-shot tasks  ti  each k-shot / n-way task containing a small amount of k (usually) of labeled support images and some amount of unlabeled query images for each of the n categories of the task  the goal of meta-learning is to find a base model that is easily adapted to the specific task at hand  so that it will generalize well to tasks built from novel unseen categories and fulfill the goal of fsc  it seems that larger architectures increase fsc performance  up to a certain size  where performance seems to saturate or even degrade  this happens since bigger backbones carry higher risk of over-fitting  it seems the overall performance of the fsc techniques cannot continue to grow by simply expanding the backbone size  in light of the above  in this paperthe author set to explore methods for architecture search  their meta-adaptation and optimization for fsc  a few-shot learning problem is one where you train a model on a small number of examples ,  and you try to classify the new examples according to their proximity to the ones that have already been trained on the previous task . sub-models metadapt controllers predict the change in connectivity that is needed in the learned graph as a function of the current task  replacing simple sequence of convolutional layers with the suggested dag ,  with its many layers and parameters  in conventional gradient descent training will result in a larger over-fitting . this is even worse for fsl  where it is harder to achieve generalization due to scarcity of the data and the domain differences between the training and test sets  the weights are optimized using sgd optimizer with learning rate 0 . tldr; the authors propose met modifiers ,  a few-shot learning approach that enables meta-learned network architecture that is adaptive to novel few-shot tasks . the goal of meta-learning is to find a base model that is easily adapted to the specific task at hand, so that it will generalize well to tasks built from novel unseen categories and fulfill the goal of fsc (see section for further review). one of such major factors is the cnn backbone architecture at the basis of all the modern fsc methods. to summarize, our contributions in this work are as follows: (1)the author show that darts-like bi-level iterative optimization of layer weights and network connections performs well for few-shot classification without suffering from overfitting due to over-parameterization; (2)the author show that adding small neural networks, metadapt controllers, that adapt the connections in the main network according to the given task further (and significantly) improves performance; (3) using the proposed method,the author obtain improvements over fsc state-of-the-art on two popular fsc benchmarks: miniimagenet and fc100 these approaches include: (i) semi-supervised approaches using additional unlabeled data 9,14; (ii) fine tuning from pre-trained models 31,62,63; (iii) applying domain transfer by borrowing examples from relevant categories or using semantic vocabularies 3,15; (iv) rendering synthetic examples 42,10,56; (v) augmenting the training examples using geometric and photometric transformations or learning adaptive augmentation strategies 21; (vi) example synthesis using generative adversarial networks (gans) 69,25,20,48,45,35,11,23,2. in 22,54 additional examples are synthesized via extracting, encoding, and transferring to the novel category instances, of the intra-class relations between pairs of instances of reference categories. the list of search space operations used in our experiments is provided in table this list includes the zero-operation and identity-operation that can fully or partially (depending on the corresponding (i,j) o ) cut the connection or make it a residual one (skip-connection). darts, at search time the training is done on the full model at each iteration where each edge is a weighted-sum of its operations according to i,j contrarily, in snas i,j are treated as probabilities of a multinomial distribution and at each iteration a single operation is sampled accordingly. (9) here i,j are after softmax normalization and summed to at test time, rather than the one-hot approximation,the author use the operation with the top probability zi,jk 1, if k argmax(i,j) 0, otherwise (10) using this methodthe author get better results for fc100 1-shot and comparable results for 5-shot, compared to vanilla metaoptnet. the proposed approach effectively applies tools from the neural architecture search (nas) literature, extended with the concept of metadapt controllers\u2019, in order to learn adaptive architectures. these tools help mitigate over-fitting to the extremely small data of the few-shot tasks and domain shift between the training set and the test set.the author demonstrate that the proposed approach successfully improves state-of-the-art results on two popular few-shot benchmarks, miniimagenet and fc100, and carefully ablate the different optimization steps and design choices of the proposed approach. some interesting future work directions include extending the proposed approach to progressively searching the full network architecture (instead of just the last block), applying the approach to other few-shot tasks such as detection and segmentation, and researching into different variants of task-adaptivity including global connections modifiers and inter block adaptive wiring.",
    "1006": "for example ,  incomplete or internally inconsistent training data can lead to false classification of user intent  failure to maintain adequate context can cause chatbots to miss anaphoric references  in the extreme case  malicious actors may provide heavily biased responses  detecting egregious conversations is a new task ,  however  there is related work that aim at measuring the general quality of the interactions in conversational systems . in addition  some of these features are contextual  meaning that they are dependent on where in the conversation they appear  qualitative analysis of the data shows that it is not unusual to find a customer asking to be transferred to a human agent . such a request might indicate that the virtual agent is not providing a satisfactory service  moreover ,  even if there are human agents  they might not be available at all times  and thus  a rejection of such a request is sometimes reasonable  but might still lead to customer frustration  qualitative analysis of the data shows that it is not unusual to find a customer asking to be transferred to a human agent  see the table below for a more detailed analysis . the goal of this work is to give developers of automated agents tools to detect and then solve problems cre-ated by exceptionally bad conversations  automated conversational agents (chatbots) are becoming widely used for various tasks such as personal assistants or as customer service agents. recent studies project that of businesses plan to use chatbots by 20201, and that chatbots will power of customer service interactions by the year this increasing usage is mainly due to advances in artificial intelligence and natural language processing (hirschberg and manning, 2015) 1url 2url along with increasingly capable chat development environments, leading to improvements in conversational richness and robustness. the customer then tries to explain what went wrong, but the chatbot has insufficient exposure to this sort of utterance to provide anything but the default response (i\u2019m not trained on that). being able to automatically detect such conversations, either in real time or through log analysis, could help to improve chatbot quality. as an aid to chatbot improvement, analysis of egregious conversations can often point to problems in training data or system logic that can be repaired. specifically,the author consider customer inputs throughout a whole conversation, and detect cues such as rephrasing, the presence of heightened emotions, and queries about whether the chatbot is a human or requests to speak to an actual human.the author also studied how robust our features were: if our features generalize well, performance should not drop much when testing company b with the classifier trained exclusively on the data from company a. although company a and company b share similar conversation engine platforms, they are completely different in terms of objectives, domain, terminology, etc. specifically, in our setting, the relevant motivations are12: (1) natural language understanding (nlu) error - the agent\u2019s intent detection is wrong, and thus the agent\u2019s response is semantically far from the customer\u2019s turn; (2) language generation (lg) limitation - the intent is detected correctly, but the customer is not satisfied by the response (for example, the response was too generic); (3) unsupported intent error - the customer\u2019s intent is not supported by the agent. in order to detect nlu errors,the author measured the similarity between the first customer turn claiming that the best answer given by the system has the highest similarity value between the customer turn and the agent answer. to detect unsupported intent errorthe author used the approach described in section as reported in table 4, rephrasing due to an unsupported intent is more common in egregious conversations (18 vs. 14), whereas, rephrasing due to generation limitations (lg limitation) is more common in 12we did not consider other motivations like automatic speech recognition (asr) errors, fallback to search, and backend failure as they are not relevant to our setting.the author did not encounter any unsupported intent errors leading to customer rephrasing, which affected the ability of the rule-based model to classify those conversations as egregious. while customer rephrasing was captured by the egr model, for the text-based model some of the intents were new (did not appear in the training data) and thus were difficult for the model to capture.the author also plan to extend the work to detect egregious conversations in real time (e.g., for escalating to a human operators), and create log analysis tools to analyze the root causes of egregious conversations and suggest possible remedies.",
    "1007": "cnns used for computer vision can be interpreted by projecting filters into image space ,  but for discrete sequence inputs cnns remain a mystery the author aim to understand the method by which the networks process and classify text the author examine common hypotheses to this problem: that filters  accompanied by global max-pooling  serve as ngram detectors the author show that filters may capture several different semantic classes of ngrams by using different activation patterns  and that global max-pooling induces behavior which separates important ngrams from the rest  the paper examines and attempts to understand how cnns process text ,  and then uses this information for the more practical goals of improving model-level and prediction-level explanations . max-pooling across the document dimensions is applied followed by a relu activation  the result is then passed to a linear layer for the final classification  datasets sentiment analysis datasets mr - sentence polarity dataset containing 200k train and 25k test evenly split reviews . common intuition suggests that each filter is homogeneous and specializes in detecting a specific class of ngrams  for example ,  a filter may specialize in detecting ngrams such as had no issues  had zero issues  and had no problems the author challenge this view and show that filters often specialize in multiple distinctly different semantic classes by utilizing activation patterns which are not necessarily maximized the author also show that filters may not only identify good ngrams  but may also actively supress bad ones  in particular , the author reveal that filters are not necessarily homogeneous: a single filter may detect several different semantic patterns  each one of them relying on a different slot activation pattern . our second theory to explain the discrepancy between the activations of naturally occurring and possible ngrams is that certain filter slots are not used to detect a class of highly activating words  but rather to rule out a class of highly negative words the author refer to these as negative ngrams  the paper proposes to interpret each cnn by visualizing its filters and interpreting the visible shapes in other words ,  defining a high-level description of what the filter detects . the paper proposes to associate each filter with the following items: 1) the class which this filter\u2019s strong signals contribute to (in the sentiment task: positive or negative); 2) the threshold value for the filter  together with its purity and coverages percentages (which essentially capture how informative this filter is); 3) a list of semantic patterns identified by this filter  each list item corresponds to a slot-activation cluster  for each clusterthe author present the top-k ngrams activating it  and for each ngramthe author specify its total activation  its slot-activation vector  and its list of bottom-k negative ngrams with their activations and slot activations  in particular  by clustering the activated ngrams according to their slot activation patterns and showing the top-k in each cluster the author get a much more refined coverage of the linguistic patterns that are captured by the filter  - maxpooling over time induces a thresholding behavior on the output of the convolution layer ,  essentially separating between features that are relevant to the final classification and features that are not . - decompose the ngram score into word-level scores by treating the convolution of a filter as a sum of word-level convolutions  allowing us to examine the word-level composition of the activation  specifically  by maximizing the word-level activations by iterating over the vocabulary the author observed that filters do not maximize activations at the word-level  but instead form slot activation patterns that give different types of ngrams similar activation strengths  this provides empirical evidence that filters are not homogeneous  by clustering high-scoring ngrams according to their slot patternsthe author can identify the groups of linguistic patterns captured by a filter  - filters sometimes opt to assign negative values to certain word activations in order to cause the ngrams which contain them to receive a low score despite having otherwise highly activating words  convolutional neural networks as well as other traditional natural language processing , even when considering relatively simple one-layer models (kim, 2014). the ability to interpret neural models can be used to increase trust in model predictions, analyze errors or improve the model the problem of interpretability in machine learning can be divided into two concrete tasks: given a trained model, model interpretability aims to supply a structured explanation which captures what the model has learned. accompanying their rising popularity, cnns have seen multiple advances in interpretability when used for computer vision tasks , which is likely different than the role it has when processing text. in this work,the author examine and attempt to understand how cnns process text, and then use this information for the more practical goals of improving model-level and prediction-level explanations. specifically, current common wisdom suggests that cnns classify text by working through the following steps (goldberg, 2016): 1) 1-dimensional convolving filters are used as ngram detectors, each filter specializing in a closely-related family of ngrams. for our empirical experiments and results presented in this workthe author use three text classification datasets for sentiment analysis, which involves classifying the input text (user reviews in all cases) between positive and negative. however, asthe author cannot measure directly which values pj influence the final decision,the author opt instead for measuring correlation between pj values and the predicted label for the vector p. the linearity of the decision function wp allows to measure exactly how much pj is weighted for the logit of label class k. the class which filter fj contributes to is cj argmaxk wkjthe author refer to class cj as the class identity of filter fj by assigning each filter a class identity cj and comparing it to the predicted labelthe author arrive at a correlation labelwhether the filter\u2019s identity class matches the final decision by the network.the author consider two hypotheses to explain this behavior: (i) each filter captures multiple semantic classes of ngrams, and each class has some dominating slots and some non-dominating slots (whichthe author define as a slot activation pattern). in order to identify case negative ngrams,the author heuristically test whether the changed words\u2019 scores directly influence the status of the activation relative to the threshold: given an already identified negative ngram, if the ngram scoresans the bottom-k negative slot activations (considering a hamming distance of k and given that there are k negative slot activations)passes the threshold, yet it does not pass the threshold by including the negative slot activations, then the ngram is considered a case negative ngram. in this sectionthe author show two practical implications of the findings above: improvements in both model-level and prediction-level interpretability of 1d cnns for text classification. in particular, by clustering the activated ngrams according to their slot activation patterns and showing the top-k in each clusters,the author get a much more refined coverage of the linguistic patterns that are captured by the filter. finally,the author can also mark cases of negative-ngrams (section 5.4), where an ngram has high slot activations for some words, but these are negated by a highly-negative slot and as a consequence are not selected by max-pooling, or are selected but do not pass the filter\u2019s threshold. first,the author have shown that maxpooling over time induces a thresholding behavior on the convolution layer\u2019s output, essentially separating between features that are relevant to the final classification and features that are not.the author decompose the ngram score into word-level scores by treating the convolution of a filter as a sum of word-level convolutions, allowing us to examine the word-level composition of the activation. specifically, by maximizing the word-level activations by iterating over the vocabulary,the author observed that filters do not maximize activations at the word-level, but instead form slot activation patterns that give different types of ngrams similar activation strengths.the author also show that filters sometimes opt to assign negative values to certain word activations in order to cause the ngrams which contain them to receive a low score despite having otherwise highly activating words. finally,the author use these findings to suggest improvements to model-based and predictionbased interpretability of cnns for text.",
    "1008": "we suggest a new idea of editorial network a mixed extractive-abstractive summarization approach ,  which is applied as a postprocessing step over a given sequence of extracted sentences . for each sentence si s (in order) the editor makes one of the three possible decisions: extract ,  abstract or reject . therefore  the editor may modify the summary s by paraphrasing or rejecting some of its sentences  resulting in a mixed-abstractive summary s  in each step  the editor considers both the sentence representations esi and asi as its input  together with two additional auxiliary representations  the first auxiliary representation is that of the whole document d itself  such a representation provides a global context for the editor to make an educated decision  the second auxiliary representation is that of the corresponding sentence  which provides a local context for the editor to make an educated decision  the editor chooses a concatenation of the two sentence representations  which allows the network to compare the two versions on the same grounds  such representations allow the network to decide which version of the sentence is more relevant  the paper proposes a soft attention mechanism that can be used to generate an abstractive version of a given document by considering a chunk of its sentences . moreover  editnet implements a novel sentence rejection decision  allowing to correct initial sentence selection decisions which are predicted to negatively effect summarization quality  automatic text summarizers condense a given piece of text into a shorter version (the summary). this is done while trying to preserve the main essence of the original text and keeping the generated summary as readable as possible. extractive methods select and order text fragments (e.g., sentences) from the original text source. such methods are relatively simpler to develop and keep the extracted fragments untouched, allowing to preserve important parts, e.g., keyphrases, facts, opinions, etc. work was done during a summer internship in ibm research ai while such methods usually generate summaries with better readability, their quality declines over longer textual inputs, which may lead to a higher redundancy moreover, such methods are sensitive to vocabulary size, making them more difficult to train and generalize a common approach for handling long text sequences in abstractive settings is through attention mechanisms, which aim to imitate the attentive reading behaviour of humans two main types of attention methods may be utilized, either soft or hard. soft attention methods first locate salient text regions within the input text and then bias the abstraction process to prefer such regions during decoding on the other hand, hard attention methods perform abstraction only on text regions that were initially selected by some extraction process compared to previous works, whose final summary is either entirely extracted or generated using an abstractive process, in this work,the author suggest a new idea of editorial network (editnet) a mixed extractive-abstractive summarization approach. editnet is applied as a post-processing step over a given input summary whose sentences were initially selected by some extractor. let s denote a summary which was extracted from a given text (document) d. the editorial process is implemented by iterating over sentences in s according to the selection order of the extractor. as another example, based on the interaction between both sentence versions with either of the local or global contexts (and possibly among the last two), the network may learn that both sentence versions may only add superfluous or redundant information to the summary, and therefore, decide to reject both.the author trained, validated and tested our approach using the non-annonymized version of the cnn/dailymail dataset following ,the author used the story highlights associated with each article as its ground truth summary. interestingly, editnet\u2019s summarization quality is quite similar to that of neusum yet, while neusum applies an extraction-only approach, summaries generated by editnet include a mixture of sentences that have been either extracted or abstracted. considering the complexity of these models, and the slow down that can incur during training and inference,the author think that editnet still provides a useful, high quality and relatively simple extension on top of standard encoder aligned decoder architectures. this demonstrates that, editnet has a high capability of utilizing abstraction, while being also able to maintain or reject the original extracted text whenever it is estimated to provide the best benefit for the summary\u2019s quality.",
    "1009": "the authors propose a high-dimensional encoding of chinese phonetic similarities to improve performance . although at first glance it may seem that phonetic similarity can only be quantified for pairs of words ,  this problem is often present in purely textual spaces  such as social media posts or text messages . correct homophones and synophones  whether used in error or incorrectly  pose challenges for a wide range of nlp tasks  such as named entity identification  text normalization and spelling correction  these tasks must therefore successfully transform incorrect words or phrases (hear  sew  so) to their phonetically similar counterparts (here  thesew)  a reliably able approach for generating phonetically similar words is equally crucial for chinese text unfortunately  most existing phonetic similarity algorithms such as soundex (philips and mean administration  and double metaphone (dm) ) are motivated by english and designed for indo-chinesean languages  words are encoded to approximate phonetic linguistic characteristics by ignoring (except ones foremost) syllables  which is appropriate where phonetic transcription is used  in contrast  the minimum speech sound of a character is represented by a single dimensional encoding consisting of two or three parts: an initial (optional)  a final or compound  and a final or compound finals  as a result  phonetic similarity approaches designed for indo-chinese languages often fall short when applied to chinese text  dimsim generates ranked candidate words similar to a seed word . similarity is measured by a phonetic distance metric based on n-dimensional encodings  an important characteristic of pinyin is that the three components ,  initial  final and tone  can be independently phonetically compared  for example  the phonetic similarity of the ie and ue is identical in the pinyin pairs xie2 ue2 and lie2 lue2  in spite of the varying tonality  english contrast  on the other hand  does not have this characteristic  for example ,  u is written as u after j  q  x uo is written as o after b  p  m  f or w . there are a total of six rewritten rules in pinyin (iso)  since these rules are fixed  it is straightforward to preprocess the pinyins according to these rules to turn them into the original form of pinyins as an internal representation before conducting the comparison  for example the author represent ju as bo as theo  after the preprocessing step the author independently compare ompon nts  this is done by grouping pinyin components into initial clusters and only annotating pairs within each cluster ,  and represent tive cluster pairs .the author also incorporate a penalty function ,  p  for pairs deviating from the manually annotated distance so that more similar pairs are penalized more highly  precision is measured via a manual annotation task on the top-ranked candidates generated by each approach . however ,  as features of art are manually assigned  these algorithms fall short in capturing the perceptual essence of phonetic similarity through empirical data . a reli- able approach for generating phonetically similar words is equally crucial for chinese text unfortunately, most existing phonetic similarity algorithms such as soundex (archives and administration, 2007) and double metaphone (dm) philips (2000) are motivated by english and designed for indo-european languages. as a result, phonetic similarity approaches designed for indo-european languages often fall short when applied to chinese text. this paper presents dimsim, a learned ndimensional phonetic encoding for chinese along with a phonetic similarity algorithm, which uses the encoding to generate and rank phonetically similar words. to address the complexity of relative phonetic similarities in pinyin components,the author propose a supervised learning approach to learn n dimensional encodings for finals and initials where n can be easily extended from one to two or higher dimensions. the learning model derives accurate encodings by jointly considering pinyin linguistic characteristics, such as place of articulation and pronunciation methods, as well as high quality annotated training data sets.the author t n use ed t distance and common sequence length con traints to g ide the pair generation; specifically,the author compare a pair of finals if the edit distance between them is or since the length of finals on average is two, an edit distance of three means a complete change to the final, resulting in pairs with the lowest similarity. motivated by phonetic transcription as a widely observed phenomenon in chinese social media and informal language,the author have designed an accurate phonetic similarity algorithm. dimsim generates phonetically similar candidate words based on learned encodings that capture the pronunciation characteristics of pinyin initial, final, and tone components. these tasks all share a dependency on reliable phonetic similarity as an intermediate step, especially for languages such as chinese where incorrect homophones and synophones abound.",
    "1010": "in this paper , the author propose a hierarchical recurrent neural network enhanced by residual learning which detects kb relations given an input question  our method uses deep residual lstms to compare questions and relation names via different levels of abstraction  additionally the author propose a simple kbqa system that integrates entity linking and our proposed relation detector to make the two components enhance each other  our experimental results show that our approach not only achieves outstanding relation detection performance  but more importantly  it helps our kbqa system achieve state-of-the-art accuracy for both single-relation (simple) and multi-relation (webqsp) qa benchmarks  the paper proposes to break the relation names into word sequences for question-relation matching . url given an input question and a set of candidate entities retrieved by an entity linker based on the question ,  our proposed relation detection model plays a key role in the kbqa process: re-ranking the entity candidates according to whether they connect to high confident relations detected from the raw question text by the relation detection model . this step is important to deal with the ambiguities normally present in entity linking results  finding the core relation (chains) for each topic entity selection from a much smaller candidate entity set after re-ranking  the above steps are followed by an optional constraint detection step  when the question cannot be answered by single relations (e g   multiple entities in the question)  finally  the highest scored query from the above 2following yih et al single-relation kbqa - split relations to word sequences for single-relation detection . however ,  while the questions are natural word sequences  how to represent relations as sequences remains a challenging problem  herethe author give an overview of two types of relation sequence representations commonly used in previous work  relation as a single token (relation-level)  in this case  each relation name is treated as a unique token  the problem with this approach is that it suffers from the low relation coverage due to limited amount of training data  thus cannot generalize well to large number of opendomain relations  relation as word sequence (word-level)  the relation is treated as a sequence of words from the tokenized relation name  it has better generalization  but from the lack of global information from the original relation names  this is because the incorrect relation contains word plays  which are more similar to the question 3 entity information used in kbqa systems as features for the final answer re-rankers  (containing word play) in the embedding space  on the other hand  if the target relation co-occurs with questions related to tv appearance in training  by treating the whole relation as a token (i e  relation id) the author could better learn the correspondence between this token and phrases like tv show link to the paper(url) example(url example) matching question to different aspects of a relation (with different abstraction levels) , the author deal with three problems as follows on learning question/relation representations .the author provide our model with both types of relation representation: word-level and relation-level  therefore  the input relation becomes rword1   rwordm1 rrel1  rrelm2  where the first m1 tokens are words (e g  episode  written) and the last m2 tokens are relation names  e  episode written or starring roles  series the author transform each token above to its word embedding-ding then use two bilstms (with shared parameters) to get their hidden representations bword1:m1 brel1:m2 (each row vector i is the concatenation between forward/backward representations at i) the author initialize the relation sequence lstms with the final state representations of the word sequence  as a back-off for unseen relations the author apply one max-pooling on these two sets of vectors and get the final relation representation hr  from table 1 the author can see that different parts of a relation could match different contexts of question texts however ,  deep bilstms do not guarantee that the two-levels of question hidden representations are comparable . this is mainly because deep bilstms do not guarantee that the two-levels of question hidden representations are comparable  the training usually falls to local optima where one layer has good matching scores and the other always has weight close to the training of deeper architectures itself  to overcome the above difficulties  they adopt the idea from residual networks for hierarchical matching by adding shortcut connections between two bilstm layers  entity reranking (first-step relation detection): use the raw question text as input for a relation detector to score all relations in the kb that are associated to the entities in the question . use the relation scores to re-rank the elk(q) and generate a shorter list el0k0 containing the top-k0 entity candidates  detect relation(s) using the reformatted question text in which the topic entity is replaced by a special token eley  combine the scores from step 2 and 3 and select the top pair (e ,  r) using constraint detection: compute similarity between q and any neighbor entity along r (e  r  rc) to add the high scoring c and rc to the query  finally  select the relative position of the entity in the question and the number of entities in the given episode to assign high scores to the candidates  url given the top-scored query generated by the previous steps , the author collect all the nodes c connecting to v (with any relation) with any relation  and generate a sub-graph associated to the original query . entity matching on sub-graph nodes the author compute a matching score between each n-gram in the input question (without overlapping the topic entity) and entity name of c (except for the node in the original query) by taking into account the maximum overlapping sequence of characters between them  if the matching score is larger than a threshold (tuned on training set) the author will add the constraint entity c (and rc) to the query by attaching it to the corresponding node on the core-chain  link to the paper(url) evaluation the authors create a new relation detection task from the webqsp dataset and select all the relations and relation chains (length 2) connected to the topic entity ,  and set the core-chain labeled in the parse as the positive label and all the others as the negative examples . they tune the following hyper-parameters on the development sets: the size of the hidden states for lstms  100  200  4009; learning rate (0 1  0 5  1 0); whether the shortcut connections are between hidden states or between maxpooling results  on webqsp ,  hr-bilstm outperforms bilstm with shortcut connections . kb relation detection is a key step in kbqa and is significantly different from general relation extraction tasks . figure illustrates the process used to parse two sample questions in a kbqa system: ; and (b) a more complex case, where some constraints need to be handled for multiple entities in the question. first, in most general relation detection tasks, the number of target relations is limited, normally smaller than in contrast, in kbqa even a small kb, like freebase2m , contains more than 6,000 relation types. nowthe author have question contexts of different lengths encoded in (1)1:n and (2) 1:n unlike the standard usage of deep bilstms that employs the representations in the final layer for prediction, herethe author expect that two layers of question representations can be complementary to each other and both should be compared to the relation representation space (hierarchical matching). in order to highlight the effect of different relation detection models on the kbqa end-task,the author also implemented another baseline that uses our kbqa system but replaces hr-bilstm with our implementation of ampcnn (for simplequestions) or the char-3-gram bicnn (for webqsp) relation detectors (second block in table 3). since the reranking step relies on the relation detection models, this shows that our hr-bilstm model contributes to the good performance in multiple ways. this is probably because our joint performance on topic entity and core-chain detection is more accurate (77.5 top-1 accuracy), leaving a huge potential (77.5 vs. 58.0) for the constraint detection module to improve. finally, like stagg, which uses multiple relation detectors for the three models used),the author also try to use the top-3 relation detectors from section as shown on the last row of table 3, this gives a significant performance boost, resulting in a new state-of-the-art result on simplequestions and a result comparable to the state-of-the-art on webqsp.",
    "1011": "in this paper , the author propose a human-in-the-loop (huml) dictionary expansion approach that employs a lightweight neural language model coupled with tight huml supervision to assist the user in building and maintaining a domain-specific dictionary from an input text corpus . the approach is based on the explore/exploit paradigm to effectively discover new instances (explore) from the text corpus as well as predict new unseen terms not currently in the corpus using the accepted dictionary entries (exploit)  the authors propose a feature agnostic approach for dictionary expansion based on lightweight neural language models ,  such as word2vec . given an input text corpus and a set of seed examples  the proposed approach runs in two phases  explore  and exploit  the first phase tries to identify similar instances to the related dictionary entries that are present in the input text corpus  using term vectors from the neural language model to calculate a similarity score  the second phase tries to construct more complex multi-term phrases  multi-term phrases are a challenge for word2vec systems as they need to be known prior to model creation to identify multi-term phrases most commonly the most commonly used phrase in the input text corpus  the co-occurrence score  i e   of a term is used to detect the same phrase  however  depending on the domain and the task  the instances evolve interest  or the example corpus may not be complete  for example  valid phrase combinations may simply not occur (e g  a joint pain may appear in the sample corpus  but acute pain may not occur in future texts from the same source  and thus are important to include in any extraction lexicon  the authors show that the proposed approach is effective in the evaluation the paper proposes a human-in-the-loop approach for dictionary extraction from text . the approach is similar to glimpse and glimpseld ,  statistical algorithms for dictionary extraction based on spot with a faster underlying matching engine  the input is a large text corpus and a set of seed examples  starting from these  it evaluates the contexts (the set of words surrounding an item) in which the seeds occur and identifies good contexts  contexts are scored retrospectively in terms of how many good results they generate  all contexts are kept which have a score over a given threshold and the candidates that appear in the dictionary are only linked to the most good contexts  the approach is based on the explore/exploit paradigm to effectively discover new instances (explore) from the text corpus and generate new unseen instances based on user feedback (exploit) . the approach runs in iterations ,  where each iteration runs first the explore phase then the exploit phase  the explore phase uses the instances available in the input dictionary to identify similar candidates that are already present in the corpus vocabulary vtc  which are then accepted or rejected by the huml  the accepted candidates are then added to the input dictionary and are used in the exploit phase as well as the next explore iteration  during the exploit phase the author use the instances in the input dictionary to construct more complex phrases that might be of interest for the user  therefore  the problem of calculating the similarity between two instances is a matter of calculating the dictionary distance between two instances in the given feature space  to do sothe author use the standard cosine similarity measure which is applied on the vectors of the instances  formally  the similarity between two terms w1 and w2  with vectors v1 and v2 is calculated as the cosine similarity between the vectors v1 and v2the author calculate the similarity between the instances in the input dictionary and all the words in the corpus vocabulary vtcthe author sort the vocabulary in descending order using the cumulative similarity score when using exploit ,  the number of newly discovered instances sharply decreases as no new base terms are introduced  thus the exploit cannot generate new instances that can be added in the dictionary . the results show that using explore and exploit alternately leads to the best performances  this paper proposes an interactive dictionary expansion tool using a lightweight neural language model . dictionary expansion is one area where close integration of humans into the discovery loop has been shown to enhance task performance substantially over more traditional post-adjudication. thus even with a system which finds similar terms (e.g., word2vec) guidance is important to keep the system focused on the subject matter expert\u2019s notion of lexicon. given an input text corpus and a set of seed examples, the proposed approach runs in two phases, explore and exploit, to identify new potential dictionary entries. the explore phase tries to identify similar instances to the dictionary entries that are present in the input text corpus, using term vectors from the neural language model to calculate a similarity score. the exploit phase tries to construct more complex multi-term phrases based on the instances already in the input dictionary. multi-term phrases are a challenge for word2vec style systems as they need to be known prior to model creation. to identify multi-term phrases, most commonly a simple phrase detection model is used, which is based on a term\u2019s co-occurrence score, i.e., terms that often appear together probably are part of the same phrase the phrase detection must be done before the model is built, and they remain unchanged after the model is built. however, these phrases are likely to occur in future texts from the same source, and thus are important to include in any entity extraction lexicon.the author use two phrase generation algorithms: (i) modify the phrases by replacing single terms with similar terms from the text corpus, e.g., abnormal behavior can be modified to strange behavior; (ii) extend the instances with terms from the text corpus that are related to the terms in the instance, e.g., abnormal blood clotting problems is a an adverse drug reaction, which doesn\u2019t appear as such in a large text corpus, however the instances abnormal blood count, blood clotting and clotting problems appear several times in the corpus, which can be used to build the more complex instance. in the first approach,the author first break each instance in to a set of single terms t t1, t2, ..., tn, then for each term ti in tthe author identify a set of similar terms tsti ts1, ts2, ..., tss in the vocabulary vtc using equation in the next step,the author build new phrases by replacing ti with a term tsi from tsti the new phrases are sorted based on the similarity score and the top-n are selected as candidates. in this experimentthe author show the importance of the promptness of the huml on the number of newly discovered instances, i.e.,the author evaluate if the user gives their feedback to the system sooner it will improve the performance of the system. in future work,the author will analyze more complex language neural network models, such as recurrent neural networks (rnn), long short term memory networks (lstm), and bidirectional lstm, which might improve the search for similar and related terms, at the expense of higher training time. furthermore, future work will include an evaluation of the approach on multiple datasets covering different domains.",
    "1012": "in this work , the author demonstrate thatthe author can cast the problem of textual grounding into a unified framework that permits efficient search over all possible bounding boxes . hence  the method is able to consider significantly more proposals and doesnt rely on a successful first stage hypothesizing bounding box proposals  finding bounding boxes in images which relate to text phrases is an important problem for human-computer interaction ,  robotics and mining knowledge bases . for example the author may want an autonomous system by using phrases such as the bottle on your left  or the top shelf lamp  while those phrases are easy to interpret for a human  they pose significant challenges for textual grounding algorithms  as interpretation of those phrases requires an understanding of objects and their relations  existing approaches for textual grounding  such as 38  take advantage of the cognitive performance improvements obtained from deep net models  more deep net models are designed to extract features from bounding boxes and textual data  which are then compared to measure their fitness  to obtain suitable bounding boxes  many of the textual grounding frameworks  make use of the region proposals  while being easy to obtain  automatic extraction of region proposals is limiting the performance of the visual grounding  because the performance of the visual grounding is inherently constrained by the quality of the proposal generation procedure  in this work the author describe an interpretable mechanism for which any image concepts  such as semantic segmentations  detections and priors  can be used as the ground truth  this paper proposes a method to explicitly ground natural language in images and videos . the search over a large number of bounding boxes allows us to retrieve an accurate bounding-box prediction for a given phrase and an image  the energy is based on a set of image concepts like semantic segmentations ,  detections or image priors  all those concepts come in the form of score maps whichthe author combine linearly before searching for the bounding box containing the highest accumulated score over the combined score map  it is trivial to add additional information to our approach by adding additional score maps  moreover  linear combination of the score maps reveals importance of score maps for specific queries as well as similarity between queries such as token snowboarder  in order to process free-form textual phrases efficiently , the author restricted the vocabulary size to the top most frequent words in the training set for the referitgame  and to the top most frequent words for 30k entities . for the referitgame the author further fine-tuned the last layer of the deeplab system to include the categories of sky  ground  building  water  tree  grass  in contrast to existing approaches which are generally based on a small set of bounding box proposals , the author efficiently search over all possible bounding boxes . for example,the author may want to guide an autonomous system by using phrases such as the bottle on your left,\u2019 or the plate in the top shelf.\u2019 while those phrases are easy to interpret for a human, they pose significant challenges for present day textual grounding algorithms, as interpretation of those phrases requires an understanding of objects and their relations. more specifically, deep net models are designed to extract features from given bounding boxes and textual data, which are then compared to measure their fitness. while being easy to obtain, automatic extraction of region proposals is limiting, because the performance of the visual grounding is inherently constrained by the quality of the proposal generation procedure. second bike from right in front painting next to the two on theleft person all the way to the right figure 1: results on the test set for grounding of textual phrases using our branch and bound based algorithm. moreover, linear combination of score maps reveals importance of score maps for specific queries as well as similarity between queries such as skier\u2019 and snowboarder.\u2019 hence the framework thatthe author discuss in the following is easy to interpret and extend to other settings. general problem formulation: for simplicitythe author use x to refer to both given input data modalities, i.e., x (q, i), with query text, q, and image, ithe author will differentiate them in the narrative. language processing: in order to process free-form textual phrases efficiently,the author restricted the vocabulary size to the top most frequent words in the training set for the referitgame, and to the top most frequent training set words for flickr 30k entities; both choices cover about of all phrases in the training set. for detection,the author use the yolo object detection system 37, to extract categories, trained on pascal voc-2012, and trained on mscoco for pose estimation,the author use the system from to extract the body part location, then post-process to get the head, upper body, lower body, and hand regions. the word vectors capture image-spatial relationship of the words, meaning items that can be replaced in an image are similar; (e.g., a snowboarder can be replaced with a skier and the overall image would still be reasonable). the inference speed can be divided into three main parts, (1) extracting image features, (2) extracting language features, and (3) computing scores. for extracting image features, grounder requires a forward pass on vgg16 for each image region, where cca and our approach requires a single forward pass which can be done in ms. for extracting language features, our method requires index lookups, which takes negligible amount of time (less than 1e-6 ms).",
    "1013": "tldr; the authors propose a new method for learning implicit generative models by performing mean and covariance matching of features extracted from pretrained deep convnets . tldr; the authors propose generative feature matching networks (gfmn) ,  an approach to train implicit generative models that does not use adversarial or online learning of kernel functions  provides stable training  and state-of-the-art results . autoencoder features: a natural choice of unsupervised method to train a feature extractor is the autoencoder (ae) framework  the decoder part of an ae consists exactly of an image generator that uses features extracted by the encoder  therefore ,  by design  the encoder network should be a good feature extractor for the purpose of generation  classifier features:the author experiment with different dcnn architectures pretrained on imagenet to play the role of the feature extractor  our hypothesis is that imagenet-based pfs are informative enough to allow the training of (crossdomain) generators by feature matching  in order to train with a mean and covariance feature matching loss ,  one needs large minibatches to obtain good mean and covariance estimates . with images larger than 3232  dcnns produce millions of features  resulting easily in memory issues the author propose to alleviate this problem by using moving averages of the difference of the means (covariances) of real and generated data  instead of computing the (memory) expensive feature matching loss in eq  1 the author keep moving averages of the difference of feature means (covariances) at layer j between real and generated data the author can now rely on vj to get better estimates of the population feature means of real and generated data while using a small minibatch of sizen for a similar result using the feature matching loss given in eq  one would need a minibatch with large size n  which is problematic for large number of features  mmd(k ,  p  q) mmd(k generative moment matching network autoencoder (gfmn) uses a gaussian kernel to perform mean and covariance matching in a pf space induced by a non-linear kernel function that is orders of magnitude larger than the ae latent code . dcnn features, usually called perceptual features (pfs), have been used in tasks such as transfer learning 40, 16, style transfer and super-resolution while there have been previous works on the use of pfs in the context of image generation and transformation 7, 17, exploration of pfs as key source of information for learning generative models is not well studied. moment matching approaches for generative modeling are based on the assumption that one can learn the data distribution by matching the moments of the model distribution to the empirical data distribution. more specifically,the author propose a simple but effective moment matching method that: (1) breaks away from the problematic min/max game completely; (2) does not use online learning of kernel functions; and (3) is very efficient with regard to both number of used moments and required minibatch size. the main advantage of ama over simple moving average (ma) is in its adaptive first and second order moments that ensure stable estimation of the moving averages vj in fact, this is a non-stationary estimation since the mean of the generated data changes in the training, and it is well known that adam works well for such online non-stationary losses in sectionthe author provide experimental results supporting: (1) the memory advantage that the ama formulation of feature matching offers over the naive implementation; (2) the stability advantage and improved generation results that ama allows compared to the naive implementation. gfmn is related to the recent body of work on mmd and moment matching based generative models 22, 8, 21, 3, the closest to our method is the generative moment matching network autoencoder (gmmnae) proposed in in gmmnae, the objective is to train a generator g that maps from a prior uniform distribution to the latent code learned by a pretrained ae, and then uses the frozen pretrained decoder to map back to image space. presents samples from gfmnvgg19 trained with celeba dataset with resolution 128128, which shows that gfmn can achieve good performance with image resolutions larger than these results also demonstrate that: (1) the same classifier (vgg19 trained on imagenet) can be successfully applied to train gfmn models across different domains; (2) perceptual features from dcnns encapsulate enough statistics to allow the learning of good generative models through moment matching.the author achieve successful non-adversarial training of implicit generative models by introducing different key ingredients: (1) moment matching on perceptual features from all layers of pretrained neural networks; (2) a more robust way to compute the moving average of the mean features by using adam optimizer, which allows us to use small minibatches; and (3) the use of perceptual features from multiple neural networks at the same time (vgg19 resnet18). in this appendix,the author present a comparison between the simple moving average (ma) and adam moving average (ama) for the case where vgg19 imagenet classifier is used as a feature extractor. this experiment uses a minibatch size of that ama has a very positive effect in the quality of generated images.",
    "1014": "the demand for items is a combined effect of form utility and time utility ,  i e   a product must both be intrinsically appealing to a consumer and the time must be right for purchase  for durable goods  time utility is a function of inter-purchase duration within product category because consumers are unlikely to purchase two items in the same category in close temporal succession  moreover  purchase data  in contrast to rating data  is implicit with non-purchases not necessarily in the same category  together  these issues give rise to the positive-unlabeled demand-aware recommendation problem indicating thatthe author pose via joint low-rank tensor completion and product category inter-purchase duration vector estimation the author further relax this problem and propose a highly scalable alternating minimization approach with whichthe author can solve problems with millions of users and millions of items in a single thread for example ,  a user might not be interested in purchasing an item because she is not satisfied with it at the time it was bought  or because she is not satisfied with the current value of the item  or because she is not satisfied with the current value of the item at the time it was bought . for example  the authors propose a model that captures these two aspects of user behavior  and that is trained to predict the desired item in the future  time-aware recommender systems exploit temporal information but do not explicitly consider the notion of time utility derived from inter-purchase durations in item categories . in particular ,  given a set of m users  n items  and l time slots the author construct a third-order binary tensor p 0  1mnl to represent the purchase history . specifically  entry p indicates that user i has purchased item j in time slot k the author assume that the n items belong to r item categories  with items in each category sharing similar inter-purchase durations 3the author use an n-dimensional vector c 1  2  rn to represent the category membership of each item  given p and c the author further generate a tensor tjk where ticjk denotes the number of time slots between user is most recent purchase within category cj until time k  if user i has not purchased within category cj until time k  ticjk is set to  in this work , the author use an underlying third-order tensor x rmnl to quantify form utility . in addition the author employ a non-negative vector d rr to measure the underlying inter-purchase duration times of the r item categories  it is understood that the inter-purchase durations for durable good categories are large  while for non-durable good categories are small  or even zero  in this study the author focus on items inherent properties and assume that the inter-purchase durations are user-independent  the overall utility can be obtained by comparing form utility and time utility  the overall utility can be obtained by comparing the form utility and time utility  to this end , the author assume that an individual\u2019s form utility does not change over time  an assumption widely used in many collaborative filtering methods .the author include these two algorithms as baselines to justify whether traditional collaborative filtering algorithms are suitable for general e-tail recommendation involving both durable and non-durable goods  since they require explicit ratings as inputs the author follow to generate numerical ratings based on the frequencies of (user  item) consumption pairs  wr-mf is essentially the positive-un durable version of pmf and has shown to be very effective in modeling the implicit feedback data  all the other three baselines  i e   cp-apr  and bptf  are tensor-based methods that can consider time utility when making recommendations the author refer to the proposed recommendation algorithm as demand-aware recommender in this paper , the author examine the problem of demand-aware recommendation in settings when inter-purchase duration within item categories affects users intention to purchase items in combination with intrinsic properties of the items themselves .the author formulate it as a tensor nuclear norm minimization problem that seeks to jointly learn the form utility tensor and a vector of inter-purchase durations  and propose a scalable optimization algorithm with a tractable time complexity  e-commerce recommender systems aim to present items with high utility to the consumers utility may be decomposed into form utility: the item is desired as it is manifested, and time utility: the item is desired at the given point in time 28; recommender systems should take both types of utility into account. a key assumption made by matrix factorization- and completion-based collaborative filtering algorithms is that the underlying rating matrix is of low-rank since only a few factors typically contribute to an individual\u2019s form utility however, a user\u2019s demand is not only driven by form utility, but is the combined effect of both form utility and time utility. an additional challenge faced by many real-world recommender systems is the one-sided sampling of implicit feedback 15, unlike the netflix-like setting that provides both positive and negative feedback (high and low ratings), no negative feedback is available in many e-commerce systems. given purchase triplets (user, item, time) and item categories, the objective is to make recommendations based on users\u2019 overall predicted combination of form utility and time utility. specifically,the author model a user\u2019s time utility for an item by comparing the time t since her most recent purchase within the item\u2019s category and the item category\u2019s underlying inter-purchase duration d; the larger the value of d t, the less likely she needs this item. compared to existing recommender systems, our work has the following contributions and advantages: (i) to the best of our knowledge, this is the first work that makes demand-aware recommendation by considering inter-purchase durations for durable and nondurable goods; (ii) the proposed algorithm is able to simultaneously infer items\u2019 inter-purchase durations and users\u2019 real-time purchase intentions, which can help e-retailers make more informed decisions on inventory planning and marketing strategy; (iii) by effectively exploiting sparsity, the proposed algorithm is extremely efficient and able to handle large-scale recommendation problems. in the first task,the author record the highest ranking of items that are within item i\u2019s category among all items at time t. since a purchase record (u, i, t) may suggest that in time slot t, user u needed an item that share similar functionalities with item i, category prediction essentially checks whether the recommendation algorithms recognize this need. although they may not perfectly reflect the true inter-purchase durations, the estimated durations clearly distinguish between durable good categories, e.g., automotive, musical instruments, and non-durable good categories, e.g., instant video, apps, and food. as a final note,the author want to point out that tmall and amazon review may not take full advantage of the proposed algorithm, since (i) their categories are relatively coarse and may contain multiple sub-categories with different durations, and (ii) the time stamps of amazon review reflect the review time instead of purchase time, and inter-review durations could be different from inter-purchase durations. in this paper,the author examine the problem of demand-aware recommendation in settings when interpurchase duration within item categories affects users\u2019 purchase intention in combination with intrinsic properties of the items themselves. on two real-world datasets, tmall and amazon review,the author show that our algorithm outperforms six state-of-the-art recommendation algorithms on the tasks of category, item, and purchase time predictions.",
    "1015": "the paper presents a neural response generation model that generates responses conditioned on a target personality . the model learns high level features based on the target personality ,  and uses them to update its hidden state  it is desirable for automated agents to be capable of generating responses that express a target personality . personality is defined as a set of traits which represent durable characteristics of a person  many models of personality exist while the most common one is the big five model (digman ,  1990) including: openness  conscientiousness  extraversion  agreeableness  and empathetic  these traits were correlated with linguistic choices including lexicon and syntax  in this paper the author study how to encode personality traits as part of neural response generation for conversational agents  our approach builds upon a sequence-to-sequence by adding an additional layer that represents the target set of personality traits  and a hidden layer that learns high-level personality based features  the response is then generated conditioned on these features  tldr; the authors propose an approach to generate responses that express a target personality without explicitly defining linguistic features . 5k conversations over customer service twitter channels  in future work , the author would like to generate responses adapted to the personality traits of the customer as well  and to apply our model to other tasks such as education systems . personality is defined as a set of traits which represent durable characteristics of a person. our approach builds upon a sequence-to-sequence by adding an additional layer that represents the target set of personality traits, and a hidden layer that learns high-level personality based features. specifically,the author focus on conversational agents for customer service; in this context, many studies examined the effect of specific personality traits of human agents on service performance. the first response (in each example), is generated by a standard seq2seq response generation system that ignores personality modeling and in effect generates the consensus response of the humans represented in the training data. the second response is generated by our system, and is aimed to generate data for an agent that expresses a high level of a specific trait.the author experimented with a dataset of 87.5k real customer-agent utterance pairs from social media. neural response generation models are based on a seq2seq architecture and employ an encoder to represent the user utterance and an attention-based decoder that generates the agent response one token at a time. neural response generation can be viewed as a sequence-to-sequence problem , where a sequence of input language tokens x x1, , xm , describing the user utterance, is mapped to a sequence of output language tokens y1, , yn , describing the agent response. now, at each token generation, the decoder updates the hidden state conditioned on the personality traits features hp, as well as on the previous hidden state, the output token and the context. cold start: in our second experiment,the author split the dataset such that of the agents only formed the validation and test sets (half of each agent\u2019s examples for each set). note that,the author extracted target personality traits for agents in the training set using their training data, or, for agents in the test set, using validation data. table shows that, in this setting,the author get better performance by utilizing personality based representation: our model achieves a relative decrease in perplexity, and a relative improvement in bleu score. results from both experiments demonstrate thatthe author can better model the linguistic variation in agent responses by conditioning on target personality traits.the author conducted a human evaluation of our personality-based model using a crowd-sourcing service.the author focused on two personality traits from the big five model that are important to customer service: agreeableness and conscientiousnessthe author extracted customer utterances from the validation set of the cold start setting described above.the author selected customer utterances that convey a negative sentiment, since re- sponses to this kind of utterances vary much.the author generated a high-trait target personality distribution (trait was either agreeableness or conscientiousness), where trait was set to a value of 0.9, and all other traits to similarly,the author created a low-trait version where trait was set to for each trait and customer utterancethe author generated a response for the high-trait and low-trait versions. each triplet methodology, the two responses were presented in a random order, and judged on a 5-point zero-sum scale. a score of (2) was assigned if one response was judged to express the trait more (less) than the other response, and (1) if one response expressed the trait somewhat more (less) than the other. after discarding ties,the author found that the high-trait responses generated by our personality-based model were judged either more expressive or somewhat more expressive than the low-trait corresponding responses in of cases.the author have presented a personality-based response generation model and tested it in customer care tasks, outperforming baseline seq2seq model.",
    "1016": "this is the first time that an end-to-end stereo pipeline from image acquisition and rectification ,  multi-scale spatio-temporal stereo correspondence  winner-take-all  to disparity regularization is implemented fully on event-based hardware . event-based computation has two main drawbacks: it is limited by the frame rate ,  which limits the resolution of frame-based cameras  and it is highly redundant  which leads to downstream data transfer and energy consumption  event-based computation has the advantage of low power consumption  but it is limited by the computation time  they use a network of spiking neurons as the nodes . the ibm truenorth is a reconfigurable ,  non-von neumann neuromorphic chip containing million spiking neurons and million synapses distributed across parallel  event-driven  neurosynaptic cores are tiled in an array  embedded in a fully asynchronous network-on-chip  depending on event dynamics and network architecture  faster tick period is possible  whichthe author take advantage of in this work to achieve as low as ms per tick  thus doubling the maximum throughput achievable  the proposed local event-based stereo correspondence algorithm is implemented end-to-end as a neuromorphic event-based algorithm . this consists of systems of equations defining the behavior of true neurons ,  encased in modules called corelets  and the subsequent composition of the inputs and outputs of these modules  the stereo rectification is defined by a pair of functions l  r which map each pixel in the left and right sensor polarity channels to a pixel in the left and right sensor(t) native resolution respectively  on truenorth  this is implemented using h w membrane neurons per sensor polarity channel  arranged in an h w retinotopic map  the events at each rectified pixel are generated through neurons which replicate corresponding sensor pixels  their potential is defined by v splp (t) where t is the time at which a sensor event is produced and t is the sensor pixel corresponding to the rectified pixel  the event rate of an event-based sensor depends on factors  such as scene contrast  sensor bias  and object velocity  to add invariance across event rates the author accumulate spikes over various temporal scales through the use of temporally overlapping sliding windows  these temporal scales are implemented through the use of neurons which cause each event to appear at its corresponding pixel multiple times  depending on the desired dilation and erosion neurons are used to denoise the input . the winner-take-all (wta) system is a feed-forward neural network that takes as input d thermometer code representations of the hadamard products for d distinct candidate disparity levels ,  and finds the disparity with the largest value  at every timestep . this is achieved using two parallel wta streams  stream calculates the winner disparities for left-to-right matching  and stream calculates the winner disparities for right-to-left matching  the outputs of each stream are represented by d retinotopic maps expressed in a fixed resolution (dvi j d(t) d 0   d 1  v  l r)  where events represent the retinotopic winner disparities for that stream  the streams are then merged to produce the disparity map performance is evaluated on synthetic datasets consisting of random dot stereograms representing a rotating synthetic 3d object ,  and two real world sets of sequences  consisting of a fast rotating fan and a rotating toy butterfly . the synthetic dataset provides dense disparity estimates  which are difficult to acquire with sparse event based cameras  the fan sequence is useful for testing the ability of the algorithm to operate on rapidly moving objects  varying orientations of the fan add continuously varying depth gradient to the dataset  it is observed that the temporal scale has a higher effect on accuracy than spatial scale . a live-feed version of the system running on nine truenorth chips is shown to calculate disparity maps per second, and the ability to increase this up to 2,000 disparities per second (subject to certain trade-offs) is demonstrated, for use with high speed event cameras, such as dvs. the main advantages of the proposed method, compared to the related work 17, 49, 45, 52, 57, are simultaneous end-to-end neuromorphic disparity calculation, low power, high throughput, low latency (9-11 ms), and linear scalability to multiple neuromorphic processors for larger input sizes. when the data in a cycle is sparse, as is the case with a dvs sensor, most neurons would not compute for most of the time, resulting in low active power this processing differs from traditional architectures that use frame-buffers and other conventional data structures; where same memory fetching and computation is repeated for each pixel every frame, independent of scene activity. total chip power is the sum of passive power, computed by multiplying the idle power by the fraction of the chip\u2019s cores under use, and active power computed by subtracting idle power from the total power measured when the system is accepting input events the rds is tested on a model using spatial windows, left-right consistency constraints, no morphological erosion/dilation after rectification, and disparity levels (0-30) plus a no-disparity\u2019 indicator (often occurring due to self-occlusions). the models that run on live davis input are operated at spike injection rate of up to 2,000hz (a new input every 1/2,000 seconds) and disparity map throughput of 400hz at a 0.5ms tick period (400 distinct disparity maps produced every second) across a cluster of truenorth chips. by adding a multiplexing spiking network to the network,the author are able to reuse each feature-extraction/wta circuit to process the disparities for different pixels, effectively decreasing the maximum disparity map throughput from 2,000hz to 400hz, requiring fewer chips to process the full image (9 truenorth chips).the author have introduced an advanced neuromorphic 3d vision system uniting a pair of davis cameras with multiple truenorth processors, to create an end-to-end, scalable, event-based stereo system. comparative advantages are low power, multi-resolution disparity calculation, scalability to live sensor feed with large input sizes, and evaluation using synthetic as well as real world fast movements and depth gradients, in neuromorphic, non von-neumann hardware. the implemented neuromorphic stereo disparity system achieves these advantages, while consuming less power per pixel per disparity map compared to the stateof-the-art the homogeneous computational substrate provides the first example of a fully end-to-end low-power, high throughput fully event-based neuromorphic stereo system capable of running on live input event streams, using a fully graph-based computation model, where no frames, arrays or other such data-structures are used.",
    "1017": "given a collection of distributions ,  two causal graphs are called interventionally equivalent if they are associated with the same family of interventional distributions  where the elements of the family are indistinguishable using the invariances obtained from a direct application of the calculus rules . e   x y  if when the remaining factors are held constant  forcing x to take a specific value affects the value of y  where x y are random variables representing some relevant features of the system  the task of the causal structure entails a search over the space of causal graphs that are compatible with the observed data; the collection of these graphs forms what is called an equivalence class  the most popular mark-directed on the data by the underlying causal structure that is used to delineate an equivalence class are conditional independence (ci) relations  these relations are the most basic type of probabilistic invariances used in the field and have been studied at large in the context of graphical models since  at least  (see also)  while cis are powerful and have been the driving force behind some of the most prominent structural learning algorithms in the field  including the pc  fci  these are specific constraints for one distribution  in this paper the author start by noting something very simple  albeit powerful  that happens when a combination of observational and experimental distributions are available: there are constraints over also find this summary at davidstutz . de(url/)  the do-calculus in causal inference consists of a set of inference rules that allows one to create a map between distributions generated by a causal graph when certain graphical conditions hold in the graph . the calculus was developed in the context of hard interventions ,  and recent work presented a generalization of this result for soft interventions  the first rule of the calculus is a d-do-see type of statement relative to a specific interventional distribution  which says that y z w ind implies the corresponding conditional independence px(yw  z) px(yw) px(yw)  note that the corollary of this rule is the one underlying most of the structure learning algorithms found in practice  which says that if some independence hold in p  this would imply a corresponding graphical separation (under faithfulness) in the causal graph  from this understanding the author make a very simple  albeit powerful observation i e   the converse of the other two rules should offer insights about the underlying graphical structure as well  for instance  if there is a latent variable p(y  x) that allows one to infer the absence of a causal relation between x and y  then there is a latent variable p(y the paper introduces the notion of an augmented causal graph (augmented causal graph) to characterize when two causal graphs are equivalent in accordance to the proposed definition . given a causal graph d and an intervention set i ,  let m be the set of augmented mags corresponding to all the causal graphs that are i-markov equivalent to d . an augmented pag ford  denoted g pag(augi(d))  is a graph such that: g has the same adjacencies as m  and any member of m does; and every non-zero mark in g is an invariant mark in m  given a causal graph d and an intervention set i the author investigate the problem of learning the causal structure underlying a phenomenon of interest from a combination of observational and experimental data . the latter, inference, attempts to leverage the causal structure to compute quantitative claims about the effect of interventions and retrospective counterfactuals, which are critical to assign credit, understand blame and responsibility, and perform judgement about fairness in decision-making. the task of learning the causal structure entails a search over the space of causal graphs that are compatible with the observed data; the collection of these graphs forms what is called an equivalence class. the most popular mark imprinted on the data by the underlying causal structure that is used to delineate an equivalence class are conditional independence (ci) relations. based on the second rule of do-calculus, one can infer that there is an open backdoor path from x to y , where the edge adjacent to x on this path has an arrowhead into x. in our setting,the author do not have access to the true graph, butthe author leverage this and the other do-constraints to reverse engineer the process and try to learn the structure. for our characterization,the author utilize an extension of the causal calculus to soft interventions introduced in under soft-interventions, the do-see test can be written as checking if px(yx) p(yx), where px is the distribution obtained after a soft intervention on x. the second observation leveraged here follows from another realization by pearl that interventions can be represented explicitly in the graphical model he then introduced whatthe author call f-nodes, which graphically encode the changes due to an intervention and the corresponding parametrization (see also 16, sec. a pag, which represents a markov equivalence class of a mag, is learnable from the independence model over the observed variables, and the fci algorithm is a standard sound and complete method to learn such an object related work: learning causal graphs from a combination of observational and interventional data has been studied in the literature 3, 11, 7, 20, 8, 12, for causally sufficient systems, the notion and characterization of interventional markov equivalence has been introduced in 9, more recently, showed that the same characterization can be used for both hard and soft interventions. consider a set of interventional distributions (pi)ii c-faithful to a causal graph d (v l), where i is a set of controlled experiments. finally,the author develop an algorithm to learn an interventional equivalence class from data, which includes new orientation rules.",
    "1018": "we present complex imperative program induction from terminal rewards (cipitr)  an advanced neural programmer that mitigates reward sparsity with auxiliary rewards  and restricts the program space to semantically correct programs using high-level constraints  kb schema  and inferred answer type  cipitr solves complex kbqa considerably more accurately than key-value memory networks and neural symbolic machines (nsm)  for moderately complex queries requiring 2to 5-step programs  cipitr scores at least higher f1 than the competing systems  tldr; the authors propose a novel neural program induction system for solving complex programs . csqa is particularly suited to study the complex program induction (cpi) challenge over other kbqa data sets because: it contains large-scale training data of question-answer pairs across diverse classes of complex queries ,  each requiring different inference large kb sub-graphs  poor state-of-the-art performance of memory networks on the massive size of the kb involved million entities and million tuples poses a scalability challenge for prior npi techniques  availability of the massive size of the kb metadata helps standardize comparisons across techniques (explained subsequently)  csqa is adapted in two ways for the cpi problem  removal of extended conversations: to be consistent with the nsm work on kbqa the author discard qa pairs that depend on the previous dialogue context  this is possible as every query is annotated with information on whether it is self-contained or depends on the previous context  the gold annotations of canonical kb entities  types  and relations available in the data set along with the queries  in order to remove a prominent source of confusion in comparing kbqa systems  although annotation accuracy affects a complete kbqa system  the model is trained with a vocabulary of operators and variable types . it first passes x through a feed-forward layer to transform its dimension to key embedding dimensionxkey  then  by computing softmax over the matrix multiplication of key and xkey  the distribution over the memory variables for lookup is obtained  xkey f(x)  xdist softmax(mx keyxkey) feasibility sampling: to the search space to meaningful programs  cipitr incorporates both high-level generic or task-specific constraints when sampling any action  the generic constraints can help it adopt more pragmatic programming styles like not repeating lines of code or avoiding syntactical errors  the task specific constraints ensure that the generated program is consistent as per the kb schema or on execution gives an answer of the desired variable type  to sample from the feasible subset using these constraints  the input sampling distribution  xdist  is elementwise transformed by a feasibility vector xfeas followed by a l1-normalization  along with the transformed distribution  the top-k entries xsampled are also returned  npi core: the query representation q is fed at the initial timestep to an environment encoding rnn ,  which gives out the environment state et at every timestep . this  along with the value embedding uvalt1 of the last output variable generated by the npi engine  is fed at every timestep into another rnn that finally outputs the program state ht  ht is then fed into the successive modules of the program induction engine as described below  outvargen: the query is first parsed into a sequence of kb-entities and non-kb words  kb entities e are embedded with the concatenated vector transe  and non-kb words with 0  the final query representation is obtained from a gru encoder as q  output variable generator: the new variable up of type u is generated by invoking the value embedding of the operator prototype arg  and by applying a feed-forward network fvtype which transforms the program state ht to a vector in rmax var  it is then element-wise multiplied with the current attention state over the variables in memory of that type  tldr; the authors propose a novel neural machine translation (nmt) framework to solve the long standing problem of generating large programs that are difficult to interpret . the key idea is to use a neural machine translation (nmt) framework to solve the problem of generating large programs that are difficult to interpret  evaluation the model is trained using the adam optimizer and tuned on the validation set . some parameters are selectively turned on/ off after a few training iterations ,  which is itself a hyperparameter  evaluation the model is evaluated on the more commonly used websp data set  though quite a few recent works on kbqa have evaluated their model on websp  the reported performance is always in a setting where the gold entities/relations are not known  they either internally handle the entity and relation problem or outsource it to some external or in-house model  which itself might have been trained with additional data  additionally  the entity/relation linker outputs are also not made public  making it difficult to set up a fair ground for evaluating the program induction model  to avoid these issues the author use the human-annotated entity/relation linking data available along with the input to the program induction model  consequently the performance reported here is not comparable to the previous works evaluated on this data set  as the query annotation is obtained here from an oracle linker  further  to gauge the proficiency of the proposed program induction model the author construct a rule-based model which is aware of the human annotated semantic form of the query that is  the inference chain of relations and the exact the authors evaluate their model against nsm ,  kvmnet  and cipitr and show that it outperforms all of them in terms of f1 score . structured knowledge bases , or multi-hop , or complex queries such as how many countries have more rivers and lakes than brazil?\u2019\u2019 complex queries require a proper assembly of selected operators from a library of graph, set, logical, and arithmetic operations into a complex procedure, and is the subject of this paper. each step of the program selects an atomic operator and a set of previously defined variables as arguments and writes the result to scratch memory, which can then be used in subsequent steps. with this approach it is now possible to first train separate modules for each of the atomic operations involved and then train a program induction model that learns to use these separately trained models and invoke the sub-modules in the correct fashion to solve the task. as well as for manipulating a physical environment program induction has also seen initial promise in translating simple natural language queries into programs executable in one or two hops over a kb to obtain answers in contrast, many of the complex queries from saha et al. main contributionsthe author present complex imperative program induction from terminal rewards\u2019\u2019 (cipitr),2 an advanced neural program induction (npi) system that is able to answer complex logical, quantitative, and comparative queries by inducing programs of length up to 7, using atomic operators and variable types. relevant statistics of the resulting data set are presented in table use of gold entity, type, and relation annotations to standardize comparisons: our focus being on the reasoning aspect of the kbqa problem,the author use the gold annotations of canonical kb entities, types, and relations available in the data set along with the the queries, in order to remove a prominent source of confusion in comparing kbqa systems (i.e., all systems take as inputs the natural language query, with spans identified with kb ids of entities, types, relations, and integers). to summarize, cipitr has the following advantages, inducing programs more efficiently and pragmatically, as illustrated by the sample outputs in table 5: generating syntactically correct programs: because of the token-by-token decoding of the program, nsm cannot restrict its search to only syntactically correct programs, but rather only resorts to a post-filtering step during training. however, at test time, it could still generate programs with wrong syntax, as shown in table for example, for the logical question, it invokes a genset with a wrong argument type none and for the quantitative count question, it invokes the setunion operator on a non-set argument. generating semantically correct programs: cipitr is capable of incorporating different generic programming styles as well as problemspecific constraints, restricting its search space to only semantically correct programs. efficient search-space exploration: owing to the different strategies used to explore the program space more intelligently, cipitr scales better to a wide variety of complex queries by using less than half of nsm\u2019s beam size. as future directions of work, cipitr can be further improved to handle the hardest question types by making the search more strategic, and can be further generalized to a diverse set of goals when training on all question categories together. additionally, further improvements are required to induce complex programs without availability of gold program input variables.",
    "1019": "dual-ces is an unsupervised query focused multi-document extractive summarizer  to this end  like ces  dual-ces utilizes the cross entropy method for selecting a subset of sentences extracted from input documents  whose combination is predicted to produce a good summary  however  unlike ces  dual-ces does not attempt to address both saliency and focus goals in a single optimization step  instead  dual-ces implements a novel two-step dual-cascade optimization approach  which utilizes two sequential saliency distillation invocations  using such an approach  dual-ces provides a fully unsupervised summarization-based pseudo-feedback  which allows to generate a focused summary that is more short in the second step  this paper proposes a novel unsupervised learning approach for the task of query-based multi-document summarization . given a set of matching documents to be summarized ,  the task is to produce a length-limited summary by extracting salient content parts in the document which are further relevant (focused) to the given query . to this end the author produce summary s (with maximum length lmax) by choosing a subset of sentences which maximizes a given quality target q(sq d)  similar to ces  it utilizes the cross entropy method for selecting the most promising subset of sentences in d  sincethe author assume an unsupervised setting  no actual reference summaries are available for training nor canthe author directly optimize an actual quality target q(sq  instead  q(sq d) is surrogated by several summary quality prediction measures qi(sq d) (i 1  2  each predictor qi(sq d) is designed to estimate the level of saliency or importance of a given candidate summary s and is presumed to correlate (up to some extent) with actual summarization quality  e g   for simplicity  various predictions are assumed to be independent and are combined into a single optimization objective by taking their product  the cross entropy method provides a generic monte-carlo optimization framework for solving hard combinatorial problems previously dual-ces does not attempt to maximize both saliency and focus goals in a single optimization step . instead ,  dual-ces implements a novel two-step dual-cascade optimization approach  which utilizes two ces-like invocations  yet  each invocation utilizes a bit different set of summary quality predictors  depending on whether the summarizer\u2019s goal should lay towards higher summary saliency or focus  in the first step  dual-ces relaxes the summary length constraint  aiming at producing a longer and more salient summary  this summary is then treated as a pseudo-effective reference from which saliency-based pseudo-feedback is distilled  such pseudo-feedback is then utilized in the second step of the cascade for setting an additional auxiliary saliency-driven goal  at the second step  similar to ces  the primary goal is actually to produce a focused summary (with maximum length limit lmax)  overall  dual-ces is simply implemented as follows: cem(qfoc(q d)  lmax  scl(q  ))  here  qsal(q  qfoc(q  scl( summary length constraint lmax and pseudo-reference summary sl generated in the previous step . dual-ces was shown to better handle the tradeoff between saliency and focus ,  providing the best summarization quality compared to other alternative state-of-the-art summarizers . the vast amounts of textual data end users need to consume motivates the need for automatic summarization an automatic summarizer gets as an input one or more documents and possibly also a limit on summary length (e.g., maximum number of words). moreover, the summarizer may also be required to satisfy a specific user information need, expressed by one or more queries. therefore, the summarizer will need to produce a focused summary which includes the most relevant information to that need. while both saliency and focus goals should be considered within a query-focused summarization setting, these goals may be actually conflicting with each other higher saliency usually comes at the expense of lower focus and vice-versa. contact author: haggaiil.ibm.com to illustrate the effect of summary length on this tradeoff, using the duc dataset, figure reports the summarization quality which was obtained by the cross entropy summarizer (ces) a state of the art unsupervised query-focused multidocument extractive summarizer saliency was measured according to cosine similarity between the summary\u2019s bigram representation and that of the input documents. focus was further measured relatively to how much the summary\u2019s induced unigram model is concentrated around query-related words. using such an approach, dual-ces tries to handle the tradeoff by gradually shifting from generating a long summary that is more salient in the first step to generating a short summary that is more focused in the second step. the ce-method provides a generic monte-carlo optimization framework for solving hard combinatorial problems previously, it was utilized for solving the sentence subset selection problem to this end, the ce-method gets as an input q(q,d), a constraint on maximum summary length l and an optional pseudo-reference summary sl, whose usage will be explained later on. for a given sentence s d, let (s) denote the likelihood that it should be included in summary s. starting with a selection policy with the highest entropy (i.e. to this end, () is incrementally learned using an importance sampling approach ., a sample of n sentence-subsets sj is generated according to the selection policy t1() which was learned in the previous iteration t the likelihood of picking a sentence s d at iteration t is estimated (via cross-entropy minimization) as follows: t(s) def n j1 q(sj q,d)tssj n j1 q(sj q,d)t (1) here, denotes the kronecker-delta (indicator) function and t denotes the (1 )-quantile ( (0, 1)) of the sample performances q(sj q,d) (j 1, 2, therefore, the likelihood of picking a sentence s d will increase when it is being included in more (subset) samples whose performance is above the current minimum required quality target value t.the author further smooth t() as follows: t() t1() (1 )t(); with 0, upon its termination, the ce-method is expected to converge to the global optimal selection policy ()the author then produce a single summary s (). to this end,the author add a predictor: qqf (sq,d) def wq p(ws), which acts as a query-anchor and measures to what extent summary s\u2019s unigram model is devoted to the information need q. the input to the second step of the cascade consists of the same set of documents d, summary length constraint lmax and the pseudo-reference summary sl that was generated in the previous step. dual-ces then utilizes such salient words for better selection of salient sentences within its second step of focused summary production. figure illustrates the (average) learning curve of its adaptive-length parameter lt. overall, dual-ces\u2019s summarization quality remains quite stable, exhibiting low sensitivity to l. similar stability was further observed for the two other duc benchmarks. in addition, figure depicts an interesting empirical outcome: dual-ces-a converges (more or less) to the best hyperparameter l value (i.e., l in table 3).",
    "1020": "the system is composed of three separate neural network blocks: prosody prediction ,  acoustic feature prediction and linear prediction coding net as a neural vocoder  the system can synthesize speech with close to natural quality while running times faster than real-time on a standard cpu  tldr; the authors propose to use the world vocoder parameters of lpcnet as input to a tts system that can adapt to new voices . each sub-phoneme element represents either a heading  a middle or a trailing part of a phoneme  lpcnet block converts the stream of acoustic feature vectors to a speech signal  each block has its own model which is trained independently for each voice  for each voice  the training and adaptation phases include the following data pre-processing steps: a grapheme-to-phoneme conversion using the frontend block  forced alignment of audio at the sub-phoneme level using proprietary acoustic modeling and speech recognition tools  extraction of textual features for prosody modeling using the front-end block  pitch detection for prosody modeling using a proprietary tool  cepstral and residual extraction using the lpcnet feature extraction tool  the lpcnet decoder is a variant of the wavernn that uses a nn model to generate speech samples from equidistant-intime input of cep ,  pitch and pitch correlation parameters . unlike other waveform generative models  such as wavenet and wavernn  the lpcnet uses its nn to predict the lpc residual (the vocal source signal) and then apply to it an lpc filter calculated from the cep  this has the advantage of better control over the output of the spectral shape since it directly depends on the lpc filter shape  the model is also more robust to the predicted residual errors since any high frequency noise is also shaped by the lpc filter  in the table below ,  you can see the differences in the results for the datasets . the system is built around three nn models for generating the prosody ,  acoustic features and the final speech signal  in recent yearsthe author are experiencing a dramatic improvement of the synthesized speech quality in tts systems, with the introduction of systems that are based on neural networks (nn). a major improvement in quality was achieved by using attention based models such as tacotron and by replacing vocoders with a nn based waveform generators such as wavenet a useful feature of systems with trainable models is the ability to adapt the tts to an unseen voice using a small amount of training data (from a few seconds to an hour of speech). this is usually done by training the system on a large number of speakers, and providing a speaker embedding vector as one of the system\u2019s inputs. using this approach allows later retraining of only a subsets of the model parameters or prediction of the speaker embedding vector 3, 4, the drawback of this approach is that the resulting systems use large nn models. this was carried out by retraining nn models that had already been trained using a large highquality voice, on a small amount of data from the new voice. in this paperthe author show thatthe author can get a considerable quality improvement by modifying a tts system that produced the world vocoder parameters to predict parameters for lpcnet as in the previous work 6,the author conduct multiple adaptation experiments, applied on multiple vctk voices and show that the new system has much better quality and similarity to the target voices but can still run much faster than real-time in a single-cpu mode.the author adopted the front-end block which is used in the ibm watson tts engine and is described in detail in the front-end performs a grapheme-to-phoneme conversion, represents each word with a set of positional and categorical linguistic features and associates the features with the phonemes contained within the word. the input features, derived from the tts front end, are comprised of 1-hot coded categorical features and standard positional features in this architecture the prosody adaptation to unseen speaker is based on a variational auto encoder (vae) utterance prosody embedding, averaged over all the speaker utterances 6, as presented on figure in the current workthe author used multi-speaker baseline models for prosody adaptation to unseen voices, as it resulted in better quality than the single speaker models. the subject is asked to rate their voice similarity, using a 4-point scale adopted from the voice conversion challenge (vcc) 16, and utilized in our previous experimentsthe author performed two tests: one with only male voices and the second with only female voices.the author can compare these results to those of the vcc hub task although the task setup and the listening tests conditions are a bit differentthe author can see that our system mos and similarity score are comparable to those of the best vcc system (n10 with quality of and similarity of where the corresponding scores for the original speech are and 95).the author have presented in this article a new tts system that addresses the challenging goals of producing high quality speech while operating at faster than real-time rate without an expensive gpu support.the author tested this system using two proprietary tts voice datasets and demonstrated that our system produces high quality speech that is comparable to larger and much slower tacotron2 wavenet systems. the task of creating a high-quality tts system out of a smaller set of audio data is even more challenging.the author have shown that our system can perform well even with datasets as small as 5-20 minutes of audio.the author demonstrated that whenthe author reduce the size of the training data, there is some graceful degradation to the quality, butthe author are still able to maintain good similarity to the original speaker. for future work,the author plan to allow voice modifications by adding control over voice parameters such as pitch, breathiness and vocal tract.",
    "1021": "sobolev independence criterion (sic) decomposes the sum of importance scores of two random variables x and y into a product of their marginals  in machine learning  feature selection is an important problem in predictive statistics and machine learning for interpretable predictive discoveries  our goal  is to design a dependency measure that is interpretable can be reliably used to control feature selection  sic relies on the statistics of both the joint distribution and the product of marginals  intuitively  the gradient of the average magnitude of the gradient with respect to a feature gives an importance score for each feature  hence  promoting its sparsity is a natural feature selection problem  in particular  the authors propose to use a penalty term that controls the sparsity of the gradient estimator on the support of the measures  this is done by introducing an importance score normalized by the perturbed critic score  also find this summary at davidstutz de(url/)  the paper proposes to learn the feature map as a deep neural network  the authors propose two methods to control false discovery rate (fdr) in feature selection: holdout randomization test (hrt) and feature selection based on conditional generative models  sic can potentially leverage the deep learning toolkit for going beyond tabular data where random forests excel  to more structured data such as time series or graph data  in experiments  they compare their model to competing models on synthetic datasets and a real-world drug resistance dataset  the paper introduces the sobolev independence criterion (sic) as a feature importance measure that can be used for feature selection  alternating optimization methods block coordinate descent (bcd) using first order methods is also known to converge to a global optima  also find this summary at davidstutz de(url/)    the authors propose a variational formulation of this problem  the model is trained using minibatch sgd  this can be done in two ways  here  the out-of-coherence region is referred to as the critic branch  feature selection is an important problem in statistics and machine learning for interpretable predictive modeling and scientific discoveries. our goal in this paper is to design a dependency measure that is interpretable and can be reliably used to control the false discovery rate in feature selection. the mutual information between two random variables x and y is the most commonly used dependency measure. for instance, the hilbert-schmidt independence criterion (hsic) uses the maximum mean discrepancy (mmd) to assess the dependency between two variables, i.e. in sectionthe author show how sic and conditional generative models can be used to control the false discovery rate using the recently introduced holdout randomization test and knockoffsthe author validate sic and its fdr control on synthetic and real datasets in sectionthe author start by motivating gradient-sparsity regularization in sic as a mean of selecting the features that maintain maximum dependency between two randoms variable x (the input) and y (the response) defined on two spaces x rdx and y rdy (in the simplest case dy 1).the author experiment with two datasets: a) complex multivariate synthetic data (sinexp), which is generated from a complex multivariate model proposed in sec 5.3, where ground truth features xi out of generate the output y through a non-linearity involving the product and composition of the cos, sin and exp functions (see appendix f.1). to increase the difficulty even further,the author introduce a pairwise correlation between all features ofthe author show results for datasets of and samples repeated times comparing performance of our models with the one of two baselines: elastic net (en) and random forest (rf).the author study the result of using the normalized importance scores j from sic for (heuristic) feature selection, against features selected by elastic net. the sic critic and regressor nn were respectively the bigcritic and regressornn described with training details in appendix f.3, while the random forest is trained with default hyper parameters from scikit-learnthe author can see that, with just j , informative features are selected for the downstream regression task, with performance comparable to those selected by elasticnet, which was trained explicitly for this task. the features selected with high j values and their overlap with the features selected by elasticnet are listed in appendix f.2 table hiv-1 drug resistance with knockoffs-sic.the author use boosted sic, by varying the batch sizes in n 10, 30, 50, and computing the geometric mean of produced by those three setups as the feature importance needed for knockoffs.the author introduced in this paper the sobolev independence criterion (sic), a dependency measure that gives rise to feature importance which can be used for feature selection and interpretable decision making.the author demonstrated the merits of sic for feature selection in extensive synthetic and real-world experiments with controlled fdr."
}